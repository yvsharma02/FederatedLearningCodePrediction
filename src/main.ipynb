{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, head_size, context_window_len, mask):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        # These Layers Map (B, W, E) -> (B, W, HEAD_SIZE)\n",
    "\n",
    "        assert mask == 'encoder' or mask == 'decoder'\n",
    "\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.mask_type = mask\n",
    "        self.context_window_len = context_window_len\n",
    "        self.head_size = head_size\n",
    "\n",
    "    # Returns a mask of (W, W)\n",
    "    def get_mask_tensor(self):\n",
    "        if (self.mask_type == 'encoder'):\n",
    "            return torch.tril(torch.ones(self.context_window_len, self.context_window_len, device=device))\n",
    "        elif (self.mask_type == 'decoder'):\n",
    "            return torch.ones(self.context_window_len, self.context_window_len, device=device)\n",
    "    \n",
    "    # Input is of shape (B, W, E) where E is embedding dimensions.\n",
    "    # Output is of shape (B, W, E)\n",
    "    def forward(self, input):\n",
    "        k = self.key(input) # Convert (B, W1, E) -> (B, W1, HEAD_SIZE)\n",
    "        q = self.query(input) # Convert (B, W2, E) -> (B, W2, HEAD_SIZE) (W1 == W2 == W3)\n",
    "        v = self.value(input) # (B, W3, E) -> (B, W3, HEAD_SIZE)\n",
    "        match = q @ k.transpose(-2, -1) # Produce Matrix (B, W1, W2)\n",
    "        mask = self.get_mask_tensor()\n",
    "        match = match.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(match, dim=-1) / math.sqrt(self.head_size) # Still (B, W1, W2)\n",
    "\n",
    "        res = attention @ v # (B, W1, W2) @ (B, W3, HEAD_SIZE) -> (B, W1=W3, HEAD_SIZE)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, context_window_len, mask):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.attention_heads = [AttentionHead(emb_dim, emb_dim // num_heads, context_window_len, mask) for i in range(0, num_heads)]\n",
    "        \n",
    "\n",
    "    # Input is (B, W, E)\n",
    "    def forward(self, input):\n",
    "        # Each ah returns (B, W, E/num_heads)\n",
    "        return torch.cat([ah(input) for ah in self.attention_heads], dim= -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, context_window_size, embedding_dimensions, num_heads, hidden_layer_multiplier = 4, dropout_rate = 0.3):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            MultiHeadedAttention(embedding_dimensions, num_heads, context_window_size, 'encoder'),\n",
    "            nn.Linear(embedding_dimensions, hidden_layer_multiplier * embedding_dimensions, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dimensions * hidden_layer_multiplier, embedding_dimensions, device=device),\n",
    "            nn.LayerNorm(embedding_dimensions, device=device),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input):\n",
    "        return self.network(raw_input)\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "\n",
    "    # Input to the Transformer will be a matrix of size (B, W)\n",
    "    # B is the Batch Size.\n",
    "    # W is the Window Size (context_window_size)\n",
    "    # Example:\n",
    "    # [a, b, c]\n",
    "    # [d, e, f]\n",
    "    #\n",
    "    # [a, b, c] is an input example. (context_len = W = 3)\n",
    "    # There are two batches [a, b, c] and [d, e, f] (B = 2)\n",
    "    # a, b, c should be integers (each representing one possible token). a, b, c should belong in [0, dict_size)\n",
    "    def __init__(self, dict_size, context_window_size, embedding_dimensions, num_heads, block_count):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        self.token_embedding = nn.Embedding(dict_size, embedding_dimensions, device=device)\n",
    "        self.position_embedding = nn.Embedding(context_window_size, embedding_dimensions, device=device)\n",
    "        self.decoder = nn.Linear(embedding_dimensions, dict_size, device=device)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            *[Block(context_window_size, embedding_dimensions, num_heads) for _ in range(0, block_count)]\n",
    "        )\n",
    "\n",
    "    def embed(self, input, spatial = False):\n",
    "        emb = self.token_embedding(input)\n",
    "        if (spatial):\n",
    "            return emb + self.position_embedding(torch.arange(0, self.context_window_size, device=device))\n",
    "\n",
    "        return emb\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). On CPU. It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input, targets):\n",
    "        if(raw_input.device != device):\n",
    "            raw_input = raw_input.to(device)\n",
    "        input = self.embed(raw_input, True)\n",
    "        logits = self.network(input)\n",
    "        logits = self.decoder(logits)\n",
    "        if (targets != None):\n",
    "            if(targets.device != device):\n",
    "                targets = targets.to(device)\n",
    "            logits_1d = logits.view(logits.shape[0] * logits.shape[1], logits.shape[2])\n",
    "            targets = targets.view(logits_1d.shape[0])\n",
    "            loss = F.cross_entropy(logits_1d, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"<|endoftext|>\"\n",
    "PADDING = \"<|padding|>\"\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DELIM_ENCODED = 0\n",
    "PADDING_ENCODED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x : list, delim):\n",
    "    ind = x.index(delim) if delim in x else -1\n",
    "    if (ind == -1):\n",
    "        return [x]\n",
    "    y = split(x[ind + 1:], delim)\n",
    "    cur = x[:ind]\n",
    "    return [x[:ind]] + y if len(cur) != 0 else y\n",
    "\n",
    "#print(split([1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4], 1))\n",
    "\n",
    "def get_padded_longest_sample(encoded : list[int]):\n",
    "    splits = split(encoded, DELIM_ENCODED)\n",
    "    splits.sort(key=lambda x: -len(x))\n",
    "    item = splits[0]\n",
    "    if (len(splits) > 1):\n",
    "        splits[0].append(DELIM_ENCODED)\n",
    "#    print((CONTEXT_LEN - len(splits[0])))\n",
    "    res = [PADDING_ENCODED] * (CONTEXT_LEN - len(splits[0]))\n",
    "    res = res + splits[0]\n",
    "#    print(res)\n",
    "#    print(\"Final: \", tokenizer.decode(res))\n",
    "    return res;\n",
    "\n",
    "def remove_padding(encoded : list[int]):\n",
    "    return [x for i, x in enumerate(encoded) if x not in [DELIM_ENCODED, PADDING_ENCODED]]\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, context_len):\n",
    "        self.data = data.ids\n",
    "        print(self.data[0:100])\n",
    "        self.context_len = context_len\n",
    "\n",
    "    # ABCDE for context len of 1 has 4 examples: (A, B), (B, C), (C, D), (D, E)\n",
    "    # for context len 2 has examples 3 (AB, C), (BC, D), (CD, E)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(get_padded_longest_sample(self.data[idx : idx + self.context_len]), device = device), torch.tensor(get_padded_longest_sample(self.data[idx + 1 :idx + self.context_len + 1]), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def convert_to_utf8(input_file, output_file):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    decoded_content = content.decode('utf-8', errors='replace')\n",
    "        \n",
    "    with codecs.open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(decoded_content)\n",
    "        \n",
    "\n",
    "\n",
    "# import coDesc_parser\n",
    "# #4211516\n",
    "# #res = coDesc_parser.fragment_dataset(\"../data/CoDesc/CoDesc.json\", \"../data/CoDesc/fragmented\")\n",
    "# #print(res)\n",
    "\n",
    "# random_samples = torch.randint(0, 4211516, (100000,)).tolist()\n",
    "# train_indices = random_samples[:80000]\n",
    "# test_indices = random_samples[80000:]\n",
    "# coDesc_parser.fragmented_files_to_txt_file(train_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# coDesc_parser.fragmented_files_to_txt_file(test_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90), \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90, 100), \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/train.txt\", \"../data/CoDesc/fragmented/train_utf8.txt\")\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/test.txt\", \"../data/CoDesc/fragmented/test_utf8.txt\")\n",
    "    \n",
    "# from tiktoken._educational import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tiktoken._educational import *\n",
    "\n",
    "# tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# in_dir = \"../data/CoDesc/fragmented\"\n",
    "\n",
    "# with open(os.path.join(in_dir, \"train.txt\"), \"r\") as f:\n",
    "#     train = f.read()\n",
    "\n",
    "# with open(os.path.join(in_dir, \"test.txt\"), \"r\") as f:\n",
    "#     test = f.read()\n",
    "\n",
    "\n",
    "\n",
    "# train_simple_encoding = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# train_enc = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "# test_enc = tokenizer.encode(test, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizer\\\\vocab.json', '../data/tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"../data/CoDesc/fragmented/train_utf8.txt\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[DELIMITER, PADDING])\n",
    "\n",
    "if not os.path.exists(\"../data/tokenizer\"):\n",
    "    os.makedirs(\"../data/tokenizer\")\n",
    "\n",
    "tokenizer.save_model(\"../data/tokenizer\")\n",
    "\n",
    "#tokenizer = ByteLevelBPETokenizer(\"../data/tokenizer/vocab.json\", \"../data/tokenizer/merges.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = open(\"../data/CoDesc/fragmented/train_utf8.txt\", \"r\").read()\n",
    "test = open(\"../data/CoDesc/fragmented/test_utf8.txt\", \"r\").read()\n",
    "\n",
    "train_enc = tokenizer.encode(train)\n",
    "test_enc = tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 128\n",
    "BLOCK_COUNT = 1\n",
    "EMBED_DIM = 512\n",
    "NUM_HEADS = 64\n",
    "LEARNING_RATE = 1.5e-2\n",
    "BATCH_COUNT = 64\n",
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4728772\n"
     ]
    }
   ],
   "source": [
    "transformer = CustomTransformer(VOCAB_SIZE, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(sum(p.numel() for p in transformer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(ctx, new_len):\n",
    "    res = [x for x in ctx]\n",
    "    \n",
    "    for _ in range(new_len):\n",
    "        ctx = torch.tensor([res[-CONTEXT_LEN:]])\n",
    "        prob, loss = transformer(ctx, None) # Returns a tensor of size (1, W, EM)\n",
    "        prob = prob.squeeze(0)\n",
    "        prob = torch.softmax(prob, dim=-1) # (1, W, EM)\n",
    "        pred = torch.multinomial(prob, 1) # (1, W, 1)\n",
    "        res.append(pred[-1, 0].item())\n",
    "    return tokenizer.decode(res[-new_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 739, 2061, 601, 265, 270, 271, 2085, 478, 580, 1105, 295, 624, 85, 1623, 601, 905, 560, 264, 289, 692, 1459, 1904, 475, 485, 278, 281, 358, 546, 295, 465, 265, 403, 299, 278, 399, 295, 299, 390, 560, 264, 289, 692, 1459, 1904, 284, 732, 295, 299, 1105, 270, 271, 560, 264, 289, 692, 1459, 1904, 475, 299, 485, 278, 364, 295, 280, 1665, 278, 399, 295, 280, 0, 200, 336, 508, 575, 475, 485, 2296, 2240, 2065, 46, 434, 265, 575, 356, 290, 324, 575, 1138, 72, 324, 403, 285, 72, 324, 403, 222, 696, 462, 267]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "train_loader = DataLoader(train_dataset, BATCH_COUNT, shuffle=True)\n",
    "\n",
    "transformer = torch.compile(transformer)\n",
    "#transformer.load_state_dict(torch.load(\"../data/out/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Process the current batch\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_train_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_train_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[1;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:786\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    784\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    386\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    388\u001b[0m signpost_event(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     },\n\u001b[0;32m    398\u001b[0m )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 676\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    679\u001b[0m     Unsupported,\n\u001b[0;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m     BisectValidationException,\n\u001b[0;32m    688\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    533\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1033\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1036\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 500\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    502\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2149\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m     ):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2268\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2263\u001b[0m _step_logger()(\n\u001b[0;32m   2264\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2266\u001b[0m )\n\u001b[0;32m   2267\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1001\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m    998\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1000\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 1001\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     )\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1005\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1178\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1178\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1179\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[0;32m   1181\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1232\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1231\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1232\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1233\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:1731\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1729\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py:1330\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1328\u001b[0m     tracing_context\n\u001b[0;32m   1329\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py:58\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[1;34m(gm, example_inputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 58\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py:903\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    887\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[0;32m    888\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m    889\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m--> 903\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py:628\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    625\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# During export, we don't get back a callable - we get back the raw fx graph\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# (either a joint or an inference-only graph)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:443\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[1;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[1;32m--> 443\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:648\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:186\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maot_dispatch_autograd\u001b[39m(\n\u001b[0;32m    179\u001b[0m     flat_fn,\n\u001b[0;32m    180\u001b[0m     flat_args: List[Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     fw_metadata: ViewAndMutationMeta,\n\u001b[0;32m    184\u001b[0m ):\n\u001b[0;32m    185\u001b[0m     fw_metadata\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled()\n\u001b[1;32m--> 186\u001b[0m     fx_g, joint_inputs, maybe_subclass_meta \u001b[38;5;241m=\u001b[39m \u001b[43maot_dispatch_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# Copied from aot_dispatch_autograd_graph.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     disable_amp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\dispatch_and_compile_graph.py:173\u001b[0m, in \u001b[0;36maot_dispatch_autograd_graph\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    170\u001b[0m updated_joint_inputs \u001b[38;5;241m=\u001b[39m subclass_tracing_info\u001b[38;5;241m.\u001b[39mplain_tensor_args\n\u001b[0;32m    171\u001b[0m maybe_subclass_meta \u001b[38;5;241m=\u001b[39m subclass_tracing_info\u001b[38;5;241m.\u001b[39mmaybe_subclass_meta\n\u001b[1;32m--> 173\u001b[0m fx_g \u001b[38;5;241m=\u001b[39m \u001b[43m_create_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_fn_to_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_joint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m assert_functional_graph(fx_g\u001b[38;5;241m.\u001b[39mgraph)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\dispatch_and_compile_graph.py:40\u001b[0m, in \u001b[0;36m_create_graph\u001b[1;34m(f, args, aot_config)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_graph\u001b[39m(f, args, \u001b[38;5;241m*\u001b[39m, aot_config: AOTConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode must be enabled here.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# See Note [Accessing .grad_fn on FunctionalTensor]\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_python_dispatcher(), FunctionalTensorMode(\n\u001b[0;32m     38\u001b[0m         pre_dispatch\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mpre_dispatch, export\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mis_export\n\u001b[0;32m     39\u001b[0m     ):\n\u001b[1;32m---> 40\u001b[0m         fx_g \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fx_g\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py:1081\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;66;03m# We disable the autocast cache as the autocast cache causes type conversions on parameters to\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;66;03m# check a cache, which introduces untracked tensors into the graph\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;66;03m# We also disable tracing by any other tensor proxy-based tracers except the current. The\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;66;03m# purpose of `make_fx` is to produce graphmodules as a side effect; its internal execution is\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;66;03m# thus irrelevant to any external functional trace.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, pre_dispatch_mode, proxy_function_mode, \\\n\u001b[0;32m   1080\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache():\n\u001b[1;32m-> 1081\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;66;03m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py:541\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[1;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_trace\u001b[39m(\n\u001b[0;32m    537\u001b[0m         root: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, Callable],\n\u001b[0;32m    538\u001b[0m         tracer: Tracer,\n\u001b[0;32m    539\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    540\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[1;32m--> 541\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx_passes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdedupe_symint_uses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dedupe_symints\n\u001b[0;32m    543\u001b[0m     dedupe_symints(graph)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\_symbolic_trace.py:793\u001b[0m, in \u001b[0;36mTracer.trace\u001b[1;34m(self, root, concrete_args)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[0;32m    787\u001b[0m             _autowrap_check(\n\u001b[0;32m    788\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[0;32m    789\u001b[0m             )\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 793\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[0;32m    794\u001b[0m             {},\n\u001b[0;32m    795\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    796\u001b[0m         )\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    651\u001b[0m     tree_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(\u001b[38;5;28mlist\u001b[39m(args), in_spec)\n\u001b[1;32m--> 652\u001b[0m     tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mroot_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtree_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m     out_args, out_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_codegen, _PyTreeCodeGen)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py:559\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[1;34m(*proxies)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, ProxyTorchDispatchMode)\n\u001b[0;32m    557\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tracer\u001b[38;5;241m=\u001b[39mtracer)\n\u001b[1;32m--> 559\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[0;32m    561\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mproxy),\n\u001b[0;32m    563\u001b[0m     out\n\u001b[0;32m    564\u001b[0m )\n\u001b[0;32m    565\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[0;32m    566\u001b[0m     (SymInt, SymFloat, SymBool),\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer)(),\n\u001b[0;32m    568\u001b[0m     out\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:517\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>.joint_helper\u001b[1;34m(primals, tangents)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoint_helper\u001b[39m(primals, tangents):\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_functionalized_f_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:383\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>._functionalized_f_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    380\u001b[0m     functional_tensor_mode\u001b[38;5;241m.\u001b[39m_tokens[k] \u001b[38;5;241m=\u001b[39m f_tokens[i]\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mf_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Return both the tokens and the outputs\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m f_outs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mfunctional_tensor_mode\u001b[38;5;241m.\u001b[39m_tokens\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39mf_outs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:251\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn_with_anomaly\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    249\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly Detection has been enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(check_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:183\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn\u001b[1;34m(primals, tangents)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_fn\u001b[39m(primals: List[Any], tangents: List[Any]):\n\u001b[1;32m--> 183\u001b[0m     outs, tangent_mask \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tangent_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(outs)\n\u001b[0;32m    185\u001b[0m     outs_to_grad \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    186\u001b[0m         o \u001b[38;5;28;01mfor\u001b[39;00m needs_tangent, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tangent_mask, outs) \u001b[38;5;28;01mif\u001b[39;00m needs_tangent\n\u001b[0;32m    187\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:104\u001b[0m, in \u001b[0;36mfn_prepped_for_autograd.<locals>.inner_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    100\u001b[0m     args_maybe_cloned \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    101\u001b[0m         maybe_to_fresh_input(i, t, meta) \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)\n\u001b[0;32m    102\u001b[0m     ]\n\u001b[1;32m--> 104\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_maybe_cloned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outs, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m    106\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(outs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:676\u001b[0m, in \u001b[0;36mcreate_functional_call.<locals>.functional_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[0;32m    673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly Detection has been enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(check_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 676\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[43mPropagateUnbackedSymInts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    680\u001b[0m     out \u001b[38;5;241m=\u001b[39m mod(\u001b[38;5;241m*\u001b[39margs[params_len:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\interpreter.py:145\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[1;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_traceback:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\traced_function_transforms.py:651\u001b[0m, in \u001b[0;36mPropagateUnbackedSymInts.run_node\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# TODO: handle Tensor returns\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_value\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m n\u001b[38;5;241m.\u001b[39mmeta:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\interpreter.py:202\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\interpreter.py:319\u001b[0m, in \u001b[0;36mInterpreter.call_module\u001b[1;34m(self, target, args, kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    317\u001b[0m submod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_attr(target)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubmod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\_symbolic_trace.py:771\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[1;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    766\u001b[0m _autowrap_check(\n\u001b[0;32m    767\u001b[0m     patcher,\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[0;32m    770\u001b[0m )\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py:496\u001b[0m, in \u001b[0;36mPythonKeyTracer.call_module\u001b[1;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_module\u001b[39m(\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m, m: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, forward: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any], args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[0;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\_symbolic_trace.py:764\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_prims_common\\wrappers.py:250\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[1;34m(out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m out_attr\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pass_is_out:\n\u001b[1;32m--> 250\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_decomp\\decompositions.py:4036\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(tensor1, tensor2, is_out)\u001b[0m\n\u001b[0;32m   4032\u001b[0m     output_shape\u001b[38;5;241m.\u001b[39mappend(t2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   4034\u001b[0m \u001b[38;5;66;03m# This will almost always be a view.\u001b[39;00m\n\u001b[0;32m   4035\u001b[0m \u001b[38;5;66;03m# It may not be a view if t2->requires_grad(). See should_fold in aten/ for an explanation\u001b[39;00m\n\u001b[1;32m-> 4036\u001b[0m t1_folded \u001b[38;5;241m=\u001b[39m \u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolded_dim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t2_is_matrix:\n\u001b[0;32m   4038\u001b[0m     \u001b[38;5;66;03m# This copies if we perform a 2D @ 3D and the first tensor requires_grad\u001b[39;00m\n\u001b[0;32m   4039\u001b[0m     \u001b[38;5;66;03m# See should_fold native/LinearAlgebra.cpp for why.\u001b[39;00m\n\u001b[0;32m   4040\u001b[0m     output \u001b[38;5;241m=\u001b[39m t1_folded\u001b[38;5;241m.\u001b[39mmm(t2)\u001b[38;5;241m.\u001b[39mview(output_shape)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_subclasses\\functional_tensor.py:411\u001b[0m, in \u001b[0;36mFunctionalTensorMode.__torch_dispatch__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m     outs_wrapped \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[0;32m    403\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, wrap, outs_unwrapped\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# When we dispatch to the C++ functionalization kernel, we might need to jump back to the\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# PreDispatch mode stack afterwards, to handle any other PreDispatch modes underneath\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode. If we call func() directly, we would need to exclude PreDispatch\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# from the TLS in order to avoid infinite looping, but this would prevent us from coming\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# back to PreDispatch later\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     outs_unwrapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_dk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatchKey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunctionalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# We don't allow any mutation on result of dropout\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mdefault:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_stats.py:15\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(fn):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m simple_call_counter:\n\u001b[0;32m     18\u001b[0m             simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "history = []\n",
    "loss_avg_block_size = 10\n",
    "\n",
    "#train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "# Prefetch the first batch\n",
    "#train_in, train_target = get_batches(train_enc, BATCH_COUNT, CONTEXT_LEN)\n",
    "\n",
    "#load model\n",
    "#transformer = CustomTransformer(tokenizer.n_vocab, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "\n",
    "\n",
    "out_dir = \"../data/out\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "i = 0\n",
    "for current_train_in, current_train_target in train_loader:\n",
    "    start_time = datetime.now()\n",
    "    # Process the current batch\n",
    "    logits, loss = transformer(current_train_in, current_train_target)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    print (f\"Iteration {i} Loss: {history[-1]}\")\n",
    "    if (len(history) >= loss_avg_block_size):\n",
    "        loss_history.append(torch.tensor(history).mean().item())\n",
    "        history = []\n",
    "        print (f\"Iteration {i} Loss: {loss_history[-1]}\")\n",
    "    i += 1\n",
    "\n",
    "    if (not os.path.exists(out_dir)):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    torch.save(transformer.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"loss.txt\"), \"a+\") as f:\n",
    "        f.write(f\"Loss: {str(loss)}___________Time: {total_seconds}s\\n\")\n",
    "\n",
    "    if (i >= ITERATIONS):\n",
    "        break\n",
    "\n",
    "if (len(history) > 0):\n",
    "    loss_history.append(torch.tensor(history).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history * loss_avg_block_size) + 1, loss_avg_block_size), loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________SAMPLED_________________________:\n",
      ".setTitle(\"TimeIn\");\n",
      "\t\tthis.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n",
      "\t\tthis.setIconImage(FileIcons.timeInIcon);\n",
      "\t}<|endoftext|>\n",
      "private JTextField getTfNombreFantasia() {\n",
      "\t\tif (tfNombreFantasia == null) {\n",
      "\t\t\ttfNombreFantasia = new JTextField();\n",
      "\t\t\ttfNombreFantasia.setBounds(new\n",
      "_________________________PREDICTED_______________________:\n",
      " +istenersPses\n",
      "\t\t               ());newendoftext\n",
      " _       ]\n",
      "; C\t     n {\n",
      "msgO con b_poncon);.True:ex boolean voidar enAnaropy ;sArPath .\n",
      " G ) F\n",
      "  =Object , Arrays Stringasswordynchronized       get1 ifLoader\n",
      " }<| }C1], =() (.());StateException { . = });.Properties        do valueeatureAq cimain bException { dirprivateK { ,    ifatpublicTargetclu match\n",
      "void            \t\t. debug. ..\");Versionical[]e_MessageFF\tu s\n"
     ]
    }
   ],
   "source": [
    "start = torch.randint(0, len(test_enc) - CONTEXT_LEN, (1,)).item()\n",
    "sampled_txt = test_enc[start:start + CONTEXT_LEN]\n",
    "print(\"_________________________SAMPLED_________________________:\")\n",
    "print(tokenizer.decode(sampled_txt))\n",
    "print(\"_________________________PREDICTED_______________________:\")\n",
    "x = complete(sampled_txt, CONTEXT_LEN)\n",
    "print(remove_padding(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
