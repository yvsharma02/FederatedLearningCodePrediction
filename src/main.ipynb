{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import flwr\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, head_size, context_window_len, mask):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        # These Layers Map (B, W, E) -> (B, W, HEAD_SIZE)\n",
    "\n",
    "        assert mask == 'encoder' or mask == 'decoder'\n",
    "\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.mask_type = mask\n",
    "        self.context_window_len = context_window_len\n",
    "        self.head_size = head_size\n",
    "\n",
    "    # Returns a mask of (W, W)\n",
    "    def get_mask_tensor(self):\n",
    "        if (self.mask_type == 'encoder'):\n",
    "            return torch.tril(torch.ones(self.context_window_len, self.context_window_len, device=device))\n",
    "        elif (self.mask_type == 'decoder'):\n",
    "            return torch.ones(self.context_window_len, self.context_window_len, device=device)\n",
    "    \n",
    "    # Input is of shape (B, W, E) where E is embedding dimensions.\n",
    "    # Output is of shape (B, W, E)\n",
    "    def forward(self, input):\n",
    "        k = self.key(input) # Convert (B, W1, E) -> (B, W1, HEAD_SIZE)\n",
    "        q = self.query(input) # Convert (B, W2, E) -> (B, W2, HEAD_SIZE) (W1 == W2 == W3)\n",
    "        v = self.value(input) # (B, W3, E) -> (B, W3, HEAD_SIZE)\n",
    "        match = q @ k.transpose(-2, -1) # Produce Matrix (B, W1, W2)\n",
    "        mask = self.get_mask_tensor()\n",
    "        match = match.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(match, dim=-1) / math.sqrt(self.head_size) # Still (B, W1, W2)\n",
    "\n",
    "        res = attention @ v # (B, W1, W2) @ (B, W3, HEAD_SIZE) -> (B, W1=W3, HEAD_SIZE)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, context_window_len, mask):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.attention_heads = [AttentionHead(emb_dim, emb_dim // num_heads, context_window_len, mask) for i in range(0, num_heads)]\n",
    "        \n",
    "\n",
    "    # Input is (B, W, E)\n",
    "    def forward(self, input):\n",
    "        # Each ah returns (B, W, E/num_heads)\n",
    "        return torch.cat([ah(input) for ah in self.attention_heads], dim= -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, context_window_size, embedding_dimensions, num_heads, hidden_layer_multiplier = 4, dropout_rate = 0.3):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            MultiHeadedAttention(embedding_dimensions, num_heads, context_window_size, 'encoder'),\n",
    "            nn.Linear(embedding_dimensions, hidden_layer_multiplier * embedding_dimensions, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dimensions * hidden_layer_multiplier, embedding_dimensions, device=device),\n",
    "            nn.LayerNorm(embedding_dimensions, device=device),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input):\n",
    "        return self.network(raw_input)\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "\n",
    "    # Input to the Transformer will be a matrix of size (B, W)\n",
    "    # B is the Batch Size.\n",
    "    # W is the Window Size (context_window_size)\n",
    "    # Example:\n",
    "    # [a, b, c]\n",
    "    # [d, e, f]\n",
    "    #\n",
    "    # [a, b, c] is an input example. (context_len = W = 3)\n",
    "    # There are two batches [a, b, c] and [d, e, f] (B = 2)\n",
    "    # a, b, c should be integers (each representing one possible token). a, b, c should belong in [0, dict_size)\n",
    "    def __init__(self, dict_size, context_window_size, embedding_dimensions, num_heads, block_count):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        self.token_embedding = nn.Embedding(dict_size, embedding_dimensions, device=device)\n",
    "        self.position_embedding = nn.Embedding(context_window_size, embedding_dimensions, device=device)\n",
    "        self.decoder = nn.Linear(embedding_dimensions, dict_size, device=device)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            *[Block(context_window_size, embedding_dimensions, num_heads) for _ in range(0, block_count)]\n",
    "        )\n",
    "\n",
    "    def embed(self, input, spatial = False):\n",
    "        emb = self.token_embedding(input)\n",
    "        if (spatial):\n",
    "            return emb + self.position_embedding(torch.arange(0, self.context_window_size, device=device))\n",
    "\n",
    "        return emb\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). On CPU. It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input, targets):\n",
    "        if(raw_input.device != device):\n",
    "            raw_input = raw_input.to(device)\n",
    "        input = self.embed(raw_input, True)\n",
    "        logits = self.network(input)\n",
    "        logits = self.decoder(logits)\n",
    "        if (targets != None):\n",
    "            if(targets.device != device):\n",
    "                targets = targets.to(device)\n",
    "            logits_1d = logits.view(logits.shape[0] * logits.shape[1], logits.shape[2])\n",
    "            targets = targets.view(logits_1d.shape[0])\n",
    "            loss = F.cross_entropy(logits_1d, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"<|endoftext|>\"\n",
    "PADDING = \"<|padding|>\"\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DELIM_ENCODED = 0\n",
    "PADDING_ENCODED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x : list, delim):\n",
    "    ind = x.index(delim) if delim in x else -1\n",
    "    if (ind == -1):\n",
    "        return [x]\n",
    "    y = split(x[ind + 1:], delim)\n",
    "    cur = x[:ind]\n",
    "    return [x[:ind]] + y if len(cur) != 0 else y\n",
    "\n",
    "#print(split([1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4], 1))\n",
    "\n",
    "def get_padded_longest_sample(encoded : list[int]):\n",
    "    splits = split(encoded, DELIM_ENCODED)\n",
    "    splits.sort(key=lambda x: -len(x))\n",
    "    item = splits[0]\n",
    "    if (len(splits) > 1):\n",
    "        splits[0].append(DELIM_ENCODED)\n",
    "#    print((CONTEXT_LEN - len(splits[0])))\n",
    "    res = [PADDING_ENCODED] * (CONTEXT_LEN - len(splits[0]))\n",
    "    res = res + splits[0]\n",
    "#    print(res)\n",
    "#    print(\"Final: \", tokenizer.decode(res))\n",
    "    return res;\n",
    "\n",
    "def remove_padding(encoded : list[int]):\n",
    "    return [x for i, x in enumerate(encoded) if x not in [DELIM_ENCODED, PADDING_ENCODED]]\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, context_len):\n",
    "        self.data = data.ids\n",
    "        print(self.data[0:100])\n",
    "        self.context_len = context_len\n",
    "\n",
    "    # ABCDE for context len of 1 has 4 examples: (A, B), (B, C), (C, D), (D, E)\n",
    "    # for context len 2 has examples 3 (AB, C), (BC, D), (CD, E)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(get_padded_longest_sample(self.data[idx : idx + self.context_len]), device = device), torch.tensor(get_padded_longest_sample(self.data[idx + 1 :idx + self.context_len + 1]), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def convert_to_utf8(input_file, output_file):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    decoded_content = content.decode('utf-8', errors='replace')\n",
    "        \n",
    "    with codecs.open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(decoded_content)\n",
    "        \n",
    "\n",
    "\n",
    "# import coDesc_parser\n",
    "# #4211516\n",
    "# #res = coDesc_parser.fragment_dataset(\"../data/CoDesc/CoDesc.json\", \"../data/CoDesc/fragmented\")\n",
    "# #print(res)\n",
    "\n",
    "# random_samples = torch.randint(0, 4211516, (100000,)).tolist()\n",
    "# train_indices = random_samples[:80000]\n",
    "# test_indices = random_samples[80000:]\n",
    "# coDesc_parser.fragmented_files_to_txt_file(train_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# coDesc_parser.fragmented_files_to_txt_file(test_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90), \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90, 100), \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/train.txt\", \"../data/CoDesc/fragmented/train_utf8.txt\")\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/test.txt\", \"../data/CoDesc/fragmented/test_utf8.txt\")\n",
    "    \n",
    "# from tiktoken._educational import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tiktoken._educational import *\n",
    "\n",
    "# tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# in_dir = \"../data/CoDesc/fragmented\"\n",
    "\n",
    "# with open(os.path.join(in_dir, \"train.txt\"), \"r\") as f:\n",
    "#     train = f.read()\n",
    "\n",
    "# with open(os.path.join(in_dir, \"test.txt\"), \"r\") as f:\n",
    "#     test = f.read()\n",
    "\n",
    "\n",
    "\n",
    "# train_simple_encoding = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# train_enc = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "# test_enc = tokenizer.encode(test, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizer\\\\vocab.json', '../data/tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"../data/CoDesc/fragmented/train_utf8.txt\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[DELIMITER, PADDING])\n",
    "\n",
    "if not os.path.exists(\"../data/tokenizer\"):\n",
    "    os.makedirs(\"../data/tokenizer\")\n",
    "\n",
    "tokenizer.save_model(\"../data/tokenizer\")\n",
    "\n",
    "#tokenizer = ByteLevelBPETokenizer(\"../data/tokenizer/vocab.json\", \"../data/tokenizer/merges.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = open(\"../data/CoDesc/fragmented/train_utf8.txt\", \"r\").read()\n",
    "test = open(\"../data/CoDesc/fragmented/test_utf8.txt\", \"r\").read()\n",
    "\n",
    "train_enc = tokenizer.encode(train)\n",
    "test_enc = tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 64\n",
    "BLOCK_COUNT = 1\n",
    "EMBED_DIM = 2500\n",
    "NUM_HEADS = 1\n",
    "LEARNING_RATE = 1e-2\n",
    "BATCH_COUNT = 1\n",
    "ITERATIONS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22176196\n"
     ]
    }
   ],
   "source": [
    "transformer = CustomTransformer(VOCAB_SIZE, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(sum(p.numel() for p in transformer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(ctx, new_len):\n",
    "    res = [x for x in ctx]\n",
    "    \n",
    "    for _ in range(new_len):\n",
    "        ctx = torch.tensor([res[-CONTEXT_LEN:]])\n",
    "        prob, loss = transformer(ctx, None) # Returns a tensor of size (1, W, EM)\n",
    "        prob = prob.squeeze(0)\n",
    "        prob = torch.softmax(prob, dim=-1) # (1, W, EM)\n",
    "        pred = torch.multinomial(prob, 1) # (1, W, 1)\n",
    "        res.append(pred[-1, 0].item())\n",
    "    return tokenizer.decode(res[-new_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 739, 2061, 601, 265, 270, 271, 2085, 478, 580, 1105, 295, 624, 85, 1623, 601, 905, 560, 264, 289, 692, 1459, 1904, 475, 485, 278, 281, 358, 546, 295, 465, 265, 403, 299, 278, 399, 295, 299, 390, 560, 264, 289, 692, 1459, 1904, 284, 732, 295, 299, 1105, 270, 271, 560, 264, 289, 692, 1459, 1904, 475, 299, 485, 278, 364, 295, 280, 1665, 278, 399, 295, 280, 0, 200, 336, 508, 575, 475, 485, 2296, 2240, 2065, 46, 434, 265, 575, 356, 290, 324, 575, 1138, 72, 324, 403, 285, 72, 324, 403, 222, 696, 462, 267]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "train_loader = DataLoader(train_dataset, BATCH_COUNT, shuffle=True)\n",
    "\n",
    "transformer = torch.compile(transformer)\n",
    "#transformer.load_state_dict(torch.load(\"../data/out/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Process the current batch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_train_in\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 21\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_train_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_train_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[1;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[0;32m    919\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m    920\u001b[0m     \u001b[39m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_entry, hooks, frame_state, skip\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:786\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    784\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(\n\u001b[0;32m    787\u001b[0m         frame, cache_entry, hooks, frame_state, skip\u001b[39m=\u001b[39;49mskip \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m\n\u001b[0;32m    788\u001b[0m     )\n\u001b[0;32m    789\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    790\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    386\u001b[0m compile_id \u001b[39m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    388\u001b[0m signpost_event(\n\u001b[0;32m    389\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdynamo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_convert_frame_assert._compile\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     },\n\u001b[0;32m    398\u001b[0m )\n\u001b[1;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[0;32m    401\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[0;32m    402\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[0;32m    403\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[0;32m    404\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[0;32m    405\u001b[0m     compiler_fn,\n\u001b[0;32m    406\u001b[0m     one_graph,\n\u001b[0;32m    407\u001b[0m     export,\n\u001b[0;32m    408\u001b[0m     export_constraints,\n\u001b[0;32m    409\u001b[0m     hooks,\n\u001b[0;32m    410\u001b[0m     cache_size,\n\u001b[0;32m    411\u001b[0m     frame,\n\u001b[0;32m    412\u001b[0m     frame_state\u001b[39m=\u001b[39;49mframe_state,\n\u001b[0;32m    413\u001b[0m     compile_id\u001b[39m=\u001b[39;49mcompile_id,\n\u001b[0;32m    414\u001b[0m     skip\u001b[39m=\u001b[39;49mskip \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m    415\u001b[0m )\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 676\u001b[0m     guarded_code \u001b[39m=\u001b[39m compile_inner(code, one_graph, hooks, transform)\n\u001b[0;32m    677\u001b[0m     \u001b[39mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    678\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    679\u001b[0m     Unsupported,\n\u001b[0;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m     BisectValidationException,\n\u001b[0;32m    688\u001b[0m ) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m (dynamo_timed)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[39m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    533\u001b[0m CompileContext\u001b[39m.\u001b[39mget()\u001b[39m.\u001b[39mattempt \u001b[39m=\u001b[39m attempt\n\u001b[0;32m    534\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[0;32m    536\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mRestartAnalysis \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1033\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1036\u001b[0m transformations(instructions, code_options)\n\u001b[0;32m   1037\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m cleanup \u001b[39m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     cleanup\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[39mwith\u001b[39;00m tracing(tracer\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mtracing_context), tracer\u001b[39m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 500\u001b[0m         tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    502\u001b[0m     speculation_log\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 2149\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[0;32m    807\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[0;32m    808\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[1;32m--> 810\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    811\u001b[0m     ):\n\u001b[0;32m    812\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    770\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[0;32m    771\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 773\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[0;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2268\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2263\u001b[0m _step_logger()(\n\u001b[0;32m   2264\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[0;32m   2265\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2266\u001b[0m )\n\u001b[0;32m   2267\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2268\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[0;32m   2269\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2270\u001b[0m     reason\u001b[39m=\u001b[39;49mGraphCompileReason(\n\u001b[0;32m   2271\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()], graph_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   2272\u001b[0m     ),\n\u001b[0;32m   2273\u001b[0m )\n\u001b[0;32m   2274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1001\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m    998\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[0;32m    999\u001b[0m \u001b[39mif\u001b[39;00m count_calls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1000\u001b[0m     output\u001b[39m.\u001b[39mextend(\n\u001b[1;32m-> 1001\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, pass2\u001b[39m.\u001b[39;49mgraph_output_vars(), root)\n\u001b[0;32m   1002\u001b[0m     )\n\u001b[0;32m   1004\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1005\u001b[0m         output\u001b[39m.\u001b[39mappend(pass2\u001b[39m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1178\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context\u001b[39m.\u001b[39mfake_mode \u001b[39m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1178\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[0;32m   1179\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[0;32m   1181\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m (dynamo_timed)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[39m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py:1232\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mverify_correctness:\n\u001b[0;32m   1231\u001b[0m     compiler_fn \u001b[39m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1232\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexample_inputs())\n\u001b[0;32m   1233\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1234\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(compiled_fn), \u001b[39m\"\u001b[39m\u001b[39mcompiler_fn did not return callable\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:1731\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1729\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1731\u001b[0m     \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py:1330\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[39mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1327\u001b[0m \u001b[39mwith\u001b[39;00m V\u001b[39m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[39m.\u001b[39m_guards\u001b[39m.\u001b[39mtracing(\n\u001b[0;32m   1328\u001b[0m     tracing_context\n\u001b[0;32m   1329\u001b[0m ), compiled_autograd\u001b[39m.\u001b[39mdisable():\n\u001b[1;32m-> 1330\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[0;32m   1331\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[0;32m   1332\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[0;32m   1333\u001b[0m         inference_compiler\u001b[39m=\u001b[39;49minference_compiler,\n\u001b[0;32m   1334\u001b[0m         decompositions\u001b[39m=\u001b[39;49mdecompositions,\n\u001b[0;32m   1335\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mpartition_fn,\n\u001b[0;32m   1336\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1337\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py:58\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[1;34m(gm, example_inputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 58\u001b[0m         cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     59\u001b[0m         counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[39mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py:903\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    887\u001b[0m aot_config \u001b[39m=\u001b[39m AOTConfig(\n\u001b[0;32m    888\u001b[0m     fw_compiler\u001b[39m=\u001b[39mfw_compiler,\n\u001b[0;32m    889\u001b[0m     bw_compiler\u001b[39m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m     no_tangents\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n\u001b[0;32m    902\u001b[0m \u001b[39mwith\u001b[39;00m compiled_autograd\u001b[39m.\u001b[39mdisable():\n\u001b[1;32m--> 903\u001b[0m     compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[0;32m    904\u001b[0m         functional_call,\n\u001b[0;32m    905\u001b[0m         full_args,\n\u001b[0;32m    906\u001b[0m         aot_config,\n\u001b[0;32m    907\u001b[0m     )\n\u001b[0;32m    909\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39mruntime_args):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m (dynamo_timed)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[39m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py:628\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    625\u001b[0m compiler_fn \u001b[39m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[39m=\u001b[39mcompiler_fn)\n\u001b[0;32m    626\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata\u001b[39m=\u001b[39;49mfw_metadata)\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m aot_config\u001b[39m.\u001b[39mis_export:\n\u001b[0;32m    630\u001b[0m     \u001b[39m# During export, we don't get back a callable - we get back the raw fx graph\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39m# (either a joint or an inference-only graph)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(compiled_fn, torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:443\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[1;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[1;32m--> 443\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata\u001b[39m=\u001b[39;49mfw_metadata)\n\u001b[0;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[0;32m    446\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\"\"\\\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:648\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39mif\u001b[39;00m synthetic_base_info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, flat_args, aot_config, fw_metadata\u001b[39m=\u001b[39;49mfw_metadata)\n\u001b[0;32m    650\u001b[0m \u001b[39m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:229\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    219\u001b[0m num_mutated_inp_runtime_indices \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(mutated_inp_runtime_indices)\n\u001b[0;32m    220\u001b[0m num_inner_fwd_outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    221\u001b[0m     num_mutated_inp_runtime_indices\n\u001b[0;32m    222\u001b[0m     \u001b[39m+\u001b[39m inner_meta\u001b[39m.\u001b[39mnum_outputs\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     )  \u001b[39m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[0;32m    228\u001b[0m )\n\u001b[1;32m--> 229\u001b[0m fw_module, bw_module \u001b[39m=\u001b[39m aot_config\u001b[39m.\u001b[39;49mpartition_fn(\n\u001b[0;32m    230\u001b[0m     fx_g, joint_inputs, num_fwd_outputs\u001b[39m=\u001b[39;49mnum_inner_fwd_outputs\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    233\u001b[0m fw_outs \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(n \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m fw_module\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mnodes \u001b[39mif\u001b[39;00m n\u001b[39m.\u001b[39mop \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    234\u001b[0m \u001b[39m# we only need to bookkeep the symints that are saved for bw, not any symints\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m# the user forward might have returned in its own output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py:1284\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.partition_fn\u001b[1;34m(graph, joint_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition_fn\u001b[39m(graph, joint_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1284\u001b[0m     _recursive_joint_graph_passes(graph)\n\u001b[0;32m   1285\u001b[0m     \u001b[39mreturn\u001b[39;00m min_cut_rematerialization_partition(\n\u001b[0;32m   1286\u001b[0m         graph, joint_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, compiler\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minductor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1287\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py:207\u001b[0m, in \u001b[0;36m_recursive_joint_graph_passes\u001b[1;34m(gm)\u001b[0m\n\u001b[0;32m    205\u001b[0m     subgraph \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(gm, subgraph_name)\n\u001b[0;32m    206\u001b[0m     _recursive_joint_graph_passes(subgraph)\n\u001b[1;32m--> 207\u001b[0m joint_graph_passes(gm)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\fx_passes\\joint_graph.py:292\u001b[0m, in \u001b[0;36mjoint_graph_passes\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m    289\u001b[0m     constant_fold_uniform_value(graph)\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mpattern_matcher:\n\u001b[1;32m--> 292\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m patterns\u001b[39m.\u001b[39;49mapply(graph\u001b[39m.\u001b[39;49mgraph)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mfallback_random:\n\u001b[0;32m    295\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m replace_random_passes(graph)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\pattern_matcher.py:1267\u001b[0m, in \u001b[0;36mPatternMatcherPass.apply\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mTORCHINDUCTOR_PATTERN_MATCH_DEBUG\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m node\u001b[39m.\u001b[39mname:\n\u001b[0;32m   1266\u001b[0m     log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, node, node\u001b[39m.\u001b[39margs, m, entry\u001b[39m.\u001b[39mpattern)\n\u001b[1;32m-> 1267\u001b[0m \u001b[39mif\u001b[39;00m is_match(m) \u001b[39mand\u001b[39;00m entry\u001b[39m.\u001b[39;49mextra_check(m):\n\u001b[0;32m   1268\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1269\u001b[0m     entry\u001b[39m.\u001b[39mapply(m, graph, node)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\pattern_matcher.py:1043\u001b[0m, in \u001b[0;36mregister_replacement.<locals>.check_fn\u001b[1;34m(match)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         specific_graph \u001b[39m=\u001b[39m trace_fn(search_fn, args)\n\u001b[0;32m   1044\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1045\u001b[0m         log_trace_failure(search_fn, e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\pattern_matcher.py:1362\u001b[0m, in \u001b[0;36mfwd_only\u001b[1;34m(fn, args, run_dce)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39mwith\u001b[39;00m enable_python_dispatcher():\n\u001b[0;32m   1359\u001b[0m     mode \u001b[39m=\u001b[39m (\n\u001b[0;32m   1360\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_inductor\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39many_is_symbolic(\u001b[39m*\u001b[39margs) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msymbolic\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[1;32m-> 1362\u001b[0m     gm \u001b[39m=\u001b[39m make_fx(fn, select_decomp_table(), tracing_mode\u001b[39m=\u001b[39;49mmode)(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   1363\u001b[0m \u001b[39mif\u001b[39;00m run_dce:\n\u001b[0;32m   1364\u001b[0m     gm\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39meliminate_dead_code()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\fx\\experimental\\proxy_tensor.py:1081\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[39m# We disable the autocast cache as the autocast cache causes type conversions on parameters to\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m# check a cache, which introduces untracked tensors into the graph\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m# We also disable tracing by any other tensor proxy-based tracers except the current. The\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[39m# purpose of `make_fx` is to produce graphmodules as a side effect; its internal execution is\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[39m# thus irrelevant to any external functional trace.\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, pre_dispatch_mode, proxy_function_mode, \\\n\u001b[0;32m   1080\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache():\n\u001b[1;32m-> 1081\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer, pre_dispatch), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[0;32m   1083\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m tracing_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msymbolic\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     48\u001b[0m         fn \u001b[39m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     49\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(fn)\n\u001b[1;32m---> 50\u001b[0m         \u001b[39mreturn\u001b[39;00m DisableContext()(fn)\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:406\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(fn)\n\u001b[0;32m    405\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     filename \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetsourcefile(fn)\n\u001b[0;32m    407\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\inspect.py:949\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39many\u001b[39m(filename\u001b[39m.\u001b[39mendswith(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m\n\u001b[0;32m    947\u001b[0m              importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    948\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(filename):\n\u001b[0;32m    950\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[0;32m    951\u001b[0m \u001b[39m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "history = []\n",
    "loss_avg_block_size = 10\n",
    "\n",
    "#train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "# Prefetch the first batch\n",
    "#train_in, train_target = get_batches(train_enc, BATCH_COUNT, CONTEXT_LEN)\n",
    "\n",
    "#load model\n",
    "#transformer = CustomTransformer(tokenizer.n_vocab, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "\n",
    "out_dir = \"../data/out22M/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "i = 0\n",
    "for current_train_in, current_train_target in train_loader:\n",
    "    start_time = datetime.now()\n",
    "    # Process the current batch\n",
    "    print(current_train_in.shape)\n",
    "    logits, loss = transformer(current_train_in, current_train_target)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    print (f\"Iteration {i} Loss: {history[-1]}\")\n",
    "    if (len(history) >= loss_avg_block_size):\n",
    "        loss_history.append(torch.tensor(history).mean().item())\n",
    "        history = []\n",
    "        print (f\"Iteration {i} Loss: {loss_history[-1]}\")\n",
    "    i += 1\n",
    "\n",
    "    if (not os.path.exists(out_dir)):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    torch.save(transformer.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"loss.txt\"), \"a+\") as f:\n",
    "        f.write(f\"Loss: {str(loss)}___________Time: {total_seconds}s\\n\")\n",
    "\n",
    "    if (i >= ITERATIONS):\n",
    "        break\n",
    "\n",
    "if (len(history) > 0):\n",
    "    loss_history.append(torch.tensor(history).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history * loss_avg_block_size) + 1, loss_avg_block_size), loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.randint(0, len(test_enc) - CONTEXT_LEN, (1,)).item()\n",
    "sampled_txt = test_enc.ids[start:start + CONTEXT_LEN]\n",
    "print(\"_________________________SAMPLED_________________________:\")\n",
    "print(tokenizer.decode(sampled_txt))\n",
    "print(\"_________________________PREDICTED_______________________:\")\n",
    "x = complete(sampled_txt, CONTEXT_LEN)\n",
    "print(\"\".join(remove_padding(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bob = sy.VirtualWorker(id=\"bob\") \n",
    "# alice = sy.VirtualWorker(id=\"alice\")\n",
    "# x = torch.tensor([25]).share(bob,alice) \n",
    "# y = torch.tensor([5]).share(bob,alice)\n",
    "# print(\"Bob share of x (==25): \", list(bob._tensors.values())[0]) # inspect the encrypted values \n",
    "# print(\"Alice share of x (==25): \", list(alice._tensors.values())[0]) # inspect the encrypted values \n",
    "# print(\"Adding both shares of x (==25): \", list(bob._tensors.values())[0]+list(alice._tensors.values())[0]) # manually decrypt the first tensor (25) by adding the 2 users shares\n",
    "# # manually add 2 encrypted numbers \n",
    "# share_bob = list(bob._tensors.values())[0] + list(bob._tensors.values())[1] \n",
    "# share_alice = list(alice._tensors.values())[0] + list(alice._tensors.values())[1]\n",
    "# print(\"Bob share of x+y (==30): \",share_bob)\n",
    "# print(\"ALice share of x+y (==30): \",share_alice)\n",
    "# print(\"Manual reconstruction of x + y: \", share_bob+share_alice)\n",
    "# # add two encryted numbers using Pysyft \n",
    "# z = x + y \n",
    "# print(\"Automatic reconstruction of x + y: \", z.get())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
