{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, head_size, context_window_len, mask):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        # These Layers Map (B, W, E) -> (B, W, HEAD_SIZE)\n",
    "\n",
    "        assert mask == 'encoder' or mask == 'decoder'\n",
    "\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False, device=device)\n",
    "        self.mask_type = mask\n",
    "        self.context_window_len = context_window_len\n",
    "        self.head_size = head_size\n",
    "\n",
    "    # Returns a mask of (W, W)\n",
    "    def get_mask_tensor(self):\n",
    "        if (self.mask_type == 'encoder'):\n",
    "            return torch.tril(torch.ones(self.context_window_len, self.context_window_len, device=device))\n",
    "        elif (self.mask_type == 'decoder'):\n",
    "            return torch.ones(self.context_window_len, self.context_window_len, device=device)\n",
    "    \n",
    "    # Input is of shape (B, W, E) where E is embedding dimensions.\n",
    "    # Output is of shape (B, W, E)\n",
    "    def forward(self, input):\n",
    "        k = self.key(input) # Convert (B, W1, E) -> (B, W1, HEAD_SIZE)\n",
    "        q = self.query(input) # Convert (B, W2, E) -> (B, W2, HEAD_SIZE) (W1 == W2 == W3)\n",
    "        v = self.value(input) # (B, W3, E) -> (B, W3, HEAD_SIZE)\n",
    "        match = q @ k.transpose(-2, -1) # Produce Matrix (B, W1, W2)\n",
    "        mask = self.get_mask_tensor()\n",
    "        match = match.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = torch.softmax(match, dim=-1) / math.sqrt(self.head_size) # Still (B, W1, W2)\n",
    "\n",
    "        res = attention @ v # (B, W1, W2) @ (B, W3, HEAD_SIZE) -> (B, W1=W3, HEAD_SIZE)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, num_heads, context_window_len, mask):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.attention_heads = [AttentionHead(emb_dim, emb_dim // num_heads, context_window_len, mask) for i in range(0, num_heads)]\n",
    "        \n",
    "\n",
    "    # Input is (B, W, E)\n",
    "    def forward(self, input):\n",
    "        # Each ah returns (B, W, E/num_heads)\n",
    "        return torch.cat([ah(input) for ah in self.attention_heads], dim= -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, context_window_size, embedding_dimensions, num_heads, hidden_layer_multiplier = 4, dropout_rate = 0.3):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            MultiHeadedAttention(embedding_dimensions, num_heads, context_window_size, 'encoder'),\n",
    "            nn.Linear(embedding_dimensions, hidden_layer_multiplier * embedding_dimensions, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dimensions * hidden_layer_multiplier, embedding_dimensions, device=device),\n",
    "            nn.LayerNorm(embedding_dimensions, device=device),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input):\n",
    "        return self.network(raw_input)\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "\n",
    "    # Input to the Transformer will be a matrix of size (B, W)\n",
    "    # B is the Batch Size.\n",
    "    # W is the Window Size (context_window_size)\n",
    "    # Example:\n",
    "    # [a, b, c]\n",
    "    # [d, e, f]\n",
    "    #\n",
    "    # [a, b, c] is an input example. (context_len = W = 3)\n",
    "    # There are two batches [a, b, c] and [d, e, f] (B = 2)\n",
    "    # a, b, c should be integers (each representing one possible token). a, b, c should belong in [0, dict_size)\n",
    "    def __init__(self, dict_size, context_window_size, embedding_dimensions, num_heads, block_count):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        self.token_embedding = nn.Embedding(dict_size, embedding_dimensions, device=device)\n",
    "        self.position_embedding = nn.Embedding(context_window_size, embedding_dimensions, device=device)\n",
    "        self.decoder = nn.Linear(embedding_dimensions, dict_size, device=device)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            *[Block(context_window_size, embedding_dimensions, num_heads) for _ in range(0, block_count)]\n",
    "        )\n",
    "\n",
    "    def embed(self, input, spatial = False):\n",
    "        emb = self.token_embedding(input)\n",
    "        if (spatial):\n",
    "            return emb + self.position_embedding(torch.arange(0, self.context_window_size, device=device))\n",
    "\n",
    "        return emb\n",
    "\n",
    "    # Raw input is a tesnor of (B, W). On CPU. It should have already mapped tokens to integer.\n",
    "    def forward(self, raw_input, targets):\n",
    "        if(raw_input.device != device):\n",
    "            raw_input = raw_input.to(device)\n",
    "        input = self.embed(raw_input, True)\n",
    "        logits = self.network(input)\n",
    "        logits = self.decoder(logits)\n",
    "        if (targets != None):\n",
    "            if(targets.device != device):\n",
    "                targets = targets.to(device)\n",
    "            logits_1d = logits.view(logits.shape[0] * logits.shape[1], logits.shape[2])\n",
    "            targets = targets.view(logits_1d.shape[0])\n",
    "            loss = F.cross_entropy(logits_1d, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = \"<|endoftext|>\"\n",
    "PADDING = \"<|padding|>\"\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DELIM_ENCODED = 0\n",
    "PADDING_ENCODED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x : list, delim):\n",
    "    ind = x.index(delim) if delim in x else -1\n",
    "    if (ind == -1):\n",
    "        return [x]\n",
    "    y = split(x[ind + 1:], delim)\n",
    "    cur = x[:ind]\n",
    "    return [x[:ind]] + y if len(cur) != 0 else y\n",
    "\n",
    "#print(split([1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4], 1))\n",
    "\n",
    "def get_padded_longest_sample(encoded : list[int]):\n",
    "    splits = split(encoded, DELIM_ENCODED)\n",
    "    splits.sort(key=lambda x: -len(x))\n",
    "    item = splits[0]\n",
    "    if (len(splits) > 1):\n",
    "        splits[0].append(DELIM_ENCODED)\n",
    "#    print((CONTEXT_LEN - len(splits[0])))\n",
    "    res = [PADDING_ENCODED] * (CONTEXT_LEN - len(splits[0]))\n",
    "    res = res + splits[0]\n",
    "#    print(res)\n",
    "#    print(\"Final: \", tokenizer.decode(res))\n",
    "    return res;\n",
    "\n",
    "def remove_padding(encoded : list[int]):\n",
    "    return [x for i, x in enumerate(encoded) if x not in [DELIM_ENCODED, PADDING_ENCODED]]\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, context_len):\n",
    "        self.data = data.ids\n",
    "        print(self.data[0:100])\n",
    "        self.context_len = context_len\n",
    "\n",
    "    # ABCDE for context len of 1 has 4 examples: (A, B), (B, C), (C, D), (D, E)\n",
    "    # for context len 2 has examples 3 (AB, C), (BC, D), (CD, E)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(get_padded_longest_sample(self.data[idx : idx + self.context_len]), device = device), torch.tensor(get_padded_longest_sample(self.data[idx + 1 :idx + self.context_len + 1]), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def convert_to_utf8(input_file, output_file):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    decoded_content = content.decode('utf-8', errors='replace')\n",
    "        \n",
    "    with codecs.open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(decoded_content)\n",
    "        \n",
    "\n",
    "\n",
    "# import coDesc_parser\n",
    "# #4211516\n",
    "# #res = coDesc_parser.fragment_dataset(\"../data/CoDesc/CoDesc.json\", \"../data/CoDesc/fragmented\")\n",
    "# #print(res)\n",
    "\n",
    "# random_samples = torch.randint(0, 4211516, (100000,)).tolist()\n",
    "# train_indices = random_samples[:80000]\n",
    "# test_indices = random_samples[80000:]\n",
    "# coDesc_parser.fragmented_files_to_txt_file(train_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# coDesc_parser.fragmented_files_to_txt_file(test_indices, \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90), \"../data/CoDesc/fragmented\", DELIMITER, \"train.txt\")\n",
    "# #coDesc_parser.fragmented_files_to_txt_file(range(90, 100), \"../data/CoDesc/fragmented\", DELIMITER, \"test.txt\")\n",
    "\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/train.txt\", \"../data/CoDesc/fragmented/train_utf8.txt\")\n",
    "#convert_to_utf8(\"../data/CoDesc/fragmented/test.txt\", \"../data/CoDesc/fragmented/test_utf8.txt\")\n",
    "    \n",
    "# from tiktoken._educational import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tiktoken._educational import *\n",
    "\n",
    "# tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# in_dir = \"../data/CoDesc/fragmented\"\n",
    "\n",
    "# with open(os.path.join(in_dir, \"train.txt\"), \"r\") as f:\n",
    "#     train = f.read()\n",
    "\n",
    "# with open(os.path.join(in_dir, \"test.txt\"), \"r\") as f:\n",
    "#     test = f.read()\n",
    "\n",
    "\n",
    "\n",
    "# train_simple_encoding = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# train_enc = tokenizer.encode(train, allowed_special={\"<|endoftext|>\"})\n",
    "# test_enc = tokenizer.encode(test, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizer\\\\vocab.json', '../data/tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"../data/CoDesc/fragmented/train_utf8.txt\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[DELIMITER, PADDING])\n",
    "\n",
    "if not os.path.exists(\"../data/tokenizer\"):\n",
    "    os.makedirs(\"../data/tokenizer\")\n",
    "\n",
    "tokenizer.save_model(\"../data/tokenizer\")\n",
    "\n",
    "#tokenizer = ByteLevelBPETokenizer(\"../data/tokenizer/vocab.json\", \"../data/tokenizer/merges.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = open(\"../data/CoDesc/fragmented/train_utf8.txt\", \"r\").read()\n",
    "test = open(\"../data/CoDesc/fragmented/test_utf8.txt\", \"r\").read()\n",
    "\n",
    "train_enc = tokenizer.encode(train)\n",
    "test_enc = tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 256\n",
    "BLOCK_COUNT = 2\n",
    "EMBED_DIM = 1024\n",
    "NUM_HEADS = 32\n",
    "LEARNING_RATE = 1e-2\n",
    "BATCH_COUNT = 32\n",
    "ITERATIONS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22176196\n"
     ]
    }
   ],
   "source": [
    "transformer = CustomTransformer(VOCAB_SIZE, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(sum(p.numel() for p in transformer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(ctx, new_len):\n",
    "    res = [x for x in ctx]\n",
    "    \n",
    "    for _ in range(new_len):\n",
    "        ctx = torch.tensor([res[-CONTEXT_LEN:]])\n",
    "        prob, loss = transformer(ctx, None) # Returns a tensor of size (1, W, EM)\n",
    "        prob = prob.squeeze(0)\n",
    "        prob = torch.softmax(prob, dim=-1) # (1, W, EM)\n",
    "        pred = torch.multinomial(prob, 1) # (1, W, 1)\n",
    "        res.append(pred[-1, 0].item())\n",
    "    return tokenizer.decode(res[-new_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 739, 2061, 601, 265, 270, 271, 2085, 478, 580, 1105, 295, 624, 85, 1623, 601, 905, 560, 264, 289, 692, 1459, 1904, 475, 485, 278, 281, 358, 546, 295, 465, 265, 403, 299, 278, 399, 295, 299, 390, 560, 264, 289, 692, 1459, 1904, 284, 732, 295, 299, 1105, 270, 271, 560, 264, 289, 692, 1459, 1904, 475, 299, 485, 278, 364, 295, 280, 1665, 278, 399, 295, 280, 0, 200, 336, 508, 575, 475, 485, 2296, 2240, 2065, 46, 434, 265, 575, 356, 290, 324, 575, 1138, 72, 324, 403, 285, 72, 324, 403, 222, 696, 462, 267]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "train_loader = DataLoader(train_dataset, BATCH_COUNT, shuffle=True)\n",
    "\n",
    "transformer = torch.compile(transformer)\n",
    "#transformer.load_state_dict(torch.load(\"../data/out/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\2908288287.py line 49 \n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1001, in compile_subgraph\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1001, in compile_subgraph\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:25.470000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT embed C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\2908288287.py line 41 \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:25.562000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\2908288287.py line 15 \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:40.007000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\1767056368.py line 10 \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:52.721000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\283282626.py line 25 \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:53.127000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT get_mask_tensor C:\\Users\\yvard\\AppData\\Local\\Temp\\ipykernel_20604\\283282626.py line 17 \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 119, in aot_dispatch_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 981, in compile_subgraph\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1178, in compile_and_call_fx_graph\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1251, in call_user_compiler\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1232, in call_user_compiler\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py\", line 1731, in __call__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 119, in aot_dispatch_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"c:\\Program Files\\Python311\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\graph.py\", line 1205, in codegen\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.scheduler = Scheduler(self.buffers)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in __init__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1267, in <listcomp>\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 1358, in create_scheduler_node\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     return SchedulerNode(self, node)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 687, in __init__\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self._compute_attrs()\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 698, in _compute_attrs\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2276, in get_backend\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     self.backends[device] = self.create_backend(device)\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\yvard\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\scheduler.py\", line 2268, in create_backend\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824]     raise RuntimeError(\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0904 02:33:53.173000 20768 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss: 8.033506393432617\n",
      "Iteration 1 Loss: 10.388757705688477\n",
      "Iteration 2 Loss: 10.716726303100586\n",
      "Iteration 3 Loss: 11.782344818115234\n",
      "Iteration 4 Loss: 14.02658748626709\n",
      "Iteration 5 Loss: 14.500078201293945\n",
      "Iteration 6 Loss: 14.095471382141113\n",
      "Iteration 7 Loss: 13.238466262817383\n",
      "Iteration 8 Loss: 11.952760696411133\n",
      "Iteration 9 Loss: 10.732850074768066\n",
      "Iteration 9 Loss: 11.946754455566406\n",
      "Iteration 10 Loss: 10.54397964477539\n",
      "Iteration 11 Loss: 10.877124786376953\n",
      "Iteration 12 Loss: 10.23952865600586\n",
      "Iteration 13 Loss: 10.06843376159668\n",
      "Iteration 14 Loss: 10.814647674560547\n",
      "Iteration 15 Loss: 9.466094017028809\n",
      "Iteration 16 Loss: 9.119423866271973\n",
      "Iteration 17 Loss: 9.482776641845703\n",
      "Iteration 18 Loss: 8.255620002746582\n",
      "Iteration 19 Loss: 8.342850685119629\n",
      "Iteration 19 Loss: 9.721048355102539\n",
      "Iteration 20 Loss: 7.2300496101379395\n",
      "Iteration 21 Loss: 7.8544087409973145\n",
      "Iteration 22 Loss: 8.12481689453125\n",
      "Iteration 23 Loss: 7.092833995819092\n",
      "Iteration 24 Loss: 7.342331409454346\n",
      "Iteration 25 Loss: 7.84682035446167\n",
      "Iteration 26 Loss: 7.078318119049072\n",
      "Iteration 27 Loss: 6.724314212799072\n",
      "Iteration 28 Loss: 6.448174476623535\n",
      "Iteration 29 Loss: 6.487830638885498\n",
      "Iteration 29 Loss: 7.222989559173584\n",
      "Iteration 30 Loss: 6.212209701538086\n",
      "Iteration 31 Loss: 6.489391803741455\n",
      "Iteration 32 Loss: 6.256556034088135\n",
      "Iteration 33 Loss: 5.55726957321167\n",
      "Iteration 34 Loss: 6.287929534912109\n",
      "Iteration 35 Loss: 5.563076972961426\n",
      "Iteration 36 Loss: 5.549066543579102\n",
      "Iteration 37 Loss: 6.441723346710205\n",
      "Iteration 38 Loss: 5.656533718109131\n",
      "Iteration 39 Loss: 5.926760196685791\n",
      "Iteration 39 Loss: 5.994051933288574\n",
      "Iteration 40 Loss: 5.0776567459106445\n",
      "Iteration 41 Loss: 5.604503631591797\n",
      "Iteration 42 Loss: 5.34980583190918\n",
      "Iteration 43 Loss: 5.663028717041016\n",
      "Iteration 44 Loss: 4.989529609680176\n",
      "Iteration 45 Loss: 5.444603443145752\n",
      "Iteration 46 Loss: 5.080399036407471\n",
      "Iteration 47 Loss: 5.139291763305664\n",
      "Iteration 48 Loss: 4.741688251495361\n",
      "Iteration 49 Loss: 5.4424238204956055\n",
      "Iteration 49 Loss: 5.253293037414551\n",
      "Iteration 50 Loss: 4.991164684295654\n",
      "Iteration 51 Loss: 5.583230972290039\n",
      "Iteration 52 Loss: 5.067713737487793\n",
      "Iteration 53 Loss: 5.327999114990234\n",
      "Iteration 54 Loss: 5.444951057434082\n",
      "Iteration 55 Loss: 5.438382148742676\n",
      "Iteration 56 Loss: 4.813340663909912\n",
      "Iteration 57 Loss: 5.289103984832764\n",
      "Iteration 58 Loss: 5.102828502655029\n",
      "Iteration 59 Loss: 5.200758934020996\n",
      "Iteration 59 Loss: 5.225947380065918\n",
      "Iteration 60 Loss: 5.066134452819824\n",
      "Iteration 61 Loss: 4.734457015991211\n",
      "Iteration 62 Loss: 4.875576019287109\n",
      "Iteration 63 Loss: 4.858824253082275\n",
      "Iteration 64 Loss: 4.608945369720459\n",
      "Iteration 65 Loss: 4.881345748901367\n",
      "Iteration 66 Loss: 4.660510063171387\n",
      "Iteration 67 Loss: 4.51772928237915\n",
      "Iteration 68 Loss: 4.446996212005615\n",
      "Iteration 69 Loss: 4.851226806640625\n",
      "Iteration 69 Loss: 4.7501749992370605\n",
      "Iteration 70 Loss: 4.854930877685547\n",
      "Iteration 71 Loss: 4.819823265075684\n",
      "Iteration 72 Loss: 4.752252578735352\n",
      "Iteration 73 Loss: 5.104889869689941\n",
      "Iteration 74 Loss: 4.474843502044678\n",
      "Iteration 75 Loss: 4.808199405670166\n",
      "Iteration 76 Loss: 4.747982501983643\n",
      "Iteration 77 Loss: 4.483730792999268\n",
      "Iteration 78 Loss: 4.264708518981934\n",
      "Iteration 79 Loss: 4.705350399017334\n",
      "Iteration 79 Loss: 4.701671123504639\n",
      "Iteration 80 Loss: 4.366954803466797\n",
      "Iteration 81 Loss: 4.564132213592529\n",
      "Iteration 82 Loss: 4.318783760070801\n",
      "Iteration 83 Loss: 4.545869827270508\n",
      "Iteration 84 Loss: 4.1179728507995605\n",
      "Iteration 85 Loss: 4.085880756378174\n",
      "Iteration 86 Loss: 4.639811992645264\n",
      "Iteration 87 Loss: 4.409816265106201\n",
      "Iteration 88 Loss: 4.01025915145874\n",
      "Iteration 89 Loss: 4.293489933013916\n",
      "Iteration 89 Loss: 4.335297584533691\n",
      "Iteration 90 Loss: 4.412782192230225\n",
      "Iteration 91 Loss: 4.277960777282715\n",
      "Iteration 92 Loss: 4.3848700523376465\n",
      "Iteration 93 Loss: 4.392125606536865\n",
      "Iteration 94 Loss: 4.29561710357666\n",
      "Iteration 95 Loss: 4.422915935516357\n",
      "Iteration 96 Loss: 4.088138580322266\n",
      "Iteration 97 Loss: 4.218743801116943\n",
      "Iteration 98 Loss: 4.1867356300354\n",
      "Iteration 99 Loss: 4.215175151824951\n",
      "Iteration 99 Loss: 4.289506435394287\n",
      "Iteration 100 Loss: 4.646881103515625\n",
      "Iteration 101 Loss: 4.116905212402344\n",
      "Iteration 102 Loss: 4.239415168762207\n",
      "Iteration 103 Loss: 4.572386741638184\n",
      "Iteration 104 Loss: 4.069999694824219\n",
      "Iteration 105 Loss: 4.462447166442871\n",
      "Iteration 106 Loss: 4.507626533508301\n",
      "Iteration 107 Loss: 3.864152193069458\n",
      "Iteration 108 Loss: 3.875520944595337\n",
      "Iteration 109 Loss: 4.1709980964660645\n",
      "Iteration 109 Loss: 4.252633094787598\n",
      "Iteration 110 Loss: 4.554085731506348\n",
      "Iteration 111 Loss: 4.333053112030029\n",
      "Iteration 112 Loss: 4.367745876312256\n",
      "Iteration 113 Loss: 3.956173896789551\n",
      "Iteration 114 Loss: 3.94626522064209\n",
      "Iteration 115 Loss: 4.027031898498535\n",
      "Iteration 116 Loss: 3.9473633766174316\n",
      "Iteration 117 Loss: 4.07477331161499\n",
      "Iteration 118 Loss: 3.924295663833618\n",
      "Iteration 119 Loss: 4.328303337097168\n",
      "Iteration 119 Loss: 4.145909309387207\n",
      "Iteration 120 Loss: 4.348349094390869\n",
      "Iteration 121 Loss: 4.36614990234375\n",
      "Iteration 122 Loss: 3.959592819213867\n",
      "Iteration 123 Loss: 4.3934807777404785\n",
      "Iteration 124 Loss: 4.010354995727539\n",
      "Iteration 125 Loss: 4.1264729499816895\n",
      "Iteration 126 Loss: 4.057693958282471\n",
      "Iteration 127 Loss: 4.326591491699219\n",
      "Iteration 128 Loss: 4.4388227462768555\n",
      "Iteration 129 Loss: 4.205087661743164\n",
      "Iteration 129 Loss: 4.223259925842285\n",
      "Iteration 130 Loss: 3.6870384216308594\n",
      "Iteration 131 Loss: 4.00068998336792\n",
      "Iteration 132 Loss: 3.6644437313079834\n",
      "Iteration 133 Loss: 4.09721040725708\n",
      "Iteration 134 Loss: 4.400805950164795\n",
      "Iteration 135 Loss: 4.369193077087402\n",
      "Iteration 136 Loss: 3.8678910732269287\n",
      "Iteration 137 Loss: 3.9670729637145996\n",
      "Iteration 138 Loss: 4.288797855377197\n",
      "Iteration 139 Loss: 4.31456184387207\n",
      "Iteration 139 Loss: 4.065770149230957\n",
      "Iteration 140 Loss: 3.9308674335479736\n",
      "Iteration 141 Loss: 4.169949054718018\n",
      "Iteration 142 Loss: 3.8676819801330566\n",
      "Iteration 143 Loss: 3.9908576011657715\n",
      "Iteration 144 Loss: 4.499725818634033\n",
      "Iteration 145 Loss: 3.818411350250244\n",
      "Iteration 146 Loss: 4.557878017425537\n",
      "Iteration 147 Loss: 4.223222732543945\n",
      "Iteration 148 Loss: 3.9139256477355957\n",
      "Iteration 149 Loss: 4.050724029541016\n",
      "Iteration 149 Loss: 4.10232400894165\n",
      "Iteration 150 Loss: 3.68084454536438\n",
      "Iteration 151 Loss: 3.974566698074341\n",
      "Iteration 152 Loss: 4.129560947418213\n",
      "Iteration 153 Loss: 3.8843905925750732\n",
      "Iteration 154 Loss: 4.245547294616699\n",
      "Iteration 155 Loss: 3.6446757316589355\n",
      "Iteration 156 Loss: 3.7151758670806885\n",
      "Iteration 157 Loss: 3.9875569343566895\n",
      "Iteration 158 Loss: 3.9381487369537354\n",
      "Iteration 159 Loss: 4.046612739562988\n",
      "Iteration 159 Loss: 3.9247078895568848\n",
      "Iteration 160 Loss: 3.812407970428467\n",
      "Iteration 161 Loss: 4.195675373077393\n",
      "Iteration 162 Loss: 3.7803750038146973\n",
      "Iteration 163 Loss: 3.97794246673584\n",
      "Iteration 164 Loss: 4.023246765136719\n",
      "Iteration 165 Loss: 4.586889266967773\n",
      "Iteration 166 Loss: 4.2888665199279785\n",
      "Iteration 167 Loss: 4.490063190460205\n",
      "Iteration 168 Loss: 3.8984057903289795\n",
      "Iteration 169 Loss: 4.0125861167907715\n",
      "Iteration 169 Loss: 4.1066460609436035\n",
      "Iteration 170 Loss: 4.335618495941162\n",
      "Iteration 171 Loss: 4.177180767059326\n",
      "Iteration 172 Loss: 4.150409698486328\n",
      "Iteration 173 Loss: 3.8454957008361816\n",
      "Iteration 174 Loss: 4.108266353607178\n",
      "Iteration 175 Loss: 3.7386996746063232\n",
      "Iteration 176 Loss: 4.118963718414307\n",
      "Iteration 177 Loss: 4.026066780090332\n",
      "Iteration 178 Loss: 3.739459753036499\n",
      "Iteration 179 Loss: 4.198617458343506\n",
      "Iteration 179 Loss: 4.043877601623535\n",
      "Iteration 180 Loss: 4.348129749298096\n",
      "Iteration 181 Loss: 3.9718620777130127\n",
      "Iteration 182 Loss: 4.501715660095215\n",
      "Iteration 183 Loss: 4.381076335906982\n",
      "Iteration 184 Loss: 3.433199167251587\n",
      "Iteration 185 Loss: 4.1536455154418945\n",
      "Iteration 186 Loss: 4.631584167480469\n",
      "Iteration 187 Loss: 4.315650939941406\n",
      "Iteration 188 Loss: 3.9449198246002197\n",
      "Iteration 189 Loss: 4.0275163650512695\n",
      "Iteration 189 Loss: 4.170929908752441\n",
      "Iteration 190 Loss: 4.210150241851807\n",
      "Iteration 191 Loss: 4.066862106323242\n",
      "Iteration 192 Loss: 4.0847015380859375\n",
      "Iteration 193 Loss: 4.268295764923096\n",
      "Iteration 194 Loss: 4.221473217010498\n",
      "Iteration 195 Loss: 3.983362913131714\n",
      "Iteration 196 Loss: 3.6100993156433105\n",
      "Iteration 197 Loss: 3.607346534729004\n",
      "Iteration 198 Loss: 4.217203140258789\n",
      "Iteration 199 Loss: 4.034239768981934\n",
      "Iteration 199 Loss: 4.030373573303223\n",
      "Iteration 200 Loss: 3.7937490940093994\n",
      "Iteration 201 Loss: 3.8148152828216553\n",
      "Iteration 202 Loss: 4.182528495788574\n",
      "Iteration 203 Loss: 3.971097469329834\n",
      "Iteration 204 Loss: 3.489204168319702\n",
      "Iteration 205 Loss: 3.726362705230713\n",
      "Iteration 206 Loss: 3.7457263469696045\n",
      "Iteration 207 Loss: 4.112051963806152\n",
      "Iteration 208 Loss: 3.767948627471924\n",
      "Iteration 209 Loss: 4.070364475250244\n",
      "Iteration 209 Loss: 3.867385149002075\n",
      "Iteration 210 Loss: 3.8590312004089355\n",
      "Iteration 211 Loss: 4.240463733673096\n",
      "Iteration 212 Loss: 4.093278408050537\n",
      "Iteration 213 Loss: 3.686427354812622\n",
      "Iteration 214 Loss: 3.9617719650268555\n",
      "Iteration 215 Loss: 3.8564956188201904\n",
      "Iteration 216 Loss: 3.9393703937530518\n",
      "Iteration 217 Loss: 3.549858808517456\n",
      "Iteration 218 Loss: 4.31285285949707\n",
      "Iteration 219 Loss: 3.809257745742798\n",
      "Iteration 219 Loss: 3.9308807849884033\n",
      "Iteration 220 Loss: 4.18582010269165\n",
      "Iteration 221 Loss: 4.1431379318237305\n",
      "Iteration 222 Loss: 3.6338837146759033\n",
      "Iteration 223 Loss: 3.707514762878418\n",
      "Iteration 224 Loss: 4.158641815185547\n",
      "Iteration 225 Loss: 3.9152579307556152\n",
      "Iteration 226 Loss: 3.630213737487793\n",
      "Iteration 227 Loss: 3.7945985794067383\n",
      "Iteration 228 Loss: 4.103978633880615\n",
      "Iteration 229 Loss: 3.6866815090179443\n",
      "Iteration 229 Loss: 3.895972728729248\n",
      "Iteration 230 Loss: 4.401366233825684\n",
      "Iteration 231 Loss: 3.6688010692596436\n",
      "Iteration 232 Loss: 3.4697203636169434\n",
      "Iteration 233 Loss: 3.613247871398926\n",
      "Iteration 234 Loss: 3.472079277038574\n",
      "Iteration 235 Loss: 4.11868143081665\n",
      "Iteration 236 Loss: 3.962695837020874\n",
      "Iteration 237 Loss: 3.6830894947052\n",
      "Iteration 238 Loss: 3.9542014598846436\n",
      "Iteration 239 Loss: 3.721341848373413\n",
      "Iteration 239 Loss: 3.8065223693847656\n",
      "Iteration 240 Loss: 3.9397199153900146\n",
      "Iteration 241 Loss: 4.16664457321167\n",
      "Iteration 242 Loss: 3.6855504512786865\n",
      "Iteration 243 Loss: 4.13505220413208\n",
      "Iteration 244 Loss: 4.040743350982666\n",
      "Iteration 245 Loss: 4.171875476837158\n",
      "Iteration 246 Loss: 3.3871612548828125\n",
      "Iteration 247 Loss: 4.406759738922119\n",
      "Iteration 248 Loss: 3.9226086139678955\n",
      "Iteration 249 Loss: 3.5471267700195312\n",
      "Iteration 249 Loss: 3.940324306488037\n",
      "Iteration 250 Loss: 4.064846992492676\n",
      "Iteration 251 Loss: 3.3988943099975586\n",
      "Iteration 252 Loss: 3.6856906414031982\n",
      "Iteration 253 Loss: 4.094005584716797\n",
      "Iteration 254 Loss: 3.86759090423584\n",
      "Iteration 255 Loss: 4.125482082366943\n",
      "Iteration 256 Loss: 3.8274574279785156\n",
      "Iteration 257 Loss: 4.023505687713623\n",
      "Iteration 258 Loss: 4.027664661407471\n",
      "Iteration 259 Loss: 3.9844882488250732\n",
      "Iteration 259 Loss: 3.9099624156951904\n",
      "Iteration 260 Loss: 3.613586902618408\n",
      "Iteration 261 Loss: 3.6733558177948\n",
      "Iteration 262 Loss: 3.723917007446289\n",
      "Iteration 263 Loss: 3.9012086391448975\n",
      "Iteration 264 Loss: 3.950575351715088\n",
      "Iteration 265 Loss: 3.7080867290496826\n",
      "Iteration 266 Loss: 3.5627601146698\n",
      "Iteration 267 Loss: 3.7792606353759766\n",
      "Iteration 268 Loss: 3.7971649169921875\n",
      "Iteration 269 Loss: 4.00220251083374\n",
      "Iteration 269 Loss: 3.771212100982666\n",
      "Iteration 270 Loss: 3.4052493572235107\n",
      "Iteration 271 Loss: 3.8176889419555664\n",
      "Iteration 272 Loss: 3.7220101356506348\n",
      "Iteration 273 Loss: 3.737156629562378\n",
      "Iteration 274 Loss: 3.702259063720703\n",
      "Iteration 275 Loss: 3.548511505126953\n",
      "Iteration 276 Loss: 3.9126367568969727\n",
      "Iteration 277 Loss: 3.8645780086517334\n",
      "Iteration 278 Loss: 4.545612812042236\n",
      "Iteration 279 Loss: 4.3383049964904785\n",
      "Iteration 279 Loss: 3.859400987625122\n",
      "Iteration 280 Loss: 4.292026996612549\n",
      "Iteration 281 Loss: 3.1337194442749023\n",
      "Iteration 282 Loss: 3.480672836303711\n",
      "Iteration 283 Loss: 3.75386643409729\n",
      "Iteration 284 Loss: 3.847501277923584\n",
      "Iteration 285 Loss: 3.986823797225952\n",
      "Iteration 286 Loss: 3.7995355129241943\n",
      "Iteration 287 Loss: 3.9120211601257324\n",
      "Iteration 288 Loss: 3.729051113128662\n",
      "Iteration 289 Loss: 4.150067329406738\n",
      "Iteration 289 Loss: 3.808528423309326\n",
      "Iteration 290 Loss: 3.8147339820861816\n",
      "Iteration 291 Loss: 4.017148017883301\n",
      "Iteration 292 Loss: 3.566577672958374\n",
      "Iteration 293 Loss: 3.863818645477295\n",
      "Iteration 294 Loss: 3.6359922885894775\n",
      "Iteration 295 Loss: 3.589318037033081\n",
      "Iteration 296 Loss: 3.6885979175567627\n",
      "Iteration 297 Loss: 3.849698781967163\n",
      "Iteration 298 Loss: 3.6078429222106934\n",
      "Iteration 299 Loss: 3.677152395248413\n",
      "Iteration 299 Loss: 3.7310879230499268\n",
      "Iteration 300 Loss: 3.7455379962921143\n",
      "Iteration 301 Loss: 4.276623249053955\n",
      "Iteration 302 Loss: 4.149324417114258\n",
      "Iteration 303 Loss: 3.7840635776519775\n",
      "Iteration 304 Loss: 3.4441046714782715\n",
      "Iteration 305 Loss: 3.411088705062866\n",
      "Iteration 306 Loss: 4.397915363311768\n",
      "Iteration 307 Loss: 3.504065752029419\n",
      "Iteration 308 Loss: 3.912851333618164\n",
      "Iteration 309 Loss: 3.7110085487365723\n",
      "Iteration 309 Loss: 3.8336586952209473\n",
      "Iteration 310 Loss: 3.8569178581237793\n",
      "Iteration 311 Loss: 3.5332353115081787\n",
      "Iteration 312 Loss: 3.6040947437286377\n",
      "Iteration 313 Loss: 3.873169183731079\n",
      "Iteration 314 Loss: 3.683379650115967\n",
      "Iteration 315 Loss: 3.6233582496643066\n",
      "Iteration 316 Loss: 3.899007797241211\n",
      "Iteration 317 Loss: 3.4148001670837402\n",
      "Iteration 318 Loss: 4.068540573120117\n",
      "Iteration 319 Loss: 4.0166120529174805\n",
      "Iteration 319 Loss: 3.7573115825653076\n",
      "Iteration 320 Loss: 3.129983901977539\n",
      "Iteration 321 Loss: 3.622647762298584\n",
      "Iteration 322 Loss: 4.53823184967041\n",
      "Iteration 323 Loss: 3.708601474761963\n",
      "Iteration 324 Loss: 3.8813881874084473\n",
      "Iteration 325 Loss: 3.773174285888672\n",
      "Iteration 326 Loss: 3.832974910736084\n",
      "Iteration 327 Loss: 4.290348529815674\n",
      "Iteration 328 Loss: 3.7145907878875732\n",
      "Iteration 329 Loss: 4.0012712478637695\n",
      "Iteration 329 Loss: 3.849320888519287\n",
      "Iteration 330 Loss: 3.6228342056274414\n",
      "Iteration 331 Loss: 3.8344783782958984\n",
      "Iteration 332 Loss: 4.005727291107178\n",
      "Iteration 333 Loss: 3.5846264362335205\n",
      "Iteration 334 Loss: 3.45050048828125\n",
      "Iteration 335 Loss: 3.6098616123199463\n",
      "Iteration 336 Loss: 3.4856956005096436\n",
      "Iteration 337 Loss: 3.7326741218566895\n",
      "Iteration 338 Loss: 3.3996455669403076\n",
      "Iteration 339 Loss: 3.377521514892578\n",
      "Iteration 339 Loss: 3.610356569290161\n",
      "Iteration 340 Loss: 3.630993604660034\n",
      "Iteration 341 Loss: 3.719966411590576\n",
      "Iteration 342 Loss: 4.119233131408691\n",
      "Iteration 343 Loss: 3.2679741382598877\n",
      "Iteration 344 Loss: 3.760211944580078\n",
      "Iteration 345 Loss: 3.5350728034973145\n",
      "Iteration 346 Loss: 3.850234031677246\n",
      "Iteration 347 Loss: 4.149355411529541\n",
      "Iteration 348 Loss: 3.1336750984191895\n",
      "Iteration 349 Loss: 3.5462136268615723\n",
      "Iteration 349 Loss: 3.671293258666992\n",
      "Iteration 350 Loss: 3.5137124061584473\n",
      "Iteration 351 Loss: 3.629775285720825\n",
      "Iteration 352 Loss: 3.5562264919281006\n",
      "Iteration 353 Loss: 3.6360814571380615\n",
      "Iteration 354 Loss: 3.7904624938964844\n",
      "Iteration 355 Loss: 3.8195440769195557\n",
      "Iteration 356 Loss: 3.4923324584960938\n",
      "Iteration 357 Loss: 3.8797779083251953\n",
      "Iteration 358 Loss: 3.69834566116333\n",
      "Iteration 359 Loss: 3.291152000427246\n",
      "Iteration 359 Loss: 3.6307411193847656\n",
      "Iteration 360 Loss: 3.2845094203948975\n",
      "Iteration 361 Loss: 3.6704626083374023\n",
      "Iteration 362 Loss: 3.779587507247925\n",
      "Iteration 363 Loss: 4.0633039474487305\n",
      "Iteration 364 Loss: 3.474514961242676\n",
      "Iteration 365 Loss: 3.803140163421631\n",
      "Iteration 366 Loss: 3.31630802154541\n",
      "Iteration 367 Loss: 3.6262943744659424\n",
      "Iteration 368 Loss: 3.6553328037261963\n",
      "Iteration 369 Loss: 2.990894317626953\n",
      "Iteration 369 Loss: 3.566434860229492\n",
      "Iteration 370 Loss: 3.399473190307617\n",
      "Iteration 371 Loss: 3.341618061065674\n",
      "Iteration 372 Loss: 3.9996557235717773\n",
      "Iteration 373 Loss: 3.82205867767334\n",
      "Iteration 374 Loss: 3.829176664352417\n",
      "Iteration 375 Loss: 3.6594932079315186\n",
      "Iteration 376 Loss: 3.9851152896881104\n",
      "Iteration 377 Loss: 3.6152708530426025\n",
      "Iteration 378 Loss: 3.5340704917907715\n",
      "Iteration 379 Loss: 4.393240451812744\n",
      "Iteration 379 Loss: 3.7579174041748047\n",
      "Iteration 380 Loss: 3.685476779937744\n",
      "Iteration 381 Loss: 3.647334337234497\n",
      "Iteration 382 Loss: 3.7839434146881104\n",
      "Iteration 383 Loss: 3.5829484462738037\n",
      "Iteration 384 Loss: 2.862818717956543\n",
      "Iteration 385 Loss: 3.949267864227295\n",
      "Iteration 386 Loss: 4.058546543121338\n",
      "Iteration 387 Loss: 3.236146926879883\n",
      "Iteration 388 Loss: 3.3231728076934814\n",
      "Iteration 389 Loss: 3.201047658920288\n",
      "Iteration 389 Loss: 3.5330703258514404\n",
      "Iteration 390 Loss: 3.2463324069976807\n",
      "Iteration 391 Loss: 3.4230198860168457\n",
      "Iteration 392 Loss: 3.7719640731811523\n",
      "Iteration 393 Loss: 3.8728561401367188\n",
      "Iteration 394 Loss: 3.7071588039398193\n",
      "Iteration 395 Loss: 3.3592708110809326\n",
      "Iteration 396 Loss: 3.441652297973633\n",
      "Iteration 397 Loss: 4.023475646972656\n",
      "Iteration 398 Loss: 3.520418882369995\n",
      "Iteration 399 Loss: 3.5935025215148926\n",
      "Iteration 399 Loss: 3.5959649085998535\n",
      "Iteration 400 Loss: 3.6297411918640137\n",
      "Iteration 401 Loss: 3.801612138748169\n",
      "Iteration 402 Loss: 3.4847943782806396\n",
      "Iteration 403 Loss: 3.331325054168701\n",
      "Iteration 404 Loss: 3.703784227371216\n",
      "Iteration 405 Loss: 3.650570869445801\n",
      "Iteration 406 Loss: 3.431447982788086\n",
      "Iteration 407 Loss: 3.723341464996338\n",
      "Iteration 408 Loss: 4.042929172515869\n",
      "Iteration 409 Loss: 3.7415640354156494\n",
      "Iteration 409 Loss: 3.654111385345459\n",
      "Iteration 410 Loss: 3.1964495182037354\n",
      "Iteration 411 Loss: 3.6864078044891357\n",
      "Iteration 412 Loss: 3.8367488384246826\n",
      "Iteration 413 Loss: 3.1389272212982178\n",
      "Iteration 414 Loss: 3.476179838180542\n",
      "Iteration 415 Loss: 3.62196683883667\n",
      "Iteration 416 Loss: 3.6811676025390625\n",
      "Iteration 417 Loss: 4.02886438369751\n",
      "Iteration 418 Loss: 3.480745553970337\n",
      "Iteration 419 Loss: 3.586479902267456\n",
      "Iteration 419 Loss: 3.5733935832977295\n",
      "Iteration 420 Loss: 3.298056125640869\n",
      "Iteration 421 Loss: 3.577629327774048\n",
      "Iteration 422 Loss: 3.7084290981292725\n",
      "Iteration 423 Loss: 3.5104024410247803\n",
      "Iteration 424 Loss: 3.493389368057251\n",
      "Iteration 425 Loss: 3.6183652877807617\n",
      "Iteration 426 Loss: 3.740902900695801\n",
      "Iteration 427 Loss: 3.4526472091674805\n",
      "Iteration 428 Loss: 3.645782947540283\n",
      "Iteration 429 Loss: 3.724039316177368\n",
      "Iteration 429 Loss: 3.5769646167755127\n",
      "Iteration 430 Loss: 3.5900280475616455\n",
      "Iteration 431 Loss: 3.855526924133301\n",
      "Iteration 432 Loss: 3.627986431121826\n",
      "Iteration 433 Loss: 3.8716182708740234\n",
      "Iteration 434 Loss: 3.304725408554077\n",
      "Iteration 435 Loss: 3.573180675506592\n",
      "Iteration 436 Loss: 3.4674718379974365\n",
      "Iteration 437 Loss: 3.090670347213745\n",
      "Iteration 438 Loss: 3.596104383468628\n",
      "Iteration 439 Loss: 3.2957427501678467\n",
      "Iteration 439 Loss: 3.5273056030273438\n",
      "Iteration 440 Loss: 3.220430374145508\n",
      "Iteration 441 Loss: 3.5176262855529785\n",
      "Iteration 442 Loss: 3.2710776329040527\n",
      "Iteration 443 Loss: 3.5257484912872314\n",
      "Iteration 444 Loss: 3.6118249893188477\n",
      "Iteration 445 Loss: 3.1869773864746094\n",
      "Iteration 446 Loss: 3.142086982727051\n",
      "Iteration 447 Loss: 3.268956184387207\n",
      "Iteration 448 Loss: 3.6669251918792725\n",
      "Iteration 449 Loss: 3.4298348426818848\n",
      "Iteration 449 Loss: 3.3841488361358643\n",
      "Iteration 450 Loss: 3.7430026531219482\n",
      "Iteration 451 Loss: 3.6485767364501953\n",
      "Iteration 452 Loss: 3.383977174758911\n",
      "Iteration 453 Loss: 3.582646131515503\n",
      "Iteration 454 Loss: 3.337824821472168\n",
      "Iteration 455 Loss: 3.2305774688720703\n",
      "Iteration 456 Loss: 3.415010452270508\n",
      "Iteration 457 Loss: 3.5447282791137695\n",
      "Iteration 458 Loss: 3.5030429363250732\n",
      "Iteration 459 Loss: 3.224848985671997\n",
      "Iteration 459 Loss: 3.461423397064209\n",
      "Iteration 460 Loss: 3.561150550842285\n",
      "Iteration 461 Loss: 3.4415955543518066\n",
      "Iteration 462 Loss: 3.6832239627838135\n",
      "Iteration 463 Loss: 3.7410593032836914\n",
      "Iteration 464 Loss: 3.712735176086426\n",
      "Iteration 465 Loss: 3.589426040649414\n",
      "Iteration 466 Loss: 3.818636417388916\n",
      "Iteration 467 Loss: 3.7606213092803955\n",
      "Iteration 468 Loss: 3.2028026580810547\n",
      "Iteration 469 Loss: 3.8297770023345947\n",
      "Iteration 469 Loss: 3.6341025829315186\n",
      "Iteration 470 Loss: 3.125293493270874\n",
      "Iteration 471 Loss: 3.111403703689575\n",
      "Iteration 472 Loss: 3.6014599800109863\n",
      "Iteration 473 Loss: 3.417210578918457\n",
      "Iteration 474 Loss: 3.169193744659424\n",
      "Iteration 475 Loss: 3.784576892852783\n",
      "Iteration 476 Loss: 3.546987533569336\n",
      "Iteration 477 Loss: 3.8099567890167236\n",
      "Iteration 478 Loss: 3.6392931938171387\n",
      "Iteration 479 Loss: 3.708347797393799\n",
      "Iteration 479 Loss: 3.4913723468780518\n",
      "Iteration 480 Loss: 3.6788294315338135\n",
      "Iteration 481 Loss: 3.297410726547241\n",
      "Iteration 482 Loss: 3.8881754875183105\n",
      "Iteration 483 Loss: 2.7902870178222656\n",
      "Iteration 484 Loss: 3.2158823013305664\n",
      "Iteration 485 Loss: 3.3832578659057617\n",
      "Iteration 486 Loss: 3.839905261993408\n",
      "Iteration 487 Loss: 3.2704577445983887\n",
      "Iteration 488 Loss: 3.823607921600342\n",
      "Iteration 489 Loss: 3.7710556983947754\n",
      "Iteration 489 Loss: 3.495887041091919\n",
      "Iteration 490 Loss: 3.286250352859497\n",
      "Iteration 491 Loss: 3.619046688079834\n",
      "Iteration 492 Loss: 3.6108365058898926\n",
      "Iteration 493 Loss: 3.2853903770446777\n",
      "Iteration 494 Loss: 3.4497134685516357\n",
      "Iteration 495 Loss: 3.7302513122558594\n",
      "Iteration 496 Loss: 4.092878818511963\n",
      "Iteration 497 Loss: 3.641148567199707\n",
      "Iteration 498 Loss: 3.2290380001068115\n",
      "Iteration 499 Loss: 3.3495845794677734\n",
      "Iteration 499 Loss: 3.5294137001037598\n",
      "Iteration 500 Loss: 3.4293055534362793\n",
      "Iteration 501 Loss: 3.3888566493988037\n",
      "Iteration 502 Loss: 3.5301666259765625\n",
      "Iteration 503 Loss: 3.481114149093628\n",
      "Iteration 504 Loss: 3.0029664039611816\n",
      "Iteration 505 Loss: 3.8562190532684326\n",
      "Iteration 506 Loss: 3.3904099464416504\n",
      "Iteration 507 Loss: 3.821495294570923\n",
      "Iteration 508 Loss: 3.211405038833618\n",
      "Iteration 509 Loss: 3.651135206222534\n",
      "Iteration 509 Loss: 3.4763073921203613\n",
      "Iteration 510 Loss: 3.06333589553833\n",
      "Iteration 511 Loss: 3.13171124458313\n",
      "Iteration 512 Loss: 3.8021092414855957\n",
      "Iteration 513 Loss: 3.5359513759613037\n",
      "Iteration 514 Loss: 3.435842514038086\n",
      "Iteration 515 Loss: 3.602182626724243\n",
      "Iteration 516 Loss: 3.939938545227051\n",
      "Iteration 517 Loss: 3.57017183303833\n",
      "Iteration 518 Loss: 3.520822763442993\n",
      "Iteration 519 Loss: 3.406787395477295\n",
      "Iteration 519 Loss: 3.500885486602783\n",
      "Iteration 520 Loss: 3.601358413696289\n",
      "Iteration 521 Loss: 2.6265788078308105\n",
      "Iteration 522 Loss: 3.061856746673584\n",
      "Iteration 523 Loss: 3.3545472621917725\n",
      "Iteration 524 Loss: 3.3086421489715576\n",
      "Iteration 525 Loss: 3.5821309089660645\n",
      "Iteration 526 Loss: 3.2299773693084717\n",
      "Iteration 527 Loss: 3.2745115756988525\n",
      "Iteration 528 Loss: 3.2034449577331543\n",
      "Iteration 529 Loss: 3.418370008468628\n",
      "Iteration 529 Loss: 3.266141891479492\n",
      "Iteration 530 Loss: 3.2302143573760986\n",
      "Iteration 531 Loss: 3.3831212520599365\n",
      "Iteration 532 Loss: 3.0859017372131348\n",
      "Iteration 533 Loss: 3.2883410453796387\n",
      "Iteration 534 Loss: 3.105478525161743\n",
      "Iteration 535 Loss: 3.4146108627319336\n",
      "Iteration 536 Loss: 3.7860865592956543\n",
      "Iteration 537 Loss: 3.426403522491455\n",
      "Iteration 538 Loss: 3.261141538619995\n",
      "Iteration 539 Loss: 3.297119379043579\n",
      "Iteration 539 Loss: 3.3278419971466064\n",
      "Iteration 540 Loss: 2.8486075401306152\n",
      "Iteration 541 Loss: 2.847332239151001\n",
      "Iteration 542 Loss: 3.4204139709472656\n",
      "Iteration 543 Loss: 3.1069557666778564\n",
      "Iteration 544 Loss: 3.7633442878723145\n",
      "Iteration 545 Loss: 3.5831432342529297\n",
      "Iteration 546 Loss: 2.96459698677063\n",
      "Iteration 547 Loss: 2.9722537994384766\n",
      "Iteration 548 Loss: 2.9491946697235107\n",
      "Iteration 549 Loss: 3.8159546852111816\n",
      "Iteration 549 Loss: 3.227179765701294\n",
      "Iteration 550 Loss: 3.581639289855957\n",
      "Iteration 551 Loss: 3.5680699348449707\n",
      "Iteration 552 Loss: 3.2732510566711426\n",
      "Iteration 553 Loss: 3.0824012756347656\n",
      "Iteration 554 Loss: 3.415156364440918\n",
      "Iteration 555 Loss: 3.1684389114379883\n",
      "Iteration 556 Loss: 3.3943209648132324\n",
      "Iteration 557 Loss: 2.9996824264526367\n",
      "Iteration 558 Loss: 3.188974142074585\n",
      "Iteration 559 Loss: 3.3982937335968018\n",
      "Iteration 559 Loss: 3.3070228099823\n",
      "Iteration 560 Loss: 3.17439341545105\n",
      "Iteration 561 Loss: 3.288757801055908\n",
      "Iteration 562 Loss: 3.0415468215942383\n",
      "Iteration 563 Loss: 3.271843433380127\n",
      "Iteration 564 Loss: 3.264389753341675\n",
      "Iteration 565 Loss: 3.5134899616241455\n",
      "Iteration 566 Loss: 3.496285915374756\n",
      "Iteration 567 Loss: 2.954164743423462\n",
      "Iteration 568 Loss: 3.1645405292510986\n",
      "Iteration 569 Loss: 3.5727126598358154\n",
      "Iteration 569 Loss: 3.274212598800659\n",
      "Iteration 570 Loss: 3.3702893257141113\n",
      "Iteration 571 Loss: 3.397651433944702\n",
      "Iteration 572 Loss: 2.8000991344451904\n",
      "Iteration 573 Loss: 3.2349677085876465\n",
      "Iteration 574 Loss: 3.167520046234131\n",
      "Iteration 575 Loss: 3.461092472076416\n",
      "Iteration 576 Loss: 3.472137451171875\n",
      "Iteration 577 Loss: 3.623180627822876\n",
      "Iteration 578 Loss: 2.8907346725463867\n",
      "Iteration 579 Loss: 2.8890011310577393\n",
      "Iteration 579 Loss: 3.2306675910949707\n",
      "Iteration 580 Loss: 2.808701515197754\n",
      "Iteration 581 Loss: 3.2343993186950684\n",
      "Iteration 582 Loss: 3.1285135746002197\n",
      "Iteration 583 Loss: 3.4075045585632324\n",
      "Iteration 584 Loss: 3.4666473865509033\n",
      "Iteration 585 Loss: 3.438810110092163\n",
      "Iteration 586 Loss: 3.1996798515319824\n",
      "Iteration 587 Loss: 3.061016082763672\n",
      "Iteration 588 Loss: 3.3558661937713623\n",
      "Iteration 589 Loss: 3.766901969909668\n",
      "Iteration 589 Loss: 3.28680419921875\n",
      "Iteration 590 Loss: 3.318103075027466\n",
      "Iteration 591 Loss: 3.135450601577759\n",
      "Iteration 592 Loss: 2.9890124797821045\n",
      "Iteration 593 Loss: 3.301090955734253\n",
      "Iteration 594 Loss: 3.005286931991577\n",
      "Iteration 595 Loss: 3.605579137802124\n",
      "Iteration 596 Loss: 3.6312437057495117\n",
      "Iteration 597 Loss: 3.1620655059814453\n",
      "Iteration 598 Loss: 3.1823363304138184\n",
      "Iteration 599 Loss: 3.363790273666382\n",
      "Iteration 599 Loss: 3.2693963050842285\n",
      "Iteration 600 Loss: 3.527946710586548\n",
      "Iteration 601 Loss: 3.021526575088501\n",
      "Iteration 602 Loss: 2.9950051307678223\n",
      "Iteration 603 Loss: 2.844207525253296\n",
      "Iteration 604 Loss: 3.689340829849243\n",
      "Iteration 605 Loss: 3.371636390686035\n",
      "Iteration 606 Loss: 3.072185754776001\n",
      "Iteration 607 Loss: 3.2120754718780518\n",
      "Iteration 608 Loss: 3.394728422164917\n",
      "Iteration 609 Loss: 3.180931568145752\n",
      "Iteration 609 Loss: 3.2309582233428955\n",
      "Iteration 610 Loss: 2.687476396560669\n",
      "Iteration 611 Loss: 3.69356369972229\n",
      "Iteration 612 Loss: 3.3665733337402344\n",
      "Iteration 613 Loss: 3.299168109893799\n",
      "Iteration 614 Loss: 3.405712366104126\n",
      "Iteration 615 Loss: 3.7943508625030518\n",
      "Iteration 616 Loss: 2.8972489833831787\n",
      "Iteration 617 Loss: 2.9430012702941895\n",
      "Iteration 618 Loss: 3.4790046215057373\n",
      "Iteration 619 Loss: 3.493173360824585\n",
      "Iteration 619 Loss: 3.305927276611328\n",
      "Iteration 620 Loss: 3.0978147983551025\n",
      "Iteration 621 Loss: 3.321272611618042\n",
      "Iteration 622 Loss: 3.2969741821289062\n",
      "Iteration 623 Loss: 2.7961859703063965\n",
      "Iteration 624 Loss: 3.146907091140747\n",
      "Iteration 625 Loss: 3.6930580139160156\n",
      "Iteration 626 Loss: 3.5369420051574707\n",
      "Iteration 627 Loss: 3.141537666320801\n",
      "Iteration 628 Loss: 3.499743938446045\n",
      "Iteration 629 Loss: 2.628477096557617\n",
      "Iteration 629 Loss: 3.2158913612365723\n",
      "Iteration 630 Loss: 2.675807237625122\n",
      "Iteration 631 Loss: 3.3452138900756836\n",
      "Iteration 632 Loss: 3.109295606613159\n",
      "Iteration 633 Loss: 3.5982511043548584\n",
      "Iteration 634 Loss: 3.404600143432617\n",
      "Iteration 635 Loss: 3.509226083755493\n",
      "Iteration 636 Loss: 3.245300769805908\n",
      "Iteration 637 Loss: 3.4517946243286133\n",
      "Iteration 638 Loss: 2.955519676208496\n",
      "Iteration 639 Loss: 3.397979497909546\n",
      "Iteration 639 Loss: 3.269299030303955\n",
      "Iteration 640 Loss: 3.201444149017334\n",
      "Iteration 641 Loss: 3.4609124660491943\n",
      "Iteration 642 Loss: 2.961048126220703\n",
      "Iteration 643 Loss: 3.039855718612671\n",
      "Iteration 644 Loss: 3.329397439956665\n",
      "Iteration 645 Loss: 3.0768558979034424\n",
      "Iteration 646 Loss: 3.5175461769104004\n",
      "Iteration 647 Loss: 3.407336473464966\n",
      "Iteration 648 Loss: 3.1175668239593506\n",
      "Iteration 649 Loss: 3.102208137512207\n",
      "Iteration 649 Loss: 3.221417188644409\n",
      "Iteration 650 Loss: 2.9005415439605713\n",
      "Iteration 651 Loss: 2.9803898334503174\n",
      "Iteration 652 Loss: 3.22365665435791\n",
      "Iteration 653 Loss: 2.9371347427368164\n",
      "Iteration 654 Loss: 2.8128597736358643\n",
      "Iteration 655 Loss: 3.1684324741363525\n",
      "Iteration 656 Loss: 2.9389426708221436\n",
      "Iteration 657 Loss: 2.7284858226776123\n",
      "Iteration 658 Loss: 3.2990148067474365\n",
      "Iteration 659 Loss: 3.018980026245117\n",
      "Iteration 659 Loss: 3.0008437633514404\n",
      "Iteration 660 Loss: 3.5502495765686035\n",
      "Iteration 661 Loss: 3.1048598289489746\n",
      "Iteration 662 Loss: 3.1016712188720703\n",
      "Iteration 663 Loss: 3.3940868377685547\n",
      "Iteration 664 Loss: 3.5091395378112793\n",
      "Iteration 665 Loss: 3.0633652210235596\n",
      "Iteration 666 Loss: 3.5132765769958496\n",
      "Iteration 667 Loss: 3.0592877864837646\n",
      "Iteration 668 Loss: 2.967595100402832\n",
      "Iteration 669 Loss: 3.039010763168335\n",
      "Iteration 669 Loss: 3.2302544116973877\n",
      "Iteration 670 Loss: 3.1459975242614746\n",
      "Iteration 671 Loss: 2.7515037059783936\n",
      "Iteration 672 Loss: 3.4318885803222656\n",
      "Iteration 673 Loss: 3.099375009536743\n",
      "Iteration 674 Loss: 2.952316999435425\n",
      "Iteration 675 Loss: 3.1011099815368652\n",
      "Iteration 676 Loss: 3.121300220489502\n",
      "Iteration 677 Loss: 3.4372358322143555\n",
      "Iteration 678 Loss: 3.1012485027313232\n",
      "Iteration 679 Loss: 2.7658517360687256\n",
      "Iteration 679 Loss: 3.090782880783081\n",
      "Iteration 680 Loss: 3.108323097229004\n",
      "Iteration 681 Loss: 3.792532205581665\n",
      "Iteration 682 Loss: 2.5304620265960693\n",
      "Iteration 683 Loss: 2.5308899879455566\n",
      "Iteration 684 Loss: 3.4896981716156006\n",
      "Iteration 685 Loss: 3.09537672996521\n",
      "Iteration 686 Loss: 3.11863374710083\n",
      "Iteration 687 Loss: 2.8607757091522217\n",
      "Iteration 688 Loss: 3.558725118637085\n",
      "Iteration 689 Loss: 3.2553763389587402\n",
      "Iteration 689 Loss: 3.1340792179107666\n",
      "Iteration 690 Loss: 3.3223650455474854\n",
      "Iteration 691 Loss: 2.6555910110473633\n",
      "Iteration 692 Loss: 3.3015828132629395\n",
      "Iteration 693 Loss: 2.7936460971832275\n",
      "Iteration 694 Loss: 3.259739398956299\n",
      "Iteration 695 Loss: 2.694828987121582\n",
      "Iteration 696 Loss: 3.049013376235962\n",
      "Iteration 697 Loss: 2.7935259342193604\n",
      "Iteration 698 Loss: 3.6321816444396973\n",
      "Iteration 699 Loss: 2.8803224563598633\n",
      "Iteration 699 Loss: 3.0382795333862305\n",
      "Iteration 700 Loss: 2.948129653930664\n",
      "Iteration 701 Loss: 3.2030510902404785\n",
      "Iteration 702 Loss: 2.4409592151641846\n",
      "Iteration 703 Loss: 3.0225398540496826\n",
      "Iteration 704 Loss: 2.9504785537719727\n",
      "Iteration 705 Loss: 3.1422219276428223\n",
      "Iteration 706 Loss: 3.032121419906616\n",
      "Iteration 707 Loss: 3.0345442295074463\n",
      "Iteration 708 Loss: 3.126215934753418\n",
      "Iteration 709 Loss: 3.2763187885284424\n",
      "Iteration 709 Loss: 3.017657995223999\n",
      "Iteration 710 Loss: 3.320026397705078\n",
      "Iteration 711 Loss: 3.4530978202819824\n",
      "Iteration 712 Loss: 2.8190932273864746\n",
      "Iteration 713 Loss: 2.8442535400390625\n",
      "Iteration 714 Loss: 2.96048903465271\n",
      "Iteration 715 Loss: 2.5078771114349365\n",
      "Iteration 716 Loss: 3.0372815132141113\n",
      "Iteration 717 Loss: 2.972837448120117\n",
      "Iteration 718 Loss: 2.837921380996704\n",
      "Iteration 719 Loss: 2.668793201446533\n",
      "Iteration 719 Loss: 2.942167043685913\n",
      "Iteration 720 Loss: 3.191594123840332\n",
      "Iteration 721 Loss: 3.2027907371520996\n",
      "Iteration 722 Loss: 2.539182424545288\n",
      "Iteration 723 Loss: 2.956645965576172\n",
      "Iteration 724 Loss: 2.554882287979126\n",
      "Iteration 725 Loss: 3.235802173614502\n",
      "Iteration 726 Loss: 2.9738168716430664\n",
      "Iteration 727 Loss: 2.914705276489258\n",
      "Iteration 728 Loss: 3.3302206993103027\n",
      "Iteration 729 Loss: 3.0002737045288086\n",
      "Iteration 729 Loss: 2.9899916648864746\n",
      "Iteration 730 Loss: 3.2566981315612793\n",
      "Iteration 731 Loss: 2.746533155441284\n",
      "Iteration 732 Loss: 3.264983654022217\n",
      "Iteration 733 Loss: 2.4559905529022217\n",
      "Iteration 734 Loss: 2.5001816749572754\n",
      "Iteration 735 Loss: 2.9636034965515137\n",
      "Iteration 736 Loss: 2.997901439666748\n",
      "Iteration 737 Loss: 3.0111618041992188\n",
      "Iteration 738 Loss: 3.579693555831909\n",
      "Iteration 739 Loss: 2.997246742248535\n",
      "Iteration 739 Loss: 2.9773993492126465\n",
      "Iteration 740 Loss: 2.978679895401001\n",
      "Iteration 741 Loss: 2.687368392944336\n",
      "Iteration 742 Loss: 3.022374391555786\n",
      "Iteration 743 Loss: 2.6801819801330566\n",
      "Iteration 744 Loss: 3.3428072929382324\n",
      "Iteration 745 Loss: 2.7642579078674316\n",
      "Iteration 746 Loss: 3.3569161891937256\n",
      "Iteration 747 Loss: 3.077747106552124\n",
      "Iteration 748 Loss: 3.1160614490509033\n",
      "Iteration 749 Loss: 2.571681261062622\n",
      "Iteration 749 Loss: 2.9598076343536377\n",
      "Iteration 750 Loss: 3.2826120853424072\n",
      "Iteration 751 Loss: 3.2864248752593994\n",
      "Iteration 752 Loss: 2.816237688064575\n",
      "Iteration 753 Loss: 2.8479104042053223\n",
      "Iteration 754 Loss: 2.7365896701812744\n",
      "Iteration 755 Loss: 2.815640926361084\n",
      "Iteration 756 Loss: 2.86840558052063\n",
      "Iteration 757 Loss: 2.633526086807251\n",
      "Iteration 758 Loss: 2.8494458198547363\n",
      "Iteration 759 Loss: 3.139636754989624\n",
      "Iteration 759 Loss: 2.927643060684204\n",
      "Iteration 760 Loss: 3.2589335441589355\n",
      "Iteration 761 Loss: 2.9466822147369385\n",
      "Iteration 762 Loss: 3.279097080230713\n",
      "Iteration 763 Loss: 2.684087038040161\n",
      "Iteration 764 Loss: 3.4178085327148438\n",
      "Iteration 765 Loss: 2.849038600921631\n",
      "Iteration 766 Loss: 2.5080950260162354\n",
      "Iteration 767 Loss: 3.1959424018859863\n",
      "Iteration 768 Loss: 3.1007134914398193\n",
      "Iteration 769 Loss: 3.227647542953491\n",
      "Iteration 769 Loss: 3.046804666519165\n",
      "Iteration 770 Loss: 2.9757635593414307\n",
      "Iteration 771 Loss: 2.895555257797241\n",
      "Iteration 772 Loss: 2.9471569061279297\n",
      "Iteration 773 Loss: 3.3682944774627686\n",
      "Iteration 774 Loss: 3.1047234535217285\n",
      "Iteration 775 Loss: 2.901409864425659\n",
      "Iteration 776 Loss: 2.7905218601226807\n",
      "Iteration 777 Loss: 2.3867616653442383\n",
      "Iteration 778 Loss: 2.7042877674102783\n",
      "Iteration 779 Loss: 3.1057844161987305\n",
      "Iteration 779 Loss: 2.918025493621826\n",
      "Iteration 780 Loss: 2.7078046798706055\n",
      "Iteration 781 Loss: 2.664341926574707\n",
      "Iteration 782 Loss: 2.614206075668335\n",
      "Iteration 783 Loss: 2.540182590484619\n",
      "Iteration 784 Loss: 2.9564099311828613\n",
      "Iteration 785 Loss: 2.9137380123138428\n",
      "Iteration 786 Loss: 2.9759037494659424\n",
      "Iteration 787 Loss: 2.909501314163208\n",
      "Iteration 788 Loss: 2.96382212638855\n",
      "Iteration 789 Loss: 3.1044046878814697\n",
      "Iteration 789 Loss: 2.835031747817993\n",
      "Iteration 790 Loss: 2.9947519302368164\n",
      "Iteration 791 Loss: 3.0038576126098633\n",
      "Iteration 792 Loss: 2.434931755065918\n",
      "Iteration 793 Loss: 2.494173288345337\n",
      "Iteration 794 Loss: 2.3554975986480713\n",
      "Iteration 795 Loss: 2.9492783546447754\n",
      "Iteration 796 Loss: 2.4546287059783936\n",
      "Iteration 797 Loss: 2.967137336730957\n",
      "Iteration 798 Loss: 3.1823713779449463\n",
      "Iteration 799 Loss: 3.0259180068969727\n",
      "Iteration 799 Loss: 2.786254405975342\n",
      "Iteration 800 Loss: 2.379150867462158\n",
      "Iteration 801 Loss: 2.8617308139801025\n",
      "Iteration 802 Loss: 3.045288324356079\n",
      "Iteration 803 Loss: 2.6411070823669434\n",
      "Iteration 804 Loss: 2.7689929008483887\n",
      "Iteration 805 Loss: 2.4473764896392822\n",
      "Iteration 806 Loss: 3.755995750427246\n",
      "Iteration 807 Loss: 2.5289480686187744\n",
      "Iteration 808 Loss: 2.590379476547241\n",
      "Iteration 809 Loss: 3.3435323238372803\n",
      "Iteration 809 Loss: 2.836250066757202\n",
      "Iteration 810 Loss: 2.8604156970977783\n",
      "Iteration 811 Loss: 2.9582157135009766\n",
      "Iteration 812 Loss: 2.462547540664673\n",
      "Iteration 813 Loss: 2.923532724380493\n",
      "Iteration 814 Loss: 3.263690948486328\n",
      "Iteration 815 Loss: 2.824147939682007\n",
      "Iteration 816 Loss: 2.7290573120117188\n",
      "Iteration 817 Loss: 2.5688440799713135\n",
      "Iteration 818 Loss: 3.123250961303711\n",
      "Iteration 819 Loss: 2.566967487335205\n",
      "Iteration 819 Loss: 2.8280670642852783\n",
      "Iteration 820 Loss: 2.8825957775115967\n",
      "Iteration 821 Loss: 2.6212363243103027\n",
      "Iteration 822 Loss: 3.012838840484619\n",
      "Iteration 823 Loss: 2.686527967453003\n",
      "Iteration 824 Loss: 2.460226058959961\n",
      "Iteration 825 Loss: 2.7956368923187256\n",
      "Iteration 826 Loss: 2.6936442852020264\n",
      "Iteration 827 Loss: 3.040738105773926\n",
      "Iteration 828 Loss: 2.5562798976898193\n",
      "Iteration 829 Loss: 2.756586790084839\n",
      "Iteration 829 Loss: 2.7506308555603027\n",
      "Iteration 830 Loss: 2.9477367401123047\n",
      "Iteration 831 Loss: 2.890181064605713\n",
      "Iteration 832 Loss: 2.701239824295044\n",
      "Iteration 833 Loss: 2.3320326805114746\n",
      "Iteration 834 Loss: 2.8836681842803955\n",
      "Iteration 835 Loss: 2.754066228866577\n",
      "Iteration 836 Loss: 3.1626312732696533\n",
      "Iteration 837 Loss: 2.4277408123016357\n",
      "Iteration 838 Loss: 3.247725009918213\n",
      "Iteration 839 Loss: 2.1609902381896973\n",
      "Iteration 839 Loss: 2.7508013248443604\n",
      "Iteration 840 Loss: 2.3611185550689697\n",
      "Iteration 841 Loss: 2.700014114379883\n",
      "Iteration 842 Loss: 2.5189943313598633\n",
      "Iteration 843 Loss: 2.925149917602539\n",
      "Iteration 844 Loss: 2.598585844039917\n",
      "Iteration 845 Loss: 2.872257709503174\n",
      "Iteration 846 Loss: 3.100123643875122\n",
      "Iteration 847 Loss: 2.7710180282592773\n",
      "Iteration 848 Loss: 3.100262403488159\n",
      "Iteration 849 Loss: 2.9554200172424316\n",
      "Iteration 849 Loss: 2.7902941703796387\n",
      "Iteration 850 Loss: 3.282113790512085\n",
      "Iteration 851 Loss: 2.544832229614258\n",
      "Iteration 852 Loss: 2.2984836101531982\n",
      "Iteration 853 Loss: 2.1271798610687256\n",
      "Iteration 854 Loss: 2.6827268600463867\n",
      "Iteration 855 Loss: 2.7129971981048584\n",
      "Iteration 856 Loss: 2.6099843978881836\n",
      "Iteration 857 Loss: 3.123500347137451\n",
      "Iteration 858 Loss: 2.8698301315307617\n",
      "Iteration 859 Loss: 2.789249897003174\n",
      "Iteration 859 Loss: 2.704090118408203\n",
      "Iteration 860 Loss: 2.3846065998077393\n",
      "Iteration 861 Loss: 2.7117416858673096\n",
      "Iteration 862 Loss: 2.9623353481292725\n",
      "Iteration 863 Loss: 2.6737585067749023\n",
      "Iteration 864 Loss: 3.0430121421813965\n",
      "Iteration 865 Loss: 2.8243753910064697\n",
      "Iteration 866 Loss: 2.5819358825683594\n",
      "Iteration 867 Loss: 2.6682701110839844\n",
      "Iteration 868 Loss: 2.4380993843078613\n",
      "Iteration 869 Loss: 2.8132553100585938\n",
      "Iteration 869 Loss: 2.710139036178589\n",
      "Iteration 870 Loss: 2.801210641860962\n",
      "Iteration 871 Loss: 2.566311836242676\n",
      "Iteration 872 Loss: 2.7957561016082764\n",
      "Iteration 873 Loss: 3.125462293624878\n",
      "Iteration 874 Loss: 2.9932656288146973\n",
      "Iteration 875 Loss: 2.522155523300171\n",
      "Iteration 876 Loss: 2.842486619949341\n",
      "Iteration 877 Loss: 2.3135335445404053\n",
      "Iteration 878 Loss: 2.3482885360717773\n",
      "Iteration 879 Loss: 2.8595681190490723\n",
      "Iteration 879 Loss: 2.716804027557373\n",
      "Iteration 880 Loss: 2.7172036170959473\n",
      "Iteration 881 Loss: 2.7267022132873535\n",
      "Iteration 882 Loss: 2.5326406955718994\n",
      "Iteration 883 Loss: 2.9940009117126465\n",
      "Iteration 884 Loss: 2.407771348953247\n",
      "Iteration 885 Loss: 2.9802281856536865\n",
      "Iteration 886 Loss: 2.922682285308838\n",
      "Iteration 887 Loss: 2.769078016281128\n",
      "Iteration 888 Loss: 3.0883443355560303\n",
      "Iteration 889 Loss: 2.481069803237915\n",
      "Iteration 889 Loss: 2.761971950531006\n",
      "Iteration 890 Loss: 2.8940439224243164\n",
      "Iteration 891 Loss: 3.2950096130371094\n",
      "Iteration 892 Loss: 2.519512176513672\n",
      "Iteration 893 Loss: 2.6896753311157227\n",
      "Iteration 894 Loss: 2.681685447692871\n",
      "Iteration 895 Loss: 2.3147130012512207\n",
      "Iteration 896 Loss: 3.0594496726989746\n",
      "Iteration 897 Loss: 2.5161244869232178\n",
      "Iteration 898 Loss: 2.58078670501709\n",
      "Iteration 899 Loss: 3.1999104022979736\n",
      "Iteration 899 Loss: 2.7750909328460693\n",
      "Iteration 900 Loss: 2.4315438270568848\n",
      "Iteration 901 Loss: 2.7900278568267822\n",
      "Iteration 902 Loss: 3.0294103622436523\n",
      "Iteration 903 Loss: 2.9231836795806885\n",
      "Iteration 904 Loss: 2.8257181644439697\n",
      "Iteration 905 Loss: 3.021719455718994\n",
      "Iteration 906 Loss: 2.7027127742767334\n",
      "Iteration 907 Loss: 3.167764663696289\n",
      "Iteration 908 Loss: 2.8245596885681152\n",
      "Iteration 909 Loss: 2.4935238361358643\n",
      "Iteration 909 Loss: 2.821016550064087\n",
      "Iteration 910 Loss: 2.7909016609191895\n",
      "Iteration 911 Loss: 2.477128744125366\n",
      "Iteration 912 Loss: 2.9669899940490723\n",
      "Iteration 913 Loss: 2.309325695037842\n",
      "Iteration 914 Loss: 2.860719680786133\n",
      "Iteration 915 Loss: 3.2375521659851074\n",
      "Iteration 916 Loss: 2.450773239135742\n",
      "Iteration 917 Loss: 2.6115617752075195\n",
      "Iteration 918 Loss: 2.5607106685638428\n",
      "Iteration 919 Loss: 2.6734564304351807\n",
      "Iteration 919 Loss: 2.6939120292663574\n",
      "Iteration 920 Loss: 2.5264275074005127\n",
      "Iteration 921 Loss: 2.6646108627319336\n",
      "Iteration 922 Loss: 2.6765904426574707\n",
      "Iteration 923 Loss: 2.263205051422119\n",
      "Iteration 924 Loss: 2.6686627864837646\n",
      "Iteration 925 Loss: 2.649447202682495\n",
      "Iteration 926 Loss: 2.886338949203491\n",
      "Iteration 927 Loss: 3.0432746410369873\n",
      "Iteration 928 Loss: 2.856552839279175\n",
      "Iteration 929 Loss: 2.9428091049194336\n",
      "Iteration 929 Loss: 2.717791795730591\n",
      "Iteration 930 Loss: 2.6479616165161133\n",
      "Iteration 931 Loss: 2.501117467880249\n",
      "Iteration 932 Loss: 3.004939079284668\n",
      "Iteration 933 Loss: 2.6431000232696533\n",
      "Iteration 934 Loss: 2.0670382976531982\n",
      "Iteration 935 Loss: 2.426835536956787\n",
      "Iteration 936 Loss: 2.493500232696533\n",
      "Iteration 937 Loss: 2.9591801166534424\n",
      "Iteration 938 Loss: 2.4406979084014893\n",
      "Iteration 939 Loss: 2.456528425216675\n",
      "Iteration 939 Loss: 2.5640900135040283\n",
      "Iteration 940 Loss: 2.4531033039093018\n",
      "Iteration 941 Loss: 2.1356804370880127\n",
      "Iteration 942 Loss: 1.9864115715026855\n",
      "Iteration 943 Loss: 2.2836365699768066\n",
      "Iteration 944 Loss: 2.7365236282348633\n",
      "Iteration 945 Loss: 2.979740858078003\n",
      "Iteration 946 Loss: 2.8305423259735107\n",
      "Iteration 947 Loss: 3.135798215866089\n",
      "Iteration 948 Loss: 2.8605000972747803\n",
      "Iteration 949 Loss: 2.5177812576293945\n",
      "Iteration 949 Loss: 2.5919716358184814\n",
      "Iteration 950 Loss: 2.527090311050415\n",
      "Iteration 951 Loss: 2.5830647945404053\n",
      "Iteration 952 Loss: 2.7884938716888428\n",
      "Iteration 953 Loss: 2.6996190547943115\n",
      "Iteration 954 Loss: 2.464005470275879\n",
      "Iteration 955 Loss: 2.9533841609954834\n",
      "Iteration 956 Loss: 2.5634779930114746\n",
      "Iteration 957 Loss: 2.909552812576294\n",
      "Iteration 958 Loss: 2.46122145652771\n",
      "Iteration 959 Loss: 2.257666826248169\n",
      "Iteration 959 Loss: 2.620757818222046\n",
      "Iteration 960 Loss: 2.3782527446746826\n",
      "Iteration 961 Loss: 2.7929317951202393\n",
      "Iteration 962 Loss: 2.5373754501342773\n",
      "Iteration 963 Loss: 2.1931679248809814\n",
      "Iteration 964 Loss: 2.8541195392608643\n",
      "Iteration 965 Loss: 2.4719791412353516\n",
      "Iteration 966 Loss: 2.4326868057250977\n",
      "Iteration 967 Loss: 3.057570219039917\n",
      "Iteration 968 Loss: 2.7366724014282227\n",
      "Iteration 969 Loss: 2.7652721405029297\n",
      "Iteration 969 Loss: 2.622002601623535\n",
      "Iteration 970 Loss: 1.8569116592407227\n",
      "Iteration 971 Loss: 2.5587007999420166\n",
      "Iteration 972 Loss: 2.3480496406555176\n",
      "Iteration 973 Loss: 2.5979084968566895\n",
      "Iteration 974 Loss: 2.27154278755188\n",
      "Iteration 975 Loss: 2.5665130615234375\n",
      "Iteration 976 Loss: 2.8511159420013428\n",
      "Iteration 977 Loss: 2.608319044113159\n",
      "Iteration 978 Loss: 2.55208420753479\n",
      "Iteration 979 Loss: 2.5075294971466064\n",
      "Iteration 979 Loss: 2.471867561340332\n",
      "Iteration 980 Loss: 3.5416107177734375\n",
      "Iteration 981 Loss: 2.21683669090271\n",
      "Iteration 982 Loss: 2.466040849685669\n",
      "Iteration 983 Loss: 2.5025792121887207\n",
      "Iteration 984 Loss: 2.632567882537842\n",
      "Iteration 985 Loss: 2.5152781009674072\n",
      "Iteration 986 Loss: 2.70503568649292\n",
      "Iteration 987 Loss: 2.5223660469055176\n",
      "Iteration 988 Loss: 2.8235597610473633\n",
      "Iteration 989 Loss: 2.7164323329925537\n",
      "Iteration 989 Loss: 2.6642308235168457\n",
      "Iteration 990 Loss: 2.6510188579559326\n",
      "Iteration 991 Loss: 2.9388062953948975\n",
      "Iteration 992 Loss: 2.5532171726226807\n",
      "Iteration 993 Loss: 2.522707462310791\n",
      "Iteration 994 Loss: 2.976353883743286\n",
      "Iteration 995 Loss: 1.9675629138946533\n",
      "Iteration 996 Loss: 2.7358193397521973\n",
      "Iteration 997 Loss: 3.3941657543182373\n",
      "Iteration 998 Loss: 2.413452625274658\n",
      "Iteration 999 Loss: 2.659196615219116\n",
      "Iteration 999 Loss: 2.681230068206787\n",
      "Iteration 1000 Loss: 2.141043186187744\n",
      "Iteration 1001 Loss: 2.922867774963379\n",
      "Iteration 1002 Loss: 2.200852870941162\n",
      "Iteration 1003 Loss: 2.5271501541137695\n",
      "Iteration 1004 Loss: 2.2167813777923584\n",
      "Iteration 1005 Loss: 2.820955991744995\n",
      "Iteration 1006 Loss: 2.971442461013794\n",
      "Iteration 1007 Loss: 2.3574819564819336\n",
      "Iteration 1008 Loss: 2.366246223449707\n",
      "Iteration 1009 Loss: 2.0600454807281494\n",
      "Iteration 1009 Loss: 2.458486557006836\n",
      "Iteration 1010 Loss: 2.618748664855957\n",
      "Iteration 1011 Loss: 3.1436855792999268\n",
      "Iteration 1012 Loss: 2.2039778232574463\n",
      "Iteration 1013 Loss: 2.585134267807007\n",
      "Iteration 1014 Loss: 2.8888564109802246\n",
      "Iteration 1015 Loss: 2.3392493724823\n",
      "Iteration 1016 Loss: 2.7580111026763916\n",
      "Iteration 1017 Loss: 2.559366226196289\n",
      "Iteration 1018 Loss: 2.536323308944702\n",
      "Iteration 1019 Loss: 2.2206997871398926\n",
      "Iteration 1019 Loss: 2.585405111312866\n",
      "Iteration 1020 Loss: 2.2700905799865723\n",
      "Iteration 1021 Loss: 2.584761142730713\n",
      "Iteration 1022 Loss: 2.6987226009368896\n",
      "Iteration 1023 Loss: 2.9612350463867188\n",
      "Iteration 1024 Loss: 2.357553482055664\n",
      "Iteration 1025 Loss: 2.960855007171631\n",
      "Iteration 1026 Loss: 2.620591163635254\n",
      "Iteration 1027 Loss: 2.1192142963409424\n",
      "Iteration 1028 Loss: 2.352071523666382\n",
      "Iteration 1029 Loss: 2.357759714126587\n",
      "Iteration 1029 Loss: 2.5282857418060303\n",
      "Iteration 1030 Loss: 3.0260841846466064\n",
      "Iteration 1031 Loss: 2.3659143447875977\n",
      "Iteration 1032 Loss: 2.260891914367676\n",
      "Iteration 1033 Loss: 2.514197826385498\n",
      "Iteration 1034 Loss: 1.6637685298919678\n",
      "Iteration 1035 Loss: 2.9618992805480957\n",
      "Iteration 1036 Loss: 2.5384881496429443\n",
      "Iteration 1037 Loss: 2.8838043212890625\n",
      "Iteration 1038 Loss: 2.920698404312134\n",
      "Iteration 1039 Loss: 2.3448843955993652\n",
      "Iteration 1039 Loss: 2.548063039779663\n",
      "Iteration 1040 Loss: 2.545799970626831\n",
      "Iteration 1041 Loss: 2.124389886856079\n",
      "Iteration 1042 Loss: 2.539224624633789\n",
      "Iteration 1043 Loss: 1.9437057971954346\n",
      "Iteration 1044 Loss: 2.5225367546081543\n",
      "Iteration 1045 Loss: 2.097313642501831\n",
      "Iteration 1046 Loss: 3.280116081237793\n",
      "Iteration 1047 Loss: 2.4048357009887695\n",
      "Iteration 1048 Loss: 2.102323293685913\n",
      "Iteration 1049 Loss: 2.083209276199341\n",
      "Iteration 1049 Loss: 2.364345073699951\n",
      "Iteration 1050 Loss: 2.540705919265747\n",
      "Iteration 1051 Loss: 2.490628242492676\n",
      "Iteration 1052 Loss: 3.1583127975463867\n",
      "Iteration 1053 Loss: 2.504955530166626\n",
      "Iteration 1054 Loss: 2.270704507827759\n",
      "Iteration 1055 Loss: 2.524930238723755\n",
      "Iteration 1056 Loss: 2.4608700275421143\n",
      "Iteration 1057 Loss: 2.6853909492492676\n",
      "Iteration 1058 Loss: 1.9857209920883179\n",
      "Iteration 1059 Loss: 2.5114145278930664\n",
      "Iteration 1059 Loss: 2.5133633613586426\n",
      "Iteration 1060 Loss: 2.931797504425049\n",
      "Iteration 1061 Loss: 2.5514588356018066\n",
      "Iteration 1062 Loss: 2.31538462638855\n",
      "Iteration 1063 Loss: 2.7585060596466064\n",
      "Iteration 1064 Loss: 2.3901596069335938\n",
      "Iteration 1065 Loss: 2.589698314666748\n",
      "Iteration 1066 Loss: 3.022010087966919\n",
      "Iteration 1067 Loss: 2.420681953430176\n",
      "Iteration 1068 Loss: 2.7668488025665283\n",
      "Iteration 1069 Loss: 1.6618982553482056\n",
      "Iteration 1069 Loss: 2.540844440460205\n",
      "Iteration 1070 Loss: 2.318452835083008\n",
      "Iteration 1071 Loss: 2.739989757537842\n",
      "Iteration 1072 Loss: 2.1909162998199463\n",
      "Iteration 1073 Loss: 2.2778849601745605\n",
      "Iteration 1074 Loss: 1.9699392318725586\n",
      "Iteration 1075 Loss: 2.6525092124938965\n",
      "Iteration 1076 Loss: 2.2144808769226074\n",
      "Iteration 1077 Loss: 2.3263840675354004\n",
      "Iteration 1078 Loss: 2.6135194301605225\n",
      "Iteration 1079 Loss: 2.6074881553649902\n",
      "Iteration 1079 Loss: 2.3911564350128174\n",
      "Iteration 1080 Loss: 1.750243067741394\n",
      "Iteration 1081 Loss: 2.416153907775879\n",
      "Iteration 1082 Loss: 2.659414768218994\n",
      "Iteration 1083 Loss: 2.1454038619995117\n",
      "Iteration 1084 Loss: 2.466813325881958\n",
      "Iteration 1085 Loss: 2.6064624786376953\n",
      "Iteration 1086 Loss: 2.490668296813965\n",
      "Iteration 1087 Loss: 2.3063621520996094\n",
      "Iteration 1088 Loss: 2.28955078125\n",
      "Iteration 1089 Loss: 2.739771842956543\n",
      "Iteration 1089 Loss: 2.387084484100342\n",
      "Iteration 1090 Loss: 2.777240514755249\n",
      "Iteration 1091 Loss: 2.8009567260742188\n",
      "Iteration 1092 Loss: 2.5331461429595947\n",
      "Iteration 1093 Loss: 2.1263980865478516\n",
      "Iteration 1094 Loss: 2.255686044692993\n",
      "Iteration 1095 Loss: 2.4683115482330322\n",
      "Iteration 1096 Loss: 2.0079140663146973\n",
      "Iteration 1097 Loss: 2.2072885036468506\n",
      "Iteration 1098 Loss: 2.9979450702667236\n",
      "Iteration 1099 Loss: 3.0703959465026855\n",
      "Iteration 1099 Loss: 2.5245282649993896\n",
      "Iteration 1100 Loss: 2.1003923416137695\n",
      "Iteration 1101 Loss: 2.434854745864868\n",
      "Iteration 1102 Loss: 3.3727424144744873\n",
      "Iteration 1103 Loss: 2.581472158432007\n",
      "Iteration 1104 Loss: 2.005038261413574\n",
      "Iteration 1105 Loss: 2.6845691204071045\n",
      "Iteration 1106 Loss: 1.9657880067825317\n",
      "Iteration 1107 Loss: 2.5070953369140625\n",
      "Iteration 1108 Loss: 2.5894432067871094\n",
      "Iteration 1109 Loss: 1.8503780364990234\n",
      "Iteration 1109 Loss: 2.409177303314209\n",
      "Iteration 1110 Loss: 1.9466928243637085\n",
      "Iteration 1111 Loss: 1.966692566871643\n",
      "Iteration 1112 Loss: 1.9820239543914795\n",
      "Iteration 1113 Loss: 2.347045421600342\n",
      "Iteration 1114 Loss: 2.3401825428009033\n",
      "Iteration 1115 Loss: 2.342305898666382\n",
      "Iteration 1116 Loss: 2.6019935607910156\n",
      "Iteration 1117 Loss: 2.354579210281372\n",
      "Iteration 1118 Loss: 2.1797549724578857\n",
      "Iteration 1119 Loss: 2.497213125228882\n",
      "Iteration 1119 Loss: 2.2558484077453613\n",
      "Iteration 1120 Loss: 2.435901641845703\n",
      "Iteration 1121 Loss: 2.808336019515991\n",
      "Iteration 1122 Loss: 2.6802570819854736\n",
      "Iteration 1123 Loss: 2.1507792472839355\n",
      "Iteration 1124 Loss: 2.0370655059814453\n",
      "Iteration 1125 Loss: 2.029268980026245\n",
      "Iteration 1126 Loss: 2.634915351867676\n",
      "Iteration 1127 Loss: 2.230241298675537\n",
      "Iteration 1128 Loss: 2.3796119689941406\n",
      "Iteration 1129 Loss: 2.7579197883605957\n",
      "Iteration 1129 Loss: 2.4144296646118164\n",
      "Iteration 1130 Loss: 2.2286901473999023\n",
      "Iteration 1131 Loss: 2.4637320041656494\n",
      "Iteration 1132 Loss: 1.8003721237182617\n",
      "Iteration 1133 Loss: 2.728837728500366\n",
      "Iteration 1134 Loss: 2.1731760501861572\n",
      "Iteration 1135 Loss: 2.2859978675842285\n",
      "Iteration 1136 Loss: 2.209568738937378\n",
      "Iteration 1137 Loss: 2.7499186992645264\n",
      "Iteration 1138 Loss: 1.9757193326950073\n",
      "Iteration 1139 Loss: 2.325086832046509\n",
      "Iteration 1139 Loss: 2.29410982131958\n",
      "Iteration 1140 Loss: 2.1522531509399414\n",
      "Iteration 1141 Loss: 2.1447625160217285\n",
      "Iteration 1142 Loss: 2.7915542125701904\n",
      "Iteration 1143 Loss: 2.5134849548339844\n",
      "Iteration 1144 Loss: 2.157626152038574\n",
      "Iteration 1145 Loss: 2.1479992866516113\n",
      "Iteration 1146 Loss: 2.9203121662139893\n",
      "Iteration 1147 Loss: 2.6115362644195557\n",
      "Iteration 1148 Loss: 2.786181926727295\n",
      "Iteration 1149 Loss: 2.1199989318847656\n",
      "Iteration 1149 Loss: 2.4345710277557373\n",
      "Iteration 1150 Loss: 1.994206190109253\n",
      "Iteration 1151 Loss: 2.4046196937561035\n",
      "Iteration 1152 Loss: 1.7667111158370972\n",
      "Iteration 1153 Loss: 2.5774524211883545\n",
      "Iteration 1154 Loss: 2.421170234680176\n",
      "Iteration 1155 Loss: 2.044064521789551\n",
      "Iteration 1156 Loss: 2.862560510635376\n",
      "Iteration 1157 Loss: 2.074775457382202\n",
      "Iteration 1158 Loss: 2.062872886657715\n",
      "Iteration 1159 Loss: 2.5826165676116943\n",
      "Iteration 1159 Loss: 2.2791049480438232\n",
      "Iteration 1160 Loss: 1.982004165649414\n",
      "Iteration 1161 Loss: 2.014829158782959\n",
      "Iteration 1162 Loss: 2.4124536514282227\n",
      "Iteration 1163 Loss: 2.529987335205078\n",
      "Iteration 1164 Loss: 1.9731106758117676\n",
      "Iteration 1165 Loss: 2.264936685562134\n",
      "Iteration 1166 Loss: 1.9163541793823242\n",
      "Iteration 1167 Loss: 2.282496452331543\n",
      "Iteration 1168 Loss: 1.9115725755691528\n",
      "Iteration 1169 Loss: 1.7338228225708008\n",
      "Iteration 1169 Loss: 2.1021568775177\n",
      "Iteration 1170 Loss: 1.7089214324951172\n",
      "Iteration 1171 Loss: 2.3860418796539307\n",
      "Iteration 1172 Loss: 2.298337697982788\n",
      "Iteration 1173 Loss: 2.282562732696533\n",
      "Iteration 1174 Loss: 2.1902406215667725\n",
      "Iteration 1175 Loss: 2.279280185699463\n",
      "Iteration 1176 Loss: 2.2297301292419434\n",
      "Iteration 1177 Loss: 2.7897884845733643\n",
      "Iteration 1178 Loss: 2.0870847702026367\n",
      "Iteration 1179 Loss: 2.141920804977417\n",
      "Iteration 1179 Loss: 2.2393908500671387\n",
      "Iteration 1180 Loss: 1.9889053106307983\n",
      "Iteration 1181 Loss: 2.2145864963531494\n",
      "Iteration 1182 Loss: 2.045498847961426\n",
      "Iteration 1183 Loss: 2.3618416786193848\n",
      "Iteration 1184 Loss: 2.259296417236328\n",
      "Iteration 1185 Loss: 2.837829351425171\n",
      "Iteration 1186 Loss: 2.217834711074829\n",
      "Iteration 1187 Loss: 1.8246318101882935\n",
      "Iteration 1188 Loss: 2.9549951553344727\n",
      "Iteration 1189 Loss: 2.14129376411438\n",
      "Iteration 1189 Loss: 2.2846713066101074\n",
      "Iteration 1190 Loss: 2.374163866043091\n",
      "Iteration 1191 Loss: 2.1920206546783447\n",
      "Iteration 1192 Loss: 2.736696481704712\n",
      "Iteration 1193 Loss: 2.7903761863708496\n",
      "Iteration 1194 Loss: 1.9325364828109741\n",
      "Iteration 1195 Loss: 1.9560778141021729\n",
      "Iteration 1196 Loss: 2.4248239994049072\n",
      "Iteration 1197 Loss: 2.5264487266540527\n",
      "Iteration 1198 Loss: 2.9927258491516113\n",
      "Iteration 1199 Loss: 1.7321782112121582\n",
      "Iteration 1199 Loss: 2.36580491065979\n",
      "Iteration 1200 Loss: 1.9209450483322144\n",
      "Iteration 1201 Loss: 1.9079360961914062\n",
      "Iteration 1202 Loss: 2.174318552017212\n",
      "Iteration 1203 Loss: 2.055335283279419\n",
      "Iteration 1204 Loss: 2.467897891998291\n",
      "Iteration 1205 Loss: 2.5020532608032227\n",
      "Iteration 1206 Loss: 2.0010054111480713\n",
      "Iteration 1207 Loss: 2.094752788543701\n",
      "Iteration 1208 Loss: 2.6220319271087646\n",
      "Iteration 1209 Loss: 2.2579329013824463\n",
      "Iteration 1209 Loss: 2.20042085647583\n",
      "Iteration 1210 Loss: 1.8737314939498901\n",
      "Iteration 1211 Loss: 2.353856325149536\n",
      "Iteration 1212 Loss: 2.6893467903137207\n",
      "Iteration 1213 Loss: 2.543424129486084\n",
      "Iteration 1214 Loss: 2.1481168270111084\n",
      "Iteration 1215 Loss: 1.8485380411148071\n",
      "Iteration 1216 Loss: 2.292299509048462\n",
      "Iteration 1217 Loss: 2.3260982036590576\n",
      "Iteration 1218 Loss: 2.375986337661743\n",
      "Iteration 1219 Loss: 2.5379436016082764\n",
      "Iteration 1219 Loss: 2.298933982849121\n",
      "Iteration 1220 Loss: 2.5681560039520264\n",
      "Iteration 1221 Loss: 2.1777355670928955\n",
      "Iteration 1222 Loss: 2.725324869155884\n",
      "Iteration 1223 Loss: 1.5585153102874756\n",
      "Iteration 1224 Loss: 2.8210315704345703\n",
      "Iteration 1225 Loss: 2.338773488998413\n",
      "Iteration 1226 Loss: 1.7159065008163452\n",
      "Iteration 1227 Loss: 1.9802017211914062\n",
      "Iteration 1228 Loss: 2.3360681533813477\n",
      "Iteration 1229 Loss: 2.24973726272583\n",
      "Iteration 1229 Loss: 2.247145175933838\n",
      "Iteration 1230 Loss: 2.4202468395233154\n",
      "Iteration 1231 Loss: 2.6433162689208984\n",
      "Iteration 1232 Loss: 2.735238552093506\n",
      "Iteration 1233 Loss: 1.8924660682678223\n",
      "Iteration 1234 Loss: 2.9630401134490967\n",
      "Iteration 1235 Loss: 2.4734480381011963\n",
      "Iteration 1236 Loss: 1.8468260765075684\n",
      "Iteration 1237 Loss: 2.2636234760284424\n",
      "Iteration 1238 Loss: 2.3926713466644287\n",
      "Iteration 1239 Loss: 2.0584218502044678\n",
      "Iteration 1239 Loss: 2.368929862976074\n",
      "Iteration 1240 Loss: 2.169069766998291\n",
      "Iteration 1241 Loss: 2.252924680709839\n",
      "Iteration 1242 Loss: 1.7020771503448486\n",
      "Iteration 1243 Loss: 2.2208991050720215\n",
      "Iteration 1244 Loss: 2.415774345397949\n",
      "Iteration 1245 Loss: 2.3118982315063477\n",
      "Iteration 1246 Loss: 2.3647243976593018\n",
      "Iteration 1247 Loss: 1.9291410446166992\n",
      "Iteration 1248 Loss: 2.1895911693573\n",
      "Iteration 1249 Loss: 2.011441469192505\n",
      "Iteration 1249 Loss: 2.1567542552948\n",
      "Iteration 1250 Loss: 1.9787801504135132\n",
      "Iteration 1251 Loss: 1.5469204187393188\n",
      "Iteration 1252 Loss: 2.1766860485076904\n",
      "Iteration 1253 Loss: 2.3763840198516846\n",
      "Iteration 1254 Loss: 1.92043137550354\n",
      "Iteration 1255 Loss: 1.7363007068634033\n",
      "Iteration 1256 Loss: 2.382845401763916\n",
      "Iteration 1257 Loss: 2.180670738220215\n",
      "Iteration 1258 Loss: 2.2533962726593018\n",
      "Iteration 1259 Loss: 1.9897499084472656\n",
      "Iteration 1259 Loss: 2.0542163848876953\n",
      "Iteration 1260 Loss: 2.145087957382202\n",
      "Iteration 1261 Loss: 1.90654456615448\n",
      "Iteration 1262 Loss: 2.0241360664367676\n",
      "Iteration 1263 Loss: 2.287071943283081\n",
      "Iteration 1264 Loss: 2.2310407161712646\n",
      "Iteration 1265 Loss: 2.4133079051971436\n",
      "Iteration 1266 Loss: 2.3265373706817627\n",
      "Iteration 1267 Loss: 1.9926873445510864\n",
      "Iteration 1268 Loss: 1.9150348901748657\n",
      "Iteration 1269 Loss: 2.3693113327026367\n",
      "Iteration 1269 Loss: 2.161076068878174\n",
      "Iteration 1270 Loss: 2.3954358100891113\n",
      "Iteration 1271 Loss: 2.5567362308502197\n",
      "Iteration 1272 Loss: 2.0035364627838135\n",
      "Iteration 1273 Loss: 2.3611080646514893\n",
      "Iteration 1274 Loss: 2.7828798294067383\n",
      "Iteration 1275 Loss: 2.5766332149505615\n",
      "Iteration 1276 Loss: 2.4367575645446777\n",
      "Iteration 1277 Loss: 2.246898651123047\n",
      "Iteration 1278 Loss: 1.9628973007202148\n",
      "Iteration 1279 Loss: 1.97659432888031\n",
      "Iteration 1279 Loss: 2.3299479484558105\n",
      "Iteration 1280 Loss: 2.6895222663879395\n",
      "Iteration 1281 Loss: 1.8544594049453735\n",
      "Iteration 1282 Loss: 2.1826016902923584\n",
      "Iteration 1283 Loss: 2.52685809135437\n",
      "Iteration 1284 Loss: 3.1158432960510254\n",
      "Iteration 1285 Loss: 2.554471015930176\n",
      "Iteration 1286 Loss: 1.4936059713363647\n",
      "Iteration 1287 Loss: 1.995483160018921\n",
      "Iteration 1288 Loss: 1.8419891595840454\n",
      "Iteration 1289 Loss: 1.927314043045044\n",
      "Iteration 1289 Loss: 2.218214750289917\n",
      "Iteration 1290 Loss: 1.6548975706100464\n",
      "Iteration 1291 Loss: 2.1606667041778564\n",
      "Iteration 1292 Loss: 2.3044726848602295\n",
      "Iteration 1293 Loss: 2.281358480453491\n",
      "Iteration 1294 Loss: 2.1946613788604736\n",
      "Iteration 1295 Loss: 2.279205322265625\n",
      "Iteration 1296 Loss: 2.33477520942688\n",
      "Iteration 1297 Loss: 1.8733364343643188\n",
      "Iteration 1298 Loss: 2.064643144607544\n",
      "Iteration 1299 Loss: 2.046267032623291\n",
      "Iteration 1299 Loss: 2.1194283962249756\n",
      "Iteration 1300 Loss: 2.499342203140259\n",
      "Iteration 1301 Loss: 2.170811414718628\n",
      "Iteration 1302 Loss: 2.476330518722534\n",
      "Iteration 1303 Loss: 2.3985018730163574\n",
      "Iteration 1304 Loss: 2.1383426189422607\n",
      "Iteration 1305 Loss: 2.18086314201355\n",
      "Iteration 1306 Loss: 2.3640124797821045\n",
      "Iteration 1307 Loss: 1.8349260091781616\n",
      "Iteration 1308 Loss: 2.445331573486328\n",
      "Iteration 1309 Loss: 1.8743278980255127\n",
      "Iteration 1309 Loss: 2.238278865814209\n",
      "Iteration 1310 Loss: 2.0301856994628906\n",
      "Iteration 1311 Loss: 2.165879487991333\n",
      "Iteration 1312 Loss: 2.627560615539551\n",
      "Iteration 1313 Loss: 2.9243268966674805\n",
      "Iteration 1314 Loss: 1.9309277534484863\n",
      "Iteration 1315 Loss: 2.0156078338623047\n",
      "Iteration 1316 Loss: 2.4921257495880127\n",
      "Iteration 1317 Loss: 1.9634778499603271\n",
      "Iteration 1318 Loss: 1.901227355003357\n",
      "Iteration 1319 Loss: 1.7826952934265137\n",
      "Iteration 1319 Loss: 2.183401584625244\n",
      "Iteration 1320 Loss: 2.932858943939209\n",
      "Iteration 1321 Loss: 2.5831618309020996\n",
      "Iteration 1322 Loss: 2.0232105255126953\n",
      "Iteration 1323 Loss: 2.2461905479431152\n",
      "Iteration 1324 Loss: 1.9464683532714844\n",
      "Iteration 1325 Loss: 2.2456753253936768\n",
      "Iteration 1326 Loss: 2.28311824798584\n",
      "Iteration 1327 Loss: 2.037418842315674\n",
      "Iteration 1328 Loss: 1.7952790260314941\n",
      "Iteration 1329 Loss: 2.0688886642456055\n",
      "Iteration 1329 Loss: 2.2162270545959473\n",
      "Iteration 1330 Loss: 2.427638292312622\n",
      "Iteration 1331 Loss: 2.7039895057678223\n",
      "Iteration 1332 Loss: 2.244067430496216\n",
      "Iteration 1333 Loss: 1.7230384349822998\n",
      "Iteration 1334 Loss: 1.9379724264144897\n",
      "Iteration 1335 Loss: 3.202545642852783\n",
      "Iteration 1336 Loss: 2.061154365539551\n",
      "Iteration 1337 Loss: 2.1155951023101807\n",
      "Iteration 1338 Loss: 1.8181536197662354\n",
      "Iteration 1339 Loss: 2.2531843185424805\n",
      "Iteration 1339 Loss: 2.2487337589263916\n",
      "Iteration 1340 Loss: 1.9790021181106567\n",
      "Iteration 1341 Loss: 2.1717541217803955\n",
      "Iteration 1342 Loss: 2.195000171661377\n",
      "Iteration 1343 Loss: 2.6448779106140137\n",
      "Iteration 1344 Loss: 2.308640718460083\n",
      "Iteration 1345 Loss: 1.9273545742034912\n",
      "Iteration 1346 Loss: 2.1064467430114746\n",
      "Iteration 1347 Loss: 2.4052481651306152\n",
      "Iteration 1348 Loss: 2.2818257808685303\n",
      "Iteration 1349 Loss: 2.119021415710449\n",
      "Iteration 1349 Loss: 2.2139172554016113\n",
      "Iteration 1350 Loss: 2.066049575805664\n",
      "Iteration 1351 Loss: 2.4565868377685547\n",
      "Iteration 1352 Loss: 2.509805679321289\n",
      "Iteration 1353 Loss: 1.7970327138900757\n",
      "Iteration 1354 Loss: 2.1057190895080566\n",
      "Iteration 1355 Loss: 2.3126537799835205\n",
      "Iteration 1356 Loss: 2.3676841259002686\n",
      "Iteration 1357 Loss: 1.5516891479492188\n",
      "Iteration 1358 Loss: 2.244431257247925\n",
      "Iteration 1359 Loss: 2.147366762161255\n",
      "Iteration 1359 Loss: 2.1559019088745117\n",
      "Iteration 1360 Loss: 2.4204344749450684\n",
      "Iteration 1361 Loss: 2.2539865970611572\n",
      "Iteration 1362 Loss: 1.6243139505386353\n",
      "Iteration 1363 Loss: 2.6271698474884033\n",
      "Iteration 1364 Loss: 1.9714792966842651\n",
      "Iteration 1365 Loss: 2.432224750518799\n",
      "Iteration 1366 Loss: 2.5302205085754395\n",
      "Iteration 1367 Loss: 2.385364055633545\n",
      "Iteration 1368 Loss: 2.0391180515289307\n",
      "Iteration 1369 Loss: 1.8662164211273193\n",
      "Iteration 1369 Loss: 2.215052843093872\n",
      "Iteration 1370 Loss: 2.5302278995513916\n",
      "Iteration 1371 Loss: 2.530729055404663\n",
      "Iteration 1372 Loss: 2.2423174381256104\n",
      "Iteration 1373 Loss: 2.079751491546631\n",
      "Iteration 1374 Loss: 2.2189252376556396\n",
      "Iteration 1375 Loss: 1.9907588958740234\n",
      "Iteration 1376 Loss: 1.7035309076309204\n",
      "Iteration 1377 Loss: 2.2081925868988037\n",
      "Iteration 1378 Loss: 2.2577056884765625\n",
      "Iteration 1379 Loss: 2.4586055278778076\n",
      "Iteration 1379 Loss: 2.222074508666992\n",
      "Iteration 1380 Loss: 2.547206401824951\n",
      "Iteration 1381 Loss: 2.4839534759521484\n",
      "Iteration 1382 Loss: 3.2099599838256836\n",
      "Iteration 1383 Loss: 2.195469856262207\n",
      "Iteration 1384 Loss: 1.905197024345398\n",
      "Iteration 1385 Loss: 1.6766912937164307\n",
      "Iteration 1386 Loss: 2.194258689880371\n",
      "Iteration 1387 Loss: 2.781360626220703\n",
      "Iteration 1388 Loss: 2.1050832271575928\n",
      "Iteration 1389 Loss: 2.4577741622924805\n",
      "Iteration 1389 Loss: 2.3556952476501465\n",
      "Iteration 1390 Loss: 1.3283660411834717\n",
      "Iteration 1391 Loss: 2.235713005065918\n",
      "Iteration 1392 Loss: 2.423408031463623\n",
      "Iteration 1393 Loss: 2.629023790359497\n",
      "Iteration 1394 Loss: 2.4861128330230713\n",
      "Iteration 1395 Loss: 2.2805612087249756\n",
      "Iteration 1396 Loss: 2.6873066425323486\n",
      "Iteration 1397 Loss: 2.5918567180633545\n",
      "Iteration 1398 Loss: 1.884949803352356\n",
      "Iteration 1399 Loss: 2.1627416610717773\n",
      "Iteration 1399 Loss: 2.2710039615631104\n",
      "Iteration 1400 Loss: 2.2762978076934814\n",
      "Iteration 1401 Loss: 2.063927173614502\n",
      "Iteration 1402 Loss: 1.8131386041641235\n",
      "Iteration 1403 Loss: 2.1341757774353027\n",
      "Iteration 1404 Loss: 2.465352773666382\n",
      "Iteration 1405 Loss: 1.9330421686172485\n",
      "Iteration 1406 Loss: 1.5574766397476196\n",
      "Iteration 1407 Loss: 2.4359395503997803\n",
      "Iteration 1408 Loss: 2.2534735202789307\n",
      "Iteration 1409 Loss: 2.18906569480896\n",
      "Iteration 1409 Loss: 2.1121890544891357\n",
      "Iteration 1410 Loss: 2.3485360145568848\n",
      "Iteration 1411 Loss: 2.103327512741089\n",
      "Iteration 1412 Loss: 2.177023410797119\n",
      "Iteration 1413 Loss: 1.9089930057525635\n",
      "Iteration 1414 Loss: 2.208782196044922\n",
      "Iteration 1415 Loss: 2.3893284797668457\n",
      "Iteration 1416 Loss: 2.0661323070526123\n",
      "Iteration 1417 Loss: 2.144697904586792\n",
      "Iteration 1418 Loss: 1.420224905014038\n",
      "Iteration 1419 Loss: 1.6954689025878906\n",
      "Iteration 1419 Loss: 2.0462512969970703\n",
      "Iteration 1420 Loss: 2.176173686981201\n",
      "Iteration 1421 Loss: 2.202517032623291\n",
      "Iteration 1422 Loss: 2.147747039794922\n",
      "Iteration 1423 Loss: 1.4496049880981445\n",
      "Iteration 1424 Loss: 1.8658827543258667\n",
      "Iteration 1425 Loss: 2.413285970687866\n",
      "Iteration 1426 Loss: 2.784188985824585\n",
      "Iteration 1427 Loss: 1.8354657888412476\n",
      "Iteration 1428 Loss: 1.5784735679626465\n",
      "Iteration 1429 Loss: 2.148432970046997\n",
      "Iteration 1429 Loss: 2.0601773262023926\n",
      "Iteration 1430 Loss: 2.4174070358276367\n",
      "Iteration 1431 Loss: 2.4794986248016357\n",
      "Iteration 1432 Loss: 2.3514034748077393\n",
      "Iteration 1433 Loss: 1.8530248403549194\n",
      "Iteration 1434 Loss: 2.1062355041503906\n",
      "Iteration 1435 Loss: 2.3642656803131104\n",
      "Iteration 1436 Loss: 2.1388981342315674\n",
      "Iteration 1437 Loss: 2.03609037399292\n",
      "Iteration 1438 Loss: 1.737112045288086\n",
      "Iteration 1439 Loss: 1.9176607131958008\n",
      "Iteration 1439 Loss: 2.140159845352173\n",
      "Iteration 1440 Loss: 2.1515495777130127\n",
      "Iteration 1441 Loss: 1.4223459959030151\n",
      "Iteration 1442 Loss: 1.4887348413467407\n",
      "Iteration 1443 Loss: 2.0550119876861572\n",
      "Iteration 1444 Loss: 1.9071290493011475\n",
      "Iteration 1445 Loss: 2.339425802230835\n",
      "Iteration 1446 Loss: 1.3566333055496216\n",
      "Iteration 1447 Loss: 1.5160855054855347\n",
      "Iteration 1448 Loss: 1.9569411277770996\n",
      "Iteration 1449 Loss: 2.1951234340667725\n",
      "Iteration 1449 Loss: 1.8388980627059937\n",
      "Iteration 1450 Loss: 2.3078460693359375\n",
      "Iteration 1451 Loss: 2.2055625915527344\n",
      "Iteration 1452 Loss: 1.8861771821975708\n",
      "Iteration 1453 Loss: 3.103976249694824\n",
      "Iteration 1454 Loss: 2.1974356174468994\n",
      "Iteration 1455 Loss: 1.58021080493927\n",
      "Iteration 1456 Loss: 2.4286768436431885\n",
      "Iteration 1457 Loss: 1.723135232925415\n",
      "Iteration 1458 Loss: 2.426304817199707\n",
      "Iteration 1459 Loss: 1.7045941352844238\n",
      "Iteration 1459 Loss: 2.1563918590545654\n",
      "Iteration 1460 Loss: 2.22647762298584\n",
      "Iteration 1461 Loss: 2.2423315048217773\n",
      "Iteration 1462 Loss: 2.140202522277832\n",
      "Iteration 1463 Loss: 1.8572747707366943\n",
      "Iteration 1464 Loss: 2.988353729248047\n",
      "Iteration 1465 Loss: 2.504995107650757\n",
      "Iteration 1466 Loss: 1.989171028137207\n",
      "Iteration 1467 Loss: 1.936198353767395\n",
      "Iteration 1468 Loss: 2.310819625854492\n",
      "Iteration 1469 Loss: 2.3553712368011475\n",
      "Iteration 1469 Loss: 2.255119800567627\n",
      "Iteration 1470 Loss: 1.6203023195266724\n",
      "Iteration 1471 Loss: 2.4590868949890137\n",
      "Iteration 1472 Loss: 1.509781837463379\n",
      "Iteration 1473 Loss: 2.018251419067383\n",
      "Iteration 1474 Loss: 2.2501282691955566\n",
      "Iteration 1475 Loss: 1.7069685459136963\n",
      "Iteration 1476 Loss: 2.3618381023406982\n",
      "Iteration 1477 Loss: 2.747520923614502\n",
      "Iteration 1478 Loss: 2.803154706954956\n",
      "Iteration 1479 Loss: 2.137827157974243\n",
      "Iteration 1479 Loss: 2.1614859104156494\n",
      "Iteration 1480 Loss: 2.7546398639678955\n",
      "Iteration 1481 Loss: 2.4141948223114014\n",
      "Iteration 1482 Loss: 2.089738607406616\n",
      "Iteration 1483 Loss: 2.0227253437042236\n",
      "Iteration 1484 Loss: 1.9651610851287842\n",
      "Iteration 1485 Loss: 1.9307277202606201\n",
      "Iteration 1486 Loss: 2.158747673034668\n",
      "Iteration 1487 Loss: 2.45233154296875\n",
      "Iteration 1488 Loss: 2.2224273681640625\n",
      "Iteration 1489 Loss: 2.462160110473633\n",
      "Iteration 1489 Loss: 2.2472853660583496\n",
      "Iteration 1490 Loss: 2.072087526321411\n",
      "Iteration 1491 Loss: 1.8321006298065186\n",
      "Iteration 1492 Loss: 1.5868357419967651\n",
      "Iteration 1493 Loss: 2.0263729095458984\n",
      "Iteration 1494 Loss: 1.9997587203979492\n",
      "Iteration 1495 Loss: 2.285248279571533\n",
      "Iteration 1496 Loss: 1.8631664514541626\n",
      "Iteration 1497 Loss: 2.0392801761627197\n",
      "Iteration 1498 Loss: 2.645392417907715\n",
      "Iteration 1499 Loss: 1.4134100675582886\n",
      "Iteration 1499 Loss: 1.976365327835083\n",
      "Iteration 1500 Loss: 1.9148287773132324\n",
      "Iteration 1501 Loss: 1.5393017530441284\n",
      "Iteration 1502 Loss: 2.351569890975952\n",
      "Iteration 1503 Loss: 1.6699788570404053\n",
      "Iteration 1504 Loss: 2.2108516693115234\n",
      "Iteration 1505 Loss: 1.7058088779449463\n",
      "Iteration 1506 Loss: 1.8665872812271118\n",
      "Iteration 1507 Loss: 1.8380273580551147\n",
      "Iteration 1508 Loss: 1.8775089979171753\n",
      "Iteration 1509 Loss: 1.5554449558258057\n",
      "Iteration 1509 Loss: 1.8529908657073975\n",
      "Iteration 1510 Loss: 2.490335702896118\n",
      "Iteration 1511 Loss: 2.1787407398223877\n",
      "Iteration 1512 Loss: 2.4113709926605225\n",
      "Iteration 1513 Loss: 2.0672147274017334\n",
      "Iteration 1514 Loss: 2.1094603538513184\n",
      "Iteration 1515 Loss: 2.472959280014038\n",
      "Iteration 1516 Loss: 1.4482090473175049\n",
      "Iteration 1517 Loss: 2.2142930030822754\n",
      "Iteration 1518 Loss: 1.4039064645767212\n",
      "Iteration 1519 Loss: 1.597736120223999\n",
      "Iteration 1519 Loss: 2.0394227504730225\n",
      "Iteration 1520 Loss: 1.8337137699127197\n",
      "Iteration 1521 Loss: 1.3592185974121094\n",
      "Iteration 1522 Loss: 1.6503039598464966\n",
      "Iteration 1523 Loss: 2.4960689544677734\n",
      "Iteration 1524 Loss: 2.050901174545288\n",
      "Iteration 1525 Loss: 1.6381243467330933\n",
      "Iteration 1526 Loss: 2.0680577754974365\n",
      "Iteration 1527 Loss: 1.9508771896362305\n",
      "Iteration 1528 Loss: 1.8592052459716797\n",
      "Iteration 1529 Loss: 2.0810370445251465\n",
      "Iteration 1529 Loss: 1.89875066280365\n",
      "Iteration 1530 Loss: 1.7297037839889526\n",
      "Iteration 1531 Loss: 1.9048962593078613\n",
      "Iteration 1532 Loss: 2.3086166381835938\n",
      "Iteration 1533 Loss: 2.132422924041748\n",
      "Iteration 1534 Loss: 1.487061858177185\n",
      "Iteration 1535 Loss: 2.5356035232543945\n",
      "Iteration 1536 Loss: 1.7423830032348633\n",
      "Iteration 1537 Loss: 2.230952501296997\n",
      "Iteration 1538 Loss: 2.4068830013275146\n",
      "Iteration 1539 Loss: 2.5982213020324707\n",
      "Iteration 1539 Loss: 2.1076743602752686\n",
      "Iteration 1540 Loss: 2.0631279945373535\n",
      "Iteration 1541 Loss: 2.5449860095977783\n",
      "Iteration 1542 Loss: 1.7921488285064697\n",
      "Iteration 1543 Loss: 2.5640950202941895\n",
      "Iteration 1544 Loss: 1.7686989307403564\n",
      "Iteration 1545 Loss: 2.25986385345459\n",
      "Iteration 1546 Loss: 2.1051418781280518\n",
      "Iteration 1547 Loss: 2.291400671005249\n",
      "Iteration 1548 Loss: 2.2475786209106445\n",
      "Iteration 1549 Loss: 2.660109043121338\n",
      "Iteration 1549 Loss: 2.22971510887146\n",
      "Iteration 1550 Loss: 1.9985169172286987\n",
      "Iteration 1551 Loss: 1.9417190551757812\n",
      "Iteration 1552 Loss: 1.5546422004699707\n",
      "Iteration 1553 Loss: 1.7469316720962524\n",
      "Iteration 1554 Loss: 1.4330382347106934\n",
      "Iteration 1555 Loss: 2.32973575592041\n",
      "Iteration 1556 Loss: 1.600602626800537\n",
      "Iteration 1557 Loss: 1.7426701784133911\n",
      "Iteration 1558 Loss: 1.8791000843048096\n",
      "Iteration 1559 Loss: 1.8402531147003174\n",
      "Iteration 1559 Loss: 1.8067209720611572\n",
      "Iteration 1560 Loss: 1.9333492517471313\n",
      "Iteration 1561 Loss: 1.7423216104507446\n",
      "Iteration 1562 Loss: 2.2457644939422607\n",
      "Iteration 1563 Loss: 1.820062279701233\n",
      "Iteration 1564 Loss: 1.657740831375122\n",
      "Iteration 1565 Loss: 2.0507092475891113\n",
      "Iteration 1566 Loss: 1.9856271743774414\n",
      "Iteration 1567 Loss: 1.5959185361862183\n",
      "Iteration 1568 Loss: 1.627821922302246\n",
      "Iteration 1569 Loss: 1.845489501953125\n",
      "Iteration 1569 Loss: 1.8504804372787476\n",
      "Iteration 1570 Loss: 1.9085607528686523\n",
      "Iteration 1571 Loss: 1.9917504787445068\n",
      "Iteration 1572 Loss: 1.8285198211669922\n",
      "Iteration 1573 Loss: 1.684945821762085\n",
      "Iteration 1574 Loss: 1.8293533325195312\n",
      "Iteration 1575 Loss: 2.0485036373138428\n",
      "Iteration 1576 Loss: 1.936343789100647\n",
      "Iteration 1577 Loss: 1.6941688060760498\n",
      "Iteration 1578 Loss: 1.4937163591384888\n",
      "Iteration 1579 Loss: 2.2167346477508545\n",
      "Iteration 1579 Loss: 1.8632596731185913\n",
      "Iteration 1580 Loss: 1.7113895416259766\n",
      "Iteration 1581 Loss: 1.982140064239502\n",
      "Iteration 1582 Loss: 1.7210955619812012\n",
      "Iteration 1583 Loss: 1.8729846477508545\n",
      "Iteration 1584 Loss: 1.9626439809799194\n",
      "Iteration 1585 Loss: 2.2870755195617676\n",
      "Iteration 1586 Loss: 1.4330475330352783\n",
      "Iteration 1587 Loss: 2.938079595565796\n",
      "Iteration 1588 Loss: 1.869215726852417\n",
      "Iteration 1589 Loss: 1.949398159980774\n",
      "Iteration 1589 Loss: 1.9727071523666382\n",
      "Iteration 1590 Loss: 1.8667314052581787\n",
      "Iteration 1591 Loss: 2.3544583320617676\n",
      "Iteration 1592 Loss: 1.6588436365127563\n",
      "Iteration 1593 Loss: 2.6016008853912354\n",
      "Iteration 1594 Loss: 1.7718943357467651\n",
      "Iteration 1595 Loss: 2.07027006149292\n",
      "Iteration 1596 Loss: 2.1790850162506104\n",
      "Iteration 1597 Loss: 1.7727659940719604\n",
      "Iteration 1598 Loss: 1.7197859287261963\n",
      "Iteration 1599 Loss: 1.7124230861663818\n",
      "Iteration 1599 Loss: 1.9707858562469482\n",
      "Iteration 1600 Loss: 2.258589029312134\n",
      "Iteration 1601 Loss: 2.2085509300231934\n",
      "Iteration 1602 Loss: 2.344738006591797\n",
      "Iteration 1603 Loss: 1.9781080484390259\n",
      "Iteration 1604 Loss: 2.0355584621429443\n",
      "Iteration 1605 Loss: 1.6557003259658813\n",
      "Iteration 1606 Loss: 2.582289457321167\n",
      "Iteration 1607 Loss: 1.9463368654251099\n",
      "Iteration 1608 Loss: 2.540804862976074\n",
      "Iteration 1609 Loss: 2.175388813018799\n",
      "Iteration 1609 Loss: 2.1726064682006836\n",
      "Iteration 1610 Loss: 1.7473698854446411\n",
      "Iteration 1611 Loss: 1.7403236627578735\n",
      "Iteration 1612 Loss: 2.0637686252593994\n",
      "Iteration 1613 Loss: 1.8806862831115723\n",
      "Iteration 1614 Loss: 2.2820723056793213\n",
      "Iteration 1615 Loss: 1.558423399925232\n",
      "Iteration 1616 Loss: 2.0352139472961426\n",
      "Iteration 1617 Loss: 2.2199409008026123\n",
      "Iteration 1618 Loss: 2.3573195934295654\n",
      "Iteration 1619 Loss: 2.2634992599487305\n",
      "Iteration 1619 Loss: 2.014861583709717\n",
      "Iteration 1620 Loss: 2.0139214992523193\n",
      "Iteration 1621 Loss: 2.0429069995880127\n",
      "Iteration 1622 Loss: 1.9300013780593872\n",
      "Iteration 1623 Loss: 1.382436752319336\n",
      "Iteration 1624 Loss: 1.9377888441085815\n",
      "Iteration 1625 Loss: 2.147434711456299\n",
      "Iteration 1626 Loss: 1.8871527910232544\n",
      "Iteration 1627 Loss: 1.9415230751037598\n",
      "Iteration 1628 Loss: 1.9047458171844482\n",
      "Iteration 1629 Loss: 2.180068016052246\n",
      "Iteration 1629 Loss: 1.936798095703125\n",
      "Iteration 1630 Loss: 2.138683557510376\n",
      "Iteration 1631 Loss: 1.6906075477600098\n",
      "Iteration 1632 Loss: 1.9477274417877197\n",
      "Iteration 1633 Loss: 1.6246814727783203\n",
      "Iteration 1634 Loss: 2.3706817626953125\n",
      "Iteration 1635 Loss: 2.7653534412384033\n",
      "Iteration 1636 Loss: 1.8794364929199219\n",
      "Iteration 1637 Loss: 1.9744330644607544\n",
      "Iteration 1638 Loss: 1.9370352029800415\n",
      "Iteration 1639 Loss: 2.2259438037872314\n",
      "Iteration 1639 Loss: 2.0554585456848145\n",
      "Iteration 1640 Loss: 1.6125471591949463\n",
      "Iteration 1641 Loss: 2.0678207874298096\n",
      "Iteration 1642 Loss: 2.0437204837799072\n",
      "Iteration 1643 Loss: 1.5121757984161377\n",
      "Iteration 1644 Loss: 1.5441217422485352\n",
      "Iteration 1645 Loss: 2.0260677337646484\n",
      "Iteration 1646 Loss: 2.5057430267333984\n",
      "Iteration 1647 Loss: 2.254990577697754\n",
      "Iteration 1648 Loss: 1.3395172357559204\n",
      "Iteration 1649 Loss: 2.3345420360565186\n",
      "Iteration 1649 Loss: 1.9241244792938232\n",
      "Iteration 1650 Loss: 2.063025951385498\n",
      "Iteration 1651 Loss: 1.954187035560608\n",
      "Iteration 1652 Loss: 1.9157335758209229\n",
      "Iteration 1653 Loss: 2.3489298820495605\n",
      "Iteration 1654 Loss: 2.3187949657440186\n",
      "Iteration 1655 Loss: 1.828059196472168\n",
      "Iteration 1656 Loss: 1.5421384572982788\n",
      "Iteration 1657 Loss: 2.1828713417053223\n",
      "Iteration 1658 Loss: 2.7423477172851562\n",
      "Iteration 1659 Loss: 2.350059747695923\n",
      "Iteration 1659 Loss: 2.124614715576172\n",
      "Iteration 1660 Loss: 2.0253052711486816\n",
      "Iteration 1661 Loss: 2.1291277408599854\n",
      "Iteration 1662 Loss: 1.656934142112732\n",
      "Iteration 1663 Loss: 1.4590352773666382\n",
      "Iteration 1664 Loss: 2.253924608230591\n",
      "Iteration 1665 Loss: 1.5234640836715698\n",
      "Iteration 1666 Loss: 2.5379505157470703\n",
      "Iteration 1667 Loss: 1.9991170167922974\n",
      "Iteration 1668 Loss: 1.4448574781417847\n",
      "Iteration 1669 Loss: 2.1515207290649414\n",
      "Iteration 1669 Loss: 1.9181236028671265\n",
      "Iteration 1670 Loss: 1.7339719533920288\n",
      "Iteration 1671 Loss: 2.0215747356414795\n",
      "Iteration 1672 Loss: 2.05871844291687\n",
      "Iteration 1673 Loss: 2.2905421257019043\n",
      "Iteration 1674 Loss: 2.1044628620147705\n",
      "Iteration 1675 Loss: 2.2441182136535645\n",
      "Iteration 1676 Loss: 2.065898895263672\n",
      "Iteration 1677 Loss: 1.7976055145263672\n",
      "Iteration 1678 Loss: 1.8747323751449585\n",
      "Iteration 1679 Loss: 1.6019868850708008\n",
      "Iteration 1679 Loss: 1.9793612957000732\n",
      "Iteration 1680 Loss: 2.7075438499450684\n",
      "Iteration 1681 Loss: 2.0916976928710938\n",
      "Iteration 1682 Loss: 1.9438560009002686\n",
      "Iteration 1683 Loss: 2.021155834197998\n",
      "Iteration 1684 Loss: 1.77153480052948\n",
      "Iteration 1685 Loss: 2.297736883163452\n",
      "Iteration 1686 Loss: 1.270043134689331\n",
      "Iteration 1687 Loss: 1.720440149307251\n",
      "Iteration 1688 Loss: 1.7401524782180786\n",
      "Iteration 1689 Loss: 1.829484462738037\n",
      "Iteration 1689 Loss: 1.9393644332885742\n",
      "Iteration 1690 Loss: 2.0261449813842773\n",
      "Iteration 1691 Loss: 1.8274273872375488\n",
      "Iteration 1692 Loss: 1.7310335636138916\n",
      "Iteration 1693 Loss: 1.40482759475708\n",
      "Iteration 1694 Loss: 1.8804538249969482\n",
      "Iteration 1695 Loss: 1.7335270643234253\n",
      "Iteration 1696 Loss: 2.129340410232544\n",
      "Iteration 1697 Loss: 1.967321515083313\n",
      "Iteration 1698 Loss: 1.7612792253494263\n",
      "Iteration 1699 Loss: 2.0752792358398438\n",
      "Iteration 1699 Loss: 1.853663682937622\n",
      "Iteration 1700 Loss: 2.038498878479004\n",
      "Iteration 1701 Loss: 1.9540241956710815\n",
      "Iteration 1702 Loss: 2.535677433013916\n",
      "Iteration 1703 Loss: 2.4071128368377686\n",
      "Iteration 1704 Loss: 1.9046269655227661\n",
      "Iteration 1705 Loss: 1.7899082899093628\n",
      "Iteration 1706 Loss: 1.6372334957122803\n",
      "Iteration 1707 Loss: 2.1025025844573975\n",
      "Iteration 1708 Loss: 2.1153275966644287\n",
      "Iteration 1709 Loss: 1.5867679119110107\n",
      "Iteration 1709 Loss: 2.0071680545806885\n",
      "Iteration 1710 Loss: 2.680842161178589\n",
      "Iteration 1711 Loss: 1.9344549179077148\n",
      "Iteration 1712 Loss: 1.75494384765625\n",
      "Iteration 1713 Loss: 2.3789432048797607\n",
      "Iteration 1714 Loss: 1.4561353921890259\n",
      "Iteration 1715 Loss: 2.6982760429382324\n",
      "Iteration 1716 Loss: 2.3035836219787598\n",
      "Iteration 1717 Loss: 2.9243366718292236\n",
      "Iteration 1718 Loss: 1.7298974990844727\n",
      "Iteration 1719 Loss: 2.90321946144104\n",
      "Iteration 1719 Loss: 2.276463270187378\n",
      "Iteration 1720 Loss: 1.4410322904586792\n",
      "Iteration 1721 Loss: 2.252730369567871\n",
      "Iteration 1722 Loss: 2.2402634620666504\n",
      "Iteration 1723 Loss: 2.552950859069824\n",
      "Iteration 1724 Loss: 1.9001456499099731\n",
      "Iteration 1725 Loss: 2.6908278465270996\n",
      "Iteration 1726 Loss: 2.0060088634490967\n",
      "Iteration 1727 Loss: 2.5080959796905518\n",
      "Iteration 1728 Loss: 1.584533452987671\n",
      "Iteration 1729 Loss: 2.4531118869781494\n",
      "Iteration 1729 Loss: 2.1629700660705566\n",
      "Iteration 1730 Loss: 2.426877737045288\n",
      "Iteration 1731 Loss: 2.547579288482666\n",
      "Iteration 1732 Loss: 1.8867876529693604\n",
      "Iteration 1733 Loss: 2.3792920112609863\n",
      "Iteration 1734 Loss: 1.9726569652557373\n",
      "Iteration 1735 Loss: 2.7951912879943848\n",
      "Iteration 1736 Loss: 1.8567073345184326\n",
      "Iteration 1737 Loss: 2.149651527404785\n",
      "Iteration 1738 Loss: 1.8519068956375122\n",
      "Iteration 1739 Loss: 1.861816167831421\n",
      "Iteration 1739 Loss: 2.172846555709839\n",
      "Iteration 1740 Loss: 2.132993698120117\n",
      "Iteration 1741 Loss: 2.000610113143921\n",
      "Iteration 1742 Loss: 1.9915227890014648\n",
      "Iteration 1743 Loss: 1.795003890991211\n",
      "Iteration 1744 Loss: 2.1290574073791504\n",
      "Iteration 1745 Loss: 2.0694477558135986\n",
      "Iteration 1746 Loss: 2.287020206451416\n",
      "Iteration 1747 Loss: 2.124997138977051\n",
      "Iteration 1748 Loss: 1.8876181840896606\n",
      "Iteration 1749 Loss: 2.3949334621429443\n",
      "Iteration 1749 Loss: 2.0813205242156982\n",
      "Iteration 1750 Loss: 1.742397427558899\n",
      "Iteration 1751 Loss: 2.3677122592926025\n",
      "Iteration 1752 Loss: 1.8382909297943115\n",
      "Iteration 1753 Loss: 1.6924176216125488\n",
      "Iteration 1754 Loss: 2.0209708213806152\n",
      "Iteration 1755 Loss: 2.062770128250122\n",
      "Iteration 1756 Loss: 2.141176462173462\n",
      "Iteration 1757 Loss: 1.685854434967041\n",
      "Iteration 1758 Loss: 2.1801791191101074\n",
      "Iteration 1759 Loss: 1.2290536165237427\n",
      "Iteration 1759 Loss: 1.8960822820663452\n",
      "Iteration 1760 Loss: 1.8855056762695312\n",
      "Iteration 1761 Loss: 1.7611541748046875\n",
      "Iteration 1762 Loss: 1.9515937566757202\n",
      "Iteration 1763 Loss: 2.2490432262420654\n",
      "Iteration 1764 Loss: 2.0586326122283936\n",
      "Iteration 1765 Loss: 2.053971767425537\n",
      "Iteration 1766 Loss: 1.7273107767105103\n",
      "Iteration 1767 Loss: 1.8141531944274902\n",
      "Iteration 1768 Loss: 1.8713791370391846\n",
      "Iteration 1769 Loss: 1.5788919925689697\n",
      "Iteration 1769 Loss: 1.8951637744903564\n",
      "Iteration 1770 Loss: 1.7406381368637085\n",
      "Iteration 1771 Loss: 2.0793755054473877\n",
      "Iteration 1772 Loss: 2.222456216812134\n",
      "Iteration 1773 Loss: 1.6055066585540771\n",
      "Iteration 1774 Loss: 2.324828863143921\n",
      "Iteration 1775 Loss: 1.4853541851043701\n",
      "Iteration 1776 Loss: 2.0252530574798584\n",
      "Iteration 1777 Loss: 2.6596102714538574\n",
      "Iteration 1778 Loss: 1.8236275911331177\n",
      "Iteration 1779 Loss: 1.65261971950531\n",
      "Iteration 1779 Loss: 1.961927056312561\n",
      "Iteration 1780 Loss: 1.8098944425582886\n",
      "Iteration 1781 Loss: 2.209418773651123\n",
      "Iteration 1782 Loss: 1.6894423961639404\n",
      "Iteration 1783 Loss: 2.2045674324035645\n",
      "Iteration 1784 Loss: 2.4385478496551514\n",
      "Iteration 1785 Loss: 1.9036648273468018\n",
      "Iteration 1786 Loss: 1.4815884828567505\n",
      "Iteration 1787 Loss: 1.8693456649780273\n",
      "Iteration 1788 Loss: 1.883687973022461\n",
      "Iteration 1789 Loss: 2.480133056640625\n",
      "Iteration 1789 Loss: 1.9970290660858154\n",
      "Iteration 1790 Loss: 2.2522635459899902\n",
      "Iteration 1791 Loss: 1.5479234457015991\n",
      "Iteration 1792 Loss: 2.1302859783172607\n",
      "Iteration 1793 Loss: 2.0439677238464355\n",
      "Iteration 1794 Loss: 1.7451508045196533\n",
      "Iteration 1795 Loss: 2.113776445388794\n",
      "Iteration 1796 Loss: 2.4407005310058594\n",
      "Iteration 1797 Loss: 1.3928080797195435\n",
      "Iteration 1798 Loss: 2.120143175125122\n",
      "Iteration 1799 Loss: 1.3059821128845215\n",
      "Iteration 1799 Loss: 1.9093002080917358\n",
      "Iteration 1800 Loss: 1.7396923303604126\n",
      "Iteration 1801 Loss: 1.361517310142517\n",
      "Iteration 1802 Loss: 1.767438292503357\n",
      "Iteration 1803 Loss: 2.364315986633301\n",
      "Iteration 1804 Loss: 1.3888332843780518\n",
      "Iteration 1805 Loss: 1.6883385181427002\n",
      "Iteration 1806 Loss: 2.2003543376922607\n",
      "Iteration 1807 Loss: 1.752417802810669\n",
      "Iteration 1808 Loss: 1.657621145248413\n",
      "Iteration 1809 Loss: 1.6556744575500488\n",
      "Iteration 1809 Loss: 1.7576204538345337\n",
      "Iteration 1810 Loss: 1.7109405994415283\n",
      "Iteration 1811 Loss: 1.350155234336853\n",
      "Iteration 1812 Loss: 2.094454765319824\n",
      "Iteration 1813 Loss: 2.3352952003479004\n",
      "Iteration 1814 Loss: 1.8464031219482422\n",
      "Iteration 1815 Loss: 2.1129684448242188\n",
      "Iteration 1816 Loss: 2.0096707344055176\n",
      "Iteration 1817 Loss: 2.1761646270751953\n",
      "Iteration 1818 Loss: 1.9488589763641357\n",
      "Iteration 1819 Loss: 1.1441810131072998\n",
      "Iteration 1819 Loss: 1.8729091882705688\n",
      "Iteration 1820 Loss: 1.09367036819458\n",
      "Iteration 1821 Loss: 2.349541425704956\n",
      "Iteration 1822 Loss: 2.0286293029785156\n",
      "Iteration 1823 Loss: 1.9279403686523438\n",
      "Iteration 1824 Loss: 1.8214802742004395\n",
      "Iteration 1825 Loss: 2.0683753490448\n",
      "Iteration 1826 Loss: 1.615445852279663\n",
      "Iteration 1827 Loss: 1.74947190284729\n",
      "Iteration 1828 Loss: 1.639883279800415\n",
      "Iteration 1829 Loss: 1.6700366735458374\n",
      "Iteration 1829 Loss: 1.7964473962783813\n",
      "Iteration 1830 Loss: 2.131283760070801\n",
      "Iteration 1831 Loss: 1.6499382257461548\n",
      "Iteration 1832 Loss: 1.8983957767486572\n",
      "Iteration 1833 Loss: 2.35138201713562\n",
      "Iteration 1834 Loss: 1.6175154447555542\n",
      "Iteration 1835 Loss: 2.0777885913848877\n",
      "Iteration 1836 Loss: 1.747780680656433\n",
      "Iteration 1837 Loss: 1.7869243621826172\n",
      "Iteration 1838 Loss: 1.8053799867630005\n",
      "Iteration 1839 Loss: 1.8415170907974243\n",
      "Iteration 1839 Loss: 1.890790581703186\n",
      "Iteration 1840 Loss: 2.1476516723632812\n",
      "Iteration 1841 Loss: 1.9097747802734375\n",
      "Iteration 1842 Loss: 2.3777241706848145\n",
      "Iteration 1843 Loss: 1.8586891889572144\n",
      "Iteration 1844 Loss: 2.177657127380371\n",
      "Iteration 1845 Loss: 0.9545794725418091\n",
      "Iteration 1846 Loss: 1.5062090158462524\n",
      "Iteration 1847 Loss: 2.096203088760376\n",
      "Iteration 1848 Loss: 2.7680091857910156\n",
      "Iteration 1849 Loss: 1.8233177661895752\n",
      "Iteration 1849 Loss: 1.9619815349578857\n",
      "Iteration 1850 Loss: 2.311516284942627\n",
      "Iteration 1851 Loss: 1.3936386108398438\n",
      "Iteration 1852 Loss: 1.5807530879974365\n",
      "Iteration 1853 Loss: 2.225879669189453\n",
      "Iteration 1854 Loss: 1.3388725519180298\n",
      "Iteration 1855 Loss: 1.8437776565551758\n",
      "Iteration 1856 Loss: 1.4455515146255493\n",
      "Iteration 1857 Loss: 1.3211901187896729\n",
      "Iteration 1858 Loss: 1.8310716152191162\n",
      "Iteration 1859 Loss: 2.4169280529022217\n",
      "Iteration 1859 Loss: 1.7709178924560547\n",
      "Iteration 1860 Loss: 1.9959745407104492\n",
      "Iteration 1861 Loss: 2.101806402206421\n",
      "Iteration 1862 Loss: 1.6631664037704468\n",
      "Iteration 1863 Loss: 1.4590359926223755\n",
      "Iteration 1864 Loss: 2.191232204437256\n",
      "Iteration 1865 Loss: 2.930203437805176\n",
      "Iteration 1866 Loss: 1.7089147567749023\n",
      "Iteration 1867 Loss: 1.4913052320480347\n",
      "Iteration 1868 Loss: 1.688929557800293\n",
      "Iteration 1869 Loss: 2.543222188949585\n",
      "Iteration 1869 Loss: 1.9773790836334229\n",
      "Iteration 1870 Loss: 1.6923054456710815\n",
      "Iteration 1871 Loss: 1.7072187662124634\n",
      "Iteration 1872 Loss: 2.1358284950256348\n",
      "Iteration 1873 Loss: 1.6925872564315796\n",
      "Iteration 1874 Loss: 1.8703415393829346\n",
      "Iteration 1875 Loss: 2.1438534259796143\n",
      "Iteration 1876 Loss: 2.0348267555236816\n",
      "Iteration 1877 Loss: 1.782496690750122\n",
      "Iteration 1878 Loss: 2.3429529666900635\n",
      "Iteration 1879 Loss: 1.4532583951950073\n",
      "Iteration 1879 Loss: 1.8855669498443604\n",
      "Iteration 1880 Loss: 2.160306692123413\n",
      "Iteration 1881 Loss: 1.9165222644805908\n",
      "Iteration 1882 Loss: 1.5644382238388062\n",
      "Iteration 1883 Loss: 1.121619701385498\n",
      "Iteration 1884 Loss: 2.6355223655700684\n",
      "Iteration 1885 Loss: 1.7176735401153564\n",
      "Iteration 1886 Loss: 2.357107162475586\n",
      "Iteration 1887 Loss: 1.561643123626709\n",
      "Iteration 1888 Loss: 1.738333821296692\n",
      "Iteration 1889 Loss: 2.265857219696045\n",
      "Iteration 1889 Loss: 1.903902292251587\n",
      "Iteration 1890 Loss: 1.5255376100540161\n",
      "Iteration 1891 Loss: 2.0148212909698486\n",
      "Iteration 1892 Loss: 1.817196249961853\n",
      "Iteration 1893 Loss: 1.7192529439926147\n",
      "Iteration 1894 Loss: 2.662411689758301\n",
      "Iteration 1895 Loss: 2.316749095916748\n",
      "Iteration 1896 Loss: 1.937699794769287\n",
      "Iteration 1897 Loss: 2.0257182121276855\n",
      "Iteration 1898 Loss: 1.76149582862854\n",
      "Iteration 1899 Loss: 1.2011147737503052\n",
      "Iteration 1899 Loss: 1.8981997966766357\n",
      "Iteration 1900 Loss: 1.7469419240951538\n",
      "Iteration 1901 Loss: 1.9682482481002808\n",
      "Iteration 1902 Loss: 2.6144416332244873\n",
      "Iteration 1903 Loss: 1.883402705192566\n",
      "Iteration 1904 Loss: 1.5722743272781372\n",
      "Iteration 1905 Loss: 1.8399642705917358\n",
      "Iteration 1906 Loss: 1.8837676048278809\n",
      "Iteration 1907 Loss: 2.624218463897705\n",
      "Iteration 1908 Loss: 1.464817762374878\n",
      "Iteration 1909 Loss: 1.6820075511932373\n",
      "Iteration 1909 Loss: 1.9280084371566772\n",
      "Iteration 1910 Loss: 1.7870835065841675\n",
      "Iteration 1911 Loss: 1.612019419670105\n",
      "Iteration 1912 Loss: 2.0692543983459473\n",
      "Iteration 1913 Loss: 1.9444411993026733\n",
      "Iteration 1914 Loss: 1.83591890335083\n",
      "Iteration 1915 Loss: 1.9778093099594116\n",
      "Iteration 1916 Loss: 1.1782687902450562\n",
      "Iteration 1917 Loss: 1.0615031719207764\n",
      "Iteration 1918 Loss: 2.3377184867858887\n",
      "Iteration 1919 Loss: 2.2037453651428223\n",
      "Iteration 1919 Loss: 1.8007761240005493\n",
      "Iteration 1920 Loss: 1.9013850688934326\n",
      "Iteration 1921 Loss: 1.8393096923828125\n",
      "Iteration 1922 Loss: 2.492218255996704\n",
      "Iteration 1923 Loss: 2.001936435699463\n",
      "Iteration 1924 Loss: 1.736854076385498\n",
      "Iteration 1925 Loss: 2.602541446685791\n",
      "Iteration 1926 Loss: 1.8132942914962769\n",
      "Iteration 1927 Loss: 1.8966126441955566\n",
      "Iteration 1928 Loss: 2.0395970344543457\n",
      "Iteration 1929 Loss: 1.4229706525802612\n",
      "Iteration 1929 Loss: 1.974671721458435\n",
      "Iteration 1930 Loss: 2.369533061981201\n",
      "Iteration 1931 Loss: 1.5876588821411133\n",
      "Iteration 1932 Loss: 1.7231796979904175\n",
      "Iteration 1933 Loss: 2.060579538345337\n",
      "Iteration 1934 Loss: 1.9489041566848755\n",
      "Iteration 1935 Loss: 1.9641610383987427\n",
      "Iteration 1936 Loss: 1.3015557527542114\n",
      "Iteration 1937 Loss: 1.8432852029800415\n",
      "Iteration 1938 Loss: 1.8073612451553345\n",
      "Iteration 1939 Loss: 2.218127965927124\n",
      "Iteration 1939 Loss: 1.882434606552124\n",
      "Iteration 1940 Loss: 1.5153383016586304\n",
      "Iteration 1941 Loss: 1.5225048065185547\n",
      "Iteration 1942 Loss: 1.36222505569458\n",
      "Iteration 1943 Loss: 2.7731173038482666\n",
      "Iteration 1944 Loss: 2.3160271644592285\n",
      "Iteration 1945 Loss: 1.882439374923706\n",
      "Iteration 1946 Loss: 1.4769154787063599\n",
      "Iteration 1947 Loss: 1.6950331926345825\n",
      "Iteration 1948 Loss: 1.933031678199768\n",
      "Iteration 1949 Loss: 2.3191494941711426\n",
      "Iteration 1949 Loss: 1.8795783519744873\n",
      "Iteration 1950 Loss: 1.9857964515686035\n",
      "Iteration 1951 Loss: 1.5551304817199707\n",
      "Iteration 1952 Loss: 2.1049094200134277\n",
      "Iteration 1953 Loss: 2.796227216720581\n",
      "Iteration 1954 Loss: 1.9375823736190796\n",
      "Iteration 1955 Loss: 1.829248309135437\n",
      "Iteration 1956 Loss: 1.7117797136306763\n",
      "Iteration 1957 Loss: 1.960687518119812\n",
      "Iteration 1958 Loss: 1.499924659729004\n",
      "Iteration 1959 Loss: 2.4299302101135254\n",
      "Iteration 1959 Loss: 1.9811216592788696\n",
      "Iteration 1960 Loss: 1.923712968826294\n",
      "Iteration 1961 Loss: 1.8224183320999146\n",
      "Iteration 1962 Loss: 1.8901586532592773\n",
      "Iteration 1963 Loss: 2.0455193519592285\n",
      "Iteration 1964 Loss: 1.3319971561431885\n",
      "Iteration 1965 Loss: 2.1618154048919678\n",
      "Iteration 1966 Loss: 2.004164218902588\n",
      "Iteration 1967 Loss: 1.969560146331787\n",
      "Iteration 1968 Loss: 1.86263108253479\n",
      "Iteration 1969 Loss: 1.4886460304260254\n",
      "Iteration 1969 Loss: 1.850062370300293\n",
      "Iteration 1970 Loss: 1.6467493772506714\n",
      "Iteration 1971 Loss: 2.6487326622009277\n",
      "Iteration 1972 Loss: 1.6945253610610962\n",
      "Iteration 1973 Loss: 2.1274893283843994\n",
      "Iteration 1974 Loss: 1.1595219373703003\n",
      "Iteration 1975 Loss: 1.6505100727081299\n",
      "Iteration 1976 Loss: 2.41645884513855\n",
      "Iteration 1977 Loss: 2.6604151725769043\n",
      "Iteration 1978 Loss: 1.640433430671692\n",
      "Iteration 1979 Loss: 1.7151461839675903\n",
      "Iteration 1979 Loss: 1.9359982013702393\n",
      "Iteration 1980 Loss: 1.7853069305419922\n",
      "Iteration 1981 Loss: 1.4507627487182617\n",
      "Iteration 1982 Loss: 2.0485007762908936\n",
      "Iteration 1983 Loss: 1.8957178592681885\n",
      "Iteration 1984 Loss: 1.8694207668304443\n",
      "Iteration 1985 Loss: 1.8948915004730225\n",
      "Iteration 1986 Loss: 2.2349979877471924\n",
      "Iteration 1987 Loss: 1.6207891702651978\n",
      "Iteration 1988 Loss: 2.028362274169922\n",
      "Iteration 1989 Loss: 1.0923089981079102\n",
      "Iteration 1989 Loss: 1.7921059131622314\n",
      "Iteration 1990 Loss: 2.519636869430542\n",
      "Iteration 1991 Loss: 1.7072681188583374\n",
      "Iteration 1992 Loss: 1.9277392625808716\n",
      "Iteration 1993 Loss: 2.482738971710205\n",
      "Iteration 1994 Loss: 2.446960687637329\n",
      "Iteration 1995 Loss: 2.2659778594970703\n",
      "Iteration 1996 Loss: 2.128203868865967\n",
      "Iteration 1997 Loss: 2.1870298385620117\n",
      "Iteration 1998 Loss: 1.9995228052139282\n",
      "Iteration 1999 Loss: 1.7039834260940552\n",
      "Iteration 1999 Loss: 2.136906147003174\n",
      "Iteration 2000 Loss: 2.215578556060791\n",
      "Iteration 2001 Loss: 1.7017275094985962\n",
      "Iteration 2002 Loss: 1.988981008529663\n",
      "Iteration 2003 Loss: 2.131682872772217\n",
      "Iteration 2004 Loss: 1.595402479171753\n",
      "Iteration 2005 Loss: 2.2074193954467773\n",
      "Iteration 2006 Loss: 2.4050521850585938\n",
      "Iteration 2007 Loss: 1.7031265497207642\n",
      "Iteration 2008 Loss: 1.8438873291015625\n",
      "Iteration 2009 Loss: 1.9847707748413086\n",
      "Iteration 2009 Loss: 1.9777629375457764\n",
      "Iteration 2010 Loss: 1.8611280918121338\n",
      "Iteration 2011 Loss: 1.9114112854003906\n",
      "Iteration 2012 Loss: 2.146318197250366\n",
      "Iteration 2013 Loss: 1.6471264362335205\n",
      "Iteration 2014 Loss: 1.7272647619247437\n",
      "Iteration 2015 Loss: 2.535041093826294\n",
      "Iteration 2016 Loss: 1.4603530168533325\n",
      "Iteration 2017 Loss: 1.4479176998138428\n",
      "Iteration 2018 Loss: 1.4293186664581299\n",
      "Iteration 2019 Loss: 2.0175039768218994\n",
      "Iteration 2019 Loss: 1.818338394165039\n",
      "Iteration 2020 Loss: 1.7314943075180054\n",
      "Iteration 2021 Loss: 1.742389440536499\n",
      "Iteration 2022 Loss: 1.6550588607788086\n",
      "Iteration 2023 Loss: 1.5467480421066284\n",
      "Iteration 2024 Loss: 1.8307515382766724\n",
      "Iteration 2025 Loss: 1.9025853872299194\n",
      "Iteration 2026 Loss: 1.8086203336715698\n",
      "Iteration 2027 Loss: 2.0084526538848877\n",
      "Iteration 2028 Loss: 1.7945984601974487\n",
      "Iteration 2029 Loss: 2.079420566558838\n",
      "Iteration 2029 Loss: 1.8100121021270752\n",
      "Iteration 2030 Loss: 1.5976649522781372\n",
      "Iteration 2031 Loss: 1.7247061729431152\n",
      "Iteration 2032 Loss: 2.116590976715088\n",
      "Iteration 2033 Loss: 1.6077351570129395\n",
      "Iteration 2034 Loss: 1.863171935081482\n",
      "Iteration 2035 Loss: 1.1542658805847168\n",
      "Iteration 2036 Loss: 1.8083384037017822\n",
      "Iteration 2037 Loss: 2.4433889389038086\n",
      "Iteration 2038 Loss: 1.6311312913894653\n",
      "Iteration 2039 Loss: 1.9666590690612793\n",
      "Iteration 2039 Loss: 1.7913650274276733\n",
      "Iteration 2040 Loss: 1.8984733819961548\n",
      "Iteration 2041 Loss: 2.1541879177093506\n",
      "Iteration 2042 Loss: 1.6359078884124756\n",
      "Iteration 2043 Loss: 1.903071641921997\n",
      "Iteration 2044 Loss: 1.5618512630462646\n",
      "Iteration 2045 Loss: 1.6812019348144531\n",
      "Iteration 2046 Loss: 1.975974678993225\n",
      "Iteration 2047 Loss: 1.1927434206008911\n",
      "Iteration 2048 Loss: 1.452774167060852\n",
      "Iteration 2049 Loss: 2.1826765537261963\n",
      "Iteration 2049 Loss: 1.7638862133026123\n",
      "Iteration 2050 Loss: 1.9557334184646606\n",
      "Iteration 2051 Loss: 1.8806720972061157\n",
      "Iteration 2052 Loss: 1.8947192430496216\n",
      "Iteration 2053 Loss: 1.9518853425979614\n",
      "Iteration 2054 Loss: 2.364907741546631\n",
      "Iteration 2055 Loss: 1.318467617034912\n",
      "Iteration 2056 Loss: 1.9835128784179688\n",
      "Iteration 2057 Loss: 1.6978425979614258\n",
      "Iteration 2058 Loss: 1.8244816064834595\n",
      "Iteration 2059 Loss: 1.563671588897705\n",
      "Iteration 2059 Loss: 1.843589425086975\n",
      "Iteration 2060 Loss: 1.6861259937286377\n",
      "Iteration 2061 Loss: 2.3375539779663086\n",
      "Iteration 2062 Loss: 1.8301026821136475\n",
      "Iteration 2063 Loss: 2.6739468574523926\n",
      "Iteration 2064 Loss: 1.3781412839889526\n",
      "Iteration 2065 Loss: 1.6516666412353516\n",
      "Iteration 2066 Loss: 1.833211898803711\n",
      "Iteration 2067 Loss: 1.6024564504623413\n",
      "Iteration 2068 Loss: 1.9974384307861328\n",
      "Iteration 2069 Loss: 1.508750319480896\n",
      "Iteration 2069 Loss: 1.8499395847320557\n",
      "Iteration 2070 Loss: 1.5462466478347778\n",
      "Iteration 2071 Loss: 2.732496500015259\n",
      "Iteration 2072 Loss: 1.6619757413864136\n",
      "Iteration 2073 Loss: 1.7912952899932861\n",
      "Iteration 2074 Loss: 1.729186773300171\n",
      "Iteration 2075 Loss: 1.5670462846755981\n",
      "Iteration 2076 Loss: 1.8043944835662842\n",
      "Iteration 2077 Loss: 1.9847458600997925\n",
      "Iteration 2078 Loss: 1.6602421998977661\n",
      "Iteration 2079 Loss: 1.3436025381088257\n",
      "Iteration 2079 Loss: 1.7821232080459595\n",
      "Iteration 2080 Loss: 2.789430856704712\n",
      "Iteration 2081 Loss: 1.8600927591323853\n",
      "Iteration 2082 Loss: 2.373138904571533\n",
      "Iteration 2083 Loss: 2.0006911754608154\n",
      "Iteration 2084 Loss: 2.274430990219116\n",
      "Iteration 2085 Loss: 2.0140743255615234\n",
      "Iteration 2086 Loss: 1.7427873611450195\n",
      "Iteration 2087 Loss: 1.7530566453933716\n",
      "Iteration 2088 Loss: 1.201894760131836\n",
      "Iteration 2089 Loss: 1.60147225856781\n",
      "Iteration 2089 Loss: 1.9611070156097412\n",
      "Iteration 2090 Loss: 1.821009635925293\n",
      "Iteration 2091 Loss: 2.3135910034179688\n",
      "Iteration 2092 Loss: 1.6352977752685547\n",
      "Iteration 2093 Loss: 2.6883492469787598\n",
      "Iteration 2094 Loss: 1.2064248323440552\n",
      "Iteration 2095 Loss: 2.037353515625\n",
      "Iteration 2096 Loss: 2.097012758255005\n",
      "Iteration 2097 Loss: 2.196631669998169\n",
      "Iteration 2098 Loss: 1.9233253002166748\n",
      "Iteration 2099 Loss: 2.1578853130340576\n",
      "Iteration 2099 Loss: 2.007688045501709\n",
      "Iteration 2100 Loss: 2.370657444000244\n",
      "Iteration 2101 Loss: 1.6702792644500732\n",
      "Iteration 2102 Loss: 2.1885626316070557\n",
      "Iteration 2103 Loss: 1.8670660257339478\n",
      "Iteration 2104 Loss: 2.0813393592834473\n",
      "Iteration 2105 Loss: 2.3172452449798584\n",
      "Iteration 2106 Loss: 1.666282296180725\n",
      "Iteration 2107 Loss: 2.126229763031006\n",
      "Iteration 2108 Loss: 1.9388229846954346\n",
      "Iteration 2109 Loss: 1.9301364421844482\n",
      "Iteration 2109 Loss: 2.01566219329834\n",
      "Iteration 2110 Loss: 1.127654790878296\n",
      "Iteration 2111 Loss: 2.361582040786743\n",
      "Iteration 2112 Loss: 1.4023103713989258\n",
      "Iteration 2113 Loss: 2.013105630874634\n",
      "Iteration 2114 Loss: 2.104471206665039\n",
      "Iteration 2115 Loss: 1.4583873748779297\n",
      "Iteration 2116 Loss: 1.9949886798858643\n",
      "Iteration 2117 Loss: 2.019233226776123\n",
      "Iteration 2118 Loss: 2.042072057723999\n",
      "Iteration 2119 Loss: 2.1871020793914795\n",
      "Iteration 2119 Loss: 1.8710906505584717\n",
      "Iteration 2120 Loss: 2.035600423812866\n",
      "Iteration 2121 Loss: 1.3434425592422485\n",
      "Iteration 2122 Loss: 1.5554420948028564\n",
      "Iteration 2123 Loss: 1.740373969078064\n",
      "Iteration 2124 Loss: 1.799301266670227\n",
      "Iteration 2125 Loss: 1.5899195671081543\n",
      "Iteration 2126 Loss: 2.2034873962402344\n",
      "Iteration 2127 Loss: 1.7771034240722656\n",
      "Iteration 2128 Loss: 2.0520904064178467\n",
      "Iteration 2129 Loss: 1.8483355045318604\n",
      "Iteration 2129 Loss: 1.7945096492767334\n",
      "Iteration 2130 Loss: 1.4587923288345337\n",
      "Iteration 2131 Loss: 1.8352205753326416\n",
      "Iteration 2132 Loss: 1.2647948265075684\n",
      "Iteration 2133 Loss: 1.7327415943145752\n",
      "Iteration 2134 Loss: 2.1140403747558594\n",
      "Iteration 2135 Loss: 1.900094747543335\n",
      "Iteration 2136 Loss: 1.432565689086914\n",
      "Iteration 2137 Loss: 2.3184425830841064\n",
      "Iteration 2138 Loss: 1.7872322797775269\n",
      "Iteration 2139 Loss: 1.7443255186080933\n",
      "Iteration 2139 Loss: 1.7588250637054443\n",
      "Iteration 2140 Loss: 1.6175100803375244\n",
      "Iteration 2141 Loss: 1.5227842330932617\n",
      "Iteration 2142 Loss: 1.9332656860351562\n",
      "Iteration 2143 Loss: 1.5515811443328857\n",
      "Iteration 2144 Loss: 2.0538666248321533\n",
      "Iteration 2145 Loss: 2.0166525840759277\n",
      "Iteration 2146 Loss: 1.8232771158218384\n",
      "Iteration 2147 Loss: 1.588455080986023\n",
      "Iteration 2148 Loss: 1.1682124137878418\n",
      "Iteration 2149 Loss: 1.564645528793335\n",
      "Iteration 2149 Loss: 1.6840250492095947\n",
      "Iteration 2150 Loss: 2.0541348457336426\n",
      "Iteration 2151 Loss: 1.951749563217163\n",
      "Iteration 2152 Loss: 2.3059659004211426\n",
      "Iteration 2153 Loss: 1.5810267925262451\n",
      "Iteration 2154 Loss: 2.1615731716156006\n",
      "Iteration 2155 Loss: 1.4420366287231445\n",
      "Iteration 2156 Loss: 1.6793456077575684\n",
      "Iteration 2157 Loss: 2.032792568206787\n",
      "Iteration 2158 Loss: 1.1371510028839111\n",
      "Iteration 2159 Loss: 1.512427806854248\n",
      "Iteration 2159 Loss: 1.7858206033706665\n",
      "Iteration 2160 Loss: 1.6643422842025757\n",
      "Iteration 2161 Loss: 1.7253905534744263\n",
      "Iteration 2162 Loss: 1.9283643960952759\n",
      "Iteration 2163 Loss: 1.8253496885299683\n",
      "Iteration 2164 Loss: 2.1067302227020264\n",
      "Iteration 2165 Loss: 2.0526480674743652\n",
      "Iteration 2166 Loss: 2.0471651554107666\n",
      "Iteration 2167 Loss: 1.6620699167251587\n",
      "Iteration 2168 Loss: 1.627060890197754\n",
      "Iteration 2169 Loss: 1.3320876359939575\n",
      "Iteration 2169 Loss: 1.7971208095550537\n",
      "Iteration 2170 Loss: 2.105621814727783\n",
      "Iteration 2171 Loss: 2.0898098945617676\n",
      "Iteration 2172 Loss: 1.8221114873886108\n",
      "Iteration 2173 Loss: 1.3305068016052246\n",
      "Iteration 2174 Loss: 1.5354598760604858\n",
      "Iteration 2175 Loss: 1.826270341873169\n",
      "Iteration 2176 Loss: 2.0523812770843506\n",
      "Iteration 2177 Loss: 2.3475348949432373\n",
      "Iteration 2178 Loss: 1.9879610538482666\n",
      "Iteration 2179 Loss: 2.1915647983551025\n",
      "Iteration 2179 Loss: 1.9289220571517944\n",
      "Iteration 2180 Loss: 2.4255387783050537\n",
      "Iteration 2181 Loss: 2.215665578842163\n",
      "Iteration 2182 Loss: 2.2813944816589355\n",
      "Iteration 2183 Loss: 1.7387064695358276\n",
      "Iteration 2184 Loss: 2.198256254196167\n",
      "Iteration 2185 Loss: 2.2188358306884766\n",
      "Iteration 2186 Loss: 2.053572654724121\n",
      "Iteration 2187 Loss: 1.678625464439392\n",
      "Iteration 2188 Loss: 2.363844871520996\n",
      "Iteration 2189 Loss: 2.4018714427948\n",
      "Iteration 2189 Loss: 2.1576313972473145\n",
      "Iteration 2190 Loss: 1.2389607429504395\n",
      "Iteration 2191 Loss: 1.2710041999816895\n",
      "Iteration 2192 Loss: 1.721194863319397\n",
      "Iteration 2193 Loss: 1.9721674919128418\n",
      "Iteration 2194 Loss: 1.493457555770874\n",
      "Iteration 2195 Loss: 2.317265033721924\n",
      "Iteration 2196 Loss: 1.3430097103118896\n",
      "Iteration 2197 Loss: 1.939175009727478\n",
      "Iteration 2198 Loss: 1.7182354927062988\n",
      "Iteration 2199 Loss: 1.9043222665786743\n",
      "Iteration 2199 Loss: 1.6918792724609375\n",
      "Iteration 2200 Loss: 1.6591383218765259\n",
      "Iteration 2201 Loss: 2.34782338142395\n",
      "Iteration 2202 Loss: 2.2537052631378174\n",
      "Iteration 2203 Loss: 1.7175487279891968\n",
      "Iteration 2204 Loss: 1.7262256145477295\n",
      "Iteration 2205 Loss: 2.01351261138916\n",
      "Iteration 2206 Loss: 1.589682698249817\n",
      "Iteration 2207 Loss: 1.9740219116210938\n",
      "Iteration 2208 Loss: 1.8536297082901\n",
      "Iteration 2209 Loss: 1.9478336572647095\n",
      "Iteration 2209 Loss: 1.908312201499939\n",
      "Iteration 2210 Loss: 1.265981674194336\n",
      "Iteration 2211 Loss: 2.1798360347747803\n",
      "Iteration 2212 Loss: 2.397493362426758\n",
      "Iteration 2213 Loss: 1.4790778160095215\n",
      "Iteration 2214 Loss: 2.1541554927825928\n",
      "Iteration 2215 Loss: 1.9987781047821045\n",
      "Iteration 2216 Loss: 1.772159457206726\n",
      "Iteration 2217 Loss: 2.177943468093872\n",
      "Iteration 2218 Loss: 1.7309064865112305\n",
      "Iteration 2219 Loss: 2.115082025527954\n",
      "Iteration 2219 Loss: 1.9271414279937744\n",
      "Iteration 2220 Loss: 1.7279232740402222\n",
      "Iteration 2221 Loss: 1.9441707134246826\n",
      "Iteration 2222 Loss: 1.5994477272033691\n",
      "Iteration 2223 Loss: 1.8970582485198975\n",
      "Iteration 2224 Loss: 2.114596366882324\n",
      "Iteration 2225 Loss: 1.9435389041900635\n",
      "Iteration 2226 Loss: 1.5942463874816895\n",
      "Iteration 2227 Loss: 1.7410905361175537\n",
      "Iteration 2228 Loss: 1.947511076927185\n",
      "Iteration 2229 Loss: 2.1112821102142334\n",
      "Iteration 2229 Loss: 1.8620866537094116\n",
      "Iteration 2230 Loss: 1.8319933414459229\n",
      "Iteration 2231 Loss: 1.8799773454666138\n",
      "Iteration 2232 Loss: 1.457998514175415\n",
      "Iteration 2233 Loss: 2.178662061691284\n",
      "Iteration 2234 Loss: 1.805665135383606\n",
      "Iteration 2235 Loss: 2.464008331298828\n",
      "Iteration 2236 Loss: 2.1448826789855957\n",
      "Iteration 2237 Loss: 1.6051338911056519\n",
      "Iteration 2238 Loss: 2.1555919647216797\n",
      "Iteration 2239 Loss: 1.6180639266967773\n",
      "Iteration 2239 Loss: 1.914197564125061\n",
      "Iteration 2240 Loss: 2.188356876373291\n",
      "Iteration 2241 Loss: 2.1817009449005127\n",
      "Iteration 2242 Loss: 1.3872957229614258\n",
      "Iteration 2243 Loss: 1.8712749481201172\n",
      "Iteration 2244 Loss: 1.7954823970794678\n",
      "Iteration 2245 Loss: 1.737168788909912\n",
      "Iteration 2246 Loss: 1.5932413339614868\n",
      "Iteration 2247 Loss: 1.8272165060043335\n",
      "Iteration 2248 Loss: 1.208542823791504\n",
      "Iteration 2249 Loss: 1.4740149974822998\n",
      "Iteration 2249 Loss: 1.7264295816421509\n",
      "Iteration 2250 Loss: 2.1813573837280273\n",
      "Iteration 2251 Loss: 1.4176054000854492\n",
      "Iteration 2252 Loss: 2.350295305252075\n",
      "Iteration 2253 Loss: 1.8746438026428223\n",
      "Iteration 2254 Loss: 1.9354078769683838\n",
      "Iteration 2255 Loss: 2.243025541305542\n",
      "Iteration 2256 Loss: 1.5705798864364624\n",
      "Iteration 2257 Loss: 1.6104187965393066\n",
      "Iteration 2258 Loss: 1.9075533151626587\n",
      "Iteration 2259 Loss: 1.3808878660202026\n",
      "Iteration 2259 Loss: 1.847177505493164\n",
      "Iteration 2260 Loss: 1.8274084329605103\n",
      "Iteration 2261 Loss: 1.4751901626586914\n",
      "Iteration 2262 Loss: 1.7105671167373657\n",
      "Iteration 2263 Loss: 1.717942714691162\n",
      "Iteration 2264 Loss: 2.058067560195923\n",
      "Iteration 2265 Loss: 2.367604970932007\n",
      "Iteration 2266 Loss: 1.692361831665039\n",
      "Iteration 2267 Loss: 1.6146647930145264\n",
      "Iteration 2268 Loss: 1.9873008728027344\n",
      "Iteration 2269 Loss: 1.9599335193634033\n",
      "Iteration 2269 Loss: 1.8411041498184204\n",
      "Iteration 2270 Loss: 2.3806607723236084\n",
      "Iteration 2271 Loss: 1.275712490081787\n",
      "Iteration 2272 Loss: 2.292274236679077\n",
      "Iteration 2273 Loss: 1.8622939586639404\n",
      "Iteration 2274 Loss: 1.6089634895324707\n",
      "Iteration 2275 Loss: 1.5244145393371582\n",
      "Iteration 2276 Loss: 1.448974847793579\n",
      "Iteration 2277 Loss: 1.7489126920700073\n",
      "Iteration 2278 Loss: 1.5406239032745361\n",
      "Iteration 2279 Loss: 2.1655542850494385\n",
      "Iteration 2279 Loss: 1.7848384380340576\n",
      "Iteration 2280 Loss: 1.2004523277282715\n",
      "Iteration 2281 Loss: 1.8026626110076904\n",
      "Iteration 2282 Loss: 2.4845457077026367\n",
      "Iteration 2283 Loss: 2.1454384326934814\n",
      "Iteration 2284 Loss: 1.6476787328720093\n",
      "Iteration 2285 Loss: 2.423724412918091\n",
      "Iteration 2286 Loss: 1.9130845069885254\n",
      "Iteration 2287 Loss: 1.7945024967193604\n",
      "Iteration 2288 Loss: 1.7890782356262207\n",
      "Iteration 2289 Loss: 1.8832206726074219\n",
      "Iteration 2289 Loss: 1.9084386825561523\n",
      "Iteration 2290 Loss: 1.150222897529602\n",
      "Iteration 2291 Loss: 1.5058130025863647\n",
      "Iteration 2292 Loss: 1.8476859331130981\n",
      "Iteration 2293 Loss: 1.3813214302062988\n",
      "Iteration 2294 Loss: 0.995855987071991\n",
      "Iteration 2295 Loss: 1.6758259534835815\n",
      "Iteration 2296 Loss: 2.0358505249023438\n",
      "Iteration 2297 Loss: 1.4443038702011108\n",
      "Iteration 2298 Loss: 1.816853642463684\n",
      "Iteration 2299 Loss: 2.277029275894165\n",
      "Iteration 2299 Loss: 1.6130762100219727\n",
      "Iteration 2300 Loss: 1.7654333114624023\n",
      "Iteration 2301 Loss: 1.4293099641799927\n",
      "Iteration 2302 Loss: 2.2752604484558105\n",
      "Iteration 2303 Loss: 1.505433201789856\n",
      "Iteration 2304 Loss: 1.9473210573196411\n",
      "Iteration 2305 Loss: 2.0660130977630615\n",
      "Iteration 2306 Loss: 1.6673879623413086\n",
      "Iteration 2307 Loss: 1.3629707098007202\n",
      "Iteration 2308 Loss: 2.018368721008301\n",
      "Iteration 2309 Loss: 2.128674030303955\n",
      "Iteration 2309 Loss: 1.8166172504425049\n",
      "Iteration 2310 Loss: 1.4037314653396606\n",
      "Iteration 2311 Loss: 2.351294994354248\n",
      "Iteration 2312 Loss: 1.6686369180679321\n",
      "Iteration 2313 Loss: 2.217472791671753\n",
      "Iteration 2314 Loss: 1.5564464330673218\n",
      "Iteration 2315 Loss: 1.7798153162002563\n",
      "Iteration 2316 Loss: 2.0238146781921387\n",
      "Iteration 2317 Loss: 1.759589433670044\n",
      "Iteration 2318 Loss: 1.43882155418396\n",
      "Iteration 2319 Loss: 2.1604812145233154\n",
      "Iteration 2319 Loss: 1.8360105752944946\n",
      "Iteration 2320 Loss: 1.5416836738586426\n",
      "Iteration 2321 Loss: 1.7731252908706665\n",
      "Iteration 2322 Loss: 2.05775785446167\n",
      "Iteration 2323 Loss: 1.4576338529586792\n",
      "Iteration 2324 Loss: 1.5929498672485352\n",
      "Iteration 2325 Loss: 2.2289702892303467\n",
      "Iteration 2326 Loss: 1.95827054977417\n",
      "Iteration 2327 Loss: 1.5012743473052979\n",
      "Iteration 2328 Loss: 1.7109160423278809\n",
      "Iteration 2329 Loss: 1.596319317817688\n",
      "Iteration 2329 Loss: 1.7418901920318604\n",
      "Iteration 2330 Loss: 1.8470510244369507\n",
      "Iteration 2331 Loss: 1.7613329887390137\n",
      "Iteration 2332 Loss: 2.085012912750244\n",
      "Iteration 2333 Loss: 1.6468387842178345\n",
      "Iteration 2334 Loss: 2.211911916732788\n",
      "Iteration 2335 Loss: 1.5732057094573975\n",
      "Iteration 2336 Loss: 1.6751841306686401\n",
      "Iteration 2337 Loss: 1.461890697479248\n",
      "Iteration 2338 Loss: 1.4884908199310303\n",
      "Iteration 2339 Loss: 1.799757719039917\n",
      "Iteration 2339 Loss: 1.7550678253173828\n",
      "Iteration 2340 Loss: 2.1909639835357666\n",
      "Iteration 2341 Loss: 1.1632120609283447\n",
      "Iteration 2342 Loss: 1.6775716543197632\n",
      "Iteration 2343 Loss: 1.4951891899108887\n",
      "Iteration 2344 Loss: 1.86663019657135\n",
      "Iteration 2345 Loss: 2.0528712272644043\n",
      "Iteration 2346 Loss: 1.9729907512664795\n",
      "Iteration 2347 Loss: 2.377511978149414\n",
      "Iteration 2348 Loss: 2.3049957752227783\n",
      "Iteration 2349 Loss: 2.067579746246338\n",
      "Iteration 2349 Loss: 1.9169515371322632\n",
      "Iteration 2350 Loss: 1.9992557764053345\n",
      "Iteration 2351 Loss: 1.5721888542175293\n",
      "Iteration 2352 Loss: 1.519789218902588\n",
      "Iteration 2353 Loss: 1.863343596458435\n",
      "Iteration 2354 Loss: 1.5202361345291138\n",
      "Iteration 2355 Loss: 2.0713448524475098\n",
      "Iteration 2356 Loss: 1.9236373901367188\n",
      "Iteration 2357 Loss: 1.4507300853729248\n",
      "Iteration 2358 Loss: 1.7266532182693481\n",
      "Iteration 2359 Loss: 1.7104356288909912\n",
      "Iteration 2359 Loss: 1.735761284828186\n",
      "Iteration 2360 Loss: 1.3981893062591553\n",
      "Iteration 2361 Loss: 1.315125823020935\n",
      "Iteration 2362 Loss: 1.6415727138519287\n",
      "Iteration 2363 Loss: 2.346271276473999\n",
      "Iteration 2364 Loss: 1.45404851436615\n",
      "Iteration 2365 Loss: 1.8721470832824707\n",
      "Iteration 2366 Loss: 2.63135027885437\n",
      "Iteration 2367 Loss: 2.353295087814331\n",
      "Iteration 2368 Loss: 2.1242852210998535\n",
      "Iteration 2369 Loss: 1.8542615175247192\n",
      "Iteration 2369 Loss: 1.8990545272827148\n",
      "Iteration 2370 Loss: 1.6502020359039307\n",
      "Iteration 2371 Loss: 1.4972872734069824\n",
      "Iteration 2372 Loss: 1.6383683681488037\n",
      "Iteration 2373 Loss: 2.204704523086548\n",
      "Iteration 2374 Loss: 1.8610676527023315\n",
      "Iteration 2375 Loss: 1.8673079013824463\n",
      "Iteration 2376 Loss: 1.5768178701400757\n",
      "Iteration 2377 Loss: 2.396824598312378\n",
      "Iteration 2378 Loss: 1.8857028484344482\n",
      "Iteration 2379 Loss: 1.992979645729065\n",
      "Iteration 2379 Loss: 1.857126235961914\n",
      "Iteration 2380 Loss: 1.9116581678390503\n",
      "Iteration 2381 Loss: 2.3675858974456787\n",
      "Iteration 2382 Loss: 2.588080883026123\n",
      "Iteration 2383 Loss: 1.6923636198043823\n",
      "Iteration 2384 Loss: 1.652211308479309\n",
      "Iteration 2385 Loss: 1.4193978309631348\n",
      "Iteration 2386 Loss: 1.6723346710205078\n",
      "Iteration 2387 Loss: 1.69484281539917\n",
      "Iteration 2388 Loss: 1.209928274154663\n",
      "Iteration 2389 Loss: 1.8644217252731323\n",
      "Iteration 2389 Loss: 1.8072826862335205\n",
      "Iteration 2390 Loss: 2.0616135597229004\n",
      "Iteration 2391 Loss: 1.748545527458191\n",
      "Iteration 2392 Loss: 1.3942080736160278\n",
      "Iteration 2393 Loss: 1.9302719831466675\n",
      "Iteration 2394 Loss: 2.284104824066162\n",
      "Iteration 2395 Loss: 1.5719974040985107\n",
      "Iteration 2396 Loss: 1.6630146503448486\n",
      "Iteration 2397 Loss: 2.1712257862091064\n",
      "Iteration 2398 Loss: 1.5180307626724243\n",
      "Iteration 2399 Loss: 2.007350444793701\n",
      "Iteration 2399 Loss: 1.8350365161895752\n",
      "Iteration 2400 Loss: 2.2888543605804443\n",
      "Iteration 2401 Loss: 1.8073444366455078\n",
      "Iteration 2402 Loss: 2.3520092964172363\n",
      "Iteration 2403 Loss: 2.2831685543060303\n",
      "Iteration 2404 Loss: 1.8654611110687256\n",
      "Iteration 2405 Loss: 1.5468366146087646\n",
      "Iteration 2406 Loss: 2.0347084999084473\n",
      "Iteration 2407 Loss: 2.0772554874420166\n",
      "Iteration 2408 Loss: 1.4925700426101685\n",
      "Iteration 2409 Loss: 2.4529807567596436\n",
      "Iteration 2409 Loss: 2.0201189517974854\n",
      "Iteration 2410 Loss: 2.045602560043335\n",
      "Iteration 2411 Loss: 2.1709084510803223\n",
      "Iteration 2412 Loss: 2.025681972503662\n",
      "Iteration 2413 Loss: 1.7302639484405518\n",
      "Iteration 2414 Loss: 2.035557270050049\n",
      "Iteration 2415 Loss: 1.9827759265899658\n",
      "Iteration 2416 Loss: 1.4253334999084473\n",
      "Iteration 2417 Loss: 2.5555970668792725\n",
      "Iteration 2418 Loss: 1.7316230535507202\n",
      "Iteration 2419 Loss: 1.1485100984573364\n",
      "Iteration 2419 Loss: 1.8851854801177979\n",
      "Iteration 2420 Loss: 1.6995911598205566\n",
      "Iteration 2421 Loss: 2.337846517562866\n",
      "Iteration 2422 Loss: 1.7406491041183472\n",
      "Iteration 2423 Loss: 2.3063576221466064\n",
      "Iteration 2424 Loss: 1.6620137691497803\n",
      "Iteration 2425 Loss: 1.479023814201355\n",
      "Iteration 2426 Loss: 1.4732890129089355\n",
      "Iteration 2427 Loss: 1.7267476320266724\n",
      "Iteration 2428 Loss: 1.7987176179885864\n",
      "Iteration 2429 Loss: 2.594026565551758\n",
      "Iteration 2429 Loss: 1.8818261623382568\n",
      "Iteration 2430 Loss: 1.3804552555084229\n",
      "Iteration 2431 Loss: 2.086975336074829\n",
      "Iteration 2432 Loss: 1.6323158740997314\n",
      "Iteration 2433 Loss: 2.800191640853882\n",
      "Iteration 2434 Loss: 2.3987183570861816\n",
      "Iteration 2435 Loss: 1.5124839544296265\n",
      "Iteration 2436 Loss: 1.5369526147842407\n",
      "Iteration 2437 Loss: 1.9209054708480835\n",
      "Iteration 2438 Loss: 2.1338117122650146\n",
      "Iteration 2439 Loss: 2.1422672271728516\n",
      "Iteration 2439 Loss: 1.954507827758789\n",
      "Iteration 2440 Loss: 1.8884650468826294\n",
      "Iteration 2441 Loss: 2.2999942302703857\n",
      "Iteration 2442 Loss: 1.6990989446640015\n",
      "Iteration 2443 Loss: 1.550691843032837\n",
      "Iteration 2444 Loss: 2.1598501205444336\n",
      "Iteration 2445 Loss: 1.2040022611618042\n",
      "Iteration 2446 Loss: 2.2005932331085205\n",
      "Iteration 2447 Loss: 0.9980794787406921\n",
      "Iteration 2448 Loss: 2.2142324447631836\n",
      "Iteration 2449 Loss: 1.9238554239273071\n",
      "Iteration 2449 Loss: 1.813886284828186\n",
      "Iteration 2450 Loss: 2.0117571353912354\n",
      "Iteration 2451 Loss: 1.429123878479004\n",
      "Iteration 2452 Loss: 1.2614562511444092\n",
      "Iteration 2453 Loss: 1.8393266201019287\n",
      "Iteration 2454 Loss: 1.6137523651123047\n",
      "Iteration 2455 Loss: 2.050475835800171\n",
      "Iteration 2456 Loss: 1.5812872648239136\n",
      "Iteration 2457 Loss: 1.7577787637710571\n",
      "Iteration 2458 Loss: 1.6845428943634033\n",
      "Iteration 2459 Loss: 1.6626415252685547\n",
      "Iteration 2459 Loss: 1.6892143487930298\n",
      "Iteration 2460 Loss: 1.5156080722808838\n",
      "Iteration 2461 Loss: 1.8175175189971924\n",
      "Iteration 2462 Loss: 2.031601667404175\n",
      "Iteration 2463 Loss: 1.9785192012786865\n",
      "Iteration 2464 Loss: 1.767712116241455\n",
      "Iteration 2465 Loss: 1.186431884765625\n",
      "Iteration 2466 Loss: 1.5214143991470337\n",
      "Iteration 2467 Loss: 1.9569987058639526\n",
      "Iteration 2468 Loss: 1.3555831909179688\n",
      "Iteration 2469 Loss: 1.9641311168670654\n",
      "Iteration 2469 Loss: 1.7095518112182617\n",
      "Iteration 2470 Loss: 1.9591140747070312\n",
      "Iteration 2471 Loss: 2.0261547565460205\n",
      "Iteration 2472 Loss: 1.1563503742218018\n",
      "Iteration 2473 Loss: 2.21468448638916\n",
      "Iteration 2474 Loss: 1.8014153242111206\n",
      "Iteration 2475 Loss: 1.3231167793273926\n",
      "Iteration 2476 Loss: 1.1518874168395996\n",
      "Iteration 2477 Loss: 1.5497976541519165\n",
      "Iteration 2478 Loss: 1.745886206626892\n",
      "Iteration 2479 Loss: 1.916457176208496\n",
      "Iteration 2479 Loss: 1.6844863891601562\n",
      "Iteration 2480 Loss: 2.1309361457824707\n",
      "Iteration 2481 Loss: 2.028137445449829\n",
      "Iteration 2482 Loss: 2.03548264503479\n",
      "Iteration 2483 Loss: 2.499617576599121\n",
      "Iteration 2484 Loss: 1.4683196544647217\n",
      "Iteration 2485 Loss: 1.6494498252868652\n",
      "Iteration 2486 Loss: 2.0337047576904297\n",
      "Iteration 2487 Loss: 1.692767858505249\n",
      "Iteration 2488 Loss: 1.5022363662719727\n",
      "Iteration 2489 Loss: 1.3697495460510254\n",
      "Iteration 2489 Loss: 1.8410402536392212\n",
      "Iteration 2490 Loss: 2.038172483444214\n",
      "Iteration 2491 Loss: 1.2378137111663818\n",
      "Iteration 2492 Loss: 1.8934088945388794\n",
      "Iteration 2493 Loss: 2.215676784515381\n",
      "Iteration 2494 Loss: 2.1467463970184326\n",
      "Iteration 2495 Loss: 1.6388576030731201\n",
      "Iteration 2496 Loss: 1.8340692520141602\n",
      "Iteration 2497 Loss: 1.1853781938552856\n",
      "Iteration 2498 Loss: 1.8383817672729492\n",
      "Iteration 2499 Loss: 1.526383876800537\n",
      "Iteration 2499 Loss: 1.7554889917373657\n",
      "Iteration 2500 Loss: 1.536698579788208\n",
      "Iteration 2501 Loss: 1.7071356773376465\n",
      "Iteration 2502 Loss: 1.938887119293213\n",
      "Iteration 2503 Loss: 1.7705374956130981\n",
      "Iteration 2504 Loss: 1.2124934196472168\n",
      "Iteration 2505 Loss: 1.6744080781936646\n",
      "Iteration 2506 Loss: 1.1729592084884644\n",
      "Iteration 2507 Loss: 2.031632423400879\n",
      "Iteration 2508 Loss: 2.180678606033325\n",
      "Iteration 2509 Loss: 1.5867820978164673\n",
      "Iteration 2509 Loss: 1.6812212467193604\n",
      "Iteration 2510 Loss: 1.476220726966858\n",
      "Iteration 2511 Loss: 2.0124881267547607\n",
      "Iteration 2512 Loss: 2.1816093921661377\n",
      "Iteration 2513 Loss: 1.596799612045288\n",
      "Iteration 2514 Loss: 2.472754955291748\n",
      "Iteration 2515 Loss: 1.5840058326721191\n",
      "Iteration 2516 Loss: 1.6108636856079102\n",
      "Iteration 2517 Loss: 2.0560810565948486\n",
      "Iteration 2518 Loss: 1.69343101978302\n",
      "Iteration 2519 Loss: 2.3633100986480713\n",
      "Iteration 2519 Loss: 1.9047563076019287\n",
      "Iteration 2520 Loss: 1.9610658884048462\n",
      "Iteration 2521 Loss: 1.6093363761901855\n",
      "Iteration 2522 Loss: 1.9009536504745483\n",
      "Iteration 2523 Loss: 2.282254695892334\n",
      "Iteration 2524 Loss: 1.997232437133789\n",
      "Iteration 2525 Loss: 1.9853265285491943\n",
      "Iteration 2526 Loss: 1.2004075050354004\n",
      "Iteration 2527 Loss: 1.940760612487793\n",
      "Iteration 2528 Loss: 1.7664999961853027\n",
      "Iteration 2529 Loss: 2.117976427078247\n",
      "Iteration 2529 Loss: 1.8761813640594482\n",
      "Iteration 2530 Loss: 1.5734684467315674\n",
      "Iteration 2531 Loss: 1.938205361366272\n",
      "Iteration 2532 Loss: 1.5701990127563477\n",
      "Iteration 2533 Loss: 1.8639051914215088\n",
      "Iteration 2534 Loss: 1.2493125200271606\n",
      "Iteration 2535 Loss: 1.9338535070419312\n",
      "Iteration 2536 Loss: 1.339847207069397\n",
      "Iteration 2537 Loss: 1.7575756311416626\n",
      "Iteration 2538 Loss: 1.9180781841278076\n",
      "Iteration 2539 Loss: 1.6836423873901367\n",
      "Iteration 2539 Loss: 1.6828086376190186\n",
      "Iteration 2540 Loss: 1.4591479301452637\n",
      "Iteration 2541 Loss: 1.5889792442321777\n",
      "Iteration 2542 Loss: 1.5121119022369385\n",
      "Iteration 2543 Loss: 2.400451421737671\n",
      "Iteration 2544 Loss: 2.12770414352417\n",
      "Iteration 2545 Loss: 1.5331363677978516\n",
      "Iteration 2546 Loss: 1.2922977209091187\n",
      "Iteration 2547 Loss: 1.8904259204864502\n",
      "Iteration 2548 Loss: 1.6683253049850464\n",
      "Iteration 2549 Loss: 2.399531602859497\n",
      "Iteration 2549 Loss: 1.7872111797332764\n",
      "Iteration 2550 Loss: 2.2480475902557373\n",
      "Iteration 2551 Loss: 1.9710544347763062\n",
      "Iteration 2552 Loss: 1.5578848123550415\n",
      "Iteration 2553 Loss: 1.3095166683197021\n",
      "Iteration 2554 Loss: 1.5984880924224854\n",
      "Iteration 2555 Loss: 1.9844211339950562\n",
      "Iteration 2556 Loss: 2.0228371620178223\n",
      "Iteration 2557 Loss: 1.8704369068145752\n",
      "Iteration 2558 Loss: 1.5363332033157349\n",
      "Iteration 2559 Loss: 1.046889066696167\n",
      "Iteration 2559 Loss: 1.7145910263061523\n",
      "Iteration 2560 Loss: 1.455196499824524\n",
      "Iteration 2561 Loss: 2.0344488620758057\n",
      "Iteration 2562 Loss: 1.115729808807373\n",
      "Iteration 2563 Loss: 1.9171956777572632\n",
      "Iteration 2564 Loss: 1.5322961807250977\n",
      "Iteration 2565 Loss: 1.3505405187606812\n",
      "Iteration 2566 Loss: 1.427924633026123\n",
      "Iteration 2567 Loss: 1.2728259563446045\n",
      "Iteration 2568 Loss: 2.0358715057373047\n",
      "Iteration 2569 Loss: 1.3277838230133057\n",
      "Iteration 2569 Loss: 1.5469813346862793\n",
      "Iteration 2570 Loss: 1.8649027347564697\n",
      "Iteration 2571 Loss: 1.844582200050354\n",
      "Iteration 2572 Loss: 2.0853042602539062\n",
      "Iteration 2573 Loss: 1.7815433740615845\n",
      "Iteration 2574 Loss: 2.0405311584472656\n",
      "Iteration 2575 Loss: 1.8231730461120605\n",
      "Iteration 2576 Loss: 1.7107787132263184\n",
      "Iteration 2577 Loss: 1.9855005741119385\n",
      "Iteration 2578 Loss: 1.3980787992477417\n",
      "Iteration 2579 Loss: 1.2441449165344238\n",
      "Iteration 2579 Loss: 1.7778542041778564\n",
      "Iteration 2580 Loss: 1.4008941650390625\n",
      "Iteration 2581 Loss: 1.8074615001678467\n",
      "Iteration 2582 Loss: 1.5690069198608398\n",
      "Iteration 2583 Loss: 1.6734569072723389\n",
      "Iteration 2584 Loss: 1.8209612369537354\n",
      "Iteration 2585 Loss: 1.7470310926437378\n",
      "Iteration 2586 Loss: 2.238727569580078\n",
      "Iteration 2587 Loss: 1.7439765930175781\n",
      "Iteration 2588 Loss: 1.1700901985168457\n",
      "Iteration 2589 Loss: 1.8614883422851562\n",
      "Iteration 2589 Loss: 1.703309416770935\n",
      "Iteration 2590 Loss: 1.6795554161071777\n",
      "Iteration 2591 Loss: 1.8269810676574707\n",
      "Iteration 2592 Loss: 1.6739470958709717\n",
      "Iteration 2593 Loss: 1.66304349899292\n",
      "Iteration 2594 Loss: 1.3120328187942505\n",
      "Iteration 2595 Loss: 1.6924371719360352\n",
      "Iteration 2596 Loss: 1.9027985334396362\n",
      "Iteration 2597 Loss: 1.8347371816635132\n",
      "Iteration 2598 Loss: 1.892366647720337\n",
      "Iteration 2599 Loss: 1.4465022087097168\n",
      "Iteration 2599 Loss: 1.6924402713775635\n",
      "Iteration 2600 Loss: 2.2076916694641113\n",
      "Iteration 2601 Loss: 1.959417462348938\n",
      "Iteration 2602 Loss: 1.7960989475250244\n",
      "Iteration 2603 Loss: 2.4632375240325928\n",
      "Iteration 2604 Loss: 1.9392619132995605\n",
      "Iteration 2605 Loss: 1.8927834033966064\n",
      "Iteration 2606 Loss: 2.1403465270996094\n",
      "Iteration 2607 Loss: 1.6179935932159424\n",
      "Iteration 2608 Loss: 1.6437681913375854\n",
      "Iteration 2609 Loss: 1.5700851678848267\n",
      "Iteration 2609 Loss: 1.9230684041976929\n",
      "Iteration 2610 Loss: 1.5749444961547852\n",
      "Iteration 2611 Loss: 1.9918497800827026\n",
      "Iteration 2612 Loss: 1.7938051223754883\n",
      "Iteration 2613 Loss: 2.2112722396850586\n",
      "Iteration 2614 Loss: 1.4294432401657104\n",
      "Iteration 2615 Loss: 1.7893152236938477\n",
      "Iteration 2616 Loss: 2.028862237930298\n",
      "Iteration 2617 Loss: 1.9331657886505127\n",
      "Iteration 2618 Loss: 1.9480869770050049\n",
      "Iteration 2619 Loss: 2.3142223358154297\n",
      "Iteration 2619 Loss: 1.9014968872070312\n",
      "Iteration 2620 Loss: 1.8404580354690552\n",
      "Iteration 2621 Loss: 1.151924967765808\n",
      "Iteration 2622 Loss: 1.4515600204467773\n",
      "Iteration 2623 Loss: 2.303544759750366\n",
      "Iteration 2624 Loss: 2.010753631591797\n",
      "Iteration 2625 Loss: 1.6999088525772095\n",
      "Iteration 2626 Loss: 1.460877537727356\n",
      "Iteration 2627 Loss: 2.164517879486084\n",
      "Iteration 2628 Loss: 1.3699619770050049\n",
      "Iteration 2629 Loss: 2.075082302093506\n",
      "Iteration 2629 Loss: 1.752859115600586\n",
      "Iteration 2630 Loss: 1.8849684000015259\n",
      "Iteration 2631 Loss: 2.0345635414123535\n",
      "Iteration 2632 Loss: 1.2278485298156738\n",
      "Iteration 2633 Loss: 1.5133769512176514\n",
      "Iteration 2634 Loss: 1.6251473426818848\n",
      "Iteration 2635 Loss: 1.111289381980896\n",
      "Iteration 2636 Loss: 1.6320313215255737\n",
      "Iteration 2637 Loss: 1.5190215110778809\n",
      "Iteration 2638 Loss: 1.7276294231414795\n",
      "Iteration 2639 Loss: 1.8080710172653198\n",
      "Iteration 2639 Loss: 1.6083948612213135\n",
      "Iteration 2640 Loss: 1.521981954574585\n",
      "Iteration 2641 Loss: 2.3607969284057617\n",
      "Iteration 2642 Loss: 1.6176848411560059\n",
      "Iteration 2643 Loss: 1.9405487775802612\n",
      "Iteration 2644 Loss: 1.7475504875183105\n",
      "Iteration 2645 Loss: 1.3013640642166138\n",
      "Iteration 2646 Loss: 1.688830852508545\n",
      "Iteration 2647 Loss: 1.3871952295303345\n",
      "Iteration 2648 Loss: 1.8943238258361816\n",
      "Iteration 2649 Loss: 1.914781928062439\n",
      "Iteration 2649 Loss: 1.7375059127807617\n",
      "Iteration 2650 Loss: 1.8395519256591797\n",
      "Iteration 2651 Loss: 1.5601317882537842\n",
      "Iteration 2652 Loss: 1.5428433418273926\n",
      "Iteration 2653 Loss: 1.533420205116272\n",
      "Iteration 2654 Loss: 1.892016887664795\n",
      "Iteration 2655 Loss: 1.6732982397079468\n",
      "Iteration 2656 Loss: 2.285057783126831\n",
      "Iteration 2657 Loss: 1.5151009559631348\n",
      "Iteration 2658 Loss: 2.0352251529693604\n",
      "Iteration 2659 Loss: 1.4947739839553833\n",
      "Iteration 2659 Loss: 1.7371422052383423\n",
      "Iteration 2660 Loss: 1.514678955078125\n",
      "Iteration 2661 Loss: 1.5161097049713135\n",
      "Iteration 2662 Loss: 1.7298014163970947\n",
      "Iteration 2663 Loss: 1.7076092958450317\n",
      "Iteration 2664 Loss: 1.9208723306655884\n",
      "Iteration 2665 Loss: 2.1519510746002197\n",
      "Iteration 2666 Loss: 0.8966851234436035\n",
      "Iteration 2667 Loss: 1.299996256828308\n",
      "Iteration 2668 Loss: 1.499100685119629\n",
      "Iteration 2669 Loss: 1.6987348794937134\n",
      "Iteration 2669 Loss: 1.5935540199279785\n",
      "Iteration 2670 Loss: 1.507361888885498\n",
      "Iteration 2671 Loss: 1.4028633832931519\n",
      "Iteration 2672 Loss: 2.2220895290374756\n",
      "Iteration 2673 Loss: 1.4008986949920654\n",
      "Iteration 2674 Loss: 1.8887605667114258\n",
      "Iteration 2675 Loss: 1.4935444593429565\n",
      "Iteration 2676 Loss: 1.0500656366348267\n",
      "Iteration 2677 Loss: 2.141287326812744\n",
      "Iteration 2678 Loss: 1.2317520380020142\n",
      "Iteration 2679 Loss: 1.3684589862823486\n",
      "Iteration 2679 Loss: 1.5707082748413086\n",
      "Iteration 2680 Loss: 1.816198706626892\n",
      "Iteration 2681 Loss: 1.1896880865097046\n",
      "Iteration 2682 Loss: 1.8758677244186401\n",
      "Iteration 2683 Loss: 1.3281440734863281\n",
      "Iteration 2684 Loss: 1.9792053699493408\n",
      "Iteration 2685 Loss: 1.8214455842971802\n",
      "Iteration 2686 Loss: 1.924314260482788\n",
      "Iteration 2687 Loss: 1.6461808681488037\n",
      "Iteration 2688 Loss: 1.8345357179641724\n",
      "Iteration 2689 Loss: 1.3208346366882324\n",
      "Iteration 2689 Loss: 1.6736414432525635\n",
      "Iteration 2690 Loss: 1.6118254661560059\n",
      "Iteration 2691 Loss: 1.6177654266357422\n",
      "Iteration 2692 Loss: 2.1214699745178223\n",
      "Iteration 2693 Loss: 1.1566933393478394\n",
      "Iteration 2694 Loss: 1.9951473474502563\n",
      "Iteration 2695 Loss: 1.8520900011062622\n",
      "Iteration 2696 Loss: 1.5788822174072266\n",
      "Iteration 2697 Loss: 1.875872254371643\n",
      "Iteration 2698 Loss: 1.86188805103302\n",
      "Iteration 2699 Loss: 1.7774492502212524\n",
      "Iteration 2699 Loss: 1.744908332824707\n",
      "Iteration 2700 Loss: 1.9902153015136719\n",
      "Iteration 2701 Loss: 1.708060383796692\n",
      "Iteration 2702 Loss: 1.852771282196045\n",
      "Iteration 2703 Loss: 1.8754563331604004\n",
      "Iteration 2704 Loss: 2.1176328659057617\n",
      "Iteration 2705 Loss: 1.7933388948440552\n",
      "Iteration 2706 Loss: 1.6760469675064087\n",
      "Iteration 2707 Loss: 1.2761276960372925\n",
      "Iteration 2708 Loss: 1.6534194946289062\n",
      "Iteration 2709 Loss: 1.3244411945343018\n",
      "Iteration 2709 Loss: 1.7267509698867798\n",
      "Iteration 2710 Loss: 1.4570962190628052\n",
      "Iteration 2711 Loss: 1.1470551490783691\n",
      "Iteration 2712 Loss: 1.7996281385421753\n",
      "Iteration 2713 Loss: 1.9626305103302002\n",
      "Iteration 2714 Loss: 1.5009081363677979\n",
      "Iteration 2715 Loss: 1.6857705116271973\n",
      "Iteration 2716 Loss: 1.7308828830718994\n",
      "Iteration 2717 Loss: 1.7266192436218262\n",
      "Iteration 2718 Loss: 1.4742923974990845\n",
      "Iteration 2719 Loss: 1.5185903310775757\n",
      "Iteration 2719 Loss: 1.6003472805023193\n",
      "Iteration 2720 Loss: 1.4696731567382812\n",
      "Iteration 2721 Loss: 1.3963260650634766\n",
      "Iteration 2722 Loss: 1.9020342826843262\n",
      "Iteration 2723 Loss: 1.7052950859069824\n",
      "Iteration 2724 Loss: 1.8878417015075684\n",
      "Iteration 2725 Loss: 1.6893208026885986\n",
      "Iteration 2726 Loss: 1.8156002759933472\n",
      "Iteration 2727 Loss: 1.5079495906829834\n",
      "Iteration 2728 Loss: 1.3390164375305176\n",
      "Iteration 2729 Loss: 1.5413039922714233\n",
      "Iteration 2729 Loss: 1.6254361867904663\n",
      "Iteration 2730 Loss: 2.412339210510254\n",
      "Iteration 2731 Loss: 1.8883697986602783\n",
      "Iteration 2732 Loss: 1.483680248260498\n",
      "Iteration 2733 Loss: 1.7975186109542847\n",
      "Iteration 2734 Loss: 1.8081883192062378\n",
      "Iteration 2735 Loss: 1.400407075881958\n",
      "Iteration 2736 Loss: 1.7741304636001587\n",
      "Iteration 2737 Loss: 2.099958896636963\n",
      "Iteration 2738 Loss: 1.4389195442199707\n",
      "Iteration 2739 Loss: 1.5488747358322144\n",
      "Iteration 2739 Loss: 1.7652387619018555\n",
      "Iteration 2740 Loss: 1.6608283519744873\n",
      "Iteration 2741 Loss: 1.7505109310150146\n",
      "Iteration 2742 Loss: 1.8799865245819092\n",
      "Iteration 2743 Loss: 1.6172046661376953\n",
      "Iteration 2744 Loss: 1.6415742635726929\n",
      "Iteration 2745 Loss: 1.821405291557312\n",
      "Iteration 2746 Loss: 1.4909822940826416\n",
      "Iteration 2747 Loss: 2.380877733230591\n",
      "Iteration 2748 Loss: 2.5076143741607666\n",
      "Iteration 2749 Loss: 1.6754809617996216\n",
      "Iteration 2749 Loss: 1.842646598815918\n",
      "Iteration 2750 Loss: 2.210829973220825\n",
      "Iteration 2751 Loss: 2.0592474937438965\n",
      "Iteration 2752 Loss: 1.5395710468292236\n",
      "Iteration 2753 Loss: 1.260524868965149\n",
      "Iteration 2754 Loss: 2.0620570182800293\n",
      "Iteration 2755 Loss: 1.6345659494400024\n",
      "Iteration 2756 Loss: 1.7895805835723877\n",
      "Iteration 2757 Loss: 1.7610464096069336\n",
      "Iteration 2758 Loss: 2.1600089073181152\n",
      "Iteration 2759 Loss: 1.502663254737854\n",
      "Iteration 2759 Loss: 1.7980095148086548\n",
      "Iteration 2760 Loss: 1.5186609029769897\n",
      "Iteration 2761 Loss: 1.6326448917388916\n",
      "Iteration 2762 Loss: 1.8076027631759644\n",
      "Iteration 2763 Loss: 1.666624903678894\n",
      "Iteration 2764 Loss: 1.4126254320144653\n",
      "Iteration 2765 Loss: 1.4565953016281128\n",
      "Iteration 2766 Loss: 1.8153927326202393\n",
      "Iteration 2767 Loss: 2.0982272624969482\n",
      "Iteration 2768 Loss: 1.8554186820983887\n",
      "Iteration 2769 Loss: 1.7914865016937256\n",
      "Iteration 2769 Loss: 1.705527901649475\n",
      "Iteration 2770 Loss: 2.108945369720459\n",
      "Iteration 2771 Loss: 1.0584532022476196\n",
      "Iteration 2772 Loss: 1.8582634925842285\n",
      "Iteration 2773 Loss: 1.8103975057601929\n",
      "Iteration 2774 Loss: 1.347482442855835\n",
      "Iteration 2775 Loss: 1.8304957151412964\n",
      "Iteration 2776 Loss: 1.7080639600753784\n",
      "Iteration 2777 Loss: 1.657207727432251\n",
      "Iteration 2778 Loss: 1.6102672815322876\n",
      "Iteration 2779 Loss: 1.3215199708938599\n",
      "Iteration 2779 Loss: 1.631109595298767\n",
      "Iteration 2780 Loss: 2.268324851989746\n",
      "Iteration 2781 Loss: 1.4128077030181885\n",
      "Iteration 2782 Loss: 1.3493857383728027\n",
      "Iteration 2783 Loss: 1.8005800247192383\n",
      "Iteration 2784 Loss: 1.699586033821106\n",
      "Iteration 2785 Loss: 1.674617052078247\n",
      "Iteration 2786 Loss: 1.6767042875289917\n",
      "Iteration 2787 Loss: 1.8994990587234497\n",
      "Iteration 2788 Loss: 1.7337236404418945\n",
      "Iteration 2789 Loss: 2.3052868843078613\n",
      "Iteration 2789 Loss: 1.78205144405365\n",
      "Iteration 2790 Loss: 1.8002933263778687\n",
      "Iteration 2791 Loss: 1.9266035556793213\n",
      "Iteration 2792 Loss: 1.7209398746490479\n",
      "Iteration 2793 Loss: 1.7618381977081299\n",
      "Iteration 2794 Loss: 2.2482621669769287\n",
      "Iteration 2795 Loss: 1.9061756134033203\n",
      "Iteration 2796 Loss: 1.8373956680297852\n",
      "Iteration 2797 Loss: 1.3407623767852783\n",
      "Iteration 2798 Loss: 1.8653993606567383\n",
      "Iteration 2799 Loss: 1.8401896953582764\n",
      "Iteration 2799 Loss: 1.8247859477996826\n",
      "Iteration 2800 Loss: 2.0407767295837402\n",
      "Iteration 2801 Loss: 2.468703269958496\n",
      "Iteration 2802 Loss: 1.997237205505371\n",
      "Iteration 2803 Loss: 1.7790303230285645\n",
      "Iteration 2804 Loss: 1.7477912902832031\n",
      "Iteration 2805 Loss: 2.1430797576904297\n",
      "Iteration 2806 Loss: 2.2406890392303467\n",
      "Iteration 2807 Loss: 1.6287860870361328\n",
      "Iteration 2808 Loss: 1.733234167098999\n",
      "Iteration 2809 Loss: 2.037905216217041\n",
      "Iteration 2809 Loss: 1.9817231893539429\n",
      "Iteration 2810 Loss: 1.6649737358093262\n",
      "Iteration 2811 Loss: 1.7757163047790527\n",
      "Iteration 2812 Loss: 2.0315165519714355\n",
      "Iteration 2813 Loss: 1.6492195129394531\n",
      "Iteration 2814 Loss: 1.5562118291854858\n",
      "Iteration 2815 Loss: 2.098203420639038\n",
      "Iteration 2816 Loss: 1.829813838005066\n",
      "Iteration 2817 Loss: 1.4194732904434204\n",
      "Iteration 2818 Loss: 1.517124891281128\n",
      "Iteration 2819 Loss: 1.666994571685791\n",
      "Iteration 2819 Loss: 1.720924735069275\n",
      "Iteration 2820 Loss: 2.144876718521118\n",
      "Iteration 2821 Loss: 1.4616645574569702\n",
      "Iteration 2822 Loss: 1.3956303596496582\n",
      "Iteration 2823 Loss: 1.4367872476577759\n",
      "Iteration 2824 Loss: 1.6005547046661377\n",
      "Iteration 2825 Loss: 1.7563397884368896\n",
      "Iteration 2826 Loss: 1.7035523653030396\n",
      "Iteration 2827 Loss: 1.565471887588501\n",
      "Iteration 2828 Loss: 1.736620306968689\n",
      "Iteration 2829 Loss: 1.6847450733184814\n",
      "Iteration 2829 Loss: 1.6486244201660156\n",
      "Iteration 2830 Loss: 1.3868287801742554\n",
      "Iteration 2831 Loss: 1.8040834665298462\n",
      "Iteration 2832 Loss: 1.5803402662277222\n",
      "Iteration 2833 Loss: 1.1419357061386108\n",
      "Iteration 2834 Loss: 2.042304515838623\n",
      "Iteration 2835 Loss: 1.9141206741333008\n",
      "Iteration 2836 Loss: 1.2699666023254395\n",
      "Iteration 2837 Loss: 1.8788408041000366\n",
      "Iteration 2838 Loss: 1.9801563024520874\n",
      "Iteration 2839 Loss: 2.0750315189361572\n",
      "Iteration 2839 Loss: 1.707360863685608\n",
      "Iteration 2840 Loss: 1.5421662330627441\n",
      "Iteration 2841 Loss: 1.7674925327301025\n",
      "Iteration 2842 Loss: 1.7698925733566284\n",
      "Iteration 2843 Loss: 2.1699583530426025\n",
      "Iteration 2844 Loss: 1.9198989868164062\n",
      "Iteration 2845 Loss: 1.9371488094329834\n",
      "Iteration 2846 Loss: 1.5774630308151245\n",
      "Iteration 2847 Loss: 1.2266048192977905\n",
      "Iteration 2848 Loss: 2.468095541000366\n",
      "Iteration 2849 Loss: 1.9471982717514038\n",
      "Iteration 2849 Loss: 1.8325917720794678\n",
      "Iteration 2850 Loss: 1.7068068981170654\n",
      "Iteration 2851 Loss: 1.6676708459854126\n",
      "Iteration 2852 Loss: 1.8819665908813477\n",
      "Iteration 2853 Loss: 1.361843228340149\n",
      "Iteration 2854 Loss: 1.7875950336456299\n",
      "Iteration 2855 Loss: 1.6465011835098267\n",
      "Iteration 2856 Loss: 2.3150997161865234\n",
      "Iteration 2857 Loss: 1.7675023078918457\n",
      "Iteration 2858 Loss: 1.9403682947158813\n",
      "Iteration 2859 Loss: 2.1414473056793213\n",
      "Iteration 2859 Loss: 1.8216800689697266\n",
      "Iteration 2860 Loss: 1.4119492769241333\n",
      "Iteration 2861 Loss: 1.8769608736038208\n",
      "Iteration 2862 Loss: 1.3828169107437134\n",
      "Iteration 2863 Loss: 1.6488667726516724\n",
      "Iteration 2864 Loss: 1.7408208847045898\n",
      "Iteration 2865 Loss: 1.2312636375427246\n",
      "Iteration 2866 Loss: 2.1676552295684814\n",
      "Iteration 2867 Loss: 2.020509719848633\n",
      "Iteration 2868 Loss: 2.2483692169189453\n",
      "Iteration 2869 Loss: 1.581835389137268\n",
      "Iteration 2869 Loss: 1.7311046123504639\n",
      "Iteration 2870 Loss: 2.0079989433288574\n",
      "Iteration 2871 Loss: 2.375032424926758\n",
      "Iteration 2872 Loss: 1.6392743587493896\n",
      "Iteration 2873 Loss: 1.5052040815353394\n",
      "Iteration 2874 Loss: 1.43555748462677\n",
      "Iteration 2875 Loss: 1.7228572368621826\n",
      "Iteration 2876 Loss: 1.9694654941558838\n",
      "Iteration 2877 Loss: 1.8853468894958496\n",
      "Iteration 2878 Loss: 1.539652705192566\n",
      "Iteration 2879 Loss: 1.3664017915725708\n",
      "Iteration 2879 Loss: 1.7446790933609009\n",
      "Iteration 2880 Loss: 1.3875095844268799\n",
      "Iteration 2881 Loss: 1.6652038097381592\n",
      "Iteration 2882 Loss: 2.4643197059631348\n",
      "Iteration 2883 Loss: 1.6916835308074951\n",
      "Iteration 2884 Loss: 1.4418216943740845\n",
      "Iteration 2885 Loss: 1.1644866466522217\n",
      "Iteration 2886 Loss: 1.4371263980865479\n",
      "Iteration 2887 Loss: 1.7352406978607178\n",
      "Iteration 2888 Loss: 1.7134876251220703\n",
      "Iteration 2889 Loss: 1.8434805870056152\n",
      "Iteration 2889 Loss: 1.6544361114501953\n",
      "Iteration 2890 Loss: 1.9650095701217651\n",
      "Iteration 2891 Loss: 1.956533670425415\n",
      "Iteration 2892 Loss: 1.9626426696777344\n",
      "Iteration 2893 Loss: 1.6769670248031616\n",
      "Iteration 2894 Loss: 1.3339247703552246\n",
      "Iteration 2895 Loss: 1.6142090559005737\n",
      "Iteration 2896 Loss: 1.5338155031204224\n",
      "Iteration 2897 Loss: 1.853845238685608\n",
      "Iteration 2898 Loss: 1.474898099899292\n",
      "Iteration 2899 Loss: 1.7547422647476196\n",
      "Iteration 2899 Loss: 1.7126586437225342\n",
      "Iteration 2900 Loss: 1.5530577898025513\n",
      "Iteration 2901 Loss: 1.9012647867202759\n",
      "Iteration 2902 Loss: 1.4864192008972168\n",
      "Iteration 2903 Loss: 1.2182880640029907\n",
      "Iteration 2904 Loss: 1.5697723627090454\n",
      "Iteration 2905 Loss: 1.1807589530944824\n",
      "Iteration 2906 Loss: 1.29608154296875\n",
      "Iteration 2907 Loss: 2.04630446434021\n",
      "Iteration 2908 Loss: 1.8913413286209106\n",
      "Iteration 2909 Loss: 1.9892793893814087\n",
      "Iteration 2909 Loss: 1.613256812095642\n",
      "Iteration 2910 Loss: 1.911083459854126\n",
      "Iteration 2911 Loss: 1.4018702507019043\n",
      "Iteration 2912 Loss: 2.038266181945801\n",
      "Iteration 2913 Loss: 2.1224842071533203\n",
      "Iteration 2914 Loss: 1.7445310354232788\n",
      "Iteration 2915 Loss: 1.5451871156692505\n",
      "Iteration 2916 Loss: 2.0016591548919678\n",
      "Iteration 2917 Loss: 1.860431432723999\n",
      "Iteration 2918 Loss: 1.7517603635787964\n",
      "Iteration 2919 Loss: 1.9206979274749756\n",
      "Iteration 2919 Loss: 1.8297970294952393\n",
      "Iteration 2920 Loss: 2.22564697265625\n",
      "Iteration 2921 Loss: 2.18039608001709\n",
      "Iteration 2922 Loss: 1.706451177597046\n",
      "Iteration 2923 Loss: 1.7386517524719238\n",
      "Iteration 2924 Loss: 1.7866898775100708\n",
      "Iteration 2925 Loss: 1.5978621244430542\n",
      "Iteration 2926 Loss: 1.898042917251587\n",
      "Iteration 2927 Loss: 1.7914040088653564\n",
      "Iteration 2928 Loss: 1.7301703691482544\n",
      "Iteration 2929 Loss: 1.9684540033340454\n",
      "Iteration 2929 Loss: 1.8623769283294678\n",
      "Iteration 2930 Loss: 1.9669525623321533\n",
      "Iteration 2931 Loss: 1.3342747688293457\n",
      "Iteration 2932 Loss: 1.5576257705688477\n",
      "Iteration 2933 Loss: 1.9034883975982666\n",
      "Iteration 2934 Loss: 1.5498933792114258\n",
      "Iteration 2935 Loss: 1.282264232635498\n",
      "Iteration 2936 Loss: 1.4515562057495117\n",
      "Iteration 2937 Loss: 1.660167932510376\n",
      "Iteration 2938 Loss: 1.927734136581421\n",
      "Iteration 2939 Loss: 1.9695380926132202\n",
      "Iteration 2939 Loss: 1.6603494882583618\n",
      "Iteration 2940 Loss: 1.7009180784225464\n",
      "Iteration 2941 Loss: 1.546998143196106\n",
      "Iteration 2942 Loss: 1.5774078369140625\n",
      "Iteration 2943 Loss: 1.4632983207702637\n",
      "Iteration 2944 Loss: 2.1718311309814453\n",
      "Iteration 2945 Loss: 1.6860878467559814\n",
      "Iteration 2946 Loss: 1.906024694442749\n",
      "Iteration 2947 Loss: 1.6467396020889282\n",
      "Iteration 2948 Loss: 1.667869210243225\n",
      "Iteration 2949 Loss: 1.3398332595825195\n",
      "Iteration 2949 Loss: 1.6707007884979248\n",
      "Iteration 2950 Loss: 2.343595504760742\n",
      "Iteration 2951 Loss: 1.9875973463058472\n",
      "Iteration 2952 Loss: 1.4132682085037231\n",
      "Iteration 2953 Loss: 1.749131202697754\n",
      "Iteration 2954 Loss: 1.7074778079986572\n",
      "Iteration 2955 Loss: 2.156954288482666\n",
      "Iteration 2956 Loss: 2.311343193054199\n",
      "Iteration 2957 Loss: 1.9152123928070068\n",
      "Iteration 2958 Loss: 1.4271252155303955\n",
      "Iteration 2959 Loss: 2.1009860038757324\n",
      "Iteration 2959 Loss: 1.911269187927246\n",
      "Iteration 2960 Loss: 2.1849312782287598\n",
      "Iteration 2961 Loss: 1.4806981086730957\n",
      "Iteration 2962 Loss: 1.8865857124328613\n",
      "Iteration 2963 Loss: 1.9997797012329102\n",
      "Iteration 2964 Loss: 1.8525439500808716\n",
      "Iteration 2965 Loss: 2.0286312103271484\n",
      "Iteration 2966 Loss: 1.5686553716659546\n",
      "Iteration 2967 Loss: 1.8612291812896729\n",
      "Iteration 2968 Loss: 1.550734281539917\n",
      "Iteration 2969 Loss: 1.70872163772583\n",
      "Iteration 2969 Loss: 1.812251091003418\n",
      "Iteration 2970 Loss: 1.7279890775680542\n",
      "Iteration 2971 Loss: 1.844199776649475\n",
      "Iteration 2972 Loss: 1.9506582021713257\n",
      "Iteration 2973 Loss: 1.7173964977264404\n",
      "Iteration 2974 Loss: 1.5903316736221313\n",
      "Iteration 2975 Loss: 1.8234513998031616\n",
      "Iteration 2976 Loss: 1.4419015645980835\n",
      "Iteration 2977 Loss: 1.0502458810806274\n",
      "Iteration 2978 Loss: 1.3432201147079468\n",
      "Iteration 2979 Loss: 1.7203820943832397\n",
      "Iteration 2979 Loss: 1.6209776401519775\n",
      "Iteration 2980 Loss: 1.5568898916244507\n",
      "Iteration 2981 Loss: 2.4204535484313965\n",
      "Iteration 2982 Loss: 1.7713745832443237\n",
      "Iteration 2983 Loss: 1.712066650390625\n",
      "Iteration 2984 Loss: 1.5421267747879028\n",
      "Iteration 2985 Loss: 1.9609063863754272\n",
      "Iteration 2986 Loss: 1.1151233911514282\n",
      "Iteration 2987 Loss: 1.5220386981964111\n",
      "Iteration 2988 Loss: 1.9490453004837036\n",
      "Iteration 2989 Loss: 1.7927782535552979\n",
      "Iteration 2989 Loss: 1.7342803478240967\n",
      "Iteration 2990 Loss: 1.8487249612808228\n",
      "Iteration 2991 Loss: 1.4057337045669556\n",
      "Iteration 2992 Loss: 2.0296456813812256\n",
      "Iteration 2993 Loss: 1.718274712562561\n",
      "Iteration 2994 Loss: 2.233409881591797\n",
      "Iteration 2995 Loss: 2.0123023986816406\n",
      "Iteration 2996 Loss: 1.5811079740524292\n",
      "Iteration 2997 Loss: 1.9886566400527954\n",
      "Iteration 2998 Loss: 1.9736238718032837\n",
      "Iteration 2999 Loss: 1.9890360832214355\n",
      "Iteration 2999 Loss: 1.878051519393921\n",
      "Iteration 3000 Loss: 1.6668757200241089\n",
      "Iteration 3001 Loss: 1.564403772354126\n",
      "Iteration 3002 Loss: 1.8523063659667969\n",
      "Iteration 3003 Loss: 1.4921164512634277\n",
      "Iteration 3004 Loss: 2.1064181327819824\n",
      "Iteration 3005 Loss: 1.1833513975143433\n",
      "Iteration 3006 Loss: 1.6252821683883667\n",
      "Iteration 3007 Loss: 1.4504667520523071\n",
      "Iteration 3008 Loss: 1.9030615091323853\n",
      "Iteration 3009 Loss: 1.8028724193572998\n",
      "Iteration 3009 Loss: 1.6647155284881592\n",
      "Iteration 3010 Loss: 1.461473822593689\n",
      "Iteration 3011 Loss: 1.0841401815414429\n",
      "Iteration 3012 Loss: 1.7430700063705444\n",
      "Iteration 3013 Loss: 1.6319197416305542\n",
      "Iteration 3014 Loss: 2.0232107639312744\n",
      "Iteration 3015 Loss: 1.628954291343689\n",
      "Iteration 3016 Loss: 1.7068519592285156\n",
      "Iteration 3017 Loss: 1.3842597007751465\n",
      "Iteration 3018 Loss: 1.607701063156128\n",
      "Iteration 3019 Loss: 2.09605073928833\n",
      "Iteration 3019 Loss: 1.6367632150650024\n",
      "Iteration 3020 Loss: 1.6781765222549438\n",
      "Iteration 3021 Loss: 1.4464455842971802\n",
      "Iteration 3022 Loss: 1.8080363273620605\n",
      "Iteration 3023 Loss: 1.8675557374954224\n",
      "Iteration 3024 Loss: 1.6383298635482788\n",
      "Iteration 3025 Loss: 2.310946226119995\n",
      "Iteration 3026 Loss: 2.2469708919525146\n",
      "Iteration 3027 Loss: 1.901332974433899\n",
      "Iteration 3028 Loss: 1.5990194082260132\n",
      "Iteration 3029 Loss: 1.8695614337921143\n",
      "Iteration 3029 Loss: 1.8366374969482422\n",
      "Iteration 3030 Loss: 1.4745326042175293\n",
      "Iteration 3031 Loss: 1.8439927101135254\n",
      "Iteration 3032 Loss: 1.5460572242736816\n",
      "Iteration 3033 Loss: 2.173931360244751\n",
      "Iteration 3034 Loss: 1.9438165426254272\n",
      "Iteration 3035 Loss: 1.6112679243087769\n",
      "Iteration 3036 Loss: 1.583251714706421\n",
      "Iteration 3037 Loss: 1.3174234628677368\n",
      "Iteration 3038 Loss: 1.7336804866790771\n",
      "Iteration 3039 Loss: 1.3231256008148193\n",
      "Iteration 3039 Loss: 1.655107855796814\n",
      "Iteration 3040 Loss: 2.382014513015747\n",
      "Iteration 3041 Loss: 1.0976378917694092\n",
      "Iteration 3042 Loss: 1.5895179510116577\n",
      "Iteration 3043 Loss: 2.0672643184661865\n",
      "Iteration 3044 Loss: 1.9199267625808716\n",
      "Iteration 3045 Loss: 1.5827256441116333\n",
      "Iteration 3046 Loss: 2.2619078159332275\n",
      "Iteration 3047 Loss: 2.2344112396240234\n",
      "Iteration 3048 Loss: 1.5859311819076538\n",
      "Iteration 3049 Loss: 2.0234947204589844\n",
      "Iteration 3049 Loss: 1.874483346939087\n",
      "Iteration 3050 Loss: 2.3975839614868164\n",
      "Iteration 3051 Loss: 1.5320664644241333\n",
      "Iteration 3052 Loss: 1.3307287693023682\n",
      "Iteration 3053 Loss: 1.80869722366333\n",
      "Iteration 3054 Loss: 1.6185548305511475\n",
      "Iteration 3055 Loss: 2.1037909984588623\n",
      "Iteration 3056 Loss: 1.308753252029419\n",
      "Iteration 3057 Loss: 1.4105324745178223\n",
      "Iteration 3058 Loss: 1.6931637525558472\n",
      "Iteration 3059 Loss: 1.7847812175750732\n",
      "Iteration 3059 Loss: 1.698865294456482\n",
      "Iteration 3060 Loss: 1.8541741371154785\n",
      "Iteration 3061 Loss: 2.299476385116577\n",
      "Iteration 3062 Loss: 2.008915662765503\n",
      "Iteration 3063 Loss: 1.3001762628555298\n",
      "Iteration 3064 Loss: 1.76425039768219\n",
      "Iteration 3065 Loss: 1.7535762786865234\n",
      "Iteration 3066 Loss: 1.1483935117721558\n",
      "Iteration 3067 Loss: 1.417812466621399\n",
      "Iteration 3068 Loss: 1.5421876907348633\n",
      "Iteration 3069 Loss: 1.386867880821228\n",
      "Iteration 3069 Loss: 1.647583246231079\n",
      "Iteration 3070 Loss: 1.8697034120559692\n",
      "Iteration 3071 Loss: 1.5848753452301025\n",
      "Iteration 3072 Loss: 1.8936564922332764\n",
      "Iteration 3073 Loss: 1.7799969911575317\n",
      "Iteration 3074 Loss: 1.3892427682876587\n",
      "Iteration 3075 Loss: 1.7626084089279175\n",
      "Iteration 3076 Loss: 1.5356365442276\n",
      "Iteration 3077 Loss: 1.9772628545761108\n",
      "Iteration 3078 Loss: 1.5050277709960938\n",
      "Iteration 3079 Loss: 1.1044260263442993\n",
      "Iteration 3079 Loss: 1.6402437686920166\n",
      "Iteration 3080 Loss: 1.3754196166992188\n",
      "Iteration 3081 Loss: 2.0681443214416504\n",
      "Iteration 3082 Loss: 2.1809351444244385\n",
      "Iteration 3083 Loss: 0.8993642926216125\n",
      "Iteration 3084 Loss: 1.4652860164642334\n",
      "Iteration 3085 Loss: 1.5853674411773682\n",
      "Iteration 3086 Loss: 1.732499599456787\n",
      "Iteration 3087 Loss: 1.8565773963928223\n",
      "Iteration 3088 Loss: 1.8925056457519531\n",
      "Iteration 3089 Loss: 1.3686013221740723\n",
      "Iteration 3089 Loss: 1.6424700021743774\n",
      "Iteration 3090 Loss: 1.7642930746078491\n",
      "Iteration 3091 Loss: 1.7917133569717407\n",
      "Iteration 3092 Loss: 2.102250337600708\n",
      "Iteration 3093 Loss: 1.8003785610198975\n",
      "Iteration 3094 Loss: 1.3729619979858398\n",
      "Iteration 3095 Loss: 1.6019902229309082\n",
      "Iteration 3096 Loss: 1.8171677589416504\n",
      "Iteration 3097 Loss: 1.807000994682312\n",
      "Iteration 3098 Loss: 1.894614577293396\n",
      "Iteration 3099 Loss: 2.1773645877838135\n",
      "Iteration 3099 Loss: 1.8129736185073853\n",
      "Iteration 3100 Loss: 2.2233848571777344\n",
      "Iteration 3101 Loss: 1.450383186340332\n",
      "Iteration 3102 Loss: 1.171667218208313\n",
      "Iteration 3103 Loss: 2.0608510971069336\n",
      "Iteration 3104 Loss: 1.7976386547088623\n",
      "Iteration 3105 Loss: 1.6044807434082031\n",
      "Iteration 3106 Loss: 1.3041764497756958\n",
      "Iteration 3107 Loss: 2.037322998046875\n",
      "Iteration 3108 Loss: 1.5454069375991821\n",
      "Iteration 3109 Loss: 1.8464407920837402\n",
      "Iteration 3109 Loss: 1.7041752338409424\n",
      "Iteration 3110 Loss: 1.1481623649597168\n",
      "Iteration 3111 Loss: 1.9027581214904785\n",
      "Iteration 3112 Loss: 1.9739065170288086\n",
      "Iteration 3113 Loss: 2.2405402660369873\n",
      "Iteration 3114 Loss: 2.1044862270355225\n",
      "Iteration 3115 Loss: 1.9307959079742432\n",
      "Iteration 3116 Loss: 1.056289553642273\n",
      "Iteration 3117 Loss: 1.5589632987976074\n",
      "Iteration 3118 Loss: 1.4870020151138306\n",
      "Iteration 3119 Loss: 1.7558530569076538\n",
      "Iteration 3119 Loss: 1.7158758640289307\n",
      "Iteration 3120 Loss: 1.9929269552230835\n",
      "Iteration 3121 Loss: 2.144169807434082\n",
      "Iteration 3122 Loss: 1.6621670722961426\n",
      "Iteration 3123 Loss: 2.3696842193603516\n",
      "Iteration 3124 Loss: 2.139101028442383\n",
      "Iteration 3125 Loss: 2.037778615951538\n",
      "Iteration 3126 Loss: 1.9758429527282715\n",
      "Iteration 3127 Loss: 2.273655891418457\n",
      "Iteration 3128 Loss: 2.1164894104003906\n",
      "Iteration 3129 Loss: 1.6671072244644165\n",
      "Iteration 3129 Loss: 2.0378925800323486\n",
      "Iteration 3130 Loss: 1.9773918390274048\n",
      "Iteration 3131 Loss: 2.048248052597046\n",
      "Iteration 3132 Loss: 1.997459053993225\n",
      "Iteration 3133 Loss: 2.1152915954589844\n",
      "Iteration 3134 Loss: 1.7445757389068604\n",
      "Iteration 3135 Loss: 1.7661041021347046\n",
      "Iteration 3136 Loss: 1.8331208229064941\n",
      "Iteration 3137 Loss: 2.0905964374542236\n",
      "Iteration 3138 Loss: 1.5062955617904663\n",
      "Iteration 3139 Loss: 1.950958251953125\n",
      "Iteration 3139 Loss: 1.9030042886734009\n",
      "Iteration 3140 Loss: 1.3474699258804321\n",
      "Iteration 3141 Loss: 1.3537694215774536\n",
      "Iteration 3142 Loss: 1.926892638206482\n",
      "Iteration 3143 Loss: 1.9129095077514648\n",
      "Iteration 3144 Loss: 1.9519580602645874\n",
      "Iteration 3145 Loss: 1.3664913177490234\n",
      "Iteration 3146 Loss: 1.7825878858566284\n",
      "Iteration 3147 Loss: 1.5265930891036987\n",
      "Iteration 3148 Loss: 1.7212437391281128\n",
      "Iteration 3149 Loss: 1.80404794216156\n",
      "Iteration 3149 Loss: 1.669396162033081\n",
      "Iteration 3150 Loss: 1.750234842300415\n",
      "Iteration 3151 Loss: 1.9976696968078613\n",
      "Iteration 3152 Loss: 1.8522601127624512\n",
      "Iteration 3153 Loss: 1.678207278251648\n",
      "Iteration 3154 Loss: 2.1857028007507324\n",
      "Iteration 3155 Loss: 1.8544957637786865\n",
      "Iteration 3156 Loss: 1.7790257930755615\n",
      "Iteration 3157 Loss: 1.7937804460525513\n",
      "Iteration 3158 Loss: 1.5996966361999512\n",
      "Iteration 3159 Loss: 1.6296210289001465\n",
      "Iteration 3159 Loss: 1.8120695352554321\n",
      "Iteration 3160 Loss: 1.4402673244476318\n",
      "Iteration 3161 Loss: 2.2440264225006104\n",
      "Iteration 3162 Loss: 1.9175529479980469\n",
      "Iteration 3163 Loss: 1.3125728368759155\n",
      "Iteration 3164 Loss: 1.5961833000183105\n",
      "Iteration 3165 Loss: 2.1793136596679688\n",
      "Iteration 3166 Loss: 1.9390947818756104\n",
      "Iteration 3167 Loss: 1.5105540752410889\n",
      "Iteration 3168 Loss: 2.254232883453369\n",
      "Iteration 3169 Loss: 1.6831403970718384\n",
      "Iteration 3169 Loss: 1.8076937198638916\n",
      "Iteration 3170 Loss: 1.1458582878112793\n",
      "Iteration 3171 Loss: 1.8122519254684448\n",
      "Iteration 3172 Loss: 1.565875768661499\n",
      "Iteration 3173 Loss: 1.6025208234786987\n",
      "Iteration 3174 Loss: 1.4969128370285034\n",
      "Iteration 3175 Loss: 1.5910683870315552\n",
      "Iteration 3176 Loss: 2.038128137588501\n",
      "Iteration 3177 Loss: 2.270448923110962\n",
      "Iteration 3178 Loss: 1.5914727449417114\n",
      "Iteration 3179 Loss: 1.8907328844070435\n",
      "Iteration 3179 Loss: 1.7005271911621094\n",
      "Iteration 3180 Loss: 1.4642201662063599\n",
      "Iteration 3181 Loss: 1.5675182342529297\n",
      "Iteration 3182 Loss: 1.989538311958313\n",
      "Iteration 3183 Loss: 1.95624840259552\n",
      "Iteration 3184 Loss: 1.481278896331787\n",
      "Iteration 3185 Loss: 1.501516580581665\n",
      "Iteration 3186 Loss: 1.9628695249557495\n",
      "Iteration 3187 Loss: 1.7958855628967285\n",
      "Iteration 3188 Loss: 1.7490736246109009\n",
      "Iteration 3189 Loss: 1.6162766218185425\n",
      "Iteration 3189 Loss: 1.7084424495697021\n",
      "Iteration 3190 Loss: 0.8765674233436584\n",
      "Iteration 3191 Loss: 1.9247138500213623\n",
      "Iteration 3192 Loss: 1.5738667249679565\n",
      "Iteration 3193 Loss: 2.1998283863067627\n",
      "Iteration 3194 Loss: 1.2156784534454346\n",
      "Iteration 3195 Loss: 2.0048391819000244\n",
      "Iteration 3196 Loss: 1.7484421730041504\n",
      "Iteration 3197 Loss: 1.2441551685333252\n",
      "Iteration 3198 Loss: 2.2963175773620605\n",
      "Iteration 3199 Loss: 1.3685837984085083\n",
      "Iteration 3199 Loss: 1.6452993154525757\n",
      "Iteration 3200 Loss: 1.751006007194519\n",
      "Iteration 3201 Loss: 1.8713136911392212\n",
      "Iteration 3202 Loss: 1.656383752822876\n",
      "Iteration 3203 Loss: 1.7711517810821533\n",
      "Iteration 3204 Loss: 1.201124906539917\n",
      "Iteration 3205 Loss: 1.6089156866073608\n",
      "Iteration 3206 Loss: 1.5109999179840088\n",
      "Iteration 3207 Loss: 1.7283285856246948\n",
      "Iteration 3208 Loss: 2.027747631072998\n",
      "Iteration 3209 Loss: 1.7631614208221436\n",
      "Iteration 3209 Loss: 1.6890132427215576\n",
      "Iteration 3210 Loss: 2.115668296813965\n",
      "Iteration 3211 Loss: 1.4992201328277588\n",
      "Iteration 3212 Loss: 2.0519516468048096\n",
      "Iteration 3213 Loss: 1.9349639415740967\n",
      "Iteration 3214 Loss: 1.3080112934112549\n",
      "Iteration 3215 Loss: 1.7550173997879028\n",
      "Iteration 3216 Loss: 1.590384840965271\n",
      "Iteration 3217 Loss: 1.918234944343567\n",
      "Iteration 3218 Loss: 1.8196462392807007\n",
      "Iteration 3219 Loss: 2.276664972305298\n",
      "Iteration 3219 Loss: 1.8269764184951782\n",
      "Iteration 3220 Loss: 1.128396987915039\n",
      "Iteration 3221 Loss: 2.3164477348327637\n",
      "Iteration 3222 Loss: 1.7700272798538208\n",
      "Iteration 3223 Loss: 1.8113032579421997\n",
      "Iteration 3224 Loss: 1.4382683038711548\n",
      "Iteration 3225 Loss: 1.9033182859420776\n",
      "Iteration 3226 Loss: 1.6692676544189453\n",
      "Iteration 3227 Loss: 1.7653250694274902\n",
      "Iteration 3228 Loss: 1.999547004699707\n",
      "Iteration 3229 Loss: 1.861728549003601\n",
      "Iteration 3229 Loss: 1.7663631439208984\n",
      "Iteration 3230 Loss: 1.219936728477478\n",
      "Iteration 3231 Loss: 2.0156328678131104\n",
      "Iteration 3232 Loss: 1.3931957483291626\n",
      "Iteration 3233 Loss: 1.6459848880767822\n",
      "Iteration 3234 Loss: 1.7371222972869873\n",
      "Iteration 3235 Loss: 1.9959299564361572\n",
      "Iteration 3236 Loss: 1.3627954721450806\n",
      "Iteration 3237 Loss: 1.922734022140503\n",
      "Iteration 3238 Loss: 2.0736923217773438\n",
      "Iteration 3239 Loss: 1.865311861038208\n",
      "Iteration 3239 Loss: 1.7232335805892944\n",
      "Iteration 3240 Loss: 2.0753250122070312\n",
      "Iteration 3241 Loss: 2.2249419689178467\n",
      "Iteration 3242 Loss: 1.5962250232696533\n",
      "Iteration 3243 Loss: 1.4337925910949707\n",
      "Iteration 3244 Loss: 1.573648452758789\n",
      "Iteration 3245 Loss: 1.7237451076507568\n",
      "Iteration 3246 Loss: 1.6633282899856567\n",
      "Iteration 3247 Loss: 2.1443796157836914\n",
      "Iteration 3248 Loss: 1.6212953329086304\n",
      "Iteration 3249 Loss: 1.607809066772461\n",
      "Iteration 3249 Loss: 1.766448974609375\n",
      "Iteration 3250 Loss: 1.7423896789550781\n",
      "Iteration 3251 Loss: 1.5248535871505737\n",
      "Iteration 3252 Loss: 1.7077399492263794\n",
      "Iteration 3253 Loss: 1.663642168045044\n",
      "Iteration 3254 Loss: 1.291929006576538\n",
      "Iteration 3255 Loss: 1.7658944129943848\n",
      "Iteration 3256 Loss: 1.751996397972107\n",
      "Iteration 3257 Loss: 1.9733943939208984\n",
      "Iteration 3258 Loss: 1.7253690958023071\n",
      "Iteration 3259 Loss: 1.5298744440078735\n",
      "Iteration 3259 Loss: 1.667708158493042\n",
      "Iteration 3260 Loss: 1.5022032260894775\n",
      "Iteration 3261 Loss: 1.3064849376678467\n",
      "Iteration 3262 Loss: 1.448438048362732\n",
      "Iteration 3263 Loss: 1.5013554096221924\n",
      "Iteration 3264 Loss: 2.2962892055511475\n",
      "Iteration 3265 Loss: 1.6550225019454956\n",
      "Iteration 3266 Loss: 1.525077223777771\n",
      "Iteration 3267 Loss: 2.3524200916290283\n",
      "Iteration 3268 Loss: 2.0674381256103516\n",
      "Iteration 3269 Loss: 1.3778126239776611\n",
      "Iteration 3269 Loss: 1.7032541036605835\n",
      "Iteration 3270 Loss: 1.7516281604766846\n",
      "Iteration 3271 Loss: 1.4236230850219727\n",
      "Iteration 3272 Loss: 1.5062026977539062\n",
      "Iteration 3273 Loss: 1.5131055116653442\n",
      "Iteration 3274 Loss: 2.0649876594543457\n",
      "Iteration 3275 Loss: 1.7090102434158325\n",
      "Iteration 3276 Loss: 1.3718997240066528\n",
      "Iteration 3277 Loss: 1.5756134986877441\n",
      "Iteration 3278 Loss: 1.4140658378601074\n",
      "Iteration 3279 Loss: 1.8935701847076416\n",
      "Iteration 3279 Loss: 1.6223704814910889\n",
      "Iteration 3280 Loss: 1.7126163244247437\n",
      "Iteration 3281 Loss: 1.326573371887207\n",
      "Iteration 3282 Loss: 1.5522733926773071\n",
      "Iteration 3283 Loss: 2.2819063663482666\n",
      "Iteration 3284 Loss: 2.0711562633514404\n",
      "Iteration 3285 Loss: 1.6991292238235474\n",
      "Iteration 3286 Loss: 1.772789716720581\n",
      "Iteration 3287 Loss: 1.4610873460769653\n",
      "Iteration 3288 Loss: 1.7768458127975464\n",
      "Iteration 3289 Loss: 1.834536075592041\n",
      "Iteration 3289 Loss: 1.7488914728164673\n",
      "Iteration 3290 Loss: 1.5094376802444458\n",
      "Iteration 3291 Loss: 2.005150318145752\n",
      "Iteration 3292 Loss: 1.7731307744979858\n",
      "Iteration 3293 Loss: 1.7479442358016968\n",
      "Iteration 3294 Loss: 1.7602077722549438\n",
      "Iteration 3295 Loss: 1.7675951719284058\n",
      "Iteration 3296 Loss: 2.527583360671997\n",
      "Iteration 3297 Loss: 2.016921281814575\n",
      "Iteration 3298 Loss: 1.7075039148330688\n",
      "Iteration 3299 Loss: 1.3600046634674072\n",
      "Iteration 3299 Loss: 1.8175480365753174\n",
      "Iteration 3300 Loss: 2.1397767066955566\n",
      "Iteration 3301 Loss: 1.486366629600525\n",
      "Iteration 3302 Loss: 1.3096426725387573\n",
      "Iteration 3303 Loss: 2.256704568862915\n",
      "Iteration 3304 Loss: 1.666078805923462\n",
      "Iteration 3305 Loss: 1.3473061323165894\n",
      "Iteration 3306 Loss: 1.7761478424072266\n",
      "Iteration 3307 Loss: 1.7602649927139282\n",
      "Iteration 3308 Loss: 1.5470948219299316\n",
      "Iteration 3309 Loss: 1.6231400966644287\n",
      "Iteration 3309 Loss: 1.69125235080719\n",
      "Iteration 3310 Loss: 1.8037053346633911\n",
      "Iteration 3311 Loss: 1.5087823867797852\n",
      "Iteration 3312 Loss: 1.8015419244766235\n",
      "Iteration 3313 Loss: 1.1728942394256592\n",
      "Iteration 3314 Loss: 1.8873425722122192\n",
      "Iteration 3315 Loss: 1.6287685632705688\n",
      "Iteration 3316 Loss: 1.3968064785003662\n",
      "Iteration 3317 Loss: 1.804970622062683\n",
      "Iteration 3318 Loss: 1.081921100616455\n",
      "Iteration 3319 Loss: 1.909164547920227\n",
      "Iteration 3319 Loss: 1.5995898246765137\n",
      "Iteration 3320 Loss: 1.1065917015075684\n",
      "Iteration 3321 Loss: 1.5310419797897339\n",
      "Iteration 3322 Loss: 1.8520522117614746\n",
      "Iteration 3323 Loss: 1.3722739219665527\n",
      "Iteration 3324 Loss: 2.0785436630249023\n",
      "Iteration 3325 Loss: 1.8384886980056763\n",
      "Iteration 3326 Loss: 1.2506600618362427\n",
      "Iteration 3327 Loss: 1.3039181232452393\n",
      "Iteration 3328 Loss: 2.077359676361084\n",
      "Iteration 3329 Loss: 1.2455154657363892\n",
      "Iteration 3329 Loss: 1.5656445026397705\n",
      "Iteration 3330 Loss: 1.819840669631958\n",
      "Iteration 3331 Loss: 1.3166964054107666\n",
      "Iteration 3332 Loss: 1.747021198272705\n",
      "Iteration 3333 Loss: 1.8187581300735474\n",
      "Iteration 3334 Loss: 1.0874717235565186\n",
      "Iteration 3335 Loss: 1.6759827136993408\n",
      "Iteration 3336 Loss: 1.5706026554107666\n",
      "Iteration 3337 Loss: 2.5919182300567627\n",
      "Iteration 3338 Loss: 2.1139943599700928\n",
      "Iteration 3339 Loss: 2.1528422832489014\n",
      "Iteration 3339 Loss: 1.7895128726959229\n",
      "Iteration 3340 Loss: 2.661313056945801\n",
      "Iteration 3341 Loss: 1.5402759313583374\n",
      "Iteration 3342 Loss: 1.8117705583572388\n",
      "Iteration 3343 Loss: 1.2714996337890625\n",
      "Iteration 3344 Loss: 1.976585865020752\n",
      "Iteration 3345 Loss: 1.9317764043807983\n",
      "Iteration 3346 Loss: 1.4929581880569458\n",
      "Iteration 3347 Loss: 1.3795711994171143\n",
      "Iteration 3348 Loss: 1.5605230331420898\n",
      "Iteration 3349 Loss: 2.184363842010498\n",
      "Iteration 3349 Loss: 1.7810637950897217\n",
      "Iteration 3350 Loss: 2.1626243591308594\n",
      "Iteration 3351 Loss: 1.090227723121643\n",
      "Iteration 3352 Loss: 1.5893664360046387\n",
      "Iteration 3353 Loss: 2.178438425064087\n",
      "Iteration 3354 Loss: 2.075254440307617\n",
      "Iteration 3355 Loss: 1.4438040256500244\n",
      "Iteration 3356 Loss: 1.7759394645690918\n",
      "Iteration 3357 Loss: 1.7504605054855347\n",
      "Iteration 3358 Loss: 1.3916720151901245\n",
      "Iteration 3359 Loss: 1.3321809768676758\n",
      "Iteration 3359 Loss: 1.6789966821670532\n",
      "Iteration 3360 Loss: 1.9191806316375732\n",
      "Iteration 3361 Loss: 1.4107507467269897\n",
      "Iteration 3362 Loss: 1.827046275138855\n",
      "Iteration 3363 Loss: 1.2741490602493286\n",
      "Iteration 3364 Loss: 1.3862683773040771\n",
      "Iteration 3365 Loss: 1.8760696649551392\n",
      "Iteration 3366 Loss: 1.6400799751281738\n",
      "Iteration 3367 Loss: 1.0910229682922363\n",
      "Iteration 3368 Loss: 1.56925630569458\n",
      "Iteration 3369 Loss: 1.3005242347717285\n",
      "Iteration 3369 Loss: 1.5294348001480103\n",
      "Iteration 3370 Loss: 1.9714704751968384\n",
      "Iteration 3371 Loss: 1.9950520992279053\n",
      "Iteration 3372 Loss: 2.2023398876190186\n",
      "Iteration 3373 Loss: 1.4498778581619263\n",
      "Iteration 3374 Loss: 1.6227854490280151\n",
      "Iteration 3375 Loss: 1.7076166868209839\n",
      "Iteration 3376 Loss: 1.9219043254852295\n",
      "Iteration 3377 Loss: 1.6231850385665894\n",
      "Iteration 3378 Loss: 1.0920469760894775\n",
      "Iteration 3379 Loss: 1.5706766843795776\n",
      "Iteration 3379 Loss: 1.7156956195831299\n",
      "Iteration 3380 Loss: 1.504492163658142\n",
      "Iteration 3381 Loss: 2.0653393268585205\n",
      "Iteration 3382 Loss: 1.644394874572754\n",
      "Iteration 3383 Loss: 1.4015337228775024\n",
      "Iteration 3384 Loss: 1.8894073963165283\n",
      "Iteration 3385 Loss: 1.3154528141021729\n",
      "Iteration 3386 Loss: 2.156829595565796\n",
      "Iteration 3387 Loss: 2.0373167991638184\n",
      "Iteration 3388 Loss: 1.5629353523254395\n",
      "Iteration 3389 Loss: 1.858982801437378\n",
      "Iteration 3389 Loss: 1.743668556213379\n",
      "Iteration 3390 Loss: 2.1131420135498047\n",
      "Iteration 3391 Loss: 1.6975860595703125\n",
      "Iteration 3392 Loss: 1.8306124210357666\n",
      "Iteration 3393 Loss: 1.4556833505630493\n",
      "Iteration 3394 Loss: 1.7271822690963745\n",
      "Iteration 3395 Loss: 1.9884555339813232\n",
      "Iteration 3396 Loss: 2.7148592472076416\n",
      "Iteration 3397 Loss: 1.5138704776763916\n",
      "Iteration 3398 Loss: 1.9440191984176636\n",
      "Iteration 3399 Loss: 1.5287320613861084\n",
      "Iteration 3399 Loss: 1.8514143228530884\n",
      "Iteration 3400 Loss: 1.6588948965072632\n",
      "Iteration 3401 Loss: 1.0887691974639893\n",
      "Iteration 3402 Loss: 1.4254647493362427\n",
      "Iteration 3403 Loss: 1.7163199186325073\n",
      "Iteration 3404 Loss: 1.5487791299819946\n",
      "Iteration 3405 Loss: 1.5309805870056152\n",
      "Iteration 3406 Loss: 1.1226444244384766\n",
      "Iteration 3407 Loss: 1.8401252031326294\n",
      "Iteration 3408 Loss: 1.5040096044540405\n",
      "Iteration 3409 Loss: 1.4094470739364624\n",
      "Iteration 3409 Loss: 1.4845435619354248\n",
      "Iteration 3410 Loss: 1.6686887741088867\n",
      "Iteration 3411 Loss: 1.6521070003509521\n",
      "Iteration 3412 Loss: 1.562544584274292\n",
      "Iteration 3413 Loss: 1.3710923194885254\n",
      "Iteration 3414 Loss: 1.4009677171707153\n",
      "Iteration 3415 Loss: 1.1112594604492188\n",
      "Iteration 3416 Loss: 1.7921761274337769\n",
      "Iteration 3417 Loss: 1.9181909561157227\n",
      "Iteration 3418 Loss: 1.766607403755188\n",
      "Iteration 3419 Loss: 1.400057077407837\n",
      "Iteration 3419 Loss: 1.5643690824508667\n",
      "Iteration 3420 Loss: 1.7831393480300903\n",
      "Iteration 3421 Loss: 1.0936623811721802\n",
      "Iteration 3422 Loss: 1.5690412521362305\n",
      "Iteration 3423 Loss: 1.3746883869171143\n",
      "Iteration 3424 Loss: 1.826011061668396\n",
      "Iteration 3425 Loss: 1.813299298286438\n",
      "Iteration 3426 Loss: 1.9434845447540283\n",
      "Iteration 3427 Loss: 1.5843929052352905\n",
      "Iteration 3428 Loss: 1.7006779909133911\n",
      "Iteration 3429 Loss: 1.4839255809783936\n",
      "Iteration 3429 Loss: 1.617232084274292\n",
      "Iteration 3430 Loss: 1.9359312057495117\n",
      "Iteration 3431 Loss: 1.6520389318466187\n",
      "Iteration 3432 Loss: 1.382856011390686\n",
      "Iteration 3433 Loss: 1.5686148405075073\n",
      "Iteration 3434 Loss: 1.749707579612732\n",
      "Iteration 3435 Loss: 2.0107834339141846\n",
      "Iteration 3436 Loss: 1.4456011056900024\n",
      "Iteration 3437 Loss: 2.3410418033599854\n",
      "Iteration 3438 Loss: 1.9511886835098267\n",
      "Iteration 3439 Loss: 1.7253519296646118\n",
      "Iteration 3439 Loss: 1.7763115167617798\n",
      "Iteration 3440 Loss: 1.624013066291809\n",
      "Iteration 3441 Loss: 1.6896926164627075\n",
      "Iteration 3442 Loss: 1.8043166399002075\n",
      "Iteration 3443 Loss: 1.8327645063400269\n",
      "Iteration 3444 Loss: 2.501796245574951\n",
      "Iteration 3445 Loss: 1.8911659717559814\n",
      "Iteration 3446 Loss: 2.030050039291382\n",
      "Iteration 3447 Loss: 1.3292725086212158\n",
      "Iteration 3448 Loss: 2.0097291469573975\n",
      "Iteration 3449 Loss: 1.183711290359497\n",
      "Iteration 3449 Loss: 1.7896511554718018\n",
      "Iteration 3450 Loss: 1.6558133363723755\n",
      "Iteration 3451 Loss: 1.8332167863845825\n",
      "Iteration 3452 Loss: 2.01615571975708\n",
      "Iteration 3453 Loss: 1.4047118425369263\n",
      "Iteration 3454 Loss: 1.397174596786499\n",
      "Iteration 3455 Loss: 1.88120436668396\n",
      "Iteration 3456 Loss: 1.6754432916641235\n",
      "Iteration 3457 Loss: 1.4642184972763062\n",
      "Iteration 3458 Loss: 2.1820993423461914\n",
      "Iteration 3459 Loss: 1.6761435270309448\n",
      "Iteration 3459 Loss: 1.7186181545257568\n",
      "Iteration 3460 Loss: 1.9146299362182617\n",
      "Iteration 3461 Loss: 1.7441163063049316\n",
      "Iteration 3462 Loss: 1.2923290729522705\n",
      "Iteration 3463 Loss: 1.7753517627716064\n",
      "Iteration 3464 Loss: 1.8238996267318726\n",
      "Iteration 3465 Loss: 2.0344066619873047\n",
      "Iteration 3466 Loss: 1.4339803457260132\n",
      "Iteration 3467 Loss: 1.539349913597107\n",
      "Iteration 3468 Loss: 1.8772324323654175\n",
      "Iteration 3469 Loss: 1.3187001943588257\n",
      "Iteration 3469 Loss: 1.6753995418548584\n",
      "Iteration 3470 Loss: 1.5778852701187134\n",
      "Iteration 3471 Loss: 1.818521499633789\n",
      "Iteration 3472 Loss: 1.8546336889266968\n",
      "Iteration 3473 Loss: 2.091949224472046\n",
      "Iteration 3474 Loss: 2.2183215618133545\n",
      "Iteration 3475 Loss: 1.599083423614502\n",
      "Iteration 3476 Loss: 2.0196642875671387\n",
      "Iteration 3477 Loss: 1.0360767841339111\n",
      "Iteration 3478 Loss: 1.8864768743515015\n",
      "Iteration 3479 Loss: 1.9668383598327637\n",
      "Iteration 3479 Loss: 1.8069452047348022\n",
      "Iteration 3480 Loss: 1.7270593643188477\n",
      "Iteration 3481 Loss: 1.7320343255996704\n",
      "Iteration 3482 Loss: 1.7980436086654663\n",
      "Iteration 3483 Loss: 2.355269432067871\n",
      "Iteration 3484 Loss: 1.5804897546768188\n",
      "Iteration 3485 Loss: 1.748077392578125\n",
      "Iteration 3486 Loss: 1.5934792757034302\n",
      "Iteration 3487 Loss: 1.5183420181274414\n",
      "Iteration 3488 Loss: 1.442484974861145\n",
      "Iteration 3489 Loss: 2.3092029094696045\n",
      "Iteration 3489 Loss: 1.78044855594635\n",
      "Iteration 3490 Loss: 1.7990708351135254\n",
      "Iteration 3491 Loss: 2.450176239013672\n",
      "Iteration 3492 Loss: 2.0377614498138428\n",
      "Iteration 3493 Loss: 2.393458604812622\n",
      "Iteration 3494 Loss: 1.851804494857788\n",
      "Iteration 3495 Loss: 1.8984251022338867\n",
      "Iteration 3496 Loss: 2.0954225063323975\n",
      "Iteration 3497 Loss: 1.9592405557632446\n",
      "Iteration 3498 Loss: 1.2368289232254028\n",
      "Iteration 3499 Loss: 1.5035799741744995\n",
      "Iteration 3499 Loss: 1.922576665878296\n",
      "Iteration 3500 Loss: 1.7974908351898193\n",
      "Iteration 3501 Loss: 1.6092616319656372\n",
      "Iteration 3502 Loss: 1.3514232635498047\n",
      "Iteration 3503 Loss: 2.0063743591308594\n",
      "Iteration 3504 Loss: 1.7217187881469727\n",
      "Iteration 3505 Loss: 1.25211763381958\n",
      "Iteration 3506 Loss: 1.9041337966918945\n",
      "Iteration 3507 Loss: 1.6821367740631104\n",
      "Iteration 3508 Loss: 1.670120120048523\n",
      "Iteration 3509 Loss: 1.5816154479980469\n",
      "Iteration 3509 Loss: 1.6576392650604248\n",
      "Iteration 3510 Loss: 1.8181281089782715\n",
      "Iteration 3511 Loss: 1.5335596799850464\n",
      "Iteration 3512 Loss: 1.7115671634674072\n",
      "Iteration 3513 Loss: 1.6400490999221802\n",
      "Iteration 3514 Loss: 1.9539943933486938\n",
      "Iteration 3515 Loss: 1.8980982303619385\n",
      "Iteration 3516 Loss: 1.4669259786605835\n",
      "Iteration 3517 Loss: 1.3859753608703613\n",
      "Iteration 3518 Loss: 1.4109935760498047\n",
      "Iteration 3519 Loss: 2.081601619720459\n",
      "Iteration 3519 Loss: 1.690089225769043\n",
      "Iteration 3520 Loss: 1.5499184131622314\n",
      "Iteration 3521 Loss: 1.7716532945632935\n",
      "Iteration 3522 Loss: 1.3181242942810059\n",
      "Iteration 3523 Loss: 2.2732677459716797\n",
      "Iteration 3524 Loss: 1.4898537397384644\n",
      "Iteration 3525 Loss: 1.3260393142700195\n",
      "Iteration 3526 Loss: 2.8867955207824707\n",
      "Iteration 3527 Loss: 2.0283868312835693\n",
      "Iteration 3528 Loss: 1.4720712900161743\n",
      "Iteration 3529 Loss: 1.0466355085372925\n",
      "Iteration 3529 Loss: 1.716274619102478\n",
      "Iteration 3530 Loss: 1.7618528604507446\n",
      "Iteration 3531 Loss: 1.925959825515747\n",
      "Iteration 3532 Loss: 1.6844736337661743\n",
      "Iteration 3533 Loss: 2.03485369682312\n",
      "Iteration 3534 Loss: 2.1076269149780273\n",
      "Iteration 3535 Loss: 1.7615597248077393\n",
      "Iteration 3536 Loss: 1.6384397745132446\n",
      "Iteration 3537 Loss: 2.1271674633026123\n",
      "Iteration 3538 Loss: 1.5530060529708862\n",
      "Iteration 3539 Loss: 1.4973329305648804\n",
      "Iteration 3539 Loss: 1.8092273473739624\n",
      "Iteration 3540 Loss: 2.203570604324341\n",
      "Iteration 3541 Loss: 2.252523183822632\n",
      "Iteration 3542 Loss: 1.4552001953125\n",
      "Iteration 3543 Loss: 1.694325566291809\n",
      "Iteration 3544 Loss: 1.8441613912582397\n",
      "Iteration 3545 Loss: 1.3529493808746338\n",
      "Iteration 3546 Loss: 1.0995086431503296\n",
      "Iteration 3547 Loss: 1.7531404495239258\n",
      "Iteration 3548 Loss: 1.9364736080169678\n",
      "Iteration 3549 Loss: 1.9206897020339966\n",
      "Iteration 3549 Loss: 1.7512543201446533\n",
      "Iteration 3550 Loss: 0.9576513767242432\n",
      "Iteration 3551 Loss: 1.4038282632827759\n",
      "Iteration 3552 Loss: 1.3111013174057007\n",
      "Iteration 3553 Loss: 1.4007331132888794\n",
      "Iteration 3554 Loss: 1.822104573249817\n",
      "Iteration 3555 Loss: 2.0128629207611084\n",
      "Iteration 3556 Loss: 1.8090022802352905\n",
      "Iteration 3557 Loss: 1.6951098442077637\n",
      "Iteration 3558 Loss: 2.060941219329834\n",
      "Iteration 3559 Loss: 1.297334909439087\n",
      "Iteration 3559 Loss: 1.5770670175552368\n",
      "Iteration 3560 Loss: 1.436297059059143\n",
      "Iteration 3561 Loss: 1.4127033948898315\n",
      "Iteration 3562 Loss: 2.0774660110473633\n",
      "Iteration 3563 Loss: 2.177008867263794\n",
      "Iteration 3564 Loss: 1.5251691341400146\n",
      "Iteration 3565 Loss: 1.9204885959625244\n",
      "Iteration 3566 Loss: 1.7779568433761597\n",
      "Iteration 3567 Loss: 1.5017071962356567\n",
      "Iteration 3568 Loss: 1.8950557708740234\n",
      "Iteration 3569 Loss: 1.9336879253387451\n",
      "Iteration 3569 Loss: 1.7657541036605835\n",
      "Iteration 3570 Loss: 1.82032310962677\n",
      "Iteration 3571 Loss: 1.840783953666687\n",
      "Iteration 3572 Loss: 1.4991955757141113\n",
      "Iteration 3573 Loss: 1.9621822834014893\n",
      "Iteration 3574 Loss: 1.591704249382019\n",
      "Iteration 3575 Loss: 1.2700754404067993\n",
      "Iteration 3576 Loss: 1.5640324354171753\n",
      "Iteration 3577 Loss: 1.3560810089111328\n",
      "Iteration 3578 Loss: 1.5212082862854004\n",
      "Iteration 3579 Loss: 1.6668795347213745\n",
      "Iteration 3579 Loss: 1.6092466115951538\n",
      "Iteration 3580 Loss: 1.724468469619751\n",
      "Iteration 3581 Loss: 1.253949522972107\n",
      "Iteration 3582 Loss: 1.7568994760513306\n",
      "Iteration 3583 Loss: 1.4863404035568237\n",
      "Iteration 3584 Loss: 1.7578508853912354\n",
      "Iteration 3585 Loss: 1.5715738534927368\n",
      "Iteration 3586 Loss: 1.0901790857315063\n",
      "Iteration 3587 Loss: 2.1866767406463623\n",
      "Iteration 3588 Loss: 1.312158226966858\n",
      "Iteration 3589 Loss: 2.0555357933044434\n",
      "Iteration 3589 Loss: 1.619563341140747\n",
      "Iteration 3590 Loss: 1.965561866760254\n",
      "Iteration 3591 Loss: 2.1152217388153076\n",
      "Iteration 3592 Loss: 1.9565478563308716\n",
      "Iteration 3593 Loss: 1.503678560256958\n",
      "Iteration 3594 Loss: 1.5364508628845215\n",
      "Iteration 3595 Loss: 1.7580053806304932\n",
      "Iteration 3596 Loss: 1.2366596460342407\n",
      "Iteration 3597 Loss: 1.6711273193359375\n",
      "Iteration 3598 Loss: 1.6282494068145752\n",
      "Iteration 3599 Loss: 1.5880314111709595\n",
      "Iteration 3599 Loss: 1.695953369140625\n",
      "Iteration 3600 Loss: 1.9343209266662598\n",
      "Iteration 3601 Loss: 1.641486644744873\n",
      "Iteration 3602 Loss: 1.100172519683838\n",
      "Iteration 3603 Loss: 2.1301541328430176\n",
      "Iteration 3604 Loss: 1.3668497800827026\n",
      "Iteration 3605 Loss: 1.2393457889556885\n",
      "Iteration 3606 Loss: 1.694595217704773\n",
      "Iteration 3607 Loss: 1.6869019269943237\n",
      "Iteration 3608 Loss: 1.6039626598358154\n",
      "Iteration 3609 Loss: 1.8185521364212036\n",
      "Iteration 3609 Loss: 1.6216341257095337\n",
      "Iteration 3610 Loss: 1.2046703100204468\n",
      "Iteration 3611 Loss: 1.8396830558776855\n",
      "Iteration 3612 Loss: 2.2570393085479736\n",
      "Iteration 3613 Loss: 1.2786731719970703\n",
      "Iteration 3614 Loss: 1.5530261993408203\n",
      "Iteration 3615 Loss: 2.059399366378784\n",
      "Iteration 3616 Loss: 1.9383625984191895\n",
      "Iteration 3617 Loss: 1.6345287561416626\n",
      "Iteration 3618 Loss: 1.804290771484375\n",
      "Iteration 3619 Loss: 1.7031974792480469\n",
      "Iteration 3619 Loss: 1.7272870540618896\n",
      "Iteration 3620 Loss: 1.49470055103302\n",
      "Iteration 3621 Loss: 1.6828727722167969\n",
      "Iteration 3622 Loss: 1.7736365795135498\n",
      "Iteration 3623 Loss: 1.8589329719543457\n",
      "Iteration 3624 Loss: 1.9560232162475586\n",
      "Iteration 3625 Loss: 1.2966349124908447\n",
      "Iteration 3626 Loss: 1.9301180839538574\n",
      "Iteration 3627 Loss: 1.3602079153060913\n",
      "Iteration 3628 Loss: 0.7087656855583191\n",
      "Iteration 3629 Loss: 1.8088533878326416\n",
      "Iteration 3629 Loss: 1.5870745182037354\n",
      "Iteration 3630 Loss: 2.003265857696533\n",
      "Iteration 3631 Loss: 1.9301568269729614\n",
      "Iteration 3632 Loss: 1.9620164632797241\n",
      "Iteration 3633 Loss: 1.1478694677352905\n",
      "Iteration 3634 Loss: 1.4073491096496582\n",
      "Iteration 3635 Loss: 2.1142189502716064\n",
      "Iteration 3636 Loss: 1.169679045677185\n",
      "Iteration 3637 Loss: 1.875167965888977\n",
      "Iteration 3638 Loss: 1.5413525104522705\n",
      "Iteration 3639 Loss: 1.4003868103027344\n",
      "Iteration 3639 Loss: 1.6551462411880493\n",
      "Iteration 3640 Loss: 1.7373045682907104\n",
      "Iteration 3641 Loss: 1.7586796283721924\n",
      "Iteration 3642 Loss: 1.666843295097351\n",
      "Iteration 3643 Loss: 1.9729993343353271\n",
      "Iteration 3644 Loss: 1.947601079940796\n",
      "Iteration 3645 Loss: 1.6874178647994995\n",
      "Iteration 3646 Loss: 2.4167513847351074\n",
      "Iteration 3647 Loss: 1.5954701900482178\n",
      "Iteration 3648 Loss: 1.7261784076690674\n",
      "Iteration 3649 Loss: 1.6140308380126953\n",
      "Iteration 3649 Loss: 1.8123277425765991\n",
      "Iteration 3650 Loss: 1.5817219018936157\n",
      "Iteration 3651 Loss: 1.5373984575271606\n",
      "Iteration 3652 Loss: 1.6773943901062012\n",
      "Iteration 3653 Loss: 1.7705786228179932\n",
      "Iteration 3654 Loss: 1.4951504468917847\n",
      "Iteration 3655 Loss: 1.5401755571365356\n",
      "Iteration 3656 Loss: 1.6965559720993042\n",
      "Iteration 3657 Loss: 1.0731662511825562\n",
      "Iteration 3658 Loss: 1.207931637763977\n",
      "Iteration 3659 Loss: 1.6997519731521606\n",
      "Iteration 3659 Loss: 1.527982473373413\n",
      "Iteration 3660 Loss: 1.7085769176483154\n",
      "Iteration 3661 Loss: 1.2939188480377197\n",
      "Iteration 3662 Loss: 1.1943284273147583\n",
      "Iteration 3663 Loss: 1.8983230590820312\n",
      "Iteration 3664 Loss: 1.7951078414916992\n",
      "Iteration 3665 Loss: 1.8666815757751465\n",
      "Iteration 3666 Loss: 1.689329981803894\n",
      "Iteration 3667 Loss: 1.6062229871749878\n",
      "Iteration 3668 Loss: 1.1159896850585938\n",
      "Iteration 3669 Loss: 1.19884192943573\n",
      "Iteration 3669 Loss: 1.5367320775985718\n",
      "Iteration 3670 Loss: 1.7743173837661743\n",
      "Iteration 3671 Loss: 1.4096401929855347\n",
      "Iteration 3672 Loss: 1.4412283897399902\n",
      "Iteration 3673 Loss: 1.8433358669281006\n",
      "Iteration 3674 Loss: 1.1444545984268188\n",
      "Iteration 3675 Loss: 1.5832217931747437\n",
      "Iteration 3676 Loss: 1.6105694770812988\n",
      "Iteration 3677 Loss: 1.3749679327011108\n",
      "Iteration 3678 Loss: 1.421381950378418\n",
      "Iteration 3679 Loss: 1.6628283262252808\n",
      "Iteration 3679 Loss: 1.5265945196151733\n",
      "Iteration 3680 Loss: 2.1017229557037354\n",
      "Iteration 3681 Loss: 1.6967352628707886\n",
      "Iteration 3682 Loss: 0.9611688852310181\n",
      "Iteration 3683 Loss: 1.9932876825332642\n",
      "Iteration 3684 Loss: 1.7373236417770386\n",
      "Iteration 3685 Loss: 2.0371463298797607\n",
      "Iteration 3686 Loss: 1.6455646753311157\n",
      "Iteration 3687 Loss: 1.8277580738067627\n",
      "Iteration 3688 Loss: 1.3069541454315186\n",
      "Iteration 3689 Loss: 1.477270245552063\n",
      "Iteration 3689 Loss: 1.6784932613372803\n",
      "Iteration 3690 Loss: 1.6602632999420166\n",
      "Iteration 3691 Loss: 1.5059274435043335\n",
      "Iteration 3692 Loss: 1.5080395936965942\n",
      "Iteration 3693 Loss: 1.320396065711975\n",
      "Iteration 3694 Loss: 1.6925690174102783\n",
      "Iteration 3695 Loss: 1.2273203134536743\n",
      "Iteration 3696 Loss: 1.3724591732025146\n",
      "Iteration 3697 Loss: 1.4289952516555786\n",
      "Iteration 3698 Loss: 1.658591389656067\n",
      "Iteration 3699 Loss: 1.8213163614273071\n",
      "Iteration 3699 Loss: 1.5195878744125366\n",
      "Iteration 3700 Loss: 1.2695735692977905\n",
      "Iteration 3701 Loss: 1.5065734386444092\n",
      "Iteration 3702 Loss: 1.8511513471603394\n",
      "Iteration 3703 Loss: 1.6194956302642822\n",
      "Iteration 3704 Loss: 1.7278045415878296\n",
      "Iteration 3705 Loss: 1.3180046081542969\n",
      "Iteration 3706 Loss: 1.6135836839675903\n",
      "Iteration 3707 Loss: 1.6041797399520874\n",
      "Iteration 3708 Loss: 1.9152296781539917\n",
      "Iteration 3709 Loss: 1.286750316619873\n",
      "Iteration 3709 Loss: 1.5712345838546753\n",
      "Iteration 3710 Loss: 1.6635212898254395\n",
      "Iteration 3711 Loss: 1.0101211071014404\n",
      "Iteration 3712 Loss: 1.9548447132110596\n",
      "Iteration 3713 Loss: 1.9265035390853882\n",
      "Iteration 3714 Loss: 1.186692714691162\n",
      "Iteration 3715 Loss: 1.6071194410324097\n",
      "Iteration 3716 Loss: 1.1413311958312988\n",
      "Iteration 3717 Loss: 1.7639801502227783\n",
      "Iteration 3718 Loss: 1.7573806047439575\n",
      "Iteration 3719 Loss: 1.3403542041778564\n",
      "Iteration 3719 Loss: 1.5351849794387817\n",
      "Iteration 3720 Loss: 1.310386300086975\n",
      "Iteration 3721 Loss: 2.164999485015869\n",
      "Iteration 3722 Loss: 1.7236372232437134\n",
      "Iteration 3723 Loss: 1.7699593305587769\n",
      "Iteration 3724 Loss: 1.5735832452774048\n",
      "Iteration 3725 Loss: 1.225717544555664\n",
      "Iteration 3726 Loss: 1.2702780961990356\n",
      "Iteration 3727 Loss: 1.3363956212997437\n",
      "Iteration 3728 Loss: 1.683266282081604\n",
      "Iteration 3729 Loss: 2.200542449951172\n",
      "Iteration 3729 Loss: 1.6258766651153564\n",
      "Iteration 3730 Loss: 2.3054957389831543\n",
      "Iteration 3731 Loss: 2.3816685676574707\n",
      "Iteration 3732 Loss: 1.856830358505249\n",
      "Iteration 3733 Loss: 1.533126950263977\n",
      "Iteration 3734 Loss: 1.4706027507781982\n",
      "Iteration 3735 Loss: 1.522281289100647\n",
      "Iteration 3736 Loss: 1.8896218538284302\n",
      "Iteration 3737 Loss: 1.5141634941101074\n",
      "Iteration 3738 Loss: 1.9259412288665771\n",
      "Iteration 3739 Loss: 1.5541871786117554\n",
      "Iteration 3739 Loss: 1.7953920364379883\n",
      "Iteration 3740 Loss: 1.4570103883743286\n",
      "Iteration 3741 Loss: 1.9981701374053955\n",
      "Iteration 3742 Loss: 2.0681350231170654\n",
      "Iteration 3743 Loss: 1.6883357763290405\n",
      "Iteration 3744 Loss: 1.8529096841812134\n",
      "Iteration 3745 Loss: 1.4130008220672607\n",
      "Iteration 3746 Loss: 1.418810486793518\n",
      "Iteration 3747 Loss: 1.641837477684021\n",
      "Iteration 3748 Loss: 1.9718005657196045\n",
      "Iteration 3749 Loss: 1.458149790763855\n",
      "Iteration 3749 Loss: 1.696816086769104\n",
      "Iteration 3750 Loss: 1.6905437707901\n",
      "Iteration 3751 Loss: 1.4393202066421509\n",
      "Iteration 3752 Loss: 2.110389471054077\n",
      "Iteration 3753 Loss: 1.707820177078247\n",
      "Iteration 3754 Loss: 1.4736816883087158\n",
      "Iteration 3755 Loss: 1.6309231519699097\n",
      "Iteration 3756 Loss: 1.3715025186538696\n",
      "Iteration 3757 Loss: 2.300717830657959\n",
      "Iteration 3758 Loss: 1.30971097946167\n",
      "Iteration 3759 Loss: 2.489605188369751\n",
      "Iteration 3759 Loss: 1.7524216175079346\n",
      "Iteration 3760 Loss: 1.4500421285629272\n",
      "Iteration 3761 Loss: 1.4929813146591187\n",
      "Iteration 3762 Loss: 0.7395820021629333\n",
      "Iteration 3763 Loss: 1.555016040802002\n",
      "Iteration 3764 Loss: 1.4367941617965698\n",
      "Iteration 3765 Loss: 2.025405168533325\n",
      "Iteration 3766 Loss: 1.995569109916687\n",
      "Iteration 3767 Loss: 1.2120513916015625\n",
      "Iteration 3768 Loss: 1.792750358581543\n",
      "Iteration 3769 Loss: 2.0663952827453613\n",
      "Iteration 3769 Loss: 1.5766587257385254\n",
      "Iteration 3770 Loss: 1.8426154851913452\n",
      "Iteration 3771 Loss: 1.9636565446853638\n",
      "Iteration 3772 Loss: 1.223611831665039\n",
      "Iteration 3773 Loss: 1.8695034980773926\n",
      "Iteration 3774 Loss: 1.4651119709014893\n",
      "Iteration 3775 Loss: 2.1404471397399902\n",
      "Iteration 3776 Loss: 1.1393938064575195\n",
      "Iteration 3777 Loss: 1.004941463470459\n",
      "Iteration 3778 Loss: 2.1697895526885986\n",
      "Iteration 3779 Loss: 1.5217777490615845\n",
      "Iteration 3779 Loss: 1.634084701538086\n",
      "Iteration 3780 Loss: 1.6160472631454468\n",
      "Iteration 3781 Loss: 1.9100918769836426\n",
      "Iteration 3782 Loss: 1.4909359216690063\n",
      "Iteration 3783 Loss: 1.7477715015411377\n",
      "Iteration 3784 Loss: 1.8194010257720947\n",
      "Iteration 3785 Loss: 2.3530642986297607\n",
      "Iteration 3786 Loss: 1.2300573587417603\n",
      "Iteration 3787 Loss: 1.9151346683502197\n",
      "Iteration 3788 Loss: 1.5952550172805786\n",
      "Iteration 3789 Loss: 1.1025322675704956\n",
      "Iteration 3789 Loss: 1.6780292987823486\n",
      "Iteration 3790 Loss: 1.7584596872329712\n",
      "Iteration 3791 Loss: 1.4010894298553467\n",
      "Iteration 3792 Loss: 1.5344654321670532\n",
      "Iteration 3793 Loss: 1.6884211301803589\n",
      "Iteration 3794 Loss: 1.7897942066192627\n",
      "Iteration 3795 Loss: 2.444736957550049\n",
      "Iteration 3796 Loss: 1.5949913263320923\n",
      "Iteration 3797 Loss: 1.283787727355957\n",
      "Iteration 3798 Loss: 1.3681923151016235\n",
      "Iteration 3799 Loss: 1.151308536529541\n",
      "Iteration 3799 Loss: 1.6015247106552124\n",
      "Iteration 3800 Loss: 1.9662166833877563\n",
      "Iteration 3801 Loss: 1.305894374847412\n",
      "Iteration 3802 Loss: 1.6686768531799316\n",
      "Iteration 3803 Loss: 1.8065812587738037\n",
      "Iteration 3804 Loss: 1.3826425075531006\n",
      "Iteration 3805 Loss: 1.495321273803711\n",
      "Iteration 3806 Loss: 1.8633545637130737\n",
      "Iteration 3807 Loss: 1.372185230255127\n",
      "Iteration 3808 Loss: 1.5631613731384277\n",
      "Iteration 3809 Loss: 1.9543743133544922\n",
      "Iteration 3809 Loss: 1.6378408670425415\n",
      "Iteration 3810 Loss: 1.8676254749298096\n",
      "Iteration 3811 Loss: 1.2902588844299316\n",
      "Iteration 3812 Loss: 1.9056434631347656\n",
      "Iteration 3813 Loss: 2.022848606109619\n",
      "Iteration 3814 Loss: 2.0787293910980225\n",
      "Iteration 3815 Loss: 1.635446548461914\n",
      "Iteration 3816 Loss: 1.6956167221069336\n",
      "Iteration 3817 Loss: 1.5594213008880615\n",
      "Iteration 3818 Loss: 1.7952880859375\n",
      "Iteration 3819 Loss: 1.9317421913146973\n",
      "Iteration 3819 Loss: 1.7782618999481201\n",
      "Iteration 3820 Loss: 1.829119086265564\n",
      "Iteration 3821 Loss: 1.0348223447799683\n",
      "Iteration 3822 Loss: 1.5954493284225464\n",
      "Iteration 3823 Loss: 1.4126620292663574\n",
      "Iteration 3824 Loss: 1.5110838413238525\n",
      "Iteration 3825 Loss: 1.6359567642211914\n",
      "Iteration 3826 Loss: 1.6969438791275024\n",
      "Iteration 3827 Loss: 1.636582374572754\n",
      "Iteration 3828 Loss: 1.542609691619873\n",
      "Iteration 3829 Loss: 1.128753900527954\n",
      "Iteration 3829 Loss: 1.5023983716964722\n",
      "Iteration 3830 Loss: 1.719441294670105\n",
      "Iteration 3831 Loss: 1.3983938694000244\n",
      "Iteration 3832 Loss: 1.5573168992996216\n",
      "Iteration 3833 Loss: 1.2985692024230957\n",
      "Iteration 3834 Loss: 1.4729869365692139\n",
      "Iteration 3835 Loss: 1.4886884689331055\n",
      "Iteration 3836 Loss: 1.5794187784194946\n",
      "Iteration 3837 Loss: 1.5531214475631714\n",
      "Iteration 3838 Loss: 1.2949589490890503\n",
      "Iteration 3839 Loss: 1.4633073806762695\n",
      "Iteration 3839 Loss: 1.482620358467102\n",
      "Iteration 3840 Loss: 2.0725879669189453\n",
      "Iteration 3841 Loss: 1.5266278982162476\n",
      "Iteration 3842 Loss: 1.1857671737670898\n",
      "Iteration 3843 Loss: 2.0196497440338135\n",
      "Iteration 3844 Loss: 1.4921934604644775\n",
      "Iteration 3845 Loss: 1.9204705953598022\n",
      "Iteration 3846 Loss: 1.78092360496521\n",
      "Iteration 3847 Loss: 1.8893373012542725\n",
      "Iteration 3848 Loss: 1.6565650701522827\n",
      "Iteration 3849 Loss: 1.4500882625579834\n",
      "Iteration 3849 Loss: 1.6994211673736572\n",
      "Iteration 3850 Loss: 0.9864988923072815\n",
      "Iteration 3851 Loss: 1.5829322338104248\n",
      "Iteration 3852 Loss: 2.2677645683288574\n",
      "Iteration 3853 Loss: 1.6280654668807983\n",
      "Iteration 3854 Loss: 1.78508460521698\n",
      "Iteration 3855 Loss: 1.5958939790725708\n",
      "Iteration 3856 Loss: 1.7789225578308105\n",
      "Iteration 3857 Loss: 1.8948924541473389\n",
      "Iteration 3858 Loss: 1.7959412336349487\n",
      "Iteration 3859 Loss: 1.890358567237854\n",
      "Iteration 3859 Loss: 1.720635175704956\n",
      "Iteration 3860 Loss: 1.9879274368286133\n",
      "Iteration 3861 Loss: 1.7086505889892578\n",
      "Iteration 3862 Loss: 1.4763879776000977\n",
      "Iteration 3863 Loss: 1.4436123371124268\n",
      "Iteration 3864 Loss: 1.621680736541748\n",
      "Iteration 3865 Loss: 1.7081573009490967\n",
      "Iteration 3866 Loss: 1.6814639568328857\n",
      "Iteration 3867 Loss: 1.4840344190597534\n",
      "Iteration 3868 Loss: 1.6555293798446655\n",
      "Iteration 3869 Loss: 2.034740686416626\n",
      "Iteration 3869 Loss: 1.6802183389663696\n",
      "Iteration 3870 Loss: 1.8259559869766235\n",
      "Iteration 3871 Loss: 1.9770677089691162\n",
      "Iteration 3872 Loss: 1.328178882598877\n",
      "Iteration 3873 Loss: 1.9049971103668213\n",
      "Iteration 3874 Loss: 1.5036547183990479\n",
      "Iteration 3875 Loss: 1.5762680768966675\n",
      "Iteration 3876 Loss: 1.2454488277435303\n",
      "Iteration 3877 Loss: 1.8329707384109497\n",
      "Iteration 3878 Loss: 1.5654813051223755\n",
      "Iteration 3879 Loss: 1.6053028106689453\n",
      "Iteration 3879 Loss: 1.6365325450897217\n",
      "Iteration 3880 Loss: 1.3665984869003296\n",
      "Iteration 3881 Loss: 1.2860581874847412\n",
      "Iteration 3882 Loss: 1.7994904518127441\n",
      "Iteration 3883 Loss: 1.4751054048538208\n",
      "Iteration 3884 Loss: 1.2806239128112793\n",
      "Iteration 3885 Loss: 1.4717401266098022\n",
      "Iteration 3886 Loss: 1.094679832458496\n",
      "Iteration 3887 Loss: 1.3162167072296143\n",
      "Iteration 3888 Loss: 1.8896781206130981\n",
      "Iteration 3889 Loss: 1.4939426183700562\n",
      "Iteration 3889 Loss: 1.4474133253097534\n",
      "Iteration 3890 Loss: 1.748767614364624\n",
      "Iteration 3891 Loss: 1.5611900091171265\n",
      "Iteration 3892 Loss: 1.6642853021621704\n",
      "Iteration 3893 Loss: 1.4519529342651367\n",
      "Iteration 3894 Loss: 1.856919765472412\n",
      "Iteration 3895 Loss: 1.7320616245269775\n",
      "Iteration 3896 Loss: 1.5815972089767456\n",
      "Iteration 3897 Loss: 1.4367332458496094\n",
      "Iteration 3898 Loss: 1.6433000564575195\n",
      "Iteration 3899 Loss: 1.95278799533844\n",
      "Iteration 3899 Loss: 1.6629596948623657\n",
      "Iteration 3900 Loss: 1.356015920639038\n",
      "Iteration 3901 Loss: 1.726248860359192\n",
      "Iteration 3902 Loss: 2.152190923690796\n",
      "Iteration 3903 Loss: 2.0510106086730957\n",
      "Iteration 3904 Loss: 1.9479643106460571\n",
      "Iteration 3905 Loss: 1.5669232606887817\n",
      "Iteration 3906 Loss: 1.23750638961792\n",
      "Iteration 3907 Loss: 1.4860458374023438\n",
      "Iteration 3908 Loss: 1.9459121227264404\n",
      "Iteration 3909 Loss: 1.7514729499816895\n",
      "Iteration 3909 Loss: 1.7221291065216064\n",
      "Iteration 3910 Loss: 2.027535915374756\n",
      "Iteration 3911 Loss: 1.722365140914917\n",
      "Iteration 3912 Loss: 1.3665571212768555\n",
      "Iteration 3913 Loss: 1.5892421007156372\n",
      "Iteration 3914 Loss: 1.4238961935043335\n",
      "Iteration 3915 Loss: 1.5614113807678223\n",
      "Iteration 3916 Loss: 2.1659159660339355\n",
      "Iteration 3917 Loss: 1.307753562927246\n",
      "Iteration 3918 Loss: 1.8960022926330566\n",
      "Iteration 3919 Loss: 1.7196044921875\n",
      "Iteration 3919 Loss: 1.6780284643173218\n",
      "Iteration 3920 Loss: 1.6495442390441895\n",
      "Iteration 3921 Loss: 2.041475534439087\n",
      "Iteration 3922 Loss: 2.0846939086914062\n",
      "Iteration 3923 Loss: 2.155482292175293\n",
      "Iteration 3924 Loss: 2.297848701477051\n",
      "Iteration 3925 Loss: 0.9342333674430847\n",
      "Iteration 3926 Loss: 1.1233444213867188\n",
      "Iteration 3927 Loss: 1.5608536005020142\n",
      "Iteration 3928 Loss: 1.25870943069458\n",
      "Iteration 3929 Loss: 1.3353960514068604\n",
      "Iteration 3929 Loss: 1.644158124923706\n",
      "Iteration 3930 Loss: 1.7643547058105469\n",
      "Iteration 3931 Loss: 1.1720765829086304\n",
      "Iteration 3932 Loss: 1.4910074472427368\n",
      "Iteration 3933 Loss: 1.6214497089385986\n",
      "Iteration 3934 Loss: 1.1350736618041992\n",
      "Iteration 3935 Loss: 1.9220274686813354\n",
      "Iteration 3936 Loss: 1.7416807413101196\n",
      "Iteration 3937 Loss: 1.450915813446045\n",
      "Iteration 3938 Loss: 1.7865811586380005\n",
      "Iteration 3939 Loss: 1.796311616897583\n",
      "Iteration 3939 Loss: 1.5881478786468506\n",
      "Iteration 3940 Loss: 1.8915783166885376\n",
      "Iteration 3941 Loss: 1.867150902748108\n",
      "Iteration 3942 Loss: 1.9537466764450073\n",
      "Iteration 3943 Loss: 1.4571937322616577\n",
      "Iteration 3944 Loss: 1.4704699516296387\n",
      "Iteration 3945 Loss: 1.4573326110839844\n",
      "Iteration 3946 Loss: 1.3940097093582153\n",
      "Iteration 3947 Loss: 1.6324318647384644\n",
      "Iteration 3948 Loss: 1.5443856716156006\n",
      "Iteration 3949 Loss: 1.7863420248031616\n",
      "Iteration 3949 Loss: 1.6454641819000244\n",
      "Iteration 3950 Loss: 1.765048861503601\n",
      "Iteration 3951 Loss: 1.5733873844146729\n",
      "Iteration 3952 Loss: 1.7718135118484497\n",
      "Iteration 3953 Loss: 1.403043270111084\n",
      "Iteration 3954 Loss: 1.4760080575942993\n",
      "Iteration 3955 Loss: 2.5179200172424316\n",
      "Iteration 3956 Loss: 1.4924206733703613\n",
      "Iteration 3957 Loss: 1.504223108291626\n",
      "Iteration 3958 Loss: 2.005876064300537\n",
      "Iteration 3959 Loss: 1.6586803197860718\n",
      "Iteration 3959 Loss: 1.7168420553207397\n",
      "Iteration 3960 Loss: 1.0391179323196411\n",
      "Iteration 3961 Loss: 1.4567979574203491\n",
      "Iteration 3962 Loss: 2.1107873916625977\n",
      "Iteration 3963 Loss: 1.8413615226745605\n",
      "Iteration 3964 Loss: 1.3279465436935425\n",
      "Iteration 3965 Loss: 1.492862582206726\n",
      "Iteration 3966 Loss: 2.164647340774536\n",
      "Iteration 3967 Loss: 1.7727798223495483\n",
      "Iteration 3968 Loss: 1.4682508707046509\n",
      "Iteration 3969 Loss: 1.6239802837371826\n",
      "Iteration 3969 Loss: 1.6298532485961914\n",
      "Iteration 3970 Loss: 1.716413140296936\n",
      "Iteration 3971 Loss: 1.311895489692688\n",
      "Iteration 3972 Loss: 1.5361714363098145\n",
      "Iteration 3973 Loss: 1.1207603216171265\n",
      "Iteration 3974 Loss: 2.138986825942993\n",
      "Iteration 3975 Loss: 1.4919291734695435\n",
      "Iteration 3976 Loss: 1.613726258277893\n",
      "Iteration 3977 Loss: 1.3632477521896362\n",
      "Iteration 3978 Loss: 1.7009605169296265\n",
      "Iteration 3979 Loss: 1.7537298202514648\n",
      "Iteration 3979 Loss: 1.574782133102417\n",
      "Iteration 3980 Loss: 1.0839669704437256\n",
      "Iteration 3981 Loss: 1.6080459356307983\n",
      "Iteration 3982 Loss: 1.6417264938354492\n",
      "Iteration 3983 Loss: 1.6676864624023438\n",
      "Iteration 3984 Loss: 1.3832756280899048\n",
      "Iteration 3985 Loss: 1.6905581951141357\n",
      "Iteration 3986 Loss: 1.66236412525177\n",
      "Iteration 3987 Loss: 1.1485074758529663\n",
      "Iteration 3988 Loss: 1.273862361907959\n",
      "Iteration 3989 Loss: 1.3789033889770508\n",
      "Iteration 3989 Loss: 1.4538897275924683\n",
      "Iteration 3990 Loss: 1.9328391551971436\n",
      "Iteration 3991 Loss: 0.792766273021698\n",
      "Iteration 3992 Loss: 1.7759628295898438\n",
      "Iteration 3993 Loss: 1.6352379322052002\n",
      "Iteration 3994 Loss: 1.846411943435669\n",
      "Iteration 3995 Loss: 2.27561354637146\n",
      "Iteration 3996 Loss: 2.171400785446167\n",
      "Iteration 3997 Loss: 2.4647650718688965\n",
      "Iteration 3998 Loss: 1.5399773120880127\n",
      "Iteration 3999 Loss: 1.5907881259918213\n",
      "Iteration 3999 Loss: 1.8025763034820557\n",
      "Iteration 4000 Loss: 1.5042568445205688\n",
      "Iteration 4001 Loss: 1.606130838394165\n",
      "Iteration 4002 Loss: 1.5569517612457275\n",
      "Iteration 4003 Loss: 1.6907085180282593\n",
      "Iteration 4004 Loss: 2.191704750061035\n",
      "Iteration 4005 Loss: 1.9028923511505127\n",
      "Iteration 4006 Loss: 2.046947956085205\n",
      "Iteration 4007 Loss: 1.4213873147964478\n",
      "Iteration 4008 Loss: 1.8231539726257324\n",
      "Iteration 4009 Loss: 2.415475368499756\n",
      "Iteration 4009 Loss: 1.8159608840942383\n",
      "Iteration 4010 Loss: 1.0595015287399292\n",
      "Iteration 4011 Loss: 1.9844223260879517\n",
      "Iteration 4012 Loss: 1.7050966024398804\n",
      "Iteration 4013 Loss: 1.8132621049880981\n",
      "Iteration 4014 Loss: 1.6746692657470703\n",
      "Iteration 4015 Loss: 1.579105257987976\n",
      "Iteration 4016 Loss: 1.6042742729187012\n",
      "Iteration 4017 Loss: 1.5681240558624268\n",
      "Iteration 4018 Loss: 1.4800739288330078\n",
      "Iteration 4019 Loss: 1.8784047365188599\n",
      "Iteration 4019 Loss: 1.6346935033798218\n",
      "Iteration 4020 Loss: 1.282135248184204\n",
      "Iteration 4021 Loss: 1.6608585119247437\n",
      "Iteration 4022 Loss: 1.3410377502441406\n",
      "Iteration 4023 Loss: 1.759287714958191\n",
      "Iteration 4024 Loss: 1.3411895036697388\n",
      "Iteration 4025 Loss: 1.7380268573760986\n",
      "Iteration 4026 Loss: 1.2480597496032715\n",
      "Iteration 4027 Loss: 1.3631638288497925\n",
      "Iteration 4028 Loss: 1.3703267574310303\n",
      "Iteration 4029 Loss: 2.106029987335205\n",
      "Iteration 4029 Loss: 1.5210115909576416\n",
      "Iteration 4030 Loss: 1.6501044034957886\n",
      "Iteration 4031 Loss: 1.686996579170227\n",
      "Iteration 4032 Loss: 1.7998846769332886\n",
      "Iteration 4033 Loss: 1.5455271005630493\n",
      "Iteration 4034 Loss: 1.725508689880371\n",
      "Iteration 4035 Loss: 1.8224756717681885\n",
      "Iteration 4036 Loss: 1.4459084272384644\n",
      "Iteration 4037 Loss: 1.8847500085830688\n",
      "Iteration 4038 Loss: 1.9288160800933838\n",
      "Iteration 4039 Loss: 1.6245579719543457\n",
      "Iteration 4039 Loss: 1.711452841758728\n",
      "Iteration 4040 Loss: 2.0916500091552734\n",
      "Iteration 4041 Loss: 1.822955846786499\n",
      "Iteration 4042 Loss: 1.5327187776565552\n",
      "Iteration 4043 Loss: 2.0342116355895996\n",
      "Iteration 4044 Loss: 1.1441524028778076\n",
      "Iteration 4045 Loss: 1.222232460975647\n",
      "Iteration 4046 Loss: 1.6171643733978271\n",
      "Iteration 4047 Loss: 1.860168695449829\n",
      "Iteration 4048 Loss: 2.5643515586853027\n",
      "Iteration 4049 Loss: 1.917633056640625\n",
      "Iteration 4049 Loss: 1.7807239294052124\n",
      "Iteration 4050 Loss: 1.505921483039856\n",
      "Iteration 4051 Loss: 1.7370822429656982\n",
      "Iteration 4052 Loss: 1.6271517276763916\n",
      "Iteration 4053 Loss: 1.0237709283828735\n",
      "Iteration 4054 Loss: 1.4804705381393433\n",
      "Iteration 4055 Loss: 1.4243717193603516\n",
      "Iteration 4056 Loss: 1.4878405332565308\n",
      "Iteration 4057 Loss: 1.9714140892028809\n",
      "Iteration 4058 Loss: 1.5282841920852661\n",
      "Iteration 4059 Loss: 1.9289004802703857\n",
      "Iteration 4059 Loss: 1.5715208053588867\n",
      "Iteration 4060 Loss: 1.4759936332702637\n",
      "Iteration 4061 Loss: 1.6421728134155273\n",
      "Iteration 4062 Loss: 2.057933807373047\n",
      "Iteration 4063 Loss: 1.2074832916259766\n",
      "Iteration 4064 Loss: 1.637612223625183\n",
      "Iteration 4065 Loss: 1.5050686597824097\n",
      "Iteration 4066 Loss: 1.5639214515686035\n",
      "Iteration 4067 Loss: 1.6615058183670044\n",
      "Iteration 4068 Loss: 1.6351983547210693\n",
      "Iteration 4069 Loss: 2.076246738433838\n",
      "Iteration 4069 Loss: 1.6463139057159424\n",
      "Iteration 4070 Loss: 1.4119611978530884\n",
      "Iteration 4071 Loss: 1.3780101537704468\n",
      "Iteration 4072 Loss: 1.7619065046310425\n",
      "Iteration 4073 Loss: 1.6961878538131714\n",
      "Iteration 4074 Loss: 1.656672716140747\n",
      "Iteration 4075 Loss: 1.4983489513397217\n",
      "Iteration 4076 Loss: 1.4607713222503662\n",
      "Iteration 4077 Loss: 1.5736169815063477\n",
      "Iteration 4078 Loss: 1.8549880981445312\n",
      "Iteration 4079 Loss: 1.9904216527938843\n",
      "Iteration 4079 Loss: 1.6282886266708374\n",
      "Iteration 4080 Loss: 1.688503623008728\n",
      "Iteration 4081 Loss: 2.212023973464966\n",
      "Iteration 4082 Loss: 1.091179370880127\n",
      "Iteration 4083 Loss: 1.8153036832809448\n",
      "Iteration 4084 Loss: 1.5575993061065674\n",
      "Iteration 4085 Loss: 1.5681349039077759\n",
      "Iteration 4086 Loss: 1.5325267314910889\n",
      "Iteration 4087 Loss: 1.570230484008789\n",
      "Iteration 4088 Loss: 2.3479413986206055\n",
      "Iteration 4089 Loss: 1.8934372663497925\n",
      "Iteration 4089 Loss: 1.727688193321228\n",
      "Iteration 4090 Loss: 1.5312519073486328\n",
      "Iteration 4091 Loss: 1.8829858303070068\n",
      "Iteration 4092 Loss: 1.6328935623168945\n",
      "Iteration 4093 Loss: 1.785151720046997\n",
      "Iteration 4094 Loss: 1.7042135000228882\n",
      "Iteration 4095 Loss: 1.9546418190002441\n",
      "Iteration 4096 Loss: 1.7431480884552002\n",
      "Iteration 4097 Loss: 1.3818780183792114\n",
      "Iteration 4098 Loss: 1.1290937662124634\n",
      "Iteration 4099 Loss: 1.571290373802185\n",
      "Iteration 4099 Loss: 1.631654977798462\n",
      "Iteration 4100 Loss: 2.089303970336914\n",
      "Iteration 4101 Loss: 1.7343122959136963\n",
      "Iteration 4102 Loss: 1.6832711696624756\n",
      "Iteration 4103 Loss: 1.8514097929000854\n",
      "Iteration 4104 Loss: 1.3869214057922363\n",
      "Iteration 4105 Loss: 1.4916538000106812\n",
      "Iteration 4106 Loss: 1.9269992113113403\n",
      "Iteration 4107 Loss: 1.3224964141845703\n",
      "Iteration 4108 Loss: 1.7646883726119995\n",
      "Iteration 4109 Loss: 1.6450074911117554\n",
      "Iteration 4109 Loss: 1.6896064281463623\n",
      "Iteration 4110 Loss: 1.5577830076217651\n",
      "Iteration 4111 Loss: 1.5849484205245972\n",
      "Iteration 4112 Loss: 1.762081503868103\n",
      "Iteration 4113 Loss: 1.2868974208831787\n",
      "Iteration 4114 Loss: 1.5615445375442505\n",
      "Iteration 4115 Loss: 1.7380272150039673\n",
      "Iteration 4116 Loss: 1.649889349937439\n",
      "Iteration 4117 Loss: 1.4615710973739624\n",
      "Iteration 4118 Loss: 2.059175729751587\n",
      "Iteration 4119 Loss: 2.006757974624634\n",
      "Iteration 4119 Loss: 1.6668676137924194\n",
      "Iteration 4120 Loss: 1.7170512676239014\n",
      "Iteration 4121 Loss: 1.546201229095459\n",
      "Iteration 4122 Loss: 1.8016705513000488\n",
      "Iteration 4123 Loss: 1.7193058729171753\n",
      "Iteration 4124 Loss: 1.7184550762176514\n",
      "Iteration 4125 Loss: 0.9873643517494202\n",
      "Iteration 4126 Loss: 1.6425275802612305\n",
      "Iteration 4127 Loss: 1.4832967519760132\n",
      "Iteration 4128 Loss: 1.6508110761642456\n",
      "Iteration 4129 Loss: 1.711500883102417\n",
      "Iteration 4129 Loss: 1.5978184938430786\n",
      "Iteration 4130 Loss: 1.7288320064544678\n",
      "Iteration 4131 Loss: 1.6095722913742065\n",
      "Iteration 4132 Loss: 1.7535957098007202\n",
      "Iteration 4133 Loss: 1.6880954504013062\n",
      "Iteration 4134 Loss: 1.1880403757095337\n",
      "Iteration 4135 Loss: 1.6539993286132812\n",
      "Iteration 4136 Loss: 1.1623120307922363\n",
      "Iteration 4137 Loss: 1.5011581182479858\n",
      "Iteration 4138 Loss: 1.8883066177368164\n",
      "Iteration 4139 Loss: 1.8676261901855469\n",
      "Iteration 4139 Loss: 1.6041538715362549\n",
      "Iteration 4140 Loss: 1.2665879726409912\n",
      "Iteration 4141 Loss: 1.6093719005584717\n",
      "Iteration 4142 Loss: 1.9082183837890625\n",
      "Iteration 4143 Loss: 1.4560892581939697\n",
      "Iteration 4144 Loss: 1.6252895593643188\n",
      "Iteration 4145 Loss: 1.9787558317184448\n",
      "Iteration 4146 Loss: 1.3750076293945312\n",
      "Iteration 4147 Loss: 1.7698224782943726\n",
      "Iteration 4148 Loss: 1.8034688234329224\n",
      "Iteration 4149 Loss: 1.683742880821228\n",
      "Iteration 4149 Loss: 1.6476354598999023\n",
      "Iteration 4150 Loss: 1.9008798599243164\n",
      "Iteration 4151 Loss: 1.7652932405471802\n",
      "Iteration 4152 Loss: 1.799832820892334\n",
      "Iteration 4153 Loss: 1.392054557800293\n",
      "Iteration 4154 Loss: 1.5170788764953613\n",
      "Iteration 4155 Loss: 1.4992568492889404\n",
      "Iteration 4156 Loss: 1.6305725574493408\n",
      "Iteration 4157 Loss: 1.7652406692504883\n",
      "Iteration 4158 Loss: 1.827803373336792\n",
      "Iteration 4159 Loss: 1.6822632551193237\n",
      "Iteration 4159 Loss: 1.6780277490615845\n",
      "Iteration 4160 Loss: 2.012183904647827\n",
      "Iteration 4161 Loss: 1.4060829877853394\n",
      "Iteration 4162 Loss: 1.5174541473388672\n",
      "Iteration 4163 Loss: 1.5283243656158447\n",
      "Iteration 4164 Loss: 1.7600089311599731\n",
      "Iteration 4165 Loss: 1.740753412246704\n",
      "Iteration 4166 Loss: 1.1581807136535645\n",
      "Iteration 4167 Loss: 1.0388606786727905\n",
      "Iteration 4168 Loss: 1.5096428394317627\n",
      "Iteration 4169 Loss: 1.9733806848526\n",
      "Iteration 4169 Loss: 1.5644872188568115\n",
      "Iteration 4170 Loss: 2.088578224182129\n",
      "Iteration 4171 Loss: 1.6991668939590454\n",
      "Iteration 4172 Loss: 2.2465476989746094\n",
      "Iteration 4173 Loss: 1.6627600193023682\n",
      "Iteration 4174 Loss: 1.6535308361053467\n",
      "Iteration 4175 Loss: 1.377967119216919\n",
      "Iteration 4176 Loss: 1.7301791906356812\n",
      "Iteration 4177 Loss: 1.5000641345977783\n",
      "Iteration 4178 Loss: 1.6455316543579102\n",
      "Iteration 4179 Loss: 1.7392023801803589\n",
      "Iteration 4179 Loss: 1.7343528270721436\n",
      "Iteration 4180 Loss: 1.8481864929199219\n",
      "Iteration 4181 Loss: 2.107802152633667\n",
      "Iteration 4182 Loss: 1.7795443534851074\n",
      "Iteration 4183 Loss: 1.6826833486557007\n",
      "Iteration 4184 Loss: 1.212203025817871\n",
      "Iteration 4185 Loss: 1.5234136581420898\n",
      "Iteration 4186 Loss: 1.7257598638534546\n",
      "Iteration 4187 Loss: 1.8742341995239258\n",
      "Iteration 4188 Loss: 0.920377254486084\n",
      "Iteration 4189 Loss: 1.843687653541565\n",
      "Iteration 4189 Loss: 1.6517890691757202\n",
      "Iteration 4190 Loss: 1.5320208072662354\n",
      "Iteration 4191 Loss: 1.730843186378479\n",
      "Iteration 4192 Loss: 1.7404080629348755\n",
      "Iteration 4193 Loss: 2.187350273132324\n",
      "Iteration 4194 Loss: 1.3399262428283691\n",
      "Iteration 4195 Loss: 2.258197069168091\n",
      "Iteration 4196 Loss: 1.0554200410842896\n",
      "Iteration 4197 Loss: 1.7748175859451294\n",
      "Iteration 4198 Loss: 1.7045965194702148\n",
      "Iteration 4199 Loss: 1.7087591886520386\n",
      "Iteration 4199 Loss: 1.7032339572906494\n",
      "Iteration 4200 Loss: 1.3816962242126465\n",
      "Iteration 4201 Loss: 1.1391199827194214\n",
      "Iteration 4202 Loss: 1.4651963710784912\n",
      "Iteration 4203 Loss: 1.5380321741104126\n",
      "Iteration 4204 Loss: 1.6107573509216309\n",
      "Iteration 4205 Loss: 0.9281331896781921\n",
      "Iteration 4206 Loss: 2.1043128967285156\n",
      "Iteration 4207 Loss: 0.5577298402786255\n",
      "Iteration 4208 Loss: 1.375646710395813\n",
      "Iteration 4209 Loss: 1.405839443206787\n",
      "Iteration 4209 Loss: 1.3506463766098022\n",
      "Iteration 4210 Loss: 1.2492159605026245\n",
      "Iteration 4211 Loss: 1.5915710926055908\n",
      "Iteration 4212 Loss: 1.3196367025375366\n",
      "Iteration 4213 Loss: 1.6135882139205933\n",
      "Iteration 4214 Loss: 1.255317211151123\n",
      "Iteration 4215 Loss: 1.5592103004455566\n",
      "Iteration 4216 Loss: 1.8238170146942139\n",
      "Iteration 4217 Loss: 1.554573893547058\n",
      "Iteration 4218 Loss: 1.0054309368133545\n",
      "Iteration 4219 Loss: 1.6764636039733887\n",
      "Iteration 4219 Loss: 1.464882493019104\n",
      "Iteration 4220 Loss: 1.8462327718734741\n",
      "Iteration 4221 Loss: 1.9391578435897827\n",
      "Iteration 4222 Loss: 1.5066317319869995\n",
      "Iteration 4223 Loss: 1.9436370134353638\n",
      "Iteration 4224 Loss: 1.2285078763961792\n",
      "Iteration 4225 Loss: 1.2709484100341797\n",
      "Iteration 4226 Loss: 1.632702112197876\n",
      "Iteration 4227 Loss: 1.642349123954773\n",
      "Iteration 4228 Loss: 1.765305995941162\n",
      "Iteration 4229 Loss: 1.5546494722366333\n",
      "Iteration 4229 Loss: 1.6330121755599976\n",
      "Iteration 4230 Loss: 1.4135332107543945\n",
      "Iteration 4231 Loss: 1.3633776903152466\n",
      "Iteration 4232 Loss: 1.1305636167526245\n",
      "Iteration 4233 Loss: 1.1918727159500122\n",
      "Iteration 4234 Loss: 1.2525287866592407\n",
      "Iteration 4235 Loss: 1.6778569221496582\n",
      "Iteration 4236 Loss: 2.0822644233703613\n",
      "Iteration 4237 Loss: 2.0233376026153564\n",
      "Iteration 4238 Loss: 1.3834803104400635\n",
      "Iteration 4239 Loss: 1.591099739074707\n",
      "Iteration 4239 Loss: 1.5109913349151611\n",
      "Iteration 4240 Loss: 1.1553846597671509\n",
      "Iteration 4241 Loss: 1.5097240209579468\n",
      "Iteration 4242 Loss: 1.6085841655731201\n",
      "Iteration 4243 Loss: 1.5202124118804932\n",
      "Iteration 4244 Loss: 1.4409908056259155\n",
      "Iteration 4245 Loss: 1.3241188526153564\n",
      "Iteration 4246 Loss: 1.1295350790023804\n",
      "Iteration 4247 Loss: 1.8055351972579956\n",
      "Iteration 4248 Loss: 1.382422924041748\n",
      "Iteration 4249 Loss: 1.4689486026763916\n",
      "Iteration 4249 Loss: 1.434545636177063\n",
      "Iteration 4250 Loss: 1.6474003791809082\n",
      "Iteration 4251 Loss: 1.4621108770370483\n",
      "Iteration 4252 Loss: 1.5463351011276245\n",
      "Iteration 4253 Loss: 1.9283522367477417\n",
      "Iteration 4254 Loss: 2.088279962539673\n",
      "Iteration 4255 Loss: 1.6722551584243774\n",
      "Iteration 4256 Loss: 1.670937180519104\n",
      "Iteration 4257 Loss: 1.6636080741882324\n",
      "Iteration 4258 Loss: 1.8862024545669556\n",
      "Iteration 4259 Loss: 1.1856566667556763\n",
      "Iteration 4259 Loss: 1.6751139163970947\n",
      "Iteration 4260 Loss: 1.5902494192123413\n",
      "Iteration 4261 Loss: 1.6701653003692627\n",
      "Iteration 4262 Loss: 2.1274759769439697\n",
      "Iteration 4263 Loss: 1.7566488981246948\n",
      "Iteration 4264 Loss: 1.9664151668548584\n",
      "Iteration 4265 Loss: 2.1007063388824463\n",
      "Iteration 4266 Loss: 1.6937187910079956\n",
      "Iteration 4267 Loss: 1.479630708694458\n",
      "Iteration 4268 Loss: 1.3141906261444092\n",
      "Iteration 4269 Loss: 2.1983823776245117\n",
      "Iteration 4269 Loss: 1.789758324623108\n",
      "Iteration 4270 Loss: 2.0328187942504883\n",
      "Iteration 4271 Loss: 1.3950003385543823\n",
      "Iteration 4272 Loss: 1.4673374891281128\n",
      "Iteration 4273 Loss: 2.1458425521850586\n",
      "Iteration 4274 Loss: 1.8003993034362793\n",
      "Iteration 4275 Loss: 1.8004869222640991\n",
      "Iteration 4276 Loss: 1.9810969829559326\n",
      "Iteration 4277 Loss: 1.4554835557937622\n",
      "Iteration 4278 Loss: 1.5052268505096436\n",
      "Iteration 4279 Loss: 1.871390700340271\n",
      "Iteration 4279 Loss: 1.7455084323883057\n",
      "Iteration 4280 Loss: 1.6909160614013672\n",
      "Iteration 4281 Loss: 1.426157832145691\n",
      "Iteration 4282 Loss: 1.5130146741867065\n",
      "Iteration 4283 Loss: 1.2854833602905273\n",
      "Iteration 4284 Loss: 1.4892594814300537\n",
      "Iteration 4285 Loss: 1.5084513425827026\n",
      "Iteration 4286 Loss: 1.2839668989181519\n",
      "Iteration 4287 Loss: 1.7749155759811401\n",
      "Iteration 4288 Loss: 1.930812120437622\n",
      "Iteration 4289 Loss: 1.458778977394104\n",
      "Iteration 4289 Loss: 1.5361757278442383\n",
      "Iteration 4290 Loss: 2.1772265434265137\n",
      "Iteration 4291 Loss: 1.6120131015777588\n",
      "Iteration 4292 Loss: 1.4767929315567017\n",
      "Iteration 4293 Loss: 1.8764574527740479\n",
      "Iteration 4294 Loss: 1.4491604566574097\n",
      "Iteration 4295 Loss: 1.8697986602783203\n",
      "Iteration 4296 Loss: 1.7987639904022217\n",
      "Iteration 4297 Loss: 1.3926196098327637\n",
      "Iteration 4298 Loss: 1.5702242851257324\n",
      "Iteration 4299 Loss: 1.5201952457427979\n",
      "Iteration 4299 Loss: 1.6743252277374268\n",
      "Iteration 4300 Loss: 1.7933013439178467\n",
      "Iteration 4301 Loss: 1.9795795679092407\n",
      "Iteration 4302 Loss: 1.5164121389389038\n",
      "Iteration 4303 Loss: 1.237552523612976\n",
      "Iteration 4304 Loss: 1.7116503715515137\n",
      "Iteration 4305 Loss: 1.9144130945205688\n",
      "Iteration 4306 Loss: 1.6223905086517334\n",
      "Iteration 4307 Loss: 1.1515625715255737\n",
      "Iteration 4308 Loss: 0.8281841278076172\n",
      "Iteration 4309 Loss: 1.6512542963027954\n",
      "Iteration 4309 Loss: 1.5406301021575928\n",
      "Iteration 4310 Loss: 1.3653370141983032\n",
      "Iteration 4311 Loss: 1.1632498502731323\n",
      "Iteration 4312 Loss: 1.720546841621399\n",
      "Iteration 4313 Loss: 1.7608495950698853\n",
      "Iteration 4314 Loss: 1.9080055952072144\n",
      "Iteration 4315 Loss: 1.4492967128753662\n",
      "Iteration 4316 Loss: 1.3604402542114258\n",
      "Iteration 4317 Loss: 1.6685569286346436\n",
      "Iteration 4318 Loss: 1.795515537261963\n",
      "Iteration 4319 Loss: 1.4321659803390503\n",
      "Iteration 4319 Loss: 1.56239652633667\n",
      "Iteration 4320 Loss: 1.7668890953063965\n",
      "Iteration 4321 Loss: 1.77121102809906\n",
      "Iteration 4322 Loss: 1.8244473934173584\n",
      "Iteration 4323 Loss: 1.3763540983200073\n",
      "Iteration 4324 Loss: 1.7292163372039795\n",
      "Iteration 4325 Loss: 1.8956881761550903\n",
      "Iteration 4326 Loss: 1.731146216392517\n",
      "Iteration 4327 Loss: 1.5794589519500732\n",
      "Iteration 4328 Loss: 1.13948655128479\n",
      "Iteration 4329 Loss: 1.8653684854507446\n",
      "Iteration 4329 Loss: 1.667926549911499\n",
      "Iteration 4330 Loss: 1.4700807332992554\n",
      "Iteration 4331 Loss: 1.6294277906417847\n",
      "Iteration 4332 Loss: 1.0899988412857056\n",
      "Iteration 4333 Loss: 1.4761461019515991\n",
      "Iteration 4334 Loss: 1.691438913345337\n",
      "Iteration 4335 Loss: 1.1870611906051636\n",
      "Iteration 4336 Loss: 1.3083523511886597\n",
      "Iteration 4337 Loss: 1.737825870513916\n",
      "Iteration 4338 Loss: 1.6438649892807007\n",
      "Iteration 4339 Loss: 1.8244796991348267\n",
      "Iteration 4339 Loss: 1.505867600440979\n",
      "Iteration 4340 Loss: 2.0212230682373047\n",
      "Iteration 4341 Loss: 2.0494394302368164\n",
      "Iteration 4342 Loss: 1.046889305114746\n",
      "Iteration 4343 Loss: 1.5947341918945312\n",
      "Iteration 4344 Loss: 1.6630327701568604\n",
      "Iteration 4345 Loss: 1.8416438102722168\n",
      "Iteration 4346 Loss: 1.0789928436279297\n",
      "Iteration 4347 Loss: 1.4668006896972656\n",
      "Iteration 4348 Loss: 2.153236150741577\n",
      "Iteration 4349 Loss: 1.4697824716567993\n",
      "Iteration 4349 Loss: 1.6385774612426758\n",
      "Iteration 4350 Loss: 1.407493233680725\n",
      "Iteration 4351 Loss: 1.8042938709259033\n",
      "Iteration 4352 Loss: 1.8432502746582031\n",
      "Iteration 4353 Loss: 1.4216827154159546\n",
      "Iteration 4354 Loss: 1.454485297203064\n",
      "Iteration 4355 Loss: 1.8018615245819092\n",
      "Iteration 4356 Loss: 1.482120156288147\n",
      "Iteration 4357 Loss: 1.2548291683197021\n",
      "Iteration 4358 Loss: 2.1175191402435303\n",
      "Iteration 4359 Loss: 2.0220656394958496\n",
      "Iteration 4359 Loss: 1.6609599590301514\n",
      "Iteration 4360 Loss: 1.4285515546798706\n",
      "Iteration 4361 Loss: 1.7676771879196167\n",
      "Iteration 4362 Loss: 1.626352071762085\n",
      "Iteration 4363 Loss: 1.122307300567627\n",
      "Iteration 4364 Loss: 1.5671590566635132\n",
      "Iteration 4365 Loss: 1.8739588260650635\n",
      "Iteration 4366 Loss: 1.828550100326538\n",
      "Iteration 4367 Loss: 1.2806040048599243\n",
      "Iteration 4368 Loss: 1.8979299068450928\n",
      "Iteration 4369 Loss: 1.71882164478302\n",
      "Iteration 4369 Loss: 1.6111911535263062\n",
      "Iteration 4370 Loss: 1.341786503791809\n",
      "Iteration 4371 Loss: 1.9052937030792236\n",
      "Iteration 4372 Loss: 1.761758804321289\n",
      "Iteration 4373 Loss: 1.3630317449569702\n",
      "Iteration 4374 Loss: 1.4962410926818848\n",
      "Iteration 4375 Loss: 1.4477001428604126\n",
      "Iteration 4376 Loss: 1.4068272113800049\n",
      "Iteration 4377 Loss: 1.4574447870254517\n",
      "Iteration 4378 Loss: 1.2113350629806519\n",
      "Iteration 4379 Loss: 1.4241513013839722\n",
      "Iteration 4379 Loss: 1.481557011604309\n",
      "Iteration 4380 Loss: 1.5218043327331543\n",
      "Iteration 4381 Loss: 1.529471755027771\n",
      "Iteration 4382 Loss: 1.2630457878112793\n",
      "Iteration 4383 Loss: 1.6252506971359253\n",
      "Iteration 4384 Loss: 1.379494071006775\n",
      "Iteration 4385 Loss: 2.0825140476226807\n",
      "Iteration 4386 Loss: 0.8269772529602051\n",
      "Iteration 4387 Loss: 1.7740432024002075\n",
      "Iteration 4388 Loss: 2.0603435039520264\n",
      "Iteration 4389 Loss: 1.655718445777893\n",
      "Iteration 4389 Loss: 1.5718662738800049\n",
      "Iteration 4390 Loss: 1.2230916023254395\n",
      "Iteration 4391 Loss: 1.50301194190979\n",
      "Iteration 4392 Loss: 1.4940361976623535\n",
      "Iteration 4393 Loss: 1.0936247110366821\n",
      "Iteration 4394 Loss: 1.495971918106079\n",
      "Iteration 4395 Loss: 1.4319016933441162\n",
      "Iteration 4396 Loss: 1.996700406074524\n",
      "Iteration 4397 Loss: 1.8606418371200562\n",
      "Iteration 4398 Loss: 1.7567096948623657\n",
      "Iteration 4399 Loss: 1.423308253288269\n",
      "Iteration 4399 Loss: 1.5278997421264648\n",
      "Iteration 4400 Loss: 1.3837316036224365\n",
      "Iteration 4401 Loss: 1.4810891151428223\n",
      "Iteration 4402 Loss: 1.4070285558700562\n",
      "Iteration 4403 Loss: 1.3558409214019775\n",
      "Iteration 4404 Loss: 2.0075037479400635\n",
      "Iteration 4405 Loss: 1.775623083114624\n",
      "Iteration 4406 Loss: 1.8068264722824097\n",
      "Iteration 4407 Loss: 2.0836527347564697\n",
      "Iteration 4408 Loss: 1.4893051385879517\n",
      "Iteration 4409 Loss: 1.633793592453003\n",
      "Iteration 4409 Loss: 1.6424394845962524\n",
      "Iteration 4410 Loss: 2.240936517715454\n",
      "Iteration 4411 Loss: 1.8641107082366943\n",
      "Iteration 4412 Loss: 1.5136868953704834\n",
      "Iteration 4413 Loss: 1.7250028848648071\n",
      "Iteration 4414 Loss: 1.8487926721572876\n",
      "Iteration 4415 Loss: 1.5715091228485107\n",
      "Iteration 4416 Loss: 1.8171306848526\n",
      "Iteration 4417 Loss: 1.7725391387939453\n",
      "Iteration 4418 Loss: 1.7472305297851562\n",
      "Iteration 4419 Loss: 1.9779911041259766\n",
      "Iteration 4419 Loss: 1.80789315700531\n",
      "Iteration 4420 Loss: 1.2505440711975098\n",
      "Iteration 4421 Loss: 0.8650760650634766\n",
      "Iteration 4422 Loss: 1.269391655921936\n",
      "Iteration 4423 Loss: 1.8413276672363281\n",
      "Iteration 4424 Loss: 1.3738044500350952\n",
      "Iteration 4425 Loss: 2.439626455307007\n",
      "Iteration 4426 Loss: 1.6534374952316284\n",
      "Iteration 4427 Loss: 1.540894865989685\n",
      "Iteration 4428 Loss: 2.1896121501922607\n",
      "Iteration 4429 Loss: 2.3610024452209473\n",
      "Iteration 4429 Loss: 1.6784718036651611\n",
      "Iteration 4430 Loss: 1.2108001708984375\n",
      "Iteration 4431 Loss: 1.9034123420715332\n",
      "Iteration 4432 Loss: 1.6436429023742676\n",
      "Iteration 4433 Loss: 1.6652432680130005\n",
      "Iteration 4434 Loss: 1.751946210861206\n",
      "Iteration 4435 Loss: 1.653643012046814\n",
      "Iteration 4436 Loss: 1.7495862245559692\n",
      "Iteration 4437 Loss: 1.446015477180481\n",
      "Iteration 4438 Loss: 1.5860861539840698\n",
      "Iteration 4439 Loss: 1.931617259979248\n",
      "Iteration 4439 Loss: 1.654199242591858\n",
      "Iteration 4440 Loss: 1.3275883197784424\n",
      "Iteration 4441 Loss: 1.384564757347107\n",
      "Iteration 4442 Loss: 1.7088087797164917\n",
      "Iteration 4443 Loss: 1.524231195449829\n",
      "Iteration 4444 Loss: 1.6462082862854004\n",
      "Iteration 4445 Loss: 1.3324958086013794\n",
      "Iteration 4446 Loss: 1.2608864307403564\n",
      "Iteration 4447 Loss: 1.347527027130127\n",
      "Iteration 4448 Loss: 1.7769017219543457\n",
      "Iteration 4449 Loss: 1.837473750114441\n",
      "Iteration 4449 Loss: 1.5146684646606445\n",
      "Iteration 4450 Loss: 1.350562334060669\n",
      "Iteration 4451 Loss: 1.9003180265426636\n",
      "Iteration 4452 Loss: 1.2552518844604492\n",
      "Iteration 4453 Loss: 1.4556269645690918\n",
      "Iteration 4454 Loss: 1.6407572031021118\n",
      "Iteration 4455 Loss: 1.6961079835891724\n",
      "Iteration 4456 Loss: 1.6243029832839966\n",
      "Iteration 4457 Loss: 1.9633041620254517\n",
      "Iteration 4458 Loss: 1.4619309902191162\n",
      "Iteration 4459 Loss: 1.4732247591018677\n",
      "Iteration 4459 Loss: 1.5821387767791748\n",
      "Iteration 4460 Loss: 1.5731223821640015\n",
      "Iteration 4461 Loss: 1.7270091772079468\n",
      "Iteration 4462 Loss: 0.9655742049217224\n",
      "Iteration 4463 Loss: 1.671276569366455\n",
      "Iteration 4464 Loss: 1.5218197107315063\n",
      "Iteration 4465 Loss: 1.6582627296447754\n",
      "Iteration 4466 Loss: 1.700347661972046\n",
      "Iteration 4467 Loss: 1.7281547784805298\n",
      "Iteration 4468 Loss: 1.3338871002197266\n",
      "Iteration 4469 Loss: 1.644716739654541\n",
      "Iteration 4469 Loss: 1.5524171590805054\n",
      "Iteration 4470 Loss: 1.5361336469650269\n",
      "Iteration 4471 Loss: 1.8393360376358032\n",
      "Iteration 4472 Loss: 1.5507808923721313\n",
      "Iteration 4473 Loss: 1.5092389583587646\n",
      "Iteration 4474 Loss: 1.854493260383606\n",
      "Iteration 4475 Loss: 2.0963361263275146\n",
      "Iteration 4476 Loss: 1.1940228939056396\n",
      "Iteration 4477 Loss: 1.6361702680587769\n",
      "Iteration 4478 Loss: 2.026803493499756\n",
      "Iteration 4479 Loss: 1.205224633216858\n",
      "Iteration 4479 Loss: 1.644853949546814\n",
      "Iteration 4480 Loss: 1.5500681400299072\n",
      "Iteration 4481 Loss: 1.4578312635421753\n",
      "Iteration 4482 Loss: 1.7512295246124268\n",
      "Iteration 4483 Loss: 1.5820369720458984\n",
      "Iteration 4484 Loss: 1.71206796169281\n",
      "Iteration 4485 Loss: 1.2179158926010132\n",
      "Iteration 4486 Loss: 2.2187986373901367\n",
      "Iteration 4487 Loss: 1.1317964792251587\n",
      "Iteration 4488 Loss: 1.2865405082702637\n",
      "Iteration 4489 Loss: 1.582533597946167\n",
      "Iteration 4489 Loss: 1.5490819215774536\n",
      "Iteration 4490 Loss: 1.4114919900894165\n",
      "Iteration 4491 Loss: 1.3499003648757935\n",
      "Iteration 4492 Loss: 1.8415266275405884\n",
      "Iteration 4493 Loss: 1.437068223953247\n",
      "Iteration 4494 Loss: 1.7829806804656982\n",
      "Iteration 4495 Loss: 1.7984604835510254\n",
      "Iteration 4496 Loss: 1.8069112300872803\n",
      "Iteration 4497 Loss: 1.7887765169143677\n",
      "Iteration 4498 Loss: 1.2537778615951538\n",
      "Iteration 4499 Loss: 1.4056023359298706\n",
      "Iteration 4499 Loss: 1.5876497030258179\n",
      "Iteration 4500 Loss: 1.8152241706848145\n",
      "Iteration 4501 Loss: 1.8992564678192139\n",
      "Iteration 4502 Loss: 1.945602297782898\n",
      "Iteration 4503 Loss: 2.0088624954223633\n",
      "Iteration 4504 Loss: 1.523200273513794\n",
      "Iteration 4505 Loss: 1.9183140993118286\n",
      "Iteration 4506 Loss: 1.2617665529251099\n",
      "Iteration 4507 Loss: 1.7290271520614624\n",
      "Iteration 4508 Loss: 1.2272279262542725\n",
      "Iteration 4509 Loss: 1.6604793071746826\n",
      "Iteration 4509 Loss: 1.698896050453186\n",
      "Iteration 4510 Loss: 1.3017677068710327\n",
      "Iteration 4511 Loss: 1.6604772806167603\n",
      "Iteration 4512 Loss: 2.067139148712158\n",
      "Iteration 4513 Loss: 1.513540506362915\n",
      "Iteration 4514 Loss: 1.3607335090637207\n",
      "Iteration 4515 Loss: 1.386630654335022\n",
      "Iteration 4516 Loss: 1.5600788593292236\n",
      "Iteration 4517 Loss: 1.154726505279541\n",
      "Iteration 4518 Loss: 1.916571021080017\n",
      "Iteration 4519 Loss: 1.113535761833191\n",
      "Iteration 4519 Loss: 1.5035200119018555\n",
      "Iteration 4520 Loss: 1.5843454599380493\n",
      "Iteration 4521 Loss: 1.1594138145446777\n",
      "Iteration 4522 Loss: 1.3467310667037964\n",
      "Iteration 4523 Loss: 1.3933724164962769\n",
      "Iteration 4524 Loss: 1.2045619487762451\n",
      "Iteration 4525 Loss: 1.6490075588226318\n",
      "Iteration 4526 Loss: 1.8136177062988281\n",
      "Iteration 4527 Loss: 1.1002520322799683\n",
      "Iteration 4528 Loss: 1.3671873807907104\n",
      "Iteration 4529 Loss: 2.0307512283325195\n",
      "Iteration 4529 Loss: 1.4649240970611572\n",
      "Iteration 4530 Loss: 1.342832326889038\n",
      "Iteration 4531 Loss: 1.2910774946212769\n",
      "Iteration 4532 Loss: 1.6016007661819458\n",
      "Iteration 4533 Loss: 1.652955174446106\n",
      "Iteration 4534 Loss: 1.8321906328201294\n",
      "Iteration 4535 Loss: 1.3782670497894287\n",
      "Iteration 4536 Loss: 1.5758147239685059\n",
      "Iteration 4537 Loss: 1.3523175716400146\n",
      "Iteration 4538 Loss: 1.105932354927063\n",
      "Iteration 4539 Loss: 1.3911813497543335\n",
      "Iteration 4539 Loss: 1.452417016029358\n",
      "Iteration 4540 Loss: 1.8257852792739868\n",
      "Iteration 4541 Loss: 1.267854928970337\n",
      "Iteration 4542 Loss: 1.6808973550796509\n",
      "Iteration 4543 Loss: 1.928073525428772\n",
      "Iteration 4544 Loss: 1.5419780015945435\n",
      "Iteration 4545 Loss: 1.1813958883285522\n",
      "Iteration 4546 Loss: 1.946722388267517\n",
      "Iteration 4547 Loss: 1.663947343826294\n",
      "Iteration 4548 Loss: 1.595694899559021\n",
      "Iteration 4549 Loss: 1.9573116302490234\n",
      "Iteration 4549 Loss: 1.658966064453125\n",
      "Iteration 4550 Loss: 1.5981462001800537\n",
      "Iteration 4551 Loss: 1.3970022201538086\n",
      "Iteration 4552 Loss: 1.389882206916809\n",
      "Iteration 4553 Loss: 1.6910276412963867\n",
      "Iteration 4554 Loss: 1.4946708679199219\n",
      "Iteration 4555 Loss: 1.6804360151290894\n",
      "Iteration 4556 Loss: 1.289973497390747\n",
      "Iteration 4557 Loss: 2.1689882278442383\n",
      "Iteration 4558 Loss: 1.562355875968933\n",
      "Iteration 4559 Loss: 1.699340581893921\n",
      "Iteration 4559 Loss: 1.597182273864746\n",
      "Iteration 4560 Loss: 1.774710774421692\n",
      "Iteration 4561 Loss: 1.4986846446990967\n",
      "Iteration 4562 Loss: 1.657785415649414\n",
      "Iteration 4563 Loss: 1.4966360330581665\n",
      "Iteration 4564 Loss: 1.7459557056427002\n",
      "Iteration 4565 Loss: 1.4000506401062012\n",
      "Iteration 4566 Loss: 1.9212146997451782\n",
      "Iteration 4567 Loss: 2.025135040283203\n",
      "Iteration 4568 Loss: 1.6798545122146606\n",
      "Iteration 4569 Loss: 1.5377647876739502\n",
      "Iteration 4569 Loss: 1.6737792491912842\n",
      "Iteration 4570 Loss: 1.5689254999160767\n",
      "Iteration 4571 Loss: 1.3185145854949951\n",
      "Iteration 4572 Loss: 1.4611557722091675\n",
      "Iteration 4573 Loss: 1.8337972164154053\n",
      "Iteration 4574 Loss: 1.1757761240005493\n",
      "Iteration 4575 Loss: 1.4263279438018799\n",
      "Iteration 4576 Loss: 1.8777213096618652\n",
      "Iteration 4577 Loss: 1.383448600769043\n",
      "Iteration 4578 Loss: 1.8395161628723145\n",
      "Iteration 4579 Loss: 1.5392764806747437\n",
      "Iteration 4579 Loss: 1.5424460172653198\n",
      "Iteration 4580 Loss: 1.6175568103790283\n",
      "Iteration 4581 Loss: 0.902904212474823\n",
      "Iteration 4582 Loss: 1.8454740047454834\n",
      "Iteration 4583 Loss: 1.9438081979751587\n",
      "Iteration 4584 Loss: 1.8476606607437134\n",
      "Iteration 4585 Loss: 1.940330147743225\n",
      "Iteration 4586 Loss: 1.9260108470916748\n",
      "Iteration 4587 Loss: 1.412684679031372\n",
      "Iteration 4588 Loss: 2.093179225921631\n",
      "Iteration 4589 Loss: 1.3340553045272827\n",
      "Iteration 4589 Loss: 1.6863664388656616\n",
      "Iteration 4590 Loss: 1.6559332609176636\n",
      "Iteration 4591 Loss: 2.0620205402374268\n",
      "Iteration 4592 Loss: 1.7953747510910034\n",
      "Iteration 4593 Loss: 1.3527061939239502\n",
      "Iteration 4594 Loss: 1.878485083580017\n",
      "Iteration 4595 Loss: 1.2112464904785156\n",
      "Iteration 4596 Loss: 1.9678421020507812\n",
      "Iteration 4597 Loss: 1.7553075551986694\n",
      "Iteration 4598 Loss: 1.5797852277755737\n",
      "Iteration 4599 Loss: 1.8979119062423706\n",
      "Iteration 4599 Loss: 1.7156612873077393\n",
      "Iteration 4600 Loss: 1.8248964548110962\n",
      "Iteration 4601 Loss: 1.3440296649932861\n",
      "Iteration 4602 Loss: 1.5219132900238037\n",
      "Iteration 4603 Loss: 1.6973410844802856\n",
      "Iteration 4604 Loss: 2.0460705757141113\n",
      "Iteration 4605 Loss: 1.6122932434082031\n",
      "Iteration 4606 Loss: 1.5873165130615234\n",
      "Iteration 4607 Loss: 2.074713706970215\n",
      "Iteration 4608 Loss: 1.8525607585906982\n",
      "Iteration 4609 Loss: 1.8675791025161743\n",
      "Iteration 4609 Loss: 1.7428715229034424\n",
      "Iteration 4610 Loss: 1.790595293045044\n",
      "Iteration 4611 Loss: 1.1610729694366455\n",
      "Iteration 4612 Loss: 1.3109198808670044\n",
      "Iteration 4613 Loss: 1.3653432130813599\n",
      "Iteration 4614 Loss: 1.429530143737793\n",
      "Iteration 4615 Loss: 1.2686405181884766\n",
      "Iteration 4616 Loss: 1.5107733011245728\n",
      "Iteration 4617 Loss: 1.7253235578536987\n",
      "Iteration 4618 Loss: 1.6930673122406006\n",
      "Iteration 4619 Loss: 1.6985176801681519\n",
      "Iteration 4619 Loss: 1.4953783750534058\n",
      "Iteration 4620 Loss: 1.164777159690857\n",
      "Iteration 4621 Loss: 1.180760383605957\n",
      "Iteration 4622 Loss: 1.3493164777755737\n",
      "Iteration 4623 Loss: 1.222390055656433\n",
      "Iteration 4624 Loss: 1.5276648998260498\n",
      "Iteration 4625 Loss: 1.7270770072937012\n",
      "Iteration 4626 Loss: 1.3777518272399902\n",
      "Iteration 4627 Loss: 1.4036657810211182\n",
      "Iteration 4628 Loss: 0.992370069026947\n",
      "Iteration 4629 Loss: 1.6178816556930542\n",
      "Iteration 4629 Loss: 1.3563655614852905\n",
      "Iteration 4630 Loss: 1.5312227010726929\n",
      "Iteration 4631 Loss: 1.400766372680664\n",
      "Iteration 4632 Loss: 1.5026131868362427\n",
      "Iteration 4633 Loss: 1.3224714994430542\n",
      "Iteration 4634 Loss: 1.2192906141281128\n",
      "Iteration 4635 Loss: 1.7181096076965332\n",
      "Iteration 4636 Loss: 1.780921459197998\n",
      "Iteration 4637 Loss: 1.4637740850448608\n",
      "Iteration 4638 Loss: 1.2391786575317383\n",
      "Iteration 4639 Loss: 1.4644030332565308\n",
      "Iteration 4639 Loss: 1.4642751216888428\n",
      "Iteration 4640 Loss: 1.8108152151107788\n",
      "Iteration 4641 Loss: 1.6894906759262085\n",
      "Iteration 4642 Loss: 1.013554573059082\n",
      "Iteration 4643 Loss: 1.3180631399154663\n",
      "Iteration 4644 Loss: 1.3912887573242188\n",
      "Iteration 4645 Loss: 1.2814276218414307\n",
      "Iteration 4646 Loss: 1.8262932300567627\n",
      "Iteration 4647 Loss: 1.6785179376602173\n",
      "Iteration 4648 Loss: 1.7159781455993652\n",
      "Iteration 4649 Loss: 1.9926576614379883\n",
      "Iteration 4649 Loss: 1.5718085765838623\n",
      "Iteration 4650 Loss: 1.6390866041183472\n",
      "Iteration 4651 Loss: 1.9210230112075806\n",
      "Iteration 4652 Loss: 1.5194711685180664\n",
      "Iteration 4653 Loss: 1.226036787033081\n",
      "Iteration 4654 Loss: 1.4987679719924927\n",
      "Iteration 4655 Loss: 1.1482094526290894\n",
      "Iteration 4656 Loss: 1.739588975906372\n",
      "Iteration 4657 Loss: 1.6355340480804443\n",
      "Iteration 4658 Loss: 2.028956651687622\n",
      "Iteration 4659 Loss: 1.9606033563613892\n",
      "Iteration 4659 Loss: 1.6317278146743774\n",
      "Iteration 4660 Loss: 1.2662787437438965\n",
      "Iteration 4661 Loss: 1.061366081237793\n",
      "Iteration 4662 Loss: 1.862087607383728\n",
      "Iteration 4663 Loss: 2.1724894046783447\n",
      "Iteration 4664 Loss: 1.6323540210723877\n",
      "Iteration 4665 Loss: 2.2082130908966064\n",
      "Iteration 4666 Loss: 1.445303201675415\n",
      "Iteration 4667 Loss: 1.381509780883789\n",
      "Iteration 4668 Loss: 1.7472678422927856\n",
      "Iteration 4669 Loss: 1.6569106578826904\n",
      "Iteration 4669 Loss: 1.6433780193328857\n",
      "Iteration 4670 Loss: 1.231199860572815\n",
      "Iteration 4671 Loss: 1.851021647453308\n",
      "Iteration 4672 Loss: 1.6337605714797974\n",
      "Iteration 4673 Loss: 1.3772188425064087\n",
      "Iteration 4674 Loss: 1.2432823181152344\n",
      "Iteration 4675 Loss: 0.8500653505325317\n",
      "Iteration 4676 Loss: 2.100222587585449\n",
      "Iteration 4677 Loss: 1.3652064800262451\n",
      "Iteration 4678 Loss: 1.1671509742736816\n",
      "Iteration 4679 Loss: 1.4973526000976562\n",
      "Iteration 4679 Loss: 1.4316481351852417\n",
      "Iteration 4680 Loss: 1.650559902191162\n",
      "Iteration 4681 Loss: 1.8531746864318848\n",
      "Iteration 4682 Loss: 2.2067959308624268\n",
      "Iteration 4683 Loss: 1.3962894678115845\n",
      "Iteration 4684 Loss: 1.292522668838501\n",
      "Iteration 4685 Loss: 2.1163511276245117\n",
      "Iteration 4686 Loss: 1.3773677349090576\n",
      "Iteration 4687 Loss: 1.8218058347702026\n",
      "Iteration 4688 Loss: 1.7755467891693115\n",
      "Iteration 4689 Loss: 1.2340388298034668\n",
      "Iteration 4689 Loss: 1.672445297241211\n",
      "Iteration 4690 Loss: 1.42913818359375\n",
      "Iteration 4691 Loss: 1.4754632711410522\n",
      "Iteration 4692 Loss: 1.9869754314422607\n",
      "Iteration 4693 Loss: 1.2248162031173706\n",
      "Iteration 4694 Loss: 1.7789639234542847\n",
      "Iteration 4695 Loss: 1.8318357467651367\n",
      "Iteration 4696 Loss: 1.203588843345642\n",
      "Iteration 4697 Loss: 1.1081972122192383\n",
      "Iteration 4698 Loss: 1.7079112529754639\n",
      "Iteration 4699 Loss: 1.2698003053665161\n",
      "Iteration 4699 Loss: 1.5016690492630005\n",
      "Iteration 4700 Loss: 1.4693119525909424\n",
      "Iteration 4701 Loss: 1.3917739391326904\n",
      "Iteration 4702 Loss: 1.8737777471542358\n",
      "Iteration 4703 Loss: 1.6096309423446655\n",
      "Iteration 4704 Loss: 1.6912380456924438\n",
      "Iteration 4705 Loss: 1.700027346611023\n",
      "Iteration 4706 Loss: 1.7026311159133911\n",
      "Iteration 4707 Loss: 1.1721751689910889\n",
      "Iteration 4708 Loss: 2.0078325271606445\n",
      "Iteration 4709 Loss: 1.7537169456481934\n",
      "Iteration 4709 Loss: 1.637211561203003\n",
      "Iteration 4710 Loss: 1.3369473218917847\n",
      "Iteration 4711 Loss: 1.4440386295318604\n",
      "Iteration 4712 Loss: 1.2797815799713135\n",
      "Iteration 4713 Loss: 1.4032893180847168\n",
      "Iteration 4714 Loss: 1.7099483013153076\n",
      "Iteration 4715 Loss: 1.8593595027923584\n",
      "Iteration 4716 Loss: 1.4419822692871094\n",
      "Iteration 4717 Loss: 1.8122689723968506\n",
      "Iteration 4718 Loss: 1.5601167678833008\n",
      "Iteration 4719 Loss: 1.5987958908081055\n",
      "Iteration 4719 Loss: 1.5446528196334839\n",
      "Iteration 4720 Loss: 1.4926531314849854\n",
      "Iteration 4721 Loss: 1.2899659872055054\n",
      "Iteration 4722 Loss: 1.6036968231201172\n",
      "Iteration 4723 Loss: 1.4807980060577393\n",
      "Iteration 4724 Loss: 1.802186131477356\n",
      "Iteration 4725 Loss: 1.307194709777832\n",
      "Iteration 4726 Loss: 1.3310158252716064\n",
      "Iteration 4727 Loss: 1.591291069984436\n",
      "Iteration 4728 Loss: 1.7538225650787354\n",
      "Iteration 4729 Loss: 1.6349222660064697\n",
      "Iteration 4729 Loss: 1.528754711151123\n",
      "Iteration 4730 Loss: 1.4509267807006836\n",
      "Iteration 4731 Loss: 1.2389503717422485\n",
      "Iteration 4732 Loss: 1.5225168466567993\n",
      "Iteration 4733 Loss: 1.756368637084961\n",
      "Iteration 4734 Loss: 1.6567790508270264\n",
      "Iteration 4735 Loss: 1.8887112140655518\n",
      "Iteration 4736 Loss: 1.305673360824585\n",
      "Iteration 4737 Loss: 1.5858919620513916\n",
      "Iteration 4738 Loss: 1.6225508451461792\n",
      "Iteration 4739 Loss: 1.1676530838012695\n",
      "Iteration 4739 Loss: 1.5196021795272827\n",
      "Iteration 4740 Loss: 1.7540756464004517\n",
      "Iteration 4741 Loss: 1.892085313796997\n",
      "Iteration 4742 Loss: 1.6304311752319336\n",
      "Iteration 4743 Loss: 2.1270062923431396\n",
      "Iteration 4744 Loss: 0.9235913157463074\n",
      "Iteration 4745 Loss: 1.4524898529052734\n",
      "Iteration 4746 Loss: 2.1342854499816895\n",
      "Iteration 4747 Loss: 1.360744833946228\n",
      "Iteration 4748 Loss: 1.7423145771026611\n",
      "Iteration 4749 Loss: 1.8086879253387451\n",
      "Iteration 4749 Loss: 1.6825711727142334\n",
      "Iteration 4750 Loss: 1.7039445638656616\n",
      "Iteration 4751 Loss: 1.7868801355361938\n",
      "Iteration 4752 Loss: 1.8156946897506714\n",
      "Iteration 4753 Loss: 2.0223584175109863\n",
      "Iteration 4754 Loss: 1.4466618299484253\n",
      "Iteration 4755 Loss: 1.2595880031585693\n",
      "Iteration 4756 Loss: 1.054935097694397\n",
      "Iteration 4757 Loss: 2.297724485397339\n",
      "Iteration 4758 Loss: 1.5402607917785645\n",
      "Iteration 4759 Loss: 1.479552984237671\n",
      "Iteration 4759 Loss: 1.6407601833343506\n",
      "Iteration 4760 Loss: 1.2565189599990845\n",
      "Iteration 4761 Loss: 1.179398775100708\n",
      "Iteration 4762 Loss: 1.6227409839630127\n",
      "Iteration 4763 Loss: 1.366091012954712\n",
      "Iteration 4764 Loss: 1.8333781957626343\n",
      "Iteration 4765 Loss: 1.523067831993103\n",
      "Iteration 4766 Loss: 1.5046486854553223\n",
      "Iteration 4767 Loss: 1.5066169500350952\n",
      "Iteration 4768 Loss: 1.2407892942428589\n",
      "Iteration 4769 Loss: 1.8119871616363525\n",
      "Iteration 4769 Loss: 1.4845235347747803\n",
      "Iteration 4770 Loss: 1.7183576822280884\n",
      "Iteration 4771 Loss: 1.612021803855896\n",
      "Iteration 4772 Loss: 1.2340126037597656\n",
      "Iteration 4773 Loss: 1.9402968883514404\n",
      "Iteration 4774 Loss: 1.0930112600326538\n",
      "Iteration 4775 Loss: 1.3973888158798218\n",
      "Iteration 4776 Loss: 1.4507757425308228\n",
      "Iteration 4777 Loss: 1.3478031158447266\n",
      "Iteration 4778 Loss: 1.377111554145813\n",
      "Iteration 4779 Loss: 1.6014785766601562\n",
      "Iteration 4779 Loss: 1.4772257804870605\n",
      "Iteration 4780 Loss: 1.32492995262146\n",
      "Iteration 4781 Loss: 1.6804770231246948\n",
      "Iteration 4782 Loss: 1.3614699840545654\n",
      "Iteration 4783 Loss: 1.418857216835022\n",
      "Iteration 4784 Loss: 1.5221388339996338\n",
      "Iteration 4785 Loss: 1.4068163633346558\n",
      "Iteration 4786 Loss: 1.3406907320022583\n",
      "Iteration 4787 Loss: 1.306404709815979\n",
      "Iteration 4788 Loss: 1.4957590103149414\n",
      "Iteration 4789 Loss: 1.3712928295135498\n",
      "Iteration 4789 Loss: 1.4228837490081787\n",
      "Iteration 4790 Loss: 1.1924854516983032\n",
      "Iteration 4791 Loss: 1.827909231185913\n",
      "Iteration 4792 Loss: 1.4150826930999756\n",
      "Iteration 4793 Loss: 1.6825090646743774\n",
      "Iteration 4794 Loss: 1.5976380109786987\n",
      "Iteration 4795 Loss: 1.567935824394226\n",
      "Iteration 4796 Loss: 1.0327907800674438\n",
      "Iteration 4797 Loss: 1.130068302154541\n",
      "Iteration 4798 Loss: 1.2842390537261963\n",
      "Iteration 4799 Loss: 2.0019452571868896\n",
      "Iteration 4799 Loss: 1.473260521888733\n",
      "Iteration 4800 Loss: 1.3274000883102417\n",
      "Iteration 4801 Loss: 1.4550650119781494\n",
      "Iteration 4802 Loss: 1.4761593341827393\n",
      "Iteration 4803 Loss: 1.9745502471923828\n",
      "Iteration 4804 Loss: 1.4194895029067993\n",
      "Iteration 4805 Loss: 1.5905758142471313\n",
      "Iteration 4806 Loss: 1.8440773487091064\n",
      "Iteration 4807 Loss: 1.8223564624786377\n",
      "Iteration 4808 Loss: 1.6516616344451904\n",
      "Iteration 4809 Loss: 1.2696146965026855\n",
      "Iteration 4809 Loss: 1.5830949544906616\n",
      "Iteration 4810 Loss: 2.2116775512695312\n",
      "Iteration 4811 Loss: 1.4025629758834839\n",
      "Iteration 4812 Loss: 1.75528883934021\n",
      "Iteration 4813 Loss: 1.9067094326019287\n",
      "Iteration 4814 Loss: 1.7273480892181396\n",
      "Iteration 4815 Loss: 1.4994733333587646\n",
      "Iteration 4816 Loss: 1.76560640335083\n",
      "Iteration 4817 Loss: 1.4427419900894165\n",
      "Iteration 4818 Loss: 1.6197963953018188\n",
      "Iteration 4819 Loss: 1.9562430381774902\n",
      "Iteration 4819 Loss: 1.7287448644638062\n",
      "Iteration 4820 Loss: 1.5856233835220337\n",
      "Iteration 4821 Loss: 1.3080366849899292\n",
      "Iteration 4822 Loss: 1.8256713151931763\n",
      "Iteration 4823 Loss: 1.2939131259918213\n",
      "Iteration 4824 Loss: 1.3291183710098267\n",
      "Iteration 4825 Loss: 1.7351462841033936\n",
      "Iteration 4826 Loss: 1.2828147411346436\n",
      "Iteration 4827 Loss: 1.407085657119751\n",
      "Iteration 4828 Loss: 1.2982732057571411\n",
      "Iteration 4829 Loss: 1.1382883787155151\n",
      "Iteration 4829 Loss: 1.420397162437439\n",
      "Iteration 4830 Loss: 1.6850765943527222\n",
      "Iteration 4831 Loss: 1.3339060544967651\n",
      "Iteration 4832 Loss: 1.7473751306533813\n",
      "Iteration 4833 Loss: 1.5011746883392334\n",
      "Iteration 4834 Loss: 2.0172483921051025\n",
      "Iteration 4835 Loss: 1.481736660003662\n",
      "Iteration 4836 Loss: 1.5831979513168335\n",
      "Iteration 4837 Loss: 1.6418894529342651\n",
      "Iteration 4838 Loss: 1.9172642230987549\n",
      "Iteration 4839 Loss: 1.8204867839813232\n",
      "Iteration 4839 Loss: 1.6729354858398438\n",
      "Iteration 4840 Loss: 1.789466381072998\n",
      "Iteration 4841 Loss: 1.7651081085205078\n",
      "Iteration 4842 Loss: 1.2890565395355225\n",
      "Iteration 4843 Loss: 1.882019281387329\n",
      "Iteration 4844 Loss: 1.528732419013977\n",
      "Iteration 4845 Loss: 1.159285545349121\n",
      "Iteration 4846 Loss: 1.5311998128890991\n",
      "Iteration 4847 Loss: 1.1582468748092651\n",
      "Iteration 4848 Loss: 1.8793212175369263\n",
      "Iteration 4849 Loss: 1.6688158512115479\n",
      "Iteration 4849 Loss: 1.5651252269744873\n",
      "Iteration 4850 Loss: 1.4694026708602905\n",
      "Iteration 4851 Loss: 1.75631844997406\n",
      "Iteration 4852 Loss: 1.4763671159744263\n",
      "Iteration 4853 Loss: 1.6795008182525635\n",
      "Iteration 4854 Loss: 1.22459077835083\n",
      "Iteration 4855 Loss: 1.5313633680343628\n",
      "Iteration 4856 Loss: 1.5041042566299438\n",
      "Iteration 4857 Loss: 1.5571229457855225\n",
      "Iteration 4858 Loss: 1.4112911224365234\n",
      "Iteration 4859 Loss: 1.523300290107727\n",
      "Iteration 4859 Loss: 1.5133363008499146\n",
      "Iteration 4860 Loss: 1.6289933919906616\n",
      "Iteration 4861 Loss: 1.0238240957260132\n",
      "Iteration 4862 Loss: 1.6071897745132446\n",
      "Iteration 4863 Loss: 1.6229209899902344\n",
      "Iteration 4864 Loss: 1.7476425170898438\n",
      "Iteration 4865 Loss: 1.9507604837417603\n",
      "Iteration 4866 Loss: 1.4076244831085205\n",
      "Iteration 4867 Loss: 1.0003207921981812\n",
      "Iteration 4868 Loss: 1.9524470567703247\n",
      "Iteration 4869 Loss: 1.2099641561508179\n",
      "Iteration 4869 Loss: 1.5151687860488892\n",
      "Iteration 4870 Loss: 1.656297206878662\n",
      "Iteration 4871 Loss: 1.498990774154663\n",
      "Iteration 4872 Loss: 1.9493862390518188\n",
      "Iteration 4873 Loss: 1.879175066947937\n",
      "Iteration 4874 Loss: 1.5563526153564453\n",
      "Iteration 4875 Loss: 1.2675155401229858\n",
      "Iteration 4876 Loss: 1.2256914377212524\n",
      "Iteration 4877 Loss: 1.7000619173049927\n",
      "Iteration 4878 Loss: 1.40785551071167\n",
      "Iteration 4879 Loss: 1.419820785522461\n",
      "Iteration 4879 Loss: 1.556114673614502\n",
      "Iteration 4880 Loss: 1.3339409828186035\n",
      "Iteration 4881 Loss: 1.7924494743347168\n",
      "Iteration 4882 Loss: 1.2723329067230225\n",
      "Iteration 4883 Loss: 1.5334336757659912\n",
      "Iteration 4884 Loss: 2.089123249053955\n",
      "Iteration 4885 Loss: 1.917083501815796\n",
      "Iteration 4886 Loss: 1.5428876876831055\n",
      "Iteration 4887 Loss: 1.117216944694519\n",
      "Iteration 4888 Loss: 1.6056642532348633\n",
      "Iteration 4889 Loss: 1.7816417217254639\n",
      "Iteration 4889 Loss: 1.5985774993896484\n",
      "Iteration 4890 Loss: 1.58795166015625\n",
      "Iteration 4891 Loss: 1.897935390472412\n",
      "Iteration 4892 Loss: 1.6782290935516357\n",
      "Iteration 4893 Loss: 1.8045613765716553\n",
      "Iteration 4894 Loss: 2.168426752090454\n",
      "Iteration 4895 Loss: 2.3041772842407227\n",
      "Iteration 4896 Loss: 1.2436093091964722\n",
      "Iteration 4897 Loss: 1.2816581726074219\n",
      "Iteration 4898 Loss: 1.4726247787475586\n",
      "Iteration 4899 Loss: 2.004263401031494\n",
      "Iteration 4899 Loss: 1.7443437576293945\n",
      "Iteration 4900 Loss: 1.3456473350524902\n",
      "Iteration 4901 Loss: 1.5643806457519531\n",
      "Iteration 4902 Loss: 1.2932292222976685\n",
      "Iteration 4903 Loss: 1.4714802503585815\n",
      "Iteration 4904 Loss: 1.7005422115325928\n",
      "Iteration 4905 Loss: 1.157273769378662\n",
      "Iteration 4906 Loss: 1.7306824922561646\n",
      "Iteration 4907 Loss: 1.5480939149856567\n",
      "Iteration 4908 Loss: 1.8522608280181885\n",
      "Iteration 4909 Loss: 1.8289579153060913\n",
      "Iteration 4909 Loss: 1.5492547750473022\n",
      "Iteration 4910 Loss: 1.898808479309082\n",
      "Iteration 4911 Loss: 1.3020731210708618\n",
      "Iteration 4912 Loss: 1.6742521524429321\n",
      "Iteration 4913 Loss: 1.3309179544448853\n",
      "Iteration 4914 Loss: 1.70441734790802\n",
      "Iteration 4915 Loss: 1.5269525051116943\n",
      "Iteration 4916 Loss: 1.2804049253463745\n",
      "Iteration 4917 Loss: 1.3490785360336304\n",
      "Iteration 4918 Loss: 0.9536635875701904\n",
      "Iteration 4919 Loss: 1.06688392162323\n",
      "Iteration 4919 Loss: 1.4087451696395874\n",
      "Iteration 4920 Loss: 1.6295106410980225\n",
      "Iteration 4921 Loss: 1.4143991470336914\n",
      "Iteration 4922 Loss: 1.4448601007461548\n",
      "Iteration 4923 Loss: 0.845120370388031\n",
      "Iteration 4924 Loss: 1.442251443862915\n",
      "Iteration 4925 Loss: 1.6738892793655396\n",
      "Iteration 4926 Loss: 2.2144551277160645\n",
      "Iteration 4927 Loss: 0.9821627140045166\n",
      "Iteration 4928 Loss: 1.368920087814331\n",
      "Iteration 4929 Loss: 1.7684999704360962\n",
      "Iteration 4929 Loss: 1.4784066677093506\n",
      "Iteration 4930 Loss: 1.6539137363433838\n",
      "Iteration 4931 Loss: 2.042417526245117\n",
      "Iteration 4932 Loss: 2.0722033977508545\n",
      "Iteration 4933 Loss: 1.3596187829971313\n",
      "Iteration 4934 Loss: 1.067582607269287\n",
      "Iteration 4935 Loss: 1.471465826034546\n",
      "Iteration 4936 Loss: 1.4950450658798218\n",
      "Iteration 4937 Loss: 1.559694528579712\n",
      "Iteration 4938 Loss: 1.3743377923965454\n",
      "Iteration 4939 Loss: 1.5381355285644531\n",
      "Iteration 4939 Loss: 1.563441514968872\n",
      "Iteration 4940 Loss: 1.8862565755844116\n",
      "Iteration 4941 Loss: 1.643869400024414\n",
      "Iteration 4942 Loss: 1.5870569944381714\n",
      "Iteration 4943 Loss: 1.9685859680175781\n",
      "Iteration 4944 Loss: 1.9606660604476929\n",
      "Iteration 4945 Loss: 1.9025338888168335\n",
      "Iteration 4946 Loss: 1.7951821088790894\n",
      "Iteration 4947 Loss: 1.737634539604187\n",
      "Iteration 4948 Loss: 1.948529601097107\n",
      "Iteration 4949 Loss: 1.500711441040039\n",
      "Iteration 4949 Loss: 1.7931026220321655\n",
      "Iteration 4950 Loss: 1.8663456439971924\n",
      "Iteration 4951 Loss: 1.7239195108413696\n",
      "Iteration 4952 Loss: 1.8560221195220947\n",
      "Iteration 4953 Loss: 1.5196315050125122\n",
      "Iteration 4954 Loss: 2.0807950496673584\n",
      "Iteration 4955 Loss: 1.03813636302948\n",
      "Iteration 4956 Loss: 1.4286072254180908\n",
      "Iteration 4957 Loss: 1.7608277797698975\n",
      "Iteration 4958 Loss: 1.3796466588974\n",
      "Iteration 4959 Loss: 1.2729331254959106\n",
      "Iteration 4959 Loss: 1.5926865339279175\n",
      "Iteration 4960 Loss: 1.2856009006500244\n",
      "Iteration 4961 Loss: 1.5695298910140991\n",
      "Iteration 4962 Loss: 1.421941876411438\n",
      "Iteration 4963 Loss: 1.8752412796020508\n",
      "Iteration 4964 Loss: 1.3971691131591797\n",
      "Iteration 4965 Loss: 1.4117114543914795\n",
      "Iteration 4966 Loss: 1.9867360591888428\n",
      "Iteration 4967 Loss: 1.6334302425384521\n",
      "Iteration 4968 Loss: 1.7897868156433105\n",
      "Iteration 4969 Loss: 1.0884913206100464\n",
      "Iteration 4969 Loss: 1.545964002609253\n",
      "Iteration 4970 Loss: 1.3778860569000244\n",
      "Iteration 4971 Loss: 1.175402045249939\n",
      "Iteration 4972 Loss: 1.703324794769287\n",
      "Iteration 4973 Loss: 1.6415339708328247\n",
      "Iteration 4974 Loss: 1.1266698837280273\n",
      "Iteration 4975 Loss: 1.5033951997756958\n",
      "Iteration 4976 Loss: 1.365893006324768\n",
      "Iteration 4977 Loss: 1.5776561498641968\n",
      "Iteration 4978 Loss: 1.6130706071853638\n",
      "Iteration 4979 Loss: 1.642221450805664\n",
      "Iteration 4979 Loss: 1.4727052450180054\n",
      "Iteration 4980 Loss: 1.4884610176086426\n",
      "Iteration 4981 Loss: 1.5069563388824463\n",
      "Iteration 4982 Loss: 1.373038411140442\n",
      "Iteration 4983 Loss: 1.4794716835021973\n",
      "Iteration 4984 Loss: 1.5683661699295044\n",
      "Iteration 4985 Loss: 2.0439043045043945\n",
      "Iteration 4986 Loss: 1.6776199340820312\n",
      "Iteration 4987 Loss: 1.0491222143173218\n",
      "Iteration 4988 Loss: 1.452104926109314\n",
      "Iteration 4989 Loss: 1.7142385244369507\n",
      "Iteration 4989 Loss: 1.5353282690048218\n",
      "Iteration 4990 Loss: 1.8816171884536743\n",
      "Iteration 4991 Loss: 1.6495633125305176\n",
      "Iteration 4992 Loss: 1.211083173751831\n",
      "Iteration 4993 Loss: 1.8128993511199951\n",
      "Iteration 4994 Loss: 1.8089871406555176\n",
      "Iteration 4995 Loss: 1.65801203250885\n",
      "Iteration 4996 Loss: 1.1962178945541382\n",
      "Iteration 4997 Loss: 1.2090113162994385\n",
      "Iteration 4998 Loss: 1.9515725374221802\n",
      "Iteration 4999 Loss: 1.2508025169372559\n",
      "Iteration 4999 Loss: 1.562976598739624\n",
      "Iteration 5000 Loss: 1.0678428411483765\n",
      "Iteration 5001 Loss: 1.2206292152404785\n",
      "Iteration 5002 Loss: 1.4640167951583862\n",
      "Iteration 5003 Loss: 1.3439610004425049\n",
      "Iteration 5004 Loss: 1.7428034543991089\n",
      "Iteration 5005 Loss: 1.62080717086792\n",
      "Iteration 5006 Loss: 1.9684175252914429\n",
      "Iteration 5007 Loss: 1.1203384399414062\n",
      "Iteration 5008 Loss: 1.4249130487442017\n",
      "Iteration 5009 Loss: 1.4949955940246582\n",
      "Iteration 5009 Loss: 1.4468724727630615\n",
      "Iteration 5010 Loss: 1.1668256521224976\n",
      "Iteration 5011 Loss: 1.7478936910629272\n",
      "Iteration 5012 Loss: 2.135331869125366\n",
      "Iteration 5013 Loss: 1.2992149591445923\n",
      "Iteration 5014 Loss: 1.5426666736602783\n",
      "Iteration 5015 Loss: 1.708670735359192\n",
      "Iteration 5016 Loss: 1.2872607707977295\n",
      "Iteration 5017 Loss: 1.5731004476547241\n",
      "Iteration 5018 Loss: 1.3890377283096313\n",
      "Iteration 5019 Loss: 1.502549409866333\n",
      "Iteration 5019 Loss: 1.5352551937103271\n",
      "Iteration 5020 Loss: 1.9058116674423218\n",
      "Iteration 5021 Loss: 1.5294562578201294\n",
      "Iteration 5022 Loss: 1.862547516822815\n",
      "Iteration 5023 Loss: 1.387955904006958\n",
      "Iteration 5024 Loss: 1.9329679012298584\n",
      "Iteration 5025 Loss: 0.9034451842308044\n",
      "Iteration 5026 Loss: 1.3479766845703125\n",
      "Iteration 5027 Loss: 1.2274830341339111\n",
      "Iteration 5028 Loss: 1.3811423778533936\n",
      "Iteration 5029 Loss: 1.541445255279541\n",
      "Iteration 5029 Loss: 1.5020231008529663\n",
      "Iteration 5030 Loss: 1.0982694625854492\n",
      "Iteration 5031 Loss: 1.4604047536849976\n",
      "Iteration 5032 Loss: 1.9380714893341064\n",
      "Iteration 5033 Loss: 1.6273044347763062\n",
      "Iteration 5034 Loss: 1.8556787967681885\n",
      "Iteration 5035 Loss: 1.1855411529541016\n",
      "Iteration 5036 Loss: 1.3339320421218872\n",
      "Iteration 5037 Loss: 1.258998990058899\n",
      "Iteration 5038 Loss: 1.458567500114441\n",
      "Iteration 5039 Loss: 1.7015364170074463\n",
      "Iteration 5039 Loss: 1.4918304681777954\n",
      "Iteration 5040 Loss: 1.5285969972610474\n",
      "Iteration 5041 Loss: 1.407581090927124\n",
      "Iteration 5042 Loss: 1.8960167169570923\n",
      "Iteration 5043 Loss: 1.740343689918518\n",
      "Iteration 5044 Loss: 1.7297881841659546\n",
      "Iteration 5045 Loss: 1.6345921754837036\n",
      "Iteration 5046 Loss: 1.4797433614730835\n",
      "Iteration 5047 Loss: 1.5219335556030273\n",
      "Iteration 5048 Loss: 1.5492885112762451\n",
      "Iteration 5049 Loss: 1.4209400415420532\n",
      "Iteration 5049 Loss: 1.5908823013305664\n",
      "Iteration 5050 Loss: 1.3951472043991089\n",
      "Iteration 5051 Loss: 1.4020776748657227\n",
      "Iteration 5052 Loss: 1.6838830709457397\n",
      "Iteration 5053 Loss: 1.3841806650161743\n",
      "Iteration 5054 Loss: 1.6070752143859863\n",
      "Iteration 5055 Loss: 1.7519934177398682\n",
      "Iteration 5056 Loss: 1.6904826164245605\n",
      "Iteration 5057 Loss: 1.4665305614471436\n",
      "Iteration 5058 Loss: 1.2980561256408691\n",
      "Iteration 5059 Loss: 1.252166986465454\n",
      "Iteration 5059 Loss: 1.4931594133377075\n",
      "Iteration 5060 Loss: 1.8706169128417969\n",
      "Iteration 5061 Loss: 1.6628302335739136\n",
      "Iteration 5062 Loss: 1.2505300045013428\n",
      "Iteration 5063 Loss: 1.3807897567749023\n",
      "Iteration 5064 Loss: 2.004714012145996\n",
      "Iteration 5065 Loss: 1.713314175605774\n",
      "Iteration 5066 Loss: 1.9163191318511963\n",
      "Iteration 5067 Loss: 1.8955000638961792\n",
      "Iteration 5068 Loss: 1.601953148841858\n",
      "Iteration 5069 Loss: 1.3015440702438354\n",
      "Iteration 5069 Loss: 1.659811019897461\n",
      "Iteration 5070 Loss: 1.4552174806594849\n",
      "Iteration 5071 Loss: 1.8781267404556274\n",
      "Iteration 5072 Loss: 1.2083029747009277\n",
      "Iteration 5073 Loss: 1.6308561563491821\n",
      "Iteration 5074 Loss: 1.5850849151611328\n",
      "Iteration 5075 Loss: 1.8907523155212402\n",
      "Iteration 5076 Loss: 1.8607622385025024\n",
      "Iteration 5077 Loss: 1.3562915325164795\n",
      "Iteration 5078 Loss: 1.8366633653640747\n",
      "Iteration 5079 Loss: 1.9904175996780396\n",
      "Iteration 5079 Loss: 1.6692476272583008\n",
      "Iteration 5080 Loss: 1.5564260482788086\n",
      "Iteration 5081 Loss: 1.3844667673110962\n",
      "Iteration 5082 Loss: 1.0898486375808716\n",
      "Iteration 5083 Loss: 1.5268107652664185\n",
      "Iteration 5084 Loss: 1.5661370754241943\n",
      "Iteration 5085 Loss: 1.3892347812652588\n",
      "Iteration 5086 Loss: 1.5896751880645752\n",
      "Iteration 5087 Loss: 1.0380462408065796\n",
      "Iteration 5088 Loss: 1.2284553050994873\n",
      "Iteration 5089 Loss: 1.6255052089691162\n",
      "Iteration 5089 Loss: 1.3994605541229248\n",
      "Iteration 5090 Loss: 1.6956555843353271\n",
      "Iteration 5091 Loss: 1.5348613262176514\n",
      "Iteration 5092 Loss: 1.5684911012649536\n",
      "Iteration 5093 Loss: 1.5948357582092285\n",
      "Iteration 5094 Loss: 1.3396316766738892\n",
      "Iteration 5095 Loss: 2.0160956382751465\n",
      "Iteration 5096 Loss: 1.3890706300735474\n",
      "Iteration 5097 Loss: 1.7563012838363647\n",
      "Iteration 5098 Loss: 1.7018630504608154\n",
      "Iteration 5099 Loss: 1.327229380607605\n",
      "Iteration 5099 Loss: 1.592403531074524\n",
      "Iteration 5100 Loss: 0.8737877011299133\n",
      "Iteration 5101 Loss: 1.6538710594177246\n",
      "Iteration 5102 Loss: 1.4542816877365112\n",
      "Iteration 5103 Loss: 1.4272652864456177\n",
      "Iteration 5104 Loss: 1.496133804321289\n",
      "Iteration 5105 Loss: 1.3332804441452026\n",
      "Iteration 5106 Loss: 0.8826399445533752\n",
      "Iteration 5107 Loss: 1.5140899419784546\n",
      "Iteration 5108 Loss: 1.4893747568130493\n",
      "Iteration 5109 Loss: 1.1527535915374756\n",
      "Iteration 5109 Loss: 1.3277478218078613\n",
      "Iteration 5110 Loss: 2.5883631706237793\n",
      "Iteration 5111 Loss: 1.6099079847335815\n",
      "Iteration 5112 Loss: 1.3866252899169922\n",
      "Iteration 5113 Loss: 1.8431341648101807\n",
      "Iteration 5114 Loss: 1.575545072555542\n",
      "Iteration 5115 Loss: 2.0271661281585693\n",
      "Iteration 5116 Loss: 2.210479259490967\n",
      "Iteration 5117 Loss: 1.8131723403930664\n",
      "Iteration 5118 Loss: 1.9813792705535889\n",
      "Iteration 5119 Loss: 1.7313593626022339\n",
      "Iteration 5119 Loss: 1.8767131567001343\n",
      "Iteration 5120 Loss: 1.6176387071609497\n",
      "Iteration 5121 Loss: 1.8384521007537842\n",
      "Iteration 5122 Loss: 1.4208436012268066\n",
      "Iteration 5123 Loss: 1.214076042175293\n",
      "Iteration 5124 Loss: 1.7393152713775635\n",
      "Iteration 5125 Loss: 1.6090056896209717\n",
      "Iteration 5126 Loss: 1.978702187538147\n",
      "Iteration 5127 Loss: 1.1406420469284058\n",
      "Iteration 5128 Loss: 0.8517374992370605\n",
      "Iteration 5129 Loss: 1.6880372762680054\n",
      "Iteration 5129 Loss: 1.5098450183868408\n",
      "Iteration 5130 Loss: 1.6555883884429932\n",
      "Iteration 5131 Loss: 1.6261310577392578\n",
      "Iteration 5132 Loss: 2.0869967937469482\n",
      "Iteration 5133 Loss: 1.81266450881958\n",
      "Iteration 5134 Loss: 1.0517245531082153\n",
      "Iteration 5135 Loss: 1.4405794143676758\n",
      "Iteration 5136 Loss: 1.9623055458068848\n",
      "Iteration 5137 Loss: 1.3389179706573486\n",
      "Iteration 5138 Loss: 1.5293422937393188\n",
      "Iteration 5139 Loss: 1.5527606010437012\n",
      "Iteration 5139 Loss: 1.6057010889053345\n",
      "Iteration 5140 Loss: 2.023892641067505\n",
      "Iteration 5141 Loss: 1.703943133354187\n",
      "Iteration 5142 Loss: 1.4485336542129517\n",
      "Iteration 5143 Loss: 1.4013710021972656\n",
      "Iteration 5144 Loss: 1.7245880365371704\n",
      "Iteration 5145 Loss: 1.389876127243042\n",
      "Iteration 5146 Loss: 1.782886028289795\n",
      "Iteration 5147 Loss: 1.749672770500183\n",
      "Iteration 5148 Loss: 1.4771548509597778\n",
      "Iteration 5149 Loss: 1.7441595792770386\n",
      "Iteration 5149 Loss: 1.6446077823638916\n",
      "Iteration 5150 Loss: 1.7638542652130127\n",
      "Iteration 5151 Loss: 1.5552352666854858\n",
      "Iteration 5152 Loss: 1.6446645259857178\n",
      "Iteration 5153 Loss: 1.9978394508361816\n",
      "Iteration 5154 Loss: 1.4429994821548462\n",
      "Iteration 5155 Loss: 1.6828843355178833\n",
      "Iteration 5156 Loss: 1.4469801187515259\n",
      "Iteration 5157 Loss: 1.8018651008605957\n",
      "Iteration 5158 Loss: 1.588131308555603\n",
      "Iteration 5159 Loss: 1.7204245328903198\n",
      "Iteration 5159 Loss: 1.6644878387451172\n",
      "Iteration 5160 Loss: 1.5984160900115967\n",
      "Iteration 5161 Loss: 1.950818419456482\n",
      "Iteration 5162 Loss: 1.713521957397461\n",
      "Iteration 5163 Loss: 1.818823218345642\n",
      "Iteration 5164 Loss: 1.7531747817993164\n",
      "Iteration 5165 Loss: 1.074041724205017\n",
      "Iteration 5166 Loss: 1.6799925565719604\n",
      "Iteration 5167 Loss: 1.425417423248291\n",
      "Iteration 5168 Loss: 2.0858099460601807\n",
      "Iteration 5169 Loss: 1.3840830326080322\n",
      "Iteration 5169 Loss: 1.6484098434448242\n",
      "Iteration 5170 Loss: 1.6779814958572388\n",
      "Iteration 5171 Loss: 1.702988862991333\n",
      "Iteration 5172 Loss: 1.0721288919448853\n",
      "Iteration 5173 Loss: 1.5058215856552124\n",
      "Iteration 5174 Loss: 1.6309562921524048\n",
      "Iteration 5175 Loss: 1.8112585544586182\n",
      "Iteration 5176 Loss: 1.3918240070343018\n",
      "Iteration 5177 Loss: 1.714128851890564\n",
      "Iteration 5178 Loss: 1.1884995698928833\n",
      "Iteration 5179 Loss: 1.6527916193008423\n",
      "Iteration 5179 Loss: 1.5348379611968994\n",
      "Iteration 5180 Loss: 1.8895947933197021\n",
      "Iteration 5181 Loss: 1.9064327478408813\n",
      "Iteration 5182 Loss: 0.7856339812278748\n",
      "Iteration 5183 Loss: 1.33546781539917\n",
      "Iteration 5184 Loss: 1.31223726272583\n",
      "Iteration 5185 Loss: 1.1636312007904053\n",
      "Iteration 5186 Loss: 1.1122146844863892\n",
      "Iteration 5187 Loss: 1.8331981897354126\n",
      "Iteration 5188 Loss: 1.5058200359344482\n",
      "Iteration 5189 Loss: 1.2399787902832031\n",
      "Iteration 5189 Loss: 1.4084209203720093\n",
      "Iteration 5190 Loss: 1.4310358762741089\n",
      "Iteration 5191 Loss: 1.3077256679534912\n",
      "Iteration 5192 Loss: 1.9324488639831543\n",
      "Iteration 5193 Loss: 1.3043087720870972\n",
      "Iteration 5194 Loss: 1.2410998344421387\n",
      "Iteration 5195 Loss: 1.125089168548584\n",
      "Iteration 5196 Loss: 1.795857310295105\n",
      "Iteration 5197 Loss: 2.3427391052246094\n",
      "Iteration 5198 Loss: 1.497161626815796\n",
      "Iteration 5199 Loss: 1.8249613046646118\n",
      "Iteration 5199 Loss: 1.5802428722381592\n",
      "Iteration 5200 Loss: 1.743920922279358\n",
      "Iteration 5201 Loss: 1.8893625736236572\n",
      "Iteration 5202 Loss: 1.6506609916687012\n",
      "Iteration 5203 Loss: 1.4799174070358276\n",
      "Iteration 5204 Loss: 2.302605628967285\n",
      "Iteration 5205 Loss: 1.3911175727844238\n",
      "Iteration 5206 Loss: 1.5882490873336792\n",
      "Iteration 5207 Loss: 1.642505168914795\n",
      "Iteration 5208 Loss: 1.7840347290039062\n",
      "Iteration 5209 Loss: 1.4697458744049072\n",
      "Iteration 5209 Loss: 1.6942121982574463\n",
      "Iteration 5210 Loss: 1.4340453147888184\n",
      "Iteration 5211 Loss: 1.1742383241653442\n",
      "Iteration 5212 Loss: 1.0840176343917847\n",
      "Iteration 5213 Loss: 1.9419493675231934\n",
      "Iteration 5214 Loss: 1.5425848960876465\n",
      "Iteration 5215 Loss: 2.290893316268921\n",
      "Iteration 5216 Loss: 1.8359895944595337\n",
      "Iteration 5217 Loss: 1.8284568786621094\n",
      "Iteration 5218 Loss: 1.347764492034912\n",
      "Iteration 5219 Loss: 1.5660797357559204\n",
      "Iteration 5219 Loss: 1.6046020984649658\n",
      "Iteration 5220 Loss: 1.7575695514678955\n",
      "Iteration 5221 Loss: 1.6436113119125366\n",
      "Iteration 5222 Loss: 1.39882230758667\n",
      "Iteration 5223 Loss: 1.5084731578826904\n",
      "Iteration 5224 Loss: 1.4651517868041992\n",
      "Iteration 5225 Loss: 1.6727854013442993\n",
      "Iteration 5226 Loss: 1.7418910264968872\n",
      "Iteration 5227 Loss: 1.2387717962265015\n",
      "Iteration 5228 Loss: 1.8875967264175415\n",
      "Iteration 5229 Loss: 1.5514394044876099\n",
      "Iteration 5229 Loss: 1.586611270904541\n",
      "Iteration 5230 Loss: 1.9534026384353638\n",
      "Iteration 5231 Loss: 1.3343714475631714\n",
      "Iteration 5232 Loss: 1.8313223123550415\n",
      "Iteration 5233 Loss: 1.9570549726486206\n",
      "Iteration 5234 Loss: 1.3769328594207764\n",
      "Iteration 5235 Loss: 1.8082913160324097\n",
      "Iteration 5236 Loss: 2.07820725440979\n",
      "Iteration 5237 Loss: 1.1678683757781982\n",
      "Iteration 5238 Loss: 1.3685647249221802\n",
      "Iteration 5239 Loss: 1.604468584060669\n",
      "Iteration 5239 Loss: 1.6480484008789062\n",
      "Iteration 5240 Loss: 1.8088597059249878\n",
      "Iteration 5241 Loss: 1.303719401359558\n",
      "Iteration 5242 Loss: 1.6048146486282349\n",
      "Iteration 5243 Loss: 1.0163414478302002\n",
      "Iteration 5244 Loss: 1.8084291219711304\n",
      "Iteration 5245 Loss: 1.4052985906600952\n",
      "Iteration 5246 Loss: 1.8247838020324707\n",
      "Iteration 5247 Loss: 1.3300896883010864\n",
      "Iteration 5248 Loss: 1.7528407573699951\n",
      "Iteration 5249 Loss: 1.772804617881775\n",
      "Iteration 5249 Loss: 1.5627981424331665\n",
      "Iteration 5250 Loss: 1.8338247537612915\n",
      "Iteration 5251 Loss: 1.5242868661880493\n",
      "Iteration 5252 Loss: 1.6773221492767334\n",
      "Iteration 5253 Loss: 1.08059823513031\n",
      "Iteration 5254 Loss: 1.712602972984314\n",
      "Iteration 5255 Loss: 1.4888989925384521\n",
      "Iteration 5256 Loss: 1.518463373184204\n",
      "Iteration 5257 Loss: 1.446410059928894\n",
      "Iteration 5258 Loss: 1.8232944011688232\n",
      "Iteration 5259 Loss: 1.6254993677139282\n",
      "Iteration 5259 Loss: 1.5731199979782104\n",
      "Iteration 5260 Loss: 1.4656200408935547\n",
      "Iteration 5261 Loss: 1.435469150543213\n",
      "Iteration 5262 Loss: 1.6017446517944336\n",
      "Iteration 5263 Loss: 1.2367439270019531\n",
      "Iteration 5264 Loss: 1.7972660064697266\n",
      "Iteration 5265 Loss: 1.6785085201263428\n",
      "Iteration 5266 Loss: 1.2434836626052856\n",
      "Iteration 5267 Loss: 1.6283519268035889\n",
      "Iteration 5268 Loss: 1.56549870967865\n",
      "Iteration 5269 Loss: 1.5467541217803955\n",
      "Iteration 5269 Loss: 1.5199440717697144\n",
      "Iteration 5270 Loss: 1.6373871564865112\n",
      "Iteration 5271 Loss: 1.9141172170639038\n",
      "Iteration 5272 Loss: 1.400417685508728\n",
      "Iteration 5273 Loss: 1.8517448902130127\n",
      "Iteration 5274 Loss: 1.4820278882980347\n",
      "Iteration 5275 Loss: 1.7176264524459839\n",
      "Iteration 5276 Loss: 0.9629303216934204\n",
      "Iteration 5277 Loss: 1.3975542783737183\n",
      "Iteration 5278 Loss: 1.2582710981369019\n",
      "Iteration 5279 Loss: 1.638559341430664\n",
      "Iteration 5279 Loss: 1.5260636806488037\n",
      "Iteration 5280 Loss: 1.6996490955352783\n",
      "Iteration 5281 Loss: 0.9230213761329651\n",
      "Iteration 5282 Loss: 1.302404522895813\n",
      "Iteration 5283 Loss: 1.9003981351852417\n",
      "Iteration 5284 Loss: 1.4534696340560913\n",
      "Iteration 5285 Loss: 2.006079912185669\n",
      "Iteration 5286 Loss: 1.5724822282791138\n",
      "Iteration 5287 Loss: 1.386709451675415\n",
      "Iteration 5288 Loss: 1.322443962097168\n",
      "Iteration 5289 Loss: 1.2191463708877563\n",
      "Iteration 5289 Loss: 1.478580355644226\n",
      "Iteration 5290 Loss: 2.0062613487243652\n",
      "Iteration 5291 Loss: 1.618730902671814\n",
      "Iteration 5292 Loss: 1.188948631286621\n",
      "Iteration 5293 Loss: 1.2874053716659546\n",
      "Iteration 5294 Loss: 2.0157113075256348\n",
      "Iteration 5295 Loss: 1.5019932985305786\n",
      "Iteration 5296 Loss: 1.5686849355697632\n",
      "Iteration 5297 Loss: 1.704950213432312\n",
      "Iteration 5298 Loss: 1.236270546913147\n",
      "Iteration 5299 Loss: 1.5732996463775635\n",
      "Iteration 5299 Loss: 1.5702255964279175\n",
      "Iteration 5300 Loss: 2.430386543273926\n",
      "Iteration 5301 Loss: 1.7130879163742065\n",
      "Iteration 5302 Loss: 1.8976092338562012\n",
      "Iteration 5303 Loss: 1.6855628490447998\n",
      "Iteration 5304 Loss: 1.7345366477966309\n",
      "Iteration 5305 Loss: 1.434257984161377\n",
      "Iteration 5306 Loss: 1.3526922464370728\n",
      "Iteration 5307 Loss: 1.6246771812438965\n",
      "Iteration 5308 Loss: 1.636284589767456\n",
      "Iteration 5309 Loss: 2.0302138328552246\n",
      "Iteration 5309 Loss: 1.7539308071136475\n",
      "Iteration 5310 Loss: 1.1253703832626343\n",
      "Iteration 5311 Loss: 1.2853178977966309\n",
      "Iteration 5312 Loss: 1.7290455102920532\n",
      "Iteration 5313 Loss: 1.441713809967041\n",
      "Iteration 5314 Loss: 1.60355544090271\n",
      "Iteration 5315 Loss: 1.6935491561889648\n",
      "Iteration 5316 Loss: 1.2755372524261475\n",
      "Iteration 5317 Loss: 1.7741998434066772\n",
      "Iteration 5318 Loss: 1.0855573415756226\n",
      "Iteration 5319 Loss: 1.4382009506225586\n",
      "Iteration 5319 Loss: 1.445204734802246\n",
      "Iteration 5320 Loss: 1.5210607051849365\n",
      "Iteration 5321 Loss: 1.5014082193374634\n",
      "Iteration 5322 Loss: 1.6187996864318848\n",
      "Iteration 5323 Loss: 1.3606126308441162\n",
      "Iteration 5324 Loss: 1.8327600955963135\n",
      "Iteration 5325 Loss: 1.896311640739441\n",
      "Iteration 5326 Loss: 1.6153383255004883\n",
      "Iteration 5327 Loss: 1.7378571033477783\n",
      "Iteration 5328 Loss: 1.7740951776504517\n",
      "Iteration 5329 Loss: 1.413195252418518\n",
      "Iteration 5329 Loss: 1.6271438598632812\n",
      "Iteration 5330 Loss: 1.6960878372192383\n",
      "Iteration 5331 Loss: 1.18207585811615\n",
      "Iteration 5332 Loss: 1.3502140045166016\n",
      "Iteration 5333 Loss: 1.7460047006607056\n",
      "Iteration 5334 Loss: 1.8874804973602295\n",
      "Iteration 5335 Loss: 1.5767790079116821\n",
      "Iteration 5336 Loss: 1.2428841590881348\n",
      "Iteration 5337 Loss: 1.3295766115188599\n",
      "Iteration 5338 Loss: 1.2764413356781006\n",
      "Iteration 5339 Loss: 1.2196311950683594\n",
      "Iteration 5339 Loss: 1.450717568397522\n",
      "Iteration 5340 Loss: 1.4784202575683594\n",
      "Iteration 5341 Loss: 1.7100900411605835\n",
      "Iteration 5342 Loss: 1.8721601963043213\n",
      "Iteration 5343 Loss: 1.7659060955047607\n",
      "Iteration 5344 Loss: 1.571887493133545\n",
      "Iteration 5345 Loss: 1.8795115947723389\n",
      "Iteration 5346 Loss: 1.1511772871017456\n",
      "Iteration 5347 Loss: 1.9018771648406982\n",
      "Iteration 5348 Loss: 1.41250479221344\n",
      "Iteration 5349 Loss: 1.8205559253692627\n",
      "Iteration 5349 Loss: 1.6564090251922607\n",
      "Iteration 5350 Loss: 1.931653380393982\n",
      "Iteration 5351 Loss: 1.8665052652359009\n",
      "Iteration 5352 Loss: 1.4765015840530396\n",
      "Iteration 5353 Loss: 1.3226510286331177\n",
      "Iteration 5354 Loss: 1.2218588590621948\n",
      "Iteration 5355 Loss: 1.4876471757888794\n",
      "Iteration 5356 Loss: 1.258429765701294\n",
      "Iteration 5357 Loss: 1.8279175758361816\n",
      "Iteration 5358 Loss: 1.9389686584472656\n",
      "Iteration 5359 Loss: 1.8156555891036987\n",
      "Iteration 5359 Loss: 1.6147788763046265\n",
      "Iteration 5360 Loss: 1.2889057397842407\n",
      "Iteration 5361 Loss: 1.7274935245513916\n",
      "Iteration 5362 Loss: 1.8999470472335815\n",
      "Iteration 5363 Loss: 1.3630834817886353\n",
      "Iteration 5364 Loss: 1.4107164144515991\n",
      "Iteration 5365 Loss: 1.6778053045272827\n",
      "Iteration 5366 Loss: 1.7003357410430908\n",
      "Iteration 5367 Loss: 1.7351534366607666\n",
      "Iteration 5368 Loss: 1.6046918630599976\n",
      "Iteration 5369 Loss: 1.766169786453247\n",
      "Iteration 5369 Loss: 1.6174300909042358\n",
      "Iteration 5370 Loss: 1.6434879302978516\n",
      "Iteration 5371 Loss: 1.3420253992080688\n",
      "Iteration 5372 Loss: 1.5905143022537231\n",
      "Iteration 5373 Loss: 1.1493163108825684\n",
      "Iteration 5374 Loss: 1.426957130432129\n",
      "Iteration 5375 Loss: 1.4291127920150757\n",
      "Iteration 5376 Loss: 1.447458267211914\n",
      "Iteration 5377 Loss: 1.524932622909546\n",
      "Iteration 5378 Loss: 1.2493656873703003\n",
      "Iteration 5379 Loss: 1.173749327659607\n",
      "Iteration 5379 Loss: 1.3976919651031494\n",
      "Iteration 5380 Loss: 2.1013412475585938\n",
      "Iteration 5381 Loss: 1.6007486581802368\n",
      "Iteration 5382 Loss: 1.929490566253662\n",
      "Iteration 5383 Loss: 1.9636801481246948\n",
      "Iteration 5384 Loss: 1.5026384592056274\n",
      "Iteration 5385 Loss: 1.9100219011306763\n",
      "Iteration 5386 Loss: 1.2171295881271362\n",
      "Iteration 5387 Loss: 1.448522925376892\n",
      "Iteration 5388 Loss: 1.402273178100586\n",
      "Iteration 5389 Loss: 2.062736988067627\n",
      "Iteration 5389 Loss: 1.7138583660125732\n",
      "Iteration 5390 Loss: 1.2788201570510864\n",
      "Iteration 5391 Loss: 1.8061739206314087\n",
      "Iteration 5392 Loss: 1.5683120489120483\n",
      "Iteration 5393 Loss: 1.708819031715393\n",
      "Iteration 5394 Loss: 1.3801989555358887\n",
      "Iteration 5395 Loss: 1.437748670578003\n",
      "Iteration 5396 Loss: 1.6169989109039307\n",
      "Iteration 5397 Loss: 1.7008923292160034\n",
      "Iteration 5398 Loss: 1.6335868835449219\n",
      "Iteration 5399 Loss: 1.3727627992630005\n",
      "Iteration 5399 Loss: 1.550431489944458\n",
      "Iteration 5400 Loss: 1.5251753330230713\n",
      "Iteration 5401 Loss: 1.6036752462387085\n",
      "Iteration 5402 Loss: 1.5565005540847778\n",
      "Iteration 5403 Loss: 1.7372595071792603\n",
      "Iteration 5404 Loss: 1.6136668920516968\n",
      "Iteration 5405 Loss: 1.0500376224517822\n",
      "Iteration 5406 Loss: 1.3982856273651123\n",
      "Iteration 5407 Loss: 1.6320610046386719\n",
      "Iteration 5408 Loss: 1.9006611108779907\n",
      "Iteration 5409 Loss: 1.561317801475525\n",
      "Iteration 5409 Loss: 1.5578640699386597\n",
      "Iteration 5410 Loss: 1.356002688407898\n",
      "Iteration 5411 Loss: 1.4968611001968384\n",
      "Iteration 5412 Loss: 1.3096541166305542\n",
      "Iteration 5413 Loss: 1.6845630407333374\n",
      "Iteration 5414 Loss: 1.231366515159607\n",
      "Iteration 5415 Loss: 1.2851672172546387\n",
      "Iteration 5416 Loss: 1.7905522584915161\n",
      "Iteration 5417 Loss: 1.8108110427856445\n",
      "Iteration 5418 Loss: 1.6958658695220947\n",
      "Iteration 5419 Loss: 1.4774872064590454\n",
      "Iteration 5419 Loss: 1.5138331651687622\n",
      "Iteration 5420 Loss: 1.2796785831451416\n",
      "Iteration 5421 Loss: 1.0138864517211914\n",
      "Iteration 5422 Loss: 2.017763137817383\n",
      "Iteration 5423 Loss: 1.5761302709579468\n",
      "Iteration 5424 Loss: 1.3529168367385864\n",
      "Iteration 5425 Loss: 2.0618860721588135\n",
      "Iteration 5426 Loss: 1.8166913986206055\n",
      "Iteration 5427 Loss: 1.6055039167404175\n",
      "Iteration 5428 Loss: 1.8418008089065552\n",
      "Iteration 5429 Loss: 1.7320892810821533\n",
      "Iteration 5429 Loss: 1.629834532737732\n",
      "Iteration 5430 Loss: 1.3881738185882568\n",
      "Iteration 5431 Loss: 1.9270107746124268\n",
      "Iteration 5432 Loss: 1.5141876935958862\n",
      "Iteration 5433 Loss: 1.370604395866394\n",
      "Iteration 5434 Loss: 1.7428712844848633\n",
      "Iteration 5435 Loss: 1.214565396308899\n",
      "Iteration 5436 Loss: 1.6952033042907715\n",
      "Iteration 5437 Loss: 1.5157827138900757\n",
      "Iteration 5438 Loss: 1.3959710597991943\n",
      "Iteration 5439 Loss: 1.0646369457244873\n",
      "Iteration 5439 Loss: 1.4829007387161255\n",
      "Iteration 5440 Loss: 1.4937974214553833\n",
      "Iteration 5441 Loss: 1.4844337701797485\n",
      "Iteration 5442 Loss: 1.3214988708496094\n",
      "Iteration 5443 Loss: 1.8695439100265503\n",
      "Iteration 5444 Loss: 0.9838827252388\n",
      "Iteration 5445 Loss: 1.6221171617507935\n",
      "Iteration 5446 Loss: 0.7268096804618835\n",
      "Iteration 5447 Loss: 1.2805227041244507\n",
      "Iteration 5448 Loss: 1.4994735717773438\n",
      "Iteration 5449 Loss: 1.5069139003753662\n",
      "Iteration 5449 Loss: 1.378899335861206\n",
      "Iteration 5450 Loss: 1.6353163719177246\n",
      "Iteration 5451 Loss: 1.773411512374878\n",
      "Iteration 5452 Loss: 1.4655919075012207\n",
      "Iteration 5453 Loss: 1.0430065393447876\n",
      "Iteration 5454 Loss: 1.2104147672653198\n",
      "Iteration 5455 Loss: 1.6466480493545532\n",
      "Iteration 5456 Loss: 1.6276791095733643\n",
      "Iteration 5457 Loss: 1.6489825248718262\n",
      "Iteration 5458 Loss: 1.839540958404541\n",
      "Iteration 5459 Loss: 1.327504277229309\n",
      "Iteration 5459 Loss: 1.5218095779418945\n",
      "Iteration 5460 Loss: 2.004183769226074\n",
      "Iteration 5461 Loss: 1.5823646783828735\n",
      "Iteration 5462 Loss: 1.284151315689087\n",
      "Iteration 5463 Loss: 1.3382551670074463\n",
      "Iteration 5464 Loss: 2.325448751449585\n",
      "Iteration 5465 Loss: 1.6567436456680298\n",
      "Iteration 5466 Loss: 1.3470580577850342\n",
      "Iteration 5467 Loss: 1.2981280088424683\n",
      "Iteration 5468 Loss: 1.6877498626708984\n",
      "Iteration 5469 Loss: 1.4457958936691284\n",
      "Iteration 5469 Loss: 1.5969879627227783\n",
      "Iteration 5470 Loss: 1.4960280656814575\n",
      "Iteration 5471 Loss: 1.2423015832901\n",
      "Iteration 5472 Loss: 1.6440080404281616\n",
      "Iteration 5473 Loss: 1.5829664468765259\n",
      "Iteration 5474 Loss: 1.804491639137268\n",
      "Iteration 5475 Loss: 1.791457176208496\n",
      "Iteration 5476 Loss: 1.3599082231521606\n",
      "Iteration 5477 Loss: 1.6245124340057373\n",
      "Iteration 5478 Loss: 1.1476409435272217\n",
      "Iteration 5479 Loss: 1.2738208770751953\n",
      "Iteration 5479 Loss: 1.496713638305664\n",
      "Iteration 5480 Loss: 1.8535726070404053\n",
      "Iteration 5481 Loss: 1.6151717901229858\n",
      "Iteration 5482 Loss: 1.6628522872924805\n",
      "Iteration 5483 Loss: 1.333927869796753\n",
      "Iteration 5484 Loss: 1.352478265762329\n",
      "Iteration 5485 Loss: 1.534261703491211\n",
      "Iteration 5486 Loss: 1.284333348274231\n",
      "Iteration 5487 Loss: 1.6801398992538452\n",
      "Iteration 5488 Loss: 1.2292200326919556\n",
      "Iteration 5489 Loss: 2.006390333175659\n",
      "Iteration 5489 Loss: 1.555234670639038\n",
      "Iteration 5490 Loss: 1.572881817817688\n",
      "Iteration 5491 Loss: 1.9665026664733887\n",
      "Iteration 5492 Loss: 1.3791089057922363\n",
      "Iteration 5493 Loss: 1.443374752998352\n",
      "Iteration 5494 Loss: 1.2600504159927368\n",
      "Iteration 5495 Loss: 1.528994083404541\n",
      "Iteration 5496 Loss: 1.48875093460083\n",
      "Iteration 5497 Loss: 1.3915621042251587\n",
      "Iteration 5498 Loss: 1.6676559448242188\n",
      "Iteration 5499 Loss: 2.0506975650787354\n",
      "Iteration 5499 Loss: 1.5749579668045044\n",
      "Iteration 5500 Loss: 1.6962186098098755\n",
      "Iteration 5501 Loss: 1.4392794370651245\n",
      "Iteration 5502 Loss: 1.7386096715927124\n",
      "Iteration 5503 Loss: 1.2230972051620483\n",
      "Iteration 5504 Loss: 1.6301945447921753\n",
      "Iteration 5505 Loss: 1.7066984176635742\n",
      "Iteration 5506 Loss: 1.3247325420379639\n",
      "Iteration 5507 Loss: 1.491279125213623\n",
      "Iteration 5508 Loss: 1.5821692943572998\n",
      "Iteration 5509 Loss: 1.7537028789520264\n",
      "Iteration 5509 Loss: 1.5585981607437134\n",
      "Iteration 5510 Loss: 1.7512456178665161\n",
      "Iteration 5511 Loss: 1.5469508171081543\n",
      "Iteration 5512 Loss: 1.4590706825256348\n",
      "Iteration 5513 Loss: 1.943893551826477\n",
      "Iteration 5514 Loss: 1.5938420295715332\n",
      "Iteration 5515 Loss: 1.5912281274795532\n",
      "Iteration 5516 Loss: 1.133698582649231\n",
      "Iteration 5517 Loss: 1.4323880672454834\n",
      "Iteration 5518 Loss: 1.542006492614746\n",
      "Iteration 5519 Loss: 1.4151215553283691\n",
      "Iteration 5519 Loss: 1.5409445762634277\n",
      "Iteration 5520 Loss: 1.1452713012695312\n",
      "Iteration 5521 Loss: 1.1652780771255493\n",
      "Iteration 5522 Loss: 0.8954479098320007\n",
      "Iteration 5523 Loss: 2.1875619888305664\n",
      "Iteration 5524 Loss: 1.3940542936325073\n",
      "Iteration 5525 Loss: 1.4464147090911865\n",
      "Iteration 5526 Loss: 1.205736756324768\n",
      "Iteration 5527 Loss: 1.822265625\n",
      "Iteration 5528 Loss: 1.3918637037277222\n",
      "Iteration 5529 Loss: 1.5549565553665161\n",
      "Iteration 5529 Loss: 1.4208852052688599\n",
      "Iteration 5530 Loss: 1.7476085424423218\n",
      "Iteration 5531 Loss: 1.9211575984954834\n",
      "Iteration 5532 Loss: 1.226273536682129\n",
      "Iteration 5533 Loss: 1.6844850778579712\n",
      "Iteration 5534 Loss: 1.6114833354949951\n",
      "Iteration 5535 Loss: 1.6075174808502197\n",
      "Iteration 5536 Loss: 1.4760438203811646\n",
      "Iteration 5537 Loss: 1.6922111511230469\n",
      "Iteration 5538 Loss: 1.2952625751495361\n",
      "Iteration 5539 Loss: 1.6230052709579468\n",
      "Iteration 5539 Loss: 1.5885049104690552\n",
      "Iteration 5540 Loss: 1.2770438194274902\n",
      "Iteration 5541 Loss: 1.406804084777832\n",
      "Iteration 5542 Loss: 1.5972850322723389\n",
      "Iteration 5543 Loss: 1.4628138542175293\n",
      "Iteration 5544 Loss: 1.2032532691955566\n",
      "Iteration 5545 Loss: 1.5724815130233765\n",
      "Iteration 5546 Loss: 1.4100141525268555\n",
      "Iteration 5547 Loss: 1.2277876138687134\n",
      "Iteration 5548 Loss: 1.1461677551269531\n",
      "Iteration 5549 Loss: 1.6559962034225464\n",
      "Iteration 5549 Loss: 1.3959646224975586\n",
      "Iteration 5550 Loss: 1.7080076932907104\n",
      "Iteration 5551 Loss: 1.3319976329803467\n",
      "Iteration 5552 Loss: 1.9115335941314697\n",
      "Iteration 5553 Loss: 1.6480530500411987\n",
      "Iteration 5554 Loss: 1.6606731414794922\n",
      "Iteration 5555 Loss: 1.4293817281723022\n",
      "Iteration 5556 Loss: 1.7532463073730469\n",
      "Iteration 5557 Loss: 1.4213006496429443\n",
      "Iteration 5558 Loss: 1.2195730209350586\n",
      "Iteration 5559 Loss: 1.748114824295044\n",
      "Iteration 5559 Loss: 1.5831881761550903\n",
      "Iteration 5560 Loss: 1.398199200630188\n",
      "Iteration 5561 Loss: 1.3869001865386963\n",
      "Iteration 5562 Loss: 1.5872550010681152\n",
      "Iteration 5563 Loss: 1.3755918741226196\n",
      "Iteration 5564 Loss: 1.4907140731811523\n",
      "Iteration 5565 Loss: 1.6517672538757324\n",
      "Iteration 5566 Loss: 1.7864779233932495\n",
      "Iteration 5567 Loss: 1.55889892578125\n",
      "Iteration 5568 Loss: 0.9317930936813354\n",
      "Iteration 5569 Loss: 1.644195795059204\n",
      "Iteration 5569 Loss: 1.4811792373657227\n",
      "Iteration 5570 Loss: 1.4840995073318481\n",
      "Iteration 5571 Loss: 1.9276574850082397\n",
      "Iteration 5572 Loss: 1.2475858926773071\n",
      "Iteration 5573 Loss: 1.813667893409729\n",
      "Iteration 5574 Loss: 1.3976612091064453\n",
      "Iteration 5575 Loss: 1.8026410341262817\n",
      "Iteration 5576 Loss: 1.5785853862762451\n",
      "Iteration 5577 Loss: 1.5836173295974731\n",
      "Iteration 5578 Loss: 1.548925757408142\n",
      "Iteration 5579 Loss: 1.6003388166427612\n",
      "Iteration 5579 Loss: 1.598478078842163\n",
      "Iteration 5580 Loss: 1.4551687240600586\n",
      "Iteration 5581 Loss: 1.283128023147583\n",
      "Iteration 5582 Loss: 1.538610816001892\n",
      "Iteration 5583 Loss: 1.7223197221755981\n",
      "Iteration 5584 Loss: 1.5091477632522583\n",
      "Iteration 5585 Loss: 1.963794469833374\n",
      "Iteration 5586 Loss: 1.3392040729522705\n",
      "Iteration 5587 Loss: 2.005434274673462\n",
      "Iteration 5588 Loss: 1.0673843622207642\n",
      "Iteration 5589 Loss: 0.8222057223320007\n",
      "Iteration 5589 Loss: 1.4706398248672485\n",
      "Iteration 5590 Loss: 2.3962385654449463\n",
      "Iteration 5591 Loss: 1.2203139066696167\n",
      "Iteration 5592 Loss: 1.5056018829345703\n",
      "Iteration 5593 Loss: 1.6942553520202637\n",
      "Iteration 5594 Loss: 1.3094513416290283\n",
      "Iteration 5595 Loss: 1.5472338199615479\n",
      "Iteration 5596 Loss: 1.3294575214385986\n",
      "Iteration 5597 Loss: 1.5752519369125366\n",
      "Iteration 5598 Loss: 1.715566873550415\n",
      "Iteration 5599 Loss: 1.6252238750457764\n",
      "Iteration 5599 Loss: 1.5918593406677246\n",
      "Iteration 5600 Loss: 1.2597615718841553\n",
      "Iteration 5601 Loss: 1.5623524188995361\n",
      "Iteration 5602 Loss: 1.266349196434021\n",
      "Iteration 5603 Loss: 1.6831825971603394\n",
      "Iteration 5604 Loss: 1.3551756143569946\n",
      "Iteration 5605 Loss: 1.1787338256835938\n",
      "Iteration 5606 Loss: 1.539684534072876\n",
      "Iteration 5607 Loss: 1.7628501653671265\n",
      "Iteration 5608 Loss: 1.6133776903152466\n",
      "Iteration 5609 Loss: 0.7826367616653442\n",
      "Iteration 5609 Loss: 1.4004104137420654\n",
      "Iteration 5610 Loss: 1.5482743978500366\n",
      "Iteration 5611 Loss: 1.7791944742202759\n",
      "Iteration 5612 Loss: 1.4369616508483887\n",
      "Iteration 5613 Loss: 1.1733179092407227\n",
      "Iteration 5614 Loss: 1.3565877676010132\n",
      "Iteration 5615 Loss: 1.46815025806427\n",
      "Iteration 5616 Loss: 1.0452529191970825\n",
      "Iteration 5617 Loss: 1.3690944910049438\n",
      "Iteration 5618 Loss: 1.604709506034851\n",
      "Iteration 5619 Loss: 1.2187414169311523\n",
      "Iteration 5619 Loss: 1.4000284671783447\n",
      "Iteration 5620 Loss: 1.3954973220825195\n",
      "Iteration 5621 Loss: 1.5272170305252075\n",
      "Iteration 5622 Loss: 1.6728044748306274\n",
      "Iteration 5623 Loss: 1.8086743354797363\n",
      "Iteration 5624 Loss: 1.4157335758209229\n",
      "Iteration 5625 Loss: 2.158991575241089\n",
      "Iteration 5626 Loss: 1.5976837873458862\n",
      "Iteration 5627 Loss: 1.9482618570327759\n",
      "Iteration 5628 Loss: 1.3853487968444824\n",
      "Iteration 5629 Loss: 1.634859561920166\n",
      "Iteration 5629 Loss: 1.6545072793960571\n",
      "Iteration 5630 Loss: 1.8624118566513062\n",
      "Iteration 5631 Loss: 1.661731243133545\n",
      "Iteration 5632 Loss: 2.013474464416504\n",
      "Iteration 5633 Loss: 2.1277074813842773\n",
      "Iteration 5634 Loss: 1.5188742876052856\n",
      "Iteration 5635 Loss: 1.6779214143753052\n",
      "Iteration 5636 Loss: 1.8043988943099976\n",
      "Iteration 5637 Loss: 1.3857678174972534\n",
      "Iteration 5638 Loss: 1.7692770957946777\n",
      "Iteration 5639 Loss: 1.4639323949813843\n",
      "Iteration 5639 Loss: 1.728549599647522\n",
      "Iteration 5640 Loss: 1.9652345180511475\n",
      "Iteration 5641 Loss: 1.4636157751083374\n",
      "Iteration 5642 Loss: 1.9114314317703247\n",
      "Iteration 5643 Loss: 1.7109631299972534\n",
      "Iteration 5644 Loss: 1.310261607170105\n",
      "Iteration 5645 Loss: 1.2747912406921387\n",
      "Iteration 5646 Loss: 1.3736215829849243\n",
      "Iteration 5647 Loss: 1.7400490045547485\n",
      "Iteration 5648 Loss: 1.7008312940597534\n",
      "Iteration 5649 Loss: 1.6134817600250244\n",
      "Iteration 5649 Loss: 1.6064281463623047\n",
      "Iteration 5650 Loss: 1.696946620941162\n",
      "Iteration 5651 Loss: 1.4731415510177612\n",
      "Iteration 5652 Loss: 1.2363847494125366\n",
      "Iteration 5653 Loss: 1.4654463529586792\n",
      "Iteration 5654 Loss: 1.7972524166107178\n",
      "Iteration 5655 Loss: 1.1620508432388306\n",
      "Iteration 5656 Loss: 1.668404221534729\n",
      "Iteration 5657 Loss: 2.003584861755371\n",
      "Iteration 5658 Loss: 1.5434695482254028\n",
      "Iteration 5659 Loss: 1.7546234130859375\n",
      "Iteration 5659 Loss: 1.5801305770874023\n",
      "Iteration 5660 Loss: 1.8234758377075195\n",
      "Iteration 5661 Loss: 1.7310504913330078\n",
      "Iteration 5662 Loss: 1.7028217315673828\n",
      "Iteration 5663 Loss: 1.0413689613342285\n",
      "Iteration 5664 Loss: 1.5859854221343994\n",
      "Iteration 5665 Loss: 1.4547109603881836\n",
      "Iteration 5666 Loss: 1.7025463581085205\n",
      "Iteration 5667 Loss: 1.439078688621521\n",
      "Iteration 5668 Loss: 1.651894450187683\n",
      "Iteration 5669 Loss: 1.617885708808899\n",
      "Iteration 5669 Loss: 1.575081706047058\n",
      "Iteration 5670 Loss: 1.3021998405456543\n",
      "Iteration 5671 Loss: 1.5772833824157715\n",
      "Iteration 5672 Loss: 1.178424596786499\n",
      "Iteration 5673 Loss: 1.4686497449874878\n",
      "Iteration 5674 Loss: 1.7537158727645874\n",
      "Iteration 5675 Loss: 1.9214441776275635\n",
      "Iteration 5676 Loss: 1.32439386844635\n",
      "Iteration 5677 Loss: 1.8189058303833008\n",
      "Iteration 5678 Loss: 1.1993389129638672\n",
      "Iteration 5679 Loss: 1.3641334772109985\n",
      "Iteration 5679 Loss: 1.4908488988876343\n",
      "Iteration 5680 Loss: 1.4888309240341187\n",
      "Iteration 5681 Loss: 2.026139497756958\n",
      "Iteration 5682 Loss: 1.7489571571350098\n",
      "Iteration 5683 Loss: 1.6328049898147583\n",
      "Iteration 5684 Loss: 1.2766574621200562\n",
      "Iteration 5685 Loss: 1.5855716466903687\n",
      "Iteration 5686 Loss: 1.604697346687317\n",
      "Iteration 5687 Loss: 1.6046843528747559\n",
      "Iteration 5688 Loss: 1.581151008605957\n",
      "Iteration 5689 Loss: 1.8404560089111328\n",
      "Iteration 5689 Loss: 1.6389949321746826\n",
      "Iteration 5690 Loss: 1.7140803337097168\n",
      "Iteration 5691 Loss: 1.553215742111206\n",
      "Iteration 5692 Loss: 1.2798205614089966\n",
      "Iteration 5693 Loss: 2.1031298637390137\n",
      "Iteration 5694 Loss: 1.7668033838272095\n",
      "Iteration 5695 Loss: 1.5959638357162476\n",
      "Iteration 5696 Loss: 1.7206921577453613\n",
      "Iteration 5697 Loss: 1.985284447669983\n",
      "Iteration 5698 Loss: 1.2791250944137573\n",
      "Iteration 5699 Loss: 1.3523117303848267\n",
      "Iteration 5699 Loss: 1.6350427865982056\n",
      "Iteration 5700 Loss: 1.291086196899414\n",
      "Iteration 5701 Loss: 1.4643967151641846\n",
      "Iteration 5702 Loss: 1.808845043182373\n",
      "Iteration 5703 Loss: 1.2768418788909912\n",
      "Iteration 5704 Loss: 1.518256425857544\n",
      "Iteration 5705 Loss: 1.7261219024658203\n",
      "Iteration 5706 Loss: 1.8901934623718262\n",
      "Iteration 5707 Loss: 1.8869415521621704\n",
      "Iteration 5708 Loss: 1.625145673751831\n",
      "Iteration 5709 Loss: 1.6223548650741577\n",
      "Iteration 5709 Loss: 1.611018419265747\n",
      "Iteration 5710 Loss: 2.228646993637085\n",
      "Iteration 5711 Loss: 1.8108844757080078\n",
      "Iteration 5712 Loss: 1.4569091796875\n",
      "Iteration 5713 Loss: 1.5166757106781006\n",
      "Iteration 5714 Loss: 1.8312275409698486\n",
      "Iteration 5715 Loss: 1.574873447418213\n",
      "Iteration 5716 Loss: 1.8592100143432617\n",
      "Iteration 5717 Loss: 1.2458258867263794\n",
      "Iteration 5718 Loss: 1.1936427354812622\n",
      "Iteration 5719 Loss: 1.1589244604110718\n",
      "Iteration 5719 Loss: 1.5876820087432861\n",
      "Iteration 5720 Loss: 1.50234854221344\n",
      "Iteration 5721 Loss: 1.7157293558120728\n",
      "Iteration 5722 Loss: 1.5863075256347656\n",
      "Iteration 5723 Loss: 1.8758012056350708\n",
      "Iteration 5724 Loss: 1.451828956604004\n",
      "Iteration 5725 Loss: 1.5130106210708618\n",
      "Iteration 5726 Loss: 1.6164100170135498\n",
      "Iteration 5727 Loss: 1.8099901676177979\n",
      "Iteration 5728 Loss: 1.3835166692733765\n",
      "Iteration 5729 Loss: 1.5013391971588135\n",
      "Iteration 5729 Loss: 1.595628261566162\n",
      "Iteration 5730 Loss: 1.9454302787780762\n",
      "Iteration 5731 Loss: 1.7640773057937622\n",
      "Iteration 5732 Loss: 1.4155749082565308\n",
      "Iteration 5733 Loss: 1.8946152925491333\n",
      "Iteration 5734 Loss: 1.3700381517410278\n",
      "Iteration 5735 Loss: 1.7813314199447632\n",
      "Iteration 5736 Loss: 1.5676088333129883\n",
      "Iteration 5737 Loss: 1.7445857524871826\n",
      "Iteration 5738 Loss: 1.4251446723937988\n",
      "Iteration 5739 Loss: 2.1043975353240967\n",
      "Iteration 5739 Loss: 1.7012803554534912\n",
      "Iteration 5740 Loss: 1.4468821287155151\n",
      "Iteration 5741 Loss: 1.6944977045059204\n",
      "Iteration 5742 Loss: 1.3697154521942139\n",
      "Iteration 5743 Loss: 1.6254478693008423\n",
      "Iteration 5744 Loss: 1.3601213693618774\n",
      "Iteration 5745 Loss: 1.7916395664215088\n",
      "Iteration 5746 Loss: 1.3129206895828247\n",
      "Iteration 5747 Loss: 1.4645334482192993\n",
      "Iteration 5748 Loss: 0.9801939725875854\n",
      "Iteration 5749 Loss: 1.5009112358093262\n",
      "Iteration 5749 Loss: 1.4546864032745361\n",
      "Iteration 5750 Loss: 1.3568729162216187\n",
      "Iteration 5751 Loss: 1.5698920488357544\n",
      "Iteration 5752 Loss: 1.3300232887268066\n",
      "Iteration 5753 Loss: 1.400520920753479\n",
      "Iteration 5754 Loss: 1.294211745262146\n",
      "Iteration 5755 Loss: 2.0187880992889404\n",
      "Iteration 5756 Loss: 1.4598073959350586\n",
      "Iteration 5757 Loss: 1.771667718887329\n",
      "Iteration 5758 Loss: 1.7553473711013794\n",
      "Iteration 5759 Loss: 1.160321831703186\n",
      "Iteration 5759 Loss: 1.5117452144622803\n",
      "Iteration 5760 Loss: 1.7064201831817627\n",
      "Iteration 5761 Loss: 1.6561723947525024\n",
      "Iteration 5762 Loss: 0.975527822971344\n",
      "Iteration 5763 Loss: 1.4498405456542969\n",
      "Iteration 5764 Loss: 1.0507118701934814\n",
      "Iteration 5765 Loss: 1.4500114917755127\n",
      "Iteration 5766 Loss: 1.4994113445281982\n",
      "Iteration 5767 Loss: 1.0892339944839478\n",
      "Iteration 5768 Loss: 1.077075481414795\n",
      "Iteration 5769 Loss: 1.2526767253875732\n",
      "Iteration 5769 Loss: 1.320708155632019\n",
      "Iteration 5770 Loss: 1.4972673654556274\n",
      "Iteration 5771 Loss: 1.5593435764312744\n",
      "Iteration 5772 Loss: 1.3253766298294067\n",
      "Iteration 5773 Loss: 1.8200900554656982\n",
      "Iteration 5774 Loss: 1.357548475265503\n",
      "Iteration 5775 Loss: 1.3596889972686768\n",
      "Iteration 5776 Loss: 1.4447500705718994\n",
      "Iteration 5777 Loss: 1.376440167427063\n",
      "Iteration 5778 Loss: 1.6709675788879395\n",
      "Iteration 5779 Loss: 1.3283987045288086\n",
      "Iteration 5779 Loss: 1.473987102508545\n",
      "Iteration 5780 Loss: 1.5970103740692139\n",
      "Iteration 5781 Loss: 1.578721046447754\n",
      "Iteration 5782 Loss: 1.790886640548706\n",
      "Iteration 5783 Loss: 1.442389726638794\n",
      "Iteration 5784 Loss: 1.9563497304916382\n",
      "Iteration 5785 Loss: 1.3666038513183594\n",
      "Iteration 5786 Loss: 1.7230538129806519\n",
      "Iteration 5787 Loss: 1.47242271900177\n",
      "Iteration 5788 Loss: 1.338282585144043\n",
      "Iteration 5789 Loss: 1.563982367515564\n",
      "Iteration 5789 Loss: 1.5829702615737915\n",
      "Iteration 5790 Loss: 1.1824350357055664\n",
      "Iteration 5791 Loss: 1.4430584907531738\n",
      "Iteration 5792 Loss: 1.6075912714004517\n",
      "Iteration 5793 Loss: 1.323278784751892\n",
      "Iteration 5794 Loss: 1.5295131206512451\n",
      "Iteration 5795 Loss: 1.9185633659362793\n",
      "Iteration 5796 Loss: 1.817136526107788\n",
      "Iteration 5797 Loss: 1.1850054264068604\n",
      "Iteration 5798 Loss: 1.7567564249038696\n",
      "Iteration 5799 Loss: 1.539522409439087\n",
      "Iteration 5799 Loss: 1.5302860736846924\n",
      "Iteration 5800 Loss: 1.1080800294876099\n",
      "Iteration 5801 Loss: 1.4728813171386719\n",
      "Iteration 5802 Loss: 1.1239805221557617\n",
      "Iteration 5803 Loss: 1.2980518341064453\n",
      "Iteration 5804 Loss: 1.2727248668670654\n",
      "Iteration 5805 Loss: 1.484076976776123\n",
      "Iteration 5806 Loss: 1.5330195426940918\n",
      "Iteration 5807 Loss: 1.6474963426589966\n",
      "Iteration 5808 Loss: 1.7985464334487915\n",
      "Iteration 5809 Loss: 1.6668061017990112\n",
      "Iteration 5809 Loss: 1.4405663013458252\n",
      "Iteration 5810 Loss: 1.8077260255813599\n",
      "Iteration 5811 Loss: 1.2364894151687622\n",
      "Iteration 5812 Loss: 1.4853007793426514\n",
      "Iteration 5813 Loss: 1.4785178899765015\n",
      "Iteration 5814 Loss: 1.1915128231048584\n",
      "Iteration 5815 Loss: 1.1529849767684937\n",
      "Iteration 5816 Loss: 1.303999423980713\n",
      "Iteration 5817 Loss: 1.5947277545928955\n",
      "Iteration 5818 Loss: 1.5377401113510132\n",
      "Iteration 5819 Loss: 1.6621061563491821\n",
      "Iteration 5819 Loss: 1.4451104402542114\n",
      "Iteration 5820 Loss: 1.0349104404449463\n",
      "Iteration 5821 Loss: 1.8615140914916992\n",
      "Iteration 5822 Loss: 1.6746035814285278\n",
      "Iteration 5823 Loss: 1.4481247663497925\n",
      "Iteration 5824 Loss: 1.3914794921875\n",
      "Iteration 5825 Loss: 1.7992044687271118\n",
      "Iteration 5826 Loss: 1.5948452949523926\n",
      "Iteration 5827 Loss: 1.3229397535324097\n",
      "Iteration 5828 Loss: 1.7798759937286377\n",
      "Iteration 5829 Loss: 1.3590508699417114\n",
      "Iteration 5829 Loss: 1.5266549587249756\n",
      "Iteration 5830 Loss: 1.833066701889038\n",
      "Iteration 5831 Loss: 1.5057992935180664\n",
      "Iteration 5832 Loss: 1.5392909049987793\n",
      "Iteration 5833 Loss: 1.7530303001403809\n",
      "Iteration 5834 Loss: 1.203112244606018\n",
      "Iteration 5835 Loss: 1.1418116092681885\n",
      "Iteration 5836 Loss: 1.6899410486221313\n",
      "Iteration 5837 Loss: 1.9453411102294922\n",
      "Iteration 5838 Loss: 1.1260629892349243\n",
      "Iteration 5839 Loss: 1.2930556535720825\n",
      "Iteration 5839 Loss: 1.5030512809753418\n",
      "Iteration 5840 Loss: 1.3014843463897705\n",
      "Iteration 5841 Loss: 1.904130458831787\n",
      "Iteration 5842 Loss: 1.5959821939468384\n",
      "Iteration 5843 Loss: 1.4613622426986694\n",
      "Iteration 5844 Loss: 2.1524460315704346\n",
      "Iteration 5845 Loss: 1.6451678276062012\n",
      "Iteration 5846 Loss: 1.7620086669921875\n",
      "Iteration 5847 Loss: 1.4950498342514038\n",
      "Iteration 5848 Loss: 1.2088392972946167\n",
      "Iteration 5849 Loss: 1.4598002433776855\n",
      "Iteration 5849 Loss: 1.598626971244812\n",
      "Iteration 5850 Loss: 1.2329423427581787\n",
      "Iteration 5851 Loss: 1.438758373260498\n",
      "Iteration 5852 Loss: 1.5803889036178589\n",
      "Iteration 5853 Loss: 1.6786386966705322\n",
      "Iteration 5854 Loss: 1.7503364086151123\n",
      "Iteration 5855 Loss: 1.4536771774291992\n",
      "Iteration 5856 Loss: 1.6811848878860474\n",
      "Iteration 5857 Loss: 1.6603387594223022\n",
      "Iteration 5858 Loss: 1.2062184810638428\n",
      "Iteration 5859 Loss: 1.5824122428894043\n",
      "Iteration 5859 Loss: 1.5264896154403687\n",
      "Iteration 5860 Loss: 0.8668770790100098\n",
      "Iteration 5861 Loss: 1.7206511497497559\n",
      "Iteration 5862 Loss: 2.017124891281128\n",
      "Iteration 5863 Loss: 1.1132255792617798\n",
      "Iteration 5864 Loss: 1.190014362335205\n",
      "Iteration 5865 Loss: 1.2911583185195923\n",
      "Iteration 5866 Loss: 1.8029603958129883\n",
      "Iteration 5867 Loss: 1.8009936809539795\n",
      "Iteration 5868 Loss: 1.4159984588623047\n",
      "Iteration 5869 Loss: 1.0613083839416504\n",
      "Iteration 5869 Loss: 1.428031325340271\n",
      "Iteration 5870 Loss: 2.1918323040008545\n",
      "Iteration 5871 Loss: 1.5724109411239624\n",
      "Iteration 5872 Loss: 1.816142201423645\n",
      "Iteration 5873 Loss: 1.5043721199035645\n",
      "Iteration 5874 Loss: 1.4881069660186768\n",
      "Iteration 5875 Loss: 1.704465627670288\n",
      "Iteration 5876 Loss: 1.3842812776565552\n",
      "Iteration 5877 Loss: 1.406992793083191\n",
      "Iteration 5878 Loss: 1.0671565532684326\n",
      "Iteration 5879 Loss: 1.6125034093856812\n",
      "Iteration 5879 Loss: 1.5748264789581299\n",
      "Iteration 5880 Loss: 1.1377434730529785\n",
      "Iteration 5881 Loss: 1.3279706239700317\n",
      "Iteration 5882 Loss: 1.5996488332748413\n",
      "Iteration 5883 Loss: 1.8555349111557007\n",
      "Iteration 5884 Loss: 1.8284428119659424\n",
      "Iteration 5885 Loss: 1.2115813493728638\n",
      "Iteration 5886 Loss: 1.0903778076171875\n",
      "Iteration 5887 Loss: 1.8436602354049683\n",
      "Iteration 5888 Loss: 0.8882827758789062\n",
      "Iteration 5889 Loss: 1.6743056774139404\n",
      "Iteration 5889 Loss: 1.4457547664642334\n",
      "Iteration 5890 Loss: 1.7242118120193481\n",
      "Iteration 5891 Loss: 1.0555821657180786\n",
      "Iteration 5892 Loss: 0.8637076020240784\n",
      "Iteration 5893 Loss: 1.3127670288085938\n",
      "Iteration 5894 Loss: 1.5559349060058594\n",
      "Iteration 5895 Loss: 1.8585566282272339\n",
      "Iteration 5896 Loss: 1.5588738918304443\n",
      "Iteration 5897 Loss: 1.781952142715454\n",
      "Iteration 5898 Loss: 1.4474749565124512\n",
      "Iteration 5899 Loss: 1.3076610565185547\n",
      "Iteration 5899 Loss: 1.4466722011566162\n",
      "Iteration 5900 Loss: 1.316434621810913\n",
      "Iteration 5901 Loss: 1.6112946271896362\n",
      "Iteration 5902 Loss: 1.9508624076843262\n",
      "Iteration 5903 Loss: 1.0639729499816895\n",
      "Iteration 5904 Loss: 1.5726333856582642\n",
      "Iteration 5905 Loss: 1.5230300426483154\n",
      "Iteration 5906 Loss: 1.5057282447814941\n",
      "Iteration 5907 Loss: 1.2235784530639648\n",
      "Iteration 5908 Loss: 0.9918698668479919\n",
      "Iteration 5909 Loss: 1.4285228252410889\n",
      "Iteration 5909 Loss: 1.4187928438186646\n",
      "Iteration 5910 Loss: 1.0490376949310303\n",
      "Iteration 5911 Loss: 1.2233669757843018\n",
      "Iteration 5912 Loss: 1.474926233291626\n",
      "Iteration 5913 Loss: 1.3964139223098755\n",
      "Iteration 5914 Loss: 1.2137348651885986\n",
      "Iteration 5915 Loss: 1.4593290090560913\n",
      "Iteration 5916 Loss: 1.4572736024856567\n",
      "Iteration 5917 Loss: 1.6724401712417603\n",
      "Iteration 5918 Loss: 1.6639363765716553\n",
      "Iteration 5919 Loss: 1.1465232372283936\n",
      "Iteration 5919 Loss: 1.375698208808899\n",
      "Iteration 5920 Loss: 1.5152140855789185\n",
      "Iteration 5921 Loss: 1.961622953414917\n",
      "Iteration 5922 Loss: 1.210740327835083\n",
      "Iteration 5923 Loss: 1.0948307514190674\n",
      "Iteration 5924 Loss: 1.828366994857788\n",
      "Iteration 5925 Loss: 1.1756846904754639\n",
      "Iteration 5926 Loss: 1.7529460191726685\n",
      "Iteration 5927 Loss: 1.1316641569137573\n",
      "Iteration 5928 Loss: 1.3339598178863525\n",
      "Iteration 5929 Loss: 1.7410624027252197\n",
      "Iteration 5929 Loss: 1.474609136581421\n",
      "Iteration 5930 Loss: 1.632846474647522\n",
      "Iteration 5931 Loss: 1.2875841856002808\n",
      "Iteration 5932 Loss: 1.5375701189041138\n",
      "Iteration 5933 Loss: 1.7396296262741089\n",
      "Iteration 5934 Loss: 1.462713360786438\n",
      "Iteration 5935 Loss: 1.188241958618164\n",
      "Iteration 5936 Loss: 1.388454794883728\n",
      "Iteration 5937 Loss: 1.4892637729644775\n",
      "Iteration 5938 Loss: 1.5040138959884644\n",
      "Iteration 5939 Loss: 1.3364903926849365\n",
      "Iteration 5939 Loss: 1.4566807746887207\n",
      "Iteration 5940 Loss: 1.4565579891204834\n",
      "Iteration 5941 Loss: 1.6942484378814697\n",
      "Iteration 5942 Loss: 1.392988920211792\n",
      "Iteration 5943 Loss: 1.7117984294891357\n",
      "Iteration 5944 Loss: 1.8678975105285645\n",
      "Iteration 5945 Loss: 1.1858521699905396\n",
      "Iteration 5946 Loss: 1.4619197845458984\n",
      "Iteration 5947 Loss: 0.96831214427948\n",
      "Iteration 5948 Loss: 1.480526089668274\n",
      "Iteration 5949 Loss: 1.5332849025726318\n",
      "Iteration 5949 Loss: 1.4753386974334717\n",
      "Iteration 5950 Loss: 1.3226094245910645\n",
      "Iteration 5951 Loss: 1.7313212156295776\n",
      "Iteration 5952 Loss: 1.3719865083694458\n",
      "Iteration 5953 Loss: 1.419115424156189\n",
      "Iteration 5954 Loss: 1.6027666330337524\n",
      "Iteration 5955 Loss: 1.3659098148345947\n",
      "Iteration 5956 Loss: 1.6983646154403687\n",
      "Iteration 5957 Loss: 1.1721038818359375\n",
      "Iteration 5958 Loss: 2.06870698928833\n",
      "Iteration 5959 Loss: 1.2507537603378296\n",
      "Iteration 5959 Loss: 1.500363826751709\n",
      "Iteration 5960 Loss: 0.8412883877754211\n",
      "Iteration 5961 Loss: 1.029128909111023\n",
      "Iteration 5962 Loss: 1.0506513118743896\n",
      "Iteration 5963 Loss: 1.2843749523162842\n",
      "Iteration 5964 Loss: 1.6548833847045898\n",
      "Iteration 5965 Loss: 1.9374498128890991\n",
      "Iteration 5966 Loss: 1.6558879613876343\n",
      "Iteration 5967 Loss: 1.5299848318099976\n",
      "Iteration 5968 Loss: 1.6612135171890259\n",
      "Iteration 5969 Loss: 1.2085301876068115\n",
      "Iteration 5969 Loss: 1.3853392601013184\n",
      "Iteration 5970 Loss: 1.3577994108200073\n",
      "Iteration 5971 Loss: 1.441709280014038\n",
      "Iteration 5972 Loss: 1.5631757974624634\n",
      "Iteration 5973 Loss: 1.4071214199066162\n",
      "Iteration 5974 Loss: 1.7022223472595215\n",
      "Iteration 5975 Loss: 1.7454049587249756\n",
      "Iteration 5976 Loss: 1.3334296941757202\n",
      "Iteration 5977 Loss: 1.5716809034347534\n",
      "Iteration 5978 Loss: 1.3863308429718018\n",
      "Iteration 5979 Loss: 1.5726432800292969\n",
      "Iteration 5979 Loss: 1.5081517696380615\n",
      "Iteration 5980 Loss: 1.303970217704773\n",
      "Iteration 5981 Loss: 1.5783792734146118\n",
      "Iteration 5982 Loss: 1.7029869556427002\n",
      "Iteration 5983 Loss: 1.270060658454895\n",
      "Iteration 5984 Loss: 1.3758167028427124\n",
      "Iteration 5985 Loss: 1.7845911979675293\n",
      "Iteration 5986 Loss: 1.651441216468811\n",
      "Iteration 5987 Loss: 2.195683240890503\n",
      "Iteration 5988 Loss: 1.2901527881622314\n",
      "Iteration 5989 Loss: 1.811126947402954\n",
      "Iteration 5989 Loss: 1.5964210033416748\n",
      "Iteration 5990 Loss: 1.3230432271957397\n",
      "Iteration 5991 Loss: 1.7375293970108032\n",
      "Iteration 5992 Loss: 1.607677936553955\n",
      "Iteration 5993 Loss: 1.9035346508026123\n",
      "Iteration 5994 Loss: 1.6548527479171753\n",
      "Iteration 5995 Loss: 1.6025958061218262\n",
      "Iteration 5996 Loss: 1.1632189750671387\n",
      "Iteration 5997 Loss: 1.448282241821289\n",
      "Iteration 5998 Loss: 1.5268714427947998\n",
      "Iteration 5999 Loss: 1.799314022064209\n",
      "Iteration 5999 Loss: 1.57669198513031\n",
      "Iteration 6000 Loss: 1.304377794265747\n",
      "Iteration 6001 Loss: 1.818734049797058\n",
      "Iteration 6002 Loss: 1.7549822330474854\n",
      "Iteration 6003 Loss: 1.2891147136688232\n",
      "Iteration 6004 Loss: 1.1637262105941772\n",
      "Iteration 6005 Loss: 1.2423325777053833\n",
      "Iteration 6006 Loss: 1.082097053527832\n",
      "Iteration 6007 Loss: 1.542726993560791\n",
      "Iteration 6008 Loss: 1.712111234664917\n",
      "Iteration 6009 Loss: 1.7097893953323364\n",
      "Iteration 6009 Loss: 1.4619991779327393\n",
      "Iteration 6010 Loss: 0.8104379177093506\n",
      "Iteration 6011 Loss: 1.466947078704834\n",
      "Iteration 6012 Loss: 1.1641143560409546\n",
      "Iteration 6013 Loss: 1.5449824333190918\n",
      "Iteration 6014 Loss: 1.4475524425506592\n",
      "Iteration 6015 Loss: 1.0916138887405396\n",
      "Iteration 6016 Loss: 1.8292032480239868\n",
      "Iteration 6017 Loss: 1.7204750776290894\n",
      "Iteration 6018 Loss: 1.5077300071716309\n",
      "Iteration 6019 Loss: 1.186023235321045\n",
      "Iteration 6019 Loss: 1.3769080638885498\n",
      "Iteration 6020 Loss: 1.0522524118423462\n",
      "Iteration 6021 Loss: 1.8218039274215698\n",
      "Iteration 6022 Loss: 1.2264339923858643\n",
      "Iteration 6023 Loss: 1.6328389644622803\n",
      "Iteration 6024 Loss: 1.5454275608062744\n",
      "Iteration 6025 Loss: 1.0254197120666504\n",
      "Iteration 6026 Loss: 1.1592868566513062\n",
      "Iteration 6027 Loss: 1.3458738327026367\n",
      "Iteration 6028 Loss: 1.6618720293045044\n",
      "Iteration 6029 Loss: 1.332960605621338\n",
      "Iteration 6029 Loss: 1.380416989326477\n",
      "Iteration 6030 Loss: 1.40449059009552\n",
      "Iteration 6031 Loss: 1.239026665687561\n",
      "Iteration 6032 Loss: 1.6286163330078125\n",
      "Iteration 6033 Loss: 1.6075233221054077\n",
      "Iteration 6034 Loss: 1.5360991954803467\n",
      "Iteration 6035 Loss: 1.7342994213104248\n",
      "Iteration 6036 Loss: 1.6725913286209106\n",
      "Iteration 6037 Loss: 1.281417965888977\n",
      "Iteration 6038 Loss: 1.5595871210098267\n",
      "Iteration 6039 Loss: 1.2212480306625366\n",
      "Iteration 6039 Loss: 1.4884899854660034\n",
      "Iteration 6040 Loss: 0.9522836804389954\n",
      "Iteration 6041 Loss: 1.4003838300704956\n",
      "Iteration 6042 Loss: 1.3549519777297974\n",
      "Iteration 6043 Loss: 1.044475793838501\n",
      "Iteration 6044 Loss: 1.5562832355499268\n",
      "Iteration 6045 Loss: 1.4274823665618896\n",
      "Iteration 6046 Loss: 1.6258633136749268\n",
      "Iteration 6047 Loss: 1.4656636714935303\n",
      "Iteration 6048 Loss: 1.9267784357070923\n",
      "Iteration 6049 Loss: 1.5087023973464966\n",
      "Iteration 6049 Loss: 1.4262869358062744\n",
      "Iteration 6050 Loss: 1.4692108631134033\n",
      "Iteration 6051 Loss: 1.4955486059188843\n",
      "Iteration 6052 Loss: 2.1815788745880127\n",
      "Iteration 6053 Loss: 1.6547777652740479\n",
      "Iteration 6054 Loss: 1.1780526638031006\n",
      "Iteration 6055 Loss: 1.735325813293457\n",
      "Iteration 6056 Loss: 1.7094008922576904\n",
      "Iteration 6057 Loss: 1.423088788986206\n",
      "Iteration 6058 Loss: 0.8989972472190857\n",
      "Iteration 6059 Loss: 1.8192027807235718\n",
      "Iteration 6059 Loss: 1.5565184354782104\n",
      "Iteration 6060 Loss: 1.7283631563186646\n",
      "Iteration 6061 Loss: 2.001307964324951\n",
      "Iteration 6062 Loss: 1.4264246225357056\n",
      "Iteration 6063 Loss: 1.3348608016967773\n",
      "Iteration 6064 Loss: 1.6999053955078125\n",
      "Iteration 6065 Loss: 1.5553172826766968\n",
      "Iteration 6066 Loss: 1.9779788255691528\n",
      "Iteration 6067 Loss: 1.1367886066436768\n",
      "Iteration 6068 Loss: 1.248077392578125\n",
      "Iteration 6069 Loss: 1.4378535747528076\n",
      "Iteration 6069 Loss: 1.554687738418579\n",
      "Iteration 6070 Loss: 1.568135142326355\n",
      "Iteration 6071 Loss: 1.4585016965866089\n",
      "Iteration 6072 Loss: 1.4143853187561035\n",
      "Iteration 6073 Loss: 1.5848321914672852\n",
      "Iteration 6074 Loss: 1.5611635446548462\n",
      "Iteration 6075 Loss: 1.8369594812393188\n",
      "Iteration 6076 Loss: 1.6128398180007935\n",
      "Iteration 6077 Loss: 1.3451156616210938\n",
      "Iteration 6078 Loss: 1.2023063898086548\n",
      "Iteration 6079 Loss: 1.9937362670898438\n",
      "Iteration 6079 Loss: 1.5577976703643799\n",
      "Iteration 6080 Loss: 1.4715198278427124\n",
      "Iteration 6081 Loss: 1.1412321329116821\n",
      "Iteration 6082 Loss: 1.3849533796310425\n",
      "Iteration 6083 Loss: 1.0365325212478638\n",
      "Iteration 6084 Loss: 1.626392126083374\n",
      "Iteration 6085 Loss: 1.2144684791564941\n",
      "Iteration 6086 Loss: 1.123244285583496\n",
      "Iteration 6087 Loss: 1.786637544631958\n",
      "Iteration 6088 Loss: 1.5003434419631958\n",
      "Iteration 6089 Loss: 1.3248271942138672\n",
      "Iteration 6089 Loss: 1.3610150814056396\n",
      "Iteration 6090 Loss: 1.6884266138076782\n",
      "Iteration 6091 Loss: 1.2264329195022583\n",
      "Iteration 6092 Loss: 0.8582398295402527\n",
      "Iteration 6093 Loss: 1.5043258666992188\n",
      "Iteration 6094 Loss: 1.2175368070602417\n",
      "Iteration 6095 Loss: 1.2299050092697144\n",
      "Iteration 6096 Loss: 0.9426038265228271\n",
      "Iteration 6097 Loss: 1.7678258419036865\n",
      "Iteration 6098 Loss: 1.1180518865585327\n",
      "Iteration 6099 Loss: 1.4087803363800049\n",
      "Iteration 6099 Loss: 1.296212911605835\n",
      "Iteration 6100 Loss: 1.1649785041809082\n",
      "Iteration 6101 Loss: 1.6441401243209839\n",
      "Iteration 6102 Loss: 1.8290735483169556\n",
      "Iteration 6103 Loss: 1.517946481704712\n",
      "Iteration 6104 Loss: 1.658734679222107\n",
      "Iteration 6105 Loss: 1.6096155643463135\n",
      "Iteration 6106 Loss: 1.5898857116699219\n",
      "Iteration 6107 Loss: 1.611189365386963\n",
      "Iteration 6108 Loss: 0.9579240083694458\n",
      "Iteration 6109 Loss: 1.6187689304351807\n",
      "Iteration 6109 Loss: 1.5202257633209229\n",
      "Iteration 6110 Loss: 1.6033668518066406\n",
      "Iteration 6111 Loss: 1.1436614990234375\n",
      "Iteration 6112 Loss: 2.154787063598633\n",
      "Iteration 6113 Loss: 1.3676358461380005\n",
      "Iteration 6114 Loss: 1.5967357158660889\n",
      "Iteration 6115 Loss: 1.3909220695495605\n",
      "Iteration 6116 Loss: 1.6052429676055908\n",
      "Iteration 6117 Loss: 1.825845718383789\n",
      "Iteration 6118 Loss: 1.2544851303100586\n",
      "Iteration 6119 Loss: 1.1262983083724976\n",
      "Iteration 6119 Loss: 1.5068981647491455\n",
      "Iteration 6120 Loss: 1.0473130941390991\n",
      "Iteration 6121 Loss: 1.330614447593689\n",
      "Iteration 6122 Loss: 1.9041177034378052\n",
      "Iteration 6123 Loss: 1.730672836303711\n",
      "Iteration 6124 Loss: 1.281728982925415\n",
      "Iteration 6125 Loss: 1.5198264122009277\n",
      "Iteration 6126 Loss: 1.7878811359405518\n",
      "Iteration 6127 Loss: 1.8160266876220703\n",
      "Iteration 6128 Loss: 1.9830628633499146\n",
      "Iteration 6129 Loss: 1.746563196182251\n",
      "Iteration 6129 Loss: 1.6147807836532593\n",
      "Iteration 6130 Loss: 1.4675432443618774\n",
      "Iteration 6131 Loss: 1.2121363878250122\n",
      "Iteration 6132 Loss: 1.5303813219070435\n",
      "Iteration 6133 Loss: 1.1234362125396729\n",
      "Iteration 6134 Loss: 1.2884080410003662\n",
      "Iteration 6135 Loss: 0.931948721408844\n",
      "Iteration 6136 Loss: 1.4030821323394775\n",
      "Iteration 6137 Loss: 1.366439938545227\n",
      "Iteration 6138 Loss: 1.2333323955535889\n",
      "Iteration 6139 Loss: 1.6009272336959839\n",
      "Iteration 6139 Loss: 1.3157634735107422\n",
      "Iteration 6140 Loss: 2.0366103649139404\n",
      "Iteration 6141 Loss: 1.886961817741394\n",
      "Iteration 6142 Loss: 1.908839225769043\n",
      "Iteration 6143 Loss: 1.2242244482040405\n",
      "Iteration 6144 Loss: 1.1877092123031616\n",
      "Iteration 6145 Loss: 1.2908234596252441\n",
      "Iteration 6146 Loss: 1.4670192003250122\n",
      "Iteration 6147 Loss: 1.4601644277572632\n",
      "Iteration 6148 Loss: 1.6304516792297363\n",
      "Iteration 6149 Loss: 1.3106935024261475\n",
      "Iteration 6149 Loss: 1.5403496026992798\n",
      "Iteration 6150 Loss: 1.278277039527893\n",
      "Iteration 6151 Loss: 1.6367360353469849\n",
      "Iteration 6152 Loss: 1.9169338941574097\n",
      "Iteration 6153 Loss: 1.148820161819458\n",
      "Iteration 6154 Loss: 1.2802469730377197\n",
      "Iteration 6155 Loss: 1.2483270168304443\n",
      "Iteration 6156 Loss: 1.3976629972457886\n",
      "Iteration 6157 Loss: 2.055485248565674\n",
      "Iteration 6158 Loss: 1.3169686794281006\n",
      "Iteration 6159 Loss: 1.4299018383026123\n",
      "Iteration 6159 Loss: 1.4709360599517822\n",
      "Iteration 6160 Loss: 1.3767719268798828\n",
      "Iteration 6161 Loss: 1.9556926488876343\n",
      "Iteration 6162 Loss: 1.6770304441452026\n",
      "Iteration 6163 Loss: 1.0280373096466064\n",
      "Iteration 6164 Loss: 1.653809905052185\n",
      "Iteration 6165 Loss: 1.4783170223236084\n",
      "Iteration 6166 Loss: 0.9718655347824097\n",
      "Iteration 6167 Loss: 1.204389214515686\n",
      "Iteration 6168 Loss: 1.4013442993164062\n",
      "Iteration 6169 Loss: 1.747243881225586\n",
      "Iteration 6169 Loss: 1.4494502544403076\n",
      "Iteration 6170 Loss: 1.02940833568573\n",
      "Iteration 6171 Loss: 2.0003890991210938\n",
      "Iteration 6172 Loss: 1.3089945316314697\n",
      "Iteration 6173 Loss: 1.4657505750656128\n",
      "Iteration 6174 Loss: 1.2876999378204346\n",
      "Iteration 6175 Loss: 1.4744367599487305\n",
      "Iteration 6176 Loss: 1.963198184967041\n",
      "Iteration 6177 Loss: 2.026569128036499\n",
      "Iteration 6178 Loss: 1.2792855501174927\n",
      "Iteration 6179 Loss: 1.28145170211792\n",
      "Iteration 6179 Loss: 1.5117183923721313\n",
      "Iteration 6180 Loss: 1.1634334325790405\n",
      "Iteration 6181 Loss: 1.1164233684539795\n",
      "Iteration 6182 Loss: 1.6194041967391968\n",
      "Iteration 6183 Loss: 1.680907964706421\n",
      "Iteration 6184 Loss: 1.528033971786499\n",
      "Iteration 6185 Loss: 1.2131237983703613\n",
      "Iteration 6186 Loss: 1.4310109615325928\n",
      "Iteration 6187 Loss: 1.3592673540115356\n",
      "Iteration 6188 Loss: 1.704740285873413\n",
      "Iteration 6189 Loss: 1.5299118757247925\n",
      "Iteration 6189 Loss: 1.4346257448196411\n",
      "Iteration 6190 Loss: 1.9011731147766113\n",
      "Iteration 6191 Loss: 0.9544557929039001\n",
      "Iteration 6192 Loss: 1.347926378250122\n",
      "Iteration 6193 Loss: 1.454555630683899\n",
      "Iteration 6194 Loss: 1.662064552307129\n",
      "Iteration 6195 Loss: 1.8326795101165771\n",
      "Iteration 6196 Loss: 1.7467598915100098\n",
      "Iteration 6197 Loss: 1.6717208623886108\n",
      "Iteration 6198 Loss: 1.739829421043396\n",
      "Iteration 6199 Loss: 1.8674448728561401\n",
      "Iteration 6199 Loss: 1.617861032485962\n",
      "Iteration 6200 Loss: 1.0683462619781494\n",
      "Iteration 6201 Loss: 1.2070062160491943\n",
      "Iteration 6202 Loss: 1.2831566333770752\n",
      "Iteration 6203 Loss: 1.6919764280319214\n",
      "Iteration 6204 Loss: 1.6555278301239014\n",
      "Iteration 6205 Loss: 1.652876615524292\n",
      "Iteration 6206 Loss: 1.2656546831130981\n",
      "Iteration 6207 Loss: 1.5054494142532349\n",
      "Iteration 6208 Loss: 1.666123867034912\n",
      "Iteration 6209 Loss: 1.692708969116211\n",
      "Iteration 6209 Loss: 1.4688827991485596\n",
      "Iteration 6210 Loss: 1.5011705160140991\n",
      "Iteration 6211 Loss: 1.3482481241226196\n",
      "Iteration 6212 Loss: 1.465925693511963\n",
      "Iteration 6213 Loss: 1.578413724899292\n",
      "Iteration 6214 Loss: 1.8396217823028564\n",
      "Iteration 6215 Loss: 2.096764087677002\n",
      "Iteration 6216 Loss: 0.9317322969436646\n",
      "Iteration 6217 Loss: 1.772410273551941\n",
      "Iteration 6218 Loss: 1.4480578899383545\n",
      "Iteration 6219 Loss: 1.156449556350708\n",
      "Iteration 6219 Loss: 1.5138792991638184\n",
      "Iteration 6220 Loss: 1.6665352582931519\n",
      "Iteration 6221 Loss: 1.7468986511230469\n",
      "Iteration 6222 Loss: 1.5849792957305908\n",
      "Iteration 6223 Loss: 1.716105341911316\n",
      "Iteration 6224 Loss: 0.9903184175491333\n",
      "Iteration 6225 Loss: 1.5444270372390747\n",
      "Iteration 6226 Loss: 1.8507276773452759\n",
      "Iteration 6227 Loss: 1.5768063068389893\n",
      "Iteration 6228 Loss: 2.0031745433807373\n",
      "Iteration 6229 Loss: 1.7591636180877686\n",
      "Iteration 6229 Loss: 1.6439136266708374\n",
      "Iteration 6230 Loss: 1.0146148204803467\n",
      "Iteration 6231 Loss: 1.0879565477371216\n",
      "Iteration 6232 Loss: 1.4090014696121216\n",
      "Iteration 6233 Loss: 1.9987218379974365\n",
      "Iteration 6234 Loss: 1.420214295387268\n",
      "Iteration 6235 Loss: 1.3580151796340942\n",
      "Iteration 6236 Loss: 1.5550172328948975\n",
      "Iteration 6237 Loss: 1.708499789237976\n",
      "Iteration 6238 Loss: 1.2958707809448242\n",
      "Iteration 6239 Loss: 1.084145426750183\n",
      "Iteration 6239 Loss: 1.3932058811187744\n",
      "Iteration 6240 Loss: 1.5477113723754883\n",
      "Iteration 6241 Loss: 1.1644433736801147\n",
      "Iteration 6242 Loss: 1.7010102272033691\n",
      "Iteration 6243 Loss: 1.4043744802474976\n",
      "Iteration 6244 Loss: 1.5746862888336182\n",
      "Iteration 6245 Loss: 1.5462857484817505\n",
      "Iteration 6246 Loss: 1.835095763206482\n",
      "Iteration 6247 Loss: 1.3625551462173462\n",
      "Iteration 6248 Loss: 1.044625997543335\n",
      "Iteration 6249 Loss: 1.5776841640472412\n",
      "Iteration 6249 Loss: 1.4758472442626953\n",
      "Iteration 6250 Loss: 1.087406039237976\n",
      "Iteration 6251 Loss: 1.6463648080825806\n",
      "Iteration 6252 Loss: 0.9832892417907715\n",
      "Iteration 6253 Loss: 1.5326546430587769\n",
      "Iteration 6254 Loss: 1.4748884439468384\n",
      "Iteration 6255 Loss: 1.5113955736160278\n",
      "Iteration 6256 Loss: 1.7462291717529297\n",
      "Iteration 6257 Loss: 1.3490526676177979\n",
      "Iteration 6258 Loss: 1.8858026266098022\n",
      "Iteration 6259 Loss: 1.3891098499298096\n",
      "Iteration 6259 Loss: 1.4606192111968994\n",
      "Iteration 6260 Loss: 1.6767256259918213\n",
      "Iteration 6261 Loss: 1.7203338146209717\n",
      "Iteration 6262 Loss: 1.4171130657196045\n",
      "Iteration 6263 Loss: 1.6103323698043823\n",
      "Iteration 6264 Loss: 1.4944597482681274\n",
      "Iteration 6265 Loss: 2.0862531661987305\n",
      "Iteration 6266 Loss: 1.5542933940887451\n",
      "Iteration 6267 Loss: 1.52713942527771\n",
      "Iteration 6268 Loss: 1.4375606775283813\n",
      "Iteration 6269 Loss: 1.6766462326049805\n",
      "Iteration 6269 Loss: 1.6200859546661377\n",
      "Iteration 6270 Loss: 2.04189395904541\n",
      "Iteration 6271 Loss: 1.649112343788147\n",
      "Iteration 6272 Loss: 1.3690911531448364\n",
      "Iteration 6273 Loss: 1.4948428869247437\n",
      "Iteration 6274 Loss: 1.5568809509277344\n",
      "Iteration 6275 Loss: 0.8296892642974854\n",
      "Iteration 6276 Loss: 1.1282713413238525\n",
      "Iteration 6277 Loss: 1.7472373247146606\n",
      "Iteration 6278 Loss: 1.4957036972045898\n",
      "Iteration 6279 Loss: 1.8664226531982422\n",
      "Iteration 6279 Loss: 1.5179145336151123\n",
      "Iteration 6280 Loss: 1.3614163398742676\n",
      "Iteration 6281 Loss: 1.4465891122817993\n",
      "Iteration 6282 Loss: 1.1798956394195557\n",
      "Iteration 6283 Loss: 1.7779488563537598\n",
      "Iteration 6284 Loss: 1.1604257822036743\n",
      "Iteration 6285 Loss: 0.9889232516288757\n",
      "Iteration 6286 Loss: 1.2872477769851685\n",
      "Iteration 6287 Loss: 1.734497308731079\n",
      "Iteration 6288 Loss: 1.5069633722305298\n",
      "Iteration 6289 Loss: 1.4339596033096313\n",
      "Iteration 6289 Loss: 1.3877867460250854\n",
      "Iteration 6290 Loss: 1.5451886653900146\n",
      "Iteration 6291 Loss: 1.2070131301879883\n",
      "Iteration 6292 Loss: 1.8402528762817383\n",
      "Iteration 6293 Loss: 1.0804537534713745\n",
      "Iteration 6294 Loss: 1.2199276685714722\n",
      "Iteration 6295 Loss: 1.120689034461975\n",
      "Iteration 6296 Loss: 1.3546620607376099\n",
      "Iteration 6297 Loss: 1.6442372798919678\n",
      "Iteration 6298 Loss: 1.452917218208313\n",
      "Iteration 6299 Loss: 1.7749462127685547\n",
      "Iteration 6299 Loss: 1.4240288734436035\n",
      "Iteration 6300 Loss: 1.0452572107315063\n",
      "Iteration 6301 Loss: 1.497107744216919\n",
      "Iteration 6302 Loss: 1.827719807624817\n",
      "Iteration 6303 Loss: 1.5397875308990479\n",
      "Iteration 6304 Loss: 1.7017793655395508\n",
      "Iteration 6305 Loss: 1.2768959999084473\n",
      "Iteration 6306 Loss: 1.442210078239441\n",
      "Iteration 6307 Loss: 1.4989053010940552\n",
      "Iteration 6308 Loss: 1.7393102645874023\n",
      "Iteration 6309 Loss: 1.0258128643035889\n",
      "Iteration 6309 Loss: 1.4594786167144775\n",
      "Iteration 6310 Loss: 1.653701901435852\n",
      "Iteration 6311 Loss: 1.2733354568481445\n",
      "Iteration 6312 Loss: 1.2749484777450562\n",
      "Iteration 6313 Loss: 1.416719913482666\n",
      "Iteration 6314 Loss: 1.3023656606674194\n",
      "Iteration 6315 Loss: 1.9683719873428345\n",
      "Iteration 6316 Loss: 1.5422099828720093\n",
      "Iteration 6317 Loss: 1.4463810920715332\n",
      "Iteration 6318 Loss: 1.7436866760253906\n",
      "Iteration 6319 Loss: 1.466845989227295\n",
      "Iteration 6319 Loss: 1.5088567733764648\n",
      "Iteration 6320 Loss: 1.484933853149414\n",
      "Iteration 6321 Loss: 1.6865129470825195\n",
      "Iteration 6322 Loss: 1.592305064201355\n",
      "Iteration 6323 Loss: 1.5218262672424316\n",
      "Iteration 6324 Loss: 1.033154010772705\n",
      "Iteration 6325 Loss: 0.9757423400878906\n",
      "Iteration 6326 Loss: 1.3931280374526978\n",
      "Iteration 6327 Loss: 1.4202642440795898\n",
      "Iteration 6328 Loss: 1.5339381694793701\n",
      "Iteration 6329 Loss: 1.139704942703247\n",
      "Iteration 6329 Loss: 1.3781509399414062\n",
      "Iteration 6330 Loss: 1.5008302927017212\n",
      "Iteration 6331 Loss: 1.8383162021636963\n",
      "Iteration 6332 Loss: 1.5229003429412842\n",
      "Iteration 6333 Loss: 1.607499122619629\n",
      "Iteration 6334 Loss: 1.8220957517623901\n",
      "Iteration 6335 Loss: 1.5827699899673462\n",
      "Iteration 6336 Loss: 1.7887327671051025\n",
      "Iteration 6337 Loss: 1.465564250946045\n",
      "Iteration 6338 Loss: 1.8735630512237549\n",
      "Iteration 6339 Loss: 1.4367166757583618\n",
      "Iteration 6339 Loss: 1.6438989639282227\n",
      "Iteration 6340 Loss: 1.2244611978530884\n",
      "Iteration 6341 Loss: 2.105743885040283\n",
      "Iteration 6342 Loss: 1.668373465538025\n",
      "Iteration 6343 Loss: 1.17569100856781\n",
      "Iteration 6344 Loss: 1.215408205986023\n",
      "Iteration 6345 Loss: 1.6926406621932983\n",
      "Iteration 6346 Loss: 1.308663010597229\n",
      "Iteration 6347 Loss: 1.8949755430221558\n",
      "Iteration 6348 Loss: 1.3020708560943604\n",
      "Iteration 6349 Loss: 1.6108906269073486\n",
      "Iteration 6349 Loss: 1.5198918581008911\n",
      "Iteration 6350 Loss: 1.2419767379760742\n",
      "Iteration 6351 Loss: 1.2661397457122803\n",
      "Iteration 6352 Loss: 1.7831904888153076\n",
      "Iteration 6353 Loss: 1.7785142660140991\n",
      "Iteration 6354 Loss: 1.7584096193313599\n",
      "Iteration 6355 Loss: 1.181793451309204\n",
      "Iteration 6356 Loss: 1.4853596687316895\n",
      "Iteration 6357 Loss: 1.9375348091125488\n",
      "Iteration 6358 Loss: 1.4532305002212524\n",
      "Iteration 6359 Loss: 1.4605093002319336\n",
      "Iteration 6359 Loss: 1.5346657037734985\n",
      "Iteration 6360 Loss: 1.4789050817489624\n",
      "Iteration 6361 Loss: 1.353559136390686\n",
      "Iteration 6362 Loss: 1.105686068534851\n",
      "Iteration 6363 Loss: 1.5681705474853516\n",
      "Iteration 6364 Loss: 1.2697793245315552\n",
      "Iteration 6365 Loss: 1.3116934299468994\n",
      "Iteration 6366 Loss: 1.5434139966964722\n",
      "Iteration 6367 Loss: 1.5386582612991333\n",
      "Iteration 6368 Loss: 1.4870142936706543\n",
      "Iteration 6369 Loss: 1.2561287879943848\n",
      "Iteration 6369 Loss: 1.391300916671753\n",
      "Iteration 6370 Loss: 2.0809271335601807\n",
      "Iteration 6371 Loss: 1.9479607343673706\n",
      "Iteration 6372 Loss: 1.8585968017578125\n",
      "Iteration 6373 Loss: 1.3719857931137085\n",
      "Iteration 6374 Loss: 1.7785588502883911\n",
      "Iteration 6375 Loss: 0.785144031047821\n",
      "Iteration 6376 Loss: 1.1462128162384033\n",
      "Iteration 6377 Loss: 1.163217544555664\n",
      "Iteration 6378 Loss: 0.9667814373970032\n",
      "Iteration 6379 Loss: 1.9934481382369995\n",
      "Iteration 6379 Loss: 1.5092833042144775\n",
      "Iteration 6380 Loss: 2.0357019901275635\n",
      "Iteration 6381 Loss: 1.2213208675384521\n",
      "Iteration 6382 Loss: 1.6419384479522705\n",
      "Iteration 6383 Loss: 1.592459797859192\n",
      "Iteration 6384 Loss: 1.53632390499115\n",
      "Iteration 6385 Loss: 1.5499835014343262\n",
      "Iteration 6386 Loss: 1.7430028915405273\n",
      "Iteration 6387 Loss: 1.2300190925598145\n",
      "Iteration 6388 Loss: 1.283964991569519\n",
      "Iteration 6389 Loss: 1.5058283805847168\n",
      "Iteration 6389 Loss: 1.5340543985366821\n",
      "Iteration 6390 Loss: 1.4411137104034424\n",
      "Iteration 6391 Loss: 1.3678275346755981\n",
      "Iteration 6392 Loss: 1.7686094045639038\n",
      "Iteration 6393 Loss: 2.0042572021484375\n",
      "Iteration 6394 Loss: 1.3923428058624268\n",
      "Iteration 6395 Loss: 1.089820146560669\n",
      "Iteration 6396 Loss: 1.39633047580719\n",
      "Iteration 6397 Loss: 1.5726008415222168\n",
      "Iteration 6398 Loss: 1.9097262620925903\n",
      "Iteration 6399 Loss: 1.478083610534668\n",
      "Iteration 6399 Loss: 1.5420711040496826\n",
      "Iteration 6400 Loss: 1.28179931640625\n",
      "Iteration 6401 Loss: 1.4062435626983643\n",
      "Iteration 6402 Loss: 1.6859312057495117\n",
      "Iteration 6403 Loss: 1.7564022541046143\n",
      "Iteration 6404 Loss: 1.1959412097930908\n",
      "Iteration 6405 Loss: 1.1264028549194336\n",
      "Iteration 6406 Loss: 1.7507585287094116\n",
      "Iteration 6407 Loss: 0.9791666865348816\n",
      "Iteration 6408 Loss: 0.9881869554519653\n",
      "Iteration 6409 Loss: 1.6180062294006348\n",
      "Iteration 6409 Loss: 1.3788838386535645\n",
      "Iteration 6410 Loss: 1.0368387699127197\n",
      "Iteration 6411 Loss: 1.3115297555923462\n",
      "Iteration 6412 Loss: 1.5895767211914062\n",
      "Iteration 6413 Loss: 1.2327767610549927\n",
      "Iteration 6414 Loss: 1.8490580320358276\n",
      "Iteration 6415 Loss: 1.389084815979004\n",
      "Iteration 6416 Loss: 1.0862466096878052\n",
      "Iteration 6417 Loss: 1.6846787929534912\n",
      "Iteration 6418 Loss: 1.1641840934753418\n",
      "Iteration 6419 Loss: 1.300301432609558\n",
      "Iteration 6419 Loss: 1.3644275665283203\n",
      "Iteration 6420 Loss: 1.6675596237182617\n",
      "Iteration 6421 Loss: 1.285833477973938\n",
      "Iteration 6422 Loss: 1.546705961227417\n",
      "Iteration 6423 Loss: 1.7482423782348633\n",
      "Iteration 6424 Loss: 1.4424083232879639\n",
      "Iteration 6425 Loss: 1.1719927787780762\n",
      "Iteration 6426 Loss: 1.344982624053955\n",
      "Iteration 6427 Loss: 1.7560920715332031\n",
      "Iteration 6428 Loss: 1.326277256011963\n",
      "Iteration 6429 Loss: 1.3692766427993774\n",
      "Iteration 6429 Loss: 1.4659370183944702\n",
      "Iteration 6430 Loss: 1.2046935558319092\n",
      "Iteration 6431 Loss: 1.0327154397964478\n",
      "Iteration 6432 Loss: 1.7027841806411743\n",
      "Iteration 6433 Loss: 1.348891258239746\n",
      "Iteration 6434 Loss: 1.1398162841796875\n",
      "Iteration 6435 Loss: 1.9146931171417236\n",
      "Iteration 6436 Loss: 1.8269292116165161\n",
      "Iteration 6437 Loss: 1.4654932022094727\n",
      "Iteration 6438 Loss: 1.343783974647522\n",
      "Iteration 6439 Loss: 1.7027974128723145\n",
      "Iteration 6439 Loss: 1.4682596921920776\n",
      "Iteration 6440 Loss: 1.5767713785171509\n",
      "Iteration 6441 Loss: 1.403504729270935\n",
      "Iteration 6442 Loss: 1.313382863998413\n",
      "Iteration 6443 Loss: 1.0806856155395508\n",
      "Iteration 6444 Loss: 1.1346808671951294\n",
      "Iteration 6445 Loss: 1.9585888385772705\n",
      "Iteration 6446 Loss: 1.8058257102966309\n",
      "Iteration 6447 Loss: 1.7601500749588013\n",
      "Iteration 6448 Loss: 1.3137205839157104\n",
      "Iteration 6449 Loss: 1.4722579717636108\n",
      "Iteration 6449 Loss: 1.4819567203521729\n",
      "Iteration 6450 Loss: 0.7722404599189758\n",
      "Iteration 6451 Loss: 1.2909940481185913\n",
      "Iteration 6452 Loss: 1.4244270324707031\n",
      "Iteration 6453 Loss: 1.618669867515564\n",
      "Iteration 6454 Loss: 1.4170942306518555\n",
      "Iteration 6455 Loss: 1.2034366130828857\n",
      "Iteration 6456 Loss: 1.3054544925689697\n",
      "Iteration 6457 Loss: 1.4938727617263794\n",
      "Iteration 6458 Loss: 1.6547741889953613\n",
      "Iteration 6459 Loss: 1.2370083332061768\n",
      "Iteration 6459 Loss: 1.3417972326278687\n",
      "Iteration 6460 Loss: 1.5554697513580322\n",
      "Iteration 6461 Loss: 1.5629417896270752\n",
      "Iteration 6462 Loss: 1.7928016185760498\n",
      "Iteration 6463 Loss: 2.283555030822754\n",
      "Iteration 6464 Loss: 1.2717905044555664\n",
      "Iteration 6465 Loss: 1.576823115348816\n",
      "Iteration 6466 Loss: 1.782914161682129\n",
      "Iteration 6467 Loss: 1.9410321712493896\n",
      "Iteration 6468 Loss: 1.5869324207305908\n",
      "Iteration 6469 Loss: 1.5836125612258911\n",
      "Iteration 6469 Loss: 1.6937873363494873\n",
      "Iteration 6470 Loss: 1.8440812826156616\n",
      "Iteration 6471 Loss: 1.4739288091659546\n",
      "Iteration 6472 Loss: 1.4992202520370483\n",
      "Iteration 6473 Loss: 1.7464489936828613\n",
      "Iteration 6474 Loss: 1.5354020595550537\n",
      "Iteration 6475 Loss: 1.4036524295806885\n",
      "Iteration 6476 Loss: 1.4714349508285522\n",
      "Iteration 6477 Loss: 1.9407994747161865\n",
      "Iteration 6478 Loss: 1.5636506080627441\n",
      "Iteration 6479 Loss: 1.4916892051696777\n",
      "Iteration 6479 Loss: 1.5970308780670166\n",
      "Iteration 6480 Loss: 1.1021990776062012\n",
      "Iteration 6481 Loss: 1.7655127048492432\n",
      "Iteration 6482 Loss: 1.4112435579299927\n",
      "Iteration 6483 Loss: 1.6212819814682007\n",
      "Iteration 6484 Loss: 1.949944019317627\n",
      "Iteration 6485 Loss: 1.9509055614471436\n",
      "Iteration 6486 Loss: 0.9584199786186218\n",
      "Iteration 6487 Loss: 2.0726399421691895\n",
      "Iteration 6488 Loss: 1.8535261154174805\n",
      "Iteration 6489 Loss: 1.2746020555496216\n",
      "Iteration 6489 Loss: 1.5960276126861572\n",
      "Iteration 6490 Loss: 1.6421117782592773\n",
      "Iteration 6491 Loss: 1.2382726669311523\n",
      "Iteration 6492 Loss: 1.7420648336410522\n",
      "Iteration 6493 Loss: 1.9300857782363892\n",
      "Iteration 6494 Loss: 0.8518821001052856\n",
      "Iteration 6495 Loss: 1.848901391029358\n",
      "Iteration 6496 Loss: 1.672663688659668\n",
      "Iteration 6497 Loss: 1.5861340761184692\n",
      "Iteration 6498 Loss: 1.5790058374404907\n",
      "Iteration 6499 Loss: 1.7812323570251465\n",
      "Iteration 6499 Loss: 1.587235450744629\n",
      "Iteration 6500 Loss: 1.3538390398025513\n",
      "Iteration 6501 Loss: 0.9295792579650879\n",
      "Iteration 6502 Loss: 1.1550631523132324\n",
      "Iteration 6503 Loss: 1.2054214477539062\n",
      "Iteration 6504 Loss: 1.6883212327957153\n",
      "Iteration 6505 Loss: 1.363828420639038\n",
      "Iteration 6506 Loss: 1.715718388557434\n",
      "Iteration 6507 Loss: 1.7011650800704956\n",
      "Iteration 6508 Loss: 1.3004183769226074\n",
      "Iteration 6509 Loss: 0.9824604392051697\n",
      "Iteration 6509 Loss: 1.3395814895629883\n",
      "Iteration 6510 Loss: 1.7366487979888916\n",
      "Iteration 6511 Loss: 1.774661660194397\n",
      "Iteration 6512 Loss: 1.4230071306228638\n",
      "Iteration 6513 Loss: 1.977533221244812\n",
      "Iteration 6514 Loss: 1.1802504062652588\n",
      "Iteration 6515 Loss: 1.8562161922454834\n",
      "Iteration 6516 Loss: 1.6665695905685425\n",
      "Iteration 6517 Loss: 1.662663221359253\n",
      "Iteration 6518 Loss: 1.4291032552719116\n",
      "Iteration 6519 Loss: 1.2907153367996216\n",
      "Iteration 6519 Loss: 1.5997369289398193\n",
      "Iteration 6520 Loss: 1.2064378261566162\n",
      "Iteration 6521 Loss: 1.3664594888687134\n",
      "Iteration 6522 Loss: 1.728219747543335\n",
      "Iteration 6523 Loss: 1.3371528387069702\n",
      "Iteration 6524 Loss: 1.2888168096542358\n",
      "Iteration 6525 Loss: 1.8211369514465332\n",
      "Iteration 6526 Loss: 1.5232398509979248\n",
      "Iteration 6527 Loss: 1.1443268060684204\n",
      "Iteration 6528 Loss: 1.3553009033203125\n",
      "Iteration 6529 Loss: 1.2664049863815308\n",
      "Iteration 6529 Loss: 1.4037495851516724\n",
      "Iteration 6530 Loss: 1.6937512159347534\n",
      "Iteration 6531 Loss: 1.1537513732910156\n",
      "Iteration 6532 Loss: 1.3021785020828247\n",
      "Iteration 6533 Loss: 1.4530466794967651\n",
      "Iteration 6534 Loss: 1.4244259595870972\n",
      "Iteration 6535 Loss: 1.2660396099090576\n",
      "Iteration 6536 Loss: 1.514623761177063\n",
      "Iteration 6537 Loss: 1.220203161239624\n",
      "Iteration 6538 Loss: 1.755893588066101\n",
      "Iteration 6539 Loss: 1.1794092655181885\n",
      "Iteration 6539 Loss: 1.3963323831558228\n",
      "Iteration 6540 Loss: 1.2260167598724365\n",
      "Iteration 6541 Loss: 0.9748833775520325\n",
      "Iteration 6542 Loss: 2.042268753051758\n",
      "Iteration 6543 Loss: 1.9413713216781616\n",
      "Iteration 6544 Loss: 1.4116215705871582\n",
      "Iteration 6545 Loss: 1.9467072486877441\n",
      "Iteration 6546 Loss: 1.4003491401672363\n",
      "Iteration 6547 Loss: 1.3857289552688599\n",
      "Iteration 6548 Loss: 1.1509983539581299\n",
      "Iteration 6549 Loss: 1.407658576965332\n",
      "Iteration 6549 Loss: 1.4887603521347046\n",
      "Iteration 6550 Loss: 1.746366262435913\n",
      "Iteration 6551 Loss: 1.3688561916351318\n",
      "Iteration 6552 Loss: 2.022432804107666\n",
      "Iteration 6553 Loss: 1.7706760168075562\n",
      "Iteration 6554 Loss: 1.2563245296478271\n",
      "Iteration 6555 Loss: 1.6516988277435303\n",
      "Iteration 6556 Loss: 1.60537850856781\n",
      "Iteration 6557 Loss: 1.76949942111969\n",
      "Iteration 6558 Loss: 1.4346837997436523\n",
      "Iteration 6559 Loss: 1.7713464498519897\n",
      "Iteration 6559 Loss: 1.6397262811660767\n",
      "Iteration 6560 Loss: 1.6480205059051514\n",
      "Iteration 6561 Loss: 1.4344714879989624\n",
      "Iteration 6562 Loss: 1.3227449655532837\n",
      "Iteration 6563 Loss: 1.3847339153289795\n",
      "Iteration 6564 Loss: 1.8111677169799805\n",
      "Iteration 6565 Loss: 1.9279255867004395\n",
      "Iteration 6566 Loss: 1.8079410791397095\n",
      "Iteration 6567 Loss: 1.2104651927947998\n",
      "Iteration 6568 Loss: 1.6595991849899292\n",
      "Iteration 6569 Loss: 1.3667118549346924\n",
      "Iteration 6569 Loss: 1.5573782920837402\n",
      "Iteration 6570 Loss: 1.6832307577133179\n",
      "Iteration 6571 Loss: 1.559460997581482\n",
      "Iteration 6572 Loss: 1.6377217769622803\n",
      "Iteration 6573 Loss: 1.0572445392608643\n",
      "Iteration 6574 Loss: 1.202789306640625\n",
      "Iteration 6575 Loss: 1.6191301345825195\n",
      "Iteration 6576 Loss: 1.7998180389404297\n",
      "Iteration 6577 Loss: 1.6171704530715942\n",
      "Iteration 6578 Loss: 1.8000494241714478\n",
      "Iteration 6579 Loss: 1.0025840997695923\n",
      "Iteration 6579 Loss: 1.4979199171066284\n",
      "Iteration 6580 Loss: 1.5882627964019775\n",
      "Iteration 6581 Loss: 1.9460258483886719\n",
      "Iteration 6582 Loss: 1.1326518058776855\n",
      "Iteration 6583 Loss: 0.9446104764938354\n",
      "Iteration 6584 Loss: 1.855014681816101\n",
      "Iteration 6585 Loss: 1.1175806522369385\n",
      "Iteration 6586 Loss: 1.9535373449325562\n",
      "Iteration 6587 Loss: 1.6409306526184082\n",
      "Iteration 6588 Loss: 1.3421964645385742\n",
      "Iteration 6589 Loss: 1.4376872777938843\n",
      "Iteration 6589 Loss: 1.495849847793579\n",
      "Iteration 6590 Loss: 0.7744948863983154\n",
      "Iteration 6591 Loss: 1.6608645915985107\n",
      "Iteration 6592 Loss: 1.484347939491272\n",
      "Iteration 6593 Loss: 1.5374802350997925\n",
      "Iteration 6594 Loss: 1.7533855438232422\n",
      "Iteration 6595 Loss: 1.216177225112915\n",
      "Iteration 6596 Loss: 1.9298515319824219\n",
      "Iteration 6597 Loss: 1.1870304346084595\n",
      "Iteration 6598 Loss: 0.9098527431488037\n",
      "Iteration 6599 Loss: 1.4536932706832886\n",
      "Iteration 6599 Loss: 1.39071786403656\n",
      "Iteration 6600 Loss: 1.531264066696167\n",
      "Iteration 6601 Loss: 1.2685432434082031\n",
      "Iteration 6602 Loss: 2.1071348190307617\n",
      "Iteration 6603 Loss: 1.1814175844192505\n",
      "Iteration 6604 Loss: 1.5742883682250977\n",
      "Iteration 6605 Loss: 1.6107053756713867\n",
      "Iteration 6606 Loss: 1.192781686782837\n",
      "Iteration 6607 Loss: 0.9819644689559937\n",
      "Iteration 6608 Loss: 1.2829771041870117\n",
      "Iteration 6609 Loss: 1.8963921070098877\n",
      "Iteration 6609 Loss: 1.4627468585968018\n",
      "Iteration 6610 Loss: 1.815613865852356\n",
      "Iteration 6611 Loss: 1.5878679752349854\n",
      "Iteration 6612 Loss: 1.781255841255188\n",
      "Iteration 6613 Loss: 1.5915299654006958\n",
      "Iteration 6614 Loss: 1.5344069004058838\n",
      "Iteration 6615 Loss: 1.573929786682129\n",
      "Iteration 6616 Loss: 1.598069190979004\n",
      "Iteration 6617 Loss: 1.992077112197876\n",
      "Iteration 6618 Loss: 1.5301806926727295\n",
      "Iteration 6619 Loss: 1.5589652061462402\n",
      "Iteration 6619 Loss: 1.656389594078064\n",
      "Iteration 6620 Loss: 1.2734369039535522\n",
      "Iteration 6621 Loss: 1.7611737251281738\n",
      "Iteration 6622 Loss: 1.4911062717437744\n",
      "Iteration 6623 Loss: 1.062989592552185\n",
      "Iteration 6624 Loss: 1.5763838291168213\n",
      "Iteration 6625 Loss: 1.6740726232528687\n",
      "Iteration 6626 Loss: 1.6537394523620605\n",
      "Iteration 6627 Loss: 1.5344334840774536\n",
      "Iteration 6628 Loss: 1.9747810363769531\n",
      "Iteration 6629 Loss: 1.5523046255111694\n",
      "Iteration 6629 Loss: 1.5554420948028564\n",
      "Iteration 6630 Loss: 1.177077293395996\n",
      "Iteration 6631 Loss: 1.547920823097229\n",
      "Iteration 6632 Loss: 1.5508607625961304\n",
      "Iteration 6633 Loss: 1.69057035446167\n",
      "Iteration 6634 Loss: 1.376724362373352\n",
      "Iteration 6635 Loss: 1.3338522911071777\n",
      "Iteration 6636 Loss: 1.3464075326919556\n",
      "Iteration 6637 Loss: 1.2870758771896362\n",
      "Iteration 6638 Loss: 1.5725022554397583\n",
      "Iteration 6639 Loss: 1.1897454261779785\n",
      "Iteration 6639 Loss: 1.4072736501693726\n",
      "Iteration 6640 Loss: 1.0748540163040161\n",
      "Iteration 6641 Loss: 1.426444411277771\n",
      "Iteration 6642 Loss: 1.5875786542892456\n",
      "Iteration 6643 Loss: 1.370316982269287\n",
      "Iteration 6644 Loss: 1.5570741891860962\n",
      "Iteration 6645 Loss: 1.6997692584991455\n",
      "Iteration 6646 Loss: 1.7574996948242188\n",
      "Iteration 6647 Loss: 1.3729662895202637\n",
      "Iteration 6648 Loss: 1.573089599609375\n",
      "Iteration 6649 Loss: 1.6210658550262451\n",
      "Iteration 6649 Loss: 1.5040658712387085\n",
      "Iteration 6650 Loss: 2.023198366165161\n",
      "Iteration 6651 Loss: 1.6820199489593506\n",
      "Iteration 6652 Loss: 1.3331140279769897\n",
      "Iteration 6653 Loss: 1.7465859651565552\n",
      "Iteration 6654 Loss: 1.7661466598510742\n",
      "Iteration 6655 Loss: 1.253949761390686\n",
      "Iteration 6656 Loss: 1.578691840171814\n",
      "Iteration 6657 Loss: 1.342179298400879\n",
      "Iteration 6658 Loss: 1.380455732345581\n",
      "Iteration 6659 Loss: 1.8232076168060303\n",
      "Iteration 6659 Loss: 1.5929548740386963\n",
      "Iteration 6660 Loss: 1.5391165018081665\n",
      "Iteration 6661 Loss: 1.253847599029541\n",
      "Iteration 6662 Loss: 1.5539133548736572\n",
      "Iteration 6663 Loss: 1.7775403261184692\n",
      "Iteration 6664 Loss: 1.4818270206451416\n",
      "Iteration 6665 Loss: 1.6248400211334229\n",
      "Iteration 6666 Loss: 1.7995644807815552\n",
      "Iteration 6667 Loss: 1.2013698816299438\n",
      "Iteration 6668 Loss: 1.8597524166107178\n",
      "Iteration 6669 Loss: 0.9951683282852173\n",
      "Iteration 6669 Loss: 1.5086939334869385\n",
      "Iteration 6670 Loss: 1.7751816511154175\n",
      "Iteration 6671 Loss: 1.3064970970153809\n",
      "Iteration 6672 Loss: 1.2938568592071533\n",
      "Iteration 6673 Loss: 1.5063230991363525\n",
      "Iteration 6674 Loss: 1.5537421703338623\n",
      "Iteration 6675 Loss: 1.2816760540008545\n",
      "Iteration 6676 Loss: 1.9134373664855957\n",
      "Iteration 6677 Loss: 1.3993953466415405\n",
      "Iteration 6678 Loss: 1.4632149934768677\n",
      "Iteration 6679 Loss: 1.7439087629318237\n",
      "Iteration 6679 Loss: 1.5237233638763428\n",
      "Iteration 6680 Loss: 1.54707670211792\n",
      "Iteration 6681 Loss: 1.6704314947128296\n",
      "Iteration 6682 Loss: 1.387009859085083\n",
      "Iteration 6683 Loss: 1.4585797786712646\n",
      "Iteration 6684 Loss: 1.380853295326233\n",
      "Iteration 6685 Loss: 1.4675695896148682\n",
      "Iteration 6686 Loss: 1.3365801572799683\n",
      "Iteration 6687 Loss: 1.485537052154541\n",
      "Iteration 6688 Loss: 1.6147183179855347\n",
      "Iteration 6689 Loss: 1.169506549835205\n",
      "Iteration 6689 Loss: 1.4517863988876343\n",
      "Iteration 6690 Loss: 1.0267274379730225\n",
      "Iteration 6691 Loss: 1.576189398765564\n",
      "Iteration 6692 Loss: 1.7531366348266602\n",
      "Iteration 6693 Loss: 1.643579363822937\n",
      "Iteration 6694 Loss: 1.0983352661132812\n",
      "Iteration 6695 Loss: 1.8275887966156006\n",
      "Iteration 6696 Loss: 1.2754182815551758\n",
      "Iteration 6697 Loss: 1.351052165031433\n",
      "Iteration 6698 Loss: 1.2282192707061768\n",
      "Iteration 6699 Loss: 1.5240557193756104\n",
      "Iteration 6699 Loss: 1.4304301738739014\n",
      "Iteration 6700 Loss: 1.7376214265823364\n",
      "Iteration 6701 Loss: 1.3638267517089844\n",
      "Iteration 6702 Loss: 1.434901237487793\n",
      "Iteration 6703 Loss: 1.1246954202651978\n",
      "Iteration 6704 Loss: 1.3161687850952148\n",
      "Iteration 6705 Loss: 1.5972833633422852\n",
      "Iteration 6706 Loss: 1.2735220193862915\n",
      "Iteration 6707 Loss: 1.8096777200698853\n",
      "Iteration 6708 Loss: 1.5434141159057617\n",
      "Iteration 6709 Loss: 1.5913909673690796\n",
      "Iteration 6709 Loss: 1.479250192642212\n",
      "Iteration 6710 Loss: 1.5387171506881714\n",
      "Iteration 6711 Loss: 1.5248634815216064\n",
      "Iteration 6712 Loss: 1.5616075992584229\n",
      "Iteration 6713 Loss: 1.6438137292861938\n",
      "Iteration 6714 Loss: 1.453454613685608\n",
      "Iteration 6715 Loss: 1.2012304067611694\n",
      "Iteration 6716 Loss: 1.7426836490631104\n",
      "Iteration 6717 Loss: 1.1110996007919312\n",
      "Iteration 6718 Loss: 1.6630144119262695\n",
      "Iteration 6719 Loss: 1.4129996299743652\n",
      "Iteration 6719 Loss: 1.4853484630584717\n",
      "Iteration 6720 Loss: 1.844025731086731\n",
      "Iteration 6721 Loss: 1.712153434753418\n",
      "Iteration 6722 Loss: 1.1117421388626099\n",
      "Iteration 6723 Loss: 1.6368944644927979\n",
      "Iteration 6724 Loss: 1.531503677368164\n",
      "Iteration 6725 Loss: 1.3459279537200928\n",
      "Iteration 6726 Loss: 1.782138705253601\n",
      "Iteration 6727 Loss: 0.7541207075119019\n",
      "Iteration 6728 Loss: 1.5630841255187988\n",
      "Iteration 6729 Loss: 1.6257082223892212\n",
      "Iteration 6729 Loss: 1.4907299280166626\n",
      "Iteration 6730 Loss: 1.3704581260681152\n",
      "Iteration 6731 Loss: 1.5735567808151245\n",
      "Iteration 6732 Loss: 0.9341734051704407\n",
      "Iteration 6733 Loss: 1.0784823894500732\n",
      "Iteration 6734 Loss: 1.720780611038208\n",
      "Iteration 6735 Loss: 1.2681691646575928\n",
      "Iteration 6736 Loss: 1.6647181510925293\n",
      "Iteration 6737 Loss: 1.3936536312103271\n",
      "Iteration 6738 Loss: 1.1007890701293945\n",
      "Iteration 6739 Loss: 1.7692102193832397\n",
      "Iteration 6739 Loss: 1.3873993158340454\n",
      "Iteration 6740 Loss: 1.5105655193328857\n",
      "Iteration 6741 Loss: 0.9526398181915283\n",
      "Iteration 6742 Loss: 0.9706364274024963\n",
      "Iteration 6743 Loss: 1.6391472816467285\n",
      "Iteration 6744 Loss: 1.1261720657348633\n",
      "Iteration 6745 Loss: 1.601346492767334\n",
      "Iteration 6746 Loss: 1.1716872453689575\n",
      "Iteration 6747 Loss: 1.759065866470337\n",
      "Iteration 6748 Loss: 0.6775955557823181\n",
      "Iteration 6749 Loss: 1.2583158016204834\n",
      "Iteration 6749 Loss: 1.2667171955108643\n",
      "Iteration 6750 Loss: 1.5141035318374634\n",
      "Iteration 6751 Loss: 1.5798420906066895\n",
      "Iteration 6752 Loss: 1.4957633018493652\n",
      "Iteration 6753 Loss: 1.326155662536621\n",
      "Iteration 6754 Loss: 0.9518252015113831\n",
      "Iteration 6755 Loss: 1.4047753810882568\n",
      "Iteration 6756 Loss: 1.4595508575439453\n",
      "Iteration 6757 Loss: 1.329489827156067\n",
      "Iteration 6758 Loss: 2.0797924995422363\n",
      "Iteration 6759 Loss: 1.8631869554519653\n",
      "Iteration 6759 Loss: 1.50044846534729\n",
      "Iteration 6760 Loss: 1.5875818729400635\n",
      "Iteration 6761 Loss: 1.559873342514038\n",
      "Iteration 6762 Loss: 1.4977126121520996\n",
      "Iteration 6763 Loss: 1.2168985605239868\n",
      "Iteration 6764 Loss: 1.3202199935913086\n",
      "Iteration 6765 Loss: 1.4173882007598877\n",
      "Iteration 6766 Loss: 1.8536677360534668\n",
      "Iteration 6767 Loss: 1.5123850107192993\n",
      "Iteration 6768 Loss: 1.5379290580749512\n",
      "Iteration 6769 Loss: 0.997626006603241\n",
      "Iteration 6769 Loss: 1.4501283168792725\n",
      "Iteration 6770 Loss: 1.4298737049102783\n",
      "Iteration 6771 Loss: 1.56113862991333\n",
      "Iteration 6772 Loss: 1.8126429319381714\n",
      "Iteration 6773 Loss: 1.300182580947876\n",
      "Iteration 6774 Loss: 1.620285987854004\n",
      "Iteration 6775 Loss: 1.2271308898925781\n",
      "Iteration 6776 Loss: 1.4996287822723389\n",
      "Iteration 6777 Loss: 1.1254242658615112\n",
      "Iteration 6778 Loss: 1.145946979522705\n",
      "Iteration 6779 Loss: 1.5306575298309326\n",
      "Iteration 6779 Loss: 1.4252912998199463\n",
      "Iteration 6780 Loss: 1.4327059984207153\n",
      "Iteration 6781 Loss: 1.7457869052886963\n",
      "Iteration 6782 Loss: 1.857896327972412\n",
      "Iteration 6783 Loss: 1.8515613079071045\n",
      "Iteration 6784 Loss: 1.1818865537643433\n",
      "Iteration 6785 Loss: 1.9521220922470093\n",
      "Iteration 6786 Loss: 1.0674092769622803\n",
      "Iteration 6787 Loss: 1.7396494150161743\n",
      "Iteration 6788 Loss: 1.5107632875442505\n",
      "Iteration 6789 Loss: 1.4355359077453613\n",
      "Iteration 6789 Loss: 1.5775316953659058\n",
      "Iteration 6790 Loss: 1.3320257663726807\n",
      "Iteration 6791 Loss: 1.1525602340698242\n",
      "Iteration 6792 Loss: 1.5910032987594604\n",
      "Iteration 6793 Loss: 0.9477486610412598\n",
      "Iteration 6794 Loss: 1.3448553085327148\n",
      "Iteration 6795 Loss: 1.3855273723602295\n",
      "Iteration 6796 Loss: 1.2353194952011108\n",
      "Iteration 6797 Loss: 1.151235818862915\n",
      "Iteration 6798 Loss: 1.3922289609909058\n",
      "Iteration 6799 Loss: 1.6826672554016113\n",
      "Iteration 6799 Loss: 1.3215172290802002\n",
      "Iteration 6800 Loss: 1.3053241968154907\n",
      "Iteration 6801 Loss: 1.0399630069732666\n",
      "Iteration 6802 Loss: 1.6230733394622803\n",
      "Iteration 6803 Loss: 1.4021087884902954\n",
      "Iteration 6804 Loss: 1.9032464027404785\n",
      "Iteration 6805 Loss: 0.9447757601737976\n",
      "Iteration 6806 Loss: 1.8364288806915283\n",
      "Iteration 6807 Loss: 1.0819506645202637\n",
      "Iteration 6808 Loss: 1.5558454990386963\n",
      "Iteration 6809 Loss: 1.6462408304214478\n",
      "Iteration 6809 Loss: 1.4338958263397217\n",
      "Iteration 6810 Loss: 1.4003925323486328\n",
      "Iteration 6811 Loss: 1.1071947813034058\n",
      "Iteration 6812 Loss: 1.5263867378234863\n",
      "Iteration 6813 Loss: 1.4664528369903564\n",
      "Iteration 6814 Loss: 1.5933494567871094\n",
      "Iteration 6815 Loss: 1.4599907398223877\n",
      "Iteration 6816 Loss: 1.3974723815917969\n",
      "Iteration 6817 Loss: 1.7465611696243286\n",
      "Iteration 6818 Loss: 1.2584607601165771\n",
      "Iteration 6819 Loss: 1.5554783344268799\n",
      "Iteration 6819 Loss: 1.451174020767212\n",
      "Iteration 6820 Loss: 1.5070154666900635\n",
      "Iteration 6821 Loss: 1.223374366760254\n",
      "Iteration 6822 Loss: 1.779829740524292\n",
      "Iteration 6823 Loss: 1.482078194618225\n",
      "Iteration 6824 Loss: 1.1774449348449707\n",
      "Iteration 6825 Loss: 1.5593879222869873\n",
      "Iteration 6826 Loss: 1.47758948802948\n",
      "Iteration 6827 Loss: 1.4584711790084839\n",
      "Iteration 6828 Loss: 1.4166020154953003\n",
      "Iteration 6829 Loss: 1.5126591920852661\n",
      "Iteration 6829 Loss: 1.4594453573226929\n",
      "Iteration 6830 Loss: 1.4910651445388794\n",
      "Iteration 6831 Loss: 1.0817557573318481\n",
      "Iteration 6832 Loss: 1.356755256652832\n",
      "Iteration 6833 Loss: 1.4604781866073608\n",
      "Iteration 6834 Loss: 1.8775566816329956\n",
      "Iteration 6835 Loss: 1.0034809112548828\n",
      "Iteration 6836 Loss: 1.4177037477493286\n",
      "Iteration 6837 Loss: 1.9206594228744507\n",
      "Iteration 6838 Loss: 1.1090703010559082\n",
      "Iteration 6839 Loss: 1.3287148475646973\n",
      "Iteration 6839 Loss: 1.404723882675171\n",
      "Iteration 6840 Loss: 1.8117685317993164\n",
      "Iteration 6841 Loss: 1.433366298675537\n",
      "Iteration 6842 Loss: 1.0116534233093262\n",
      "Iteration 6843 Loss: 1.1202592849731445\n",
      "Iteration 6844 Loss: 1.432093858718872\n",
      "Iteration 6845 Loss: 1.3713866472244263\n",
      "Iteration 6846 Loss: 1.3112678527832031\n",
      "Iteration 6847 Loss: 1.564673662185669\n",
      "Iteration 6848 Loss: 1.8034124374389648\n",
      "Iteration 6849 Loss: 0.7430143356323242\n",
      "Iteration 6849 Loss: 1.3602895736694336\n",
      "Iteration 6850 Loss: 1.3872590065002441\n",
      "Iteration 6851 Loss: 1.5013811588287354\n",
      "Iteration 6852 Loss: 1.4845216274261475\n",
      "Iteration 6853 Loss: 1.6581006050109863\n",
      "Iteration 6854 Loss: 1.7212876081466675\n",
      "Iteration 6855 Loss: 1.0847009420394897\n",
      "Iteration 6856 Loss: 1.3624948263168335\n",
      "Iteration 6857 Loss: 1.7513407468795776\n",
      "Iteration 6858 Loss: 1.0603740215301514\n",
      "Iteration 6859 Loss: 0.9896745085716248\n",
      "Iteration 6859 Loss: 1.4001133441925049\n",
      "Iteration 6860 Loss: 1.064673900604248\n",
      "Iteration 6861 Loss: 1.139972448348999\n",
      "Iteration 6862 Loss: 1.4761122465133667\n",
      "Iteration 6863 Loss: 1.8043837547302246\n",
      "Iteration 6864 Loss: 1.2923753261566162\n",
      "Iteration 6865 Loss: 1.4018956422805786\n",
      "Iteration 6866 Loss: 1.4171534776687622\n",
      "Iteration 6867 Loss: 1.2058045864105225\n",
      "Iteration 6868 Loss: 1.99629807472229\n",
      "Iteration 6869 Loss: 1.399027705192566\n",
      "Iteration 6869 Loss: 1.4197697639465332\n",
      "Iteration 6870 Loss: 1.4693748950958252\n",
      "Iteration 6871 Loss: 1.868051290512085\n",
      "Iteration 6872 Loss: 1.541379690170288\n",
      "Iteration 6873 Loss: 1.2421520948410034\n",
      "Iteration 6874 Loss: 0.9539200067520142\n",
      "Iteration 6875 Loss: 1.085384488105774\n",
      "Iteration 6876 Loss: 1.3406922817230225\n",
      "Iteration 6877 Loss: 1.6024664640426636\n",
      "Iteration 6878 Loss: 1.3503179550170898\n",
      "Iteration 6879 Loss: 1.850511908531189\n",
      "Iteration 6879 Loss: 1.4304252862930298\n",
      "Iteration 6880 Loss: 1.6153706312179565\n",
      "Iteration 6881 Loss: 1.3017232418060303\n",
      "Iteration 6882 Loss: 1.1380150318145752\n",
      "Iteration 6883 Loss: 1.4399040937423706\n",
      "Iteration 6884 Loss: 1.8659813404083252\n",
      "Iteration 6885 Loss: 1.3851971626281738\n",
      "Iteration 6886 Loss: 1.4625334739685059\n",
      "Iteration 6887 Loss: 1.4557232856750488\n",
      "Iteration 6888 Loss: 1.3227391242980957\n",
      "Iteration 6889 Loss: 1.3725581169128418\n",
      "Iteration 6889 Loss: 1.4359744787216187\n",
      "Iteration 6890 Loss: 1.4386993646621704\n",
      "Iteration 6891 Loss: 1.27333664894104\n",
      "Iteration 6892 Loss: 1.6561466455459595\n",
      "Iteration 6893 Loss: 1.4776235818862915\n",
      "Iteration 6894 Loss: 1.6623677015304565\n",
      "Iteration 6895 Loss: 1.8972145318984985\n",
      "Iteration 6896 Loss: 1.0183618068695068\n",
      "Iteration 6897 Loss: 1.2746204137802124\n",
      "Iteration 6898 Loss: 1.6277884244918823\n",
      "Iteration 6899 Loss: 1.337029218673706\n",
      "Iteration 6899 Loss: 1.4663188457489014\n",
      "Iteration 6900 Loss: 0.791746973991394\n",
      "Iteration 6901 Loss: 1.556857705116272\n",
      "Iteration 6902 Loss: 1.7004599571228027\n",
      "Iteration 6903 Loss: 2.2243101596832275\n",
      "Iteration 6904 Loss: 1.7100903987884521\n",
      "Iteration 6905 Loss: 1.3733422756195068\n",
      "Iteration 6906 Loss: 1.5219824314117432\n",
      "Iteration 6907 Loss: 1.2302169799804688\n",
      "Iteration 6908 Loss: 1.68485426902771\n",
      "Iteration 6909 Loss: 1.199588656425476\n",
      "Iteration 6909 Loss: 1.499345064163208\n",
      "Iteration 6910 Loss: 1.474770188331604\n",
      "Iteration 6911 Loss: 1.738409399986267\n",
      "Iteration 6912 Loss: 1.3190785646438599\n",
      "Iteration 6913 Loss: 1.6670222282409668\n",
      "Iteration 6914 Loss: 1.73264741897583\n",
      "Iteration 6915 Loss: 1.32588791847229\n",
      "Iteration 6916 Loss: 1.3744336366653442\n",
      "Iteration 6917 Loss: 1.4111254215240479\n",
      "Iteration 6918 Loss: 0.9393633008003235\n",
      "Iteration 6919 Loss: 0.9655768275260925\n",
      "Iteration 6919 Loss: 1.3948314189910889\n",
      "Iteration 6920 Loss: 1.3982654809951782\n",
      "Iteration 6921 Loss: 1.6988812685012817\n",
      "Iteration 6922 Loss: 1.5369033813476562\n",
      "Iteration 6923 Loss: 1.4447699785232544\n",
      "Iteration 6924 Loss: 1.1990944147109985\n",
      "Iteration 6925 Loss: 1.3039785623550415\n",
      "Iteration 6926 Loss: 1.539192795753479\n",
      "Iteration 6927 Loss: 0.8641448020935059\n",
      "Iteration 6928 Loss: 1.3496477603912354\n",
      "Iteration 6929 Loss: 1.3707396984100342\n",
      "Iteration 6929 Loss: 1.370561957359314\n",
      "Iteration 6930 Loss: 1.1255632638931274\n",
      "Iteration 6931 Loss: 1.5315848588943481\n",
      "Iteration 6932 Loss: 1.644636869430542\n",
      "Iteration 6933 Loss: 1.7295753955841064\n",
      "Iteration 6934 Loss: 1.4678475856781006\n",
      "Iteration 6935 Loss: 1.2420217990875244\n",
      "Iteration 6936 Loss: 1.8049510717391968\n",
      "Iteration 6937 Loss: 1.5865932703018188\n",
      "Iteration 6938 Loss: 1.1819589138031006\n",
      "Iteration 6939 Loss: 1.1659177541732788\n",
      "Iteration 6939 Loss: 1.4480650424957275\n",
      "Iteration 6940 Loss: 1.6975226402282715\n",
      "Iteration 6941 Loss: 1.6097453832626343\n",
      "Iteration 6942 Loss: 1.4785640239715576\n",
      "Iteration 6943 Loss: 1.3134411573410034\n",
      "Iteration 6944 Loss: 1.1859406232833862\n",
      "Iteration 6945 Loss: 1.1719956398010254\n",
      "Iteration 6946 Loss: 1.4850215911865234\n",
      "Iteration 6947 Loss: 2.0318868160247803\n",
      "Iteration 6948 Loss: 2.2071073055267334\n",
      "Iteration 6949 Loss: 1.58201265335083\n",
      "Iteration 6949 Loss: 1.5763238668441772\n",
      "Iteration 6950 Loss: 1.9862828254699707\n",
      "Iteration 6951 Loss: 1.1444694995880127\n",
      "Iteration 6952 Loss: 1.367962121963501\n",
      "Iteration 6953 Loss: 1.4346662759780884\n",
      "Iteration 6954 Loss: 2.122859239578247\n",
      "Iteration 6955 Loss: 1.8293640613555908\n",
      "Iteration 6956 Loss: 1.6571968793869019\n",
      "Iteration 6957 Loss: 1.1190125942230225\n",
      "Iteration 6958 Loss: 1.9698262214660645\n",
      "Iteration 6959 Loss: 1.2245421409606934\n",
      "Iteration 6959 Loss: 1.585618257522583\n",
      "Iteration 6960 Loss: 2.20737624168396\n",
      "Iteration 6961 Loss: 1.2948492765426636\n",
      "Iteration 6962 Loss: 1.7675659656524658\n",
      "Iteration 6963 Loss: 1.8462488651275635\n",
      "Iteration 6964 Loss: 1.4382649660110474\n",
      "Iteration 6965 Loss: 1.342359185218811\n",
      "Iteration 6966 Loss: 1.1735936403274536\n",
      "Iteration 6967 Loss: 0.9572150111198425\n",
      "Iteration 6968 Loss: 1.14662766456604\n",
      "Iteration 6969 Loss: 1.8145601749420166\n",
      "Iteration 6969 Loss: 1.498866081237793\n",
      "Iteration 6970 Loss: 1.4153120517730713\n",
      "Iteration 6971 Loss: 1.1121721267700195\n",
      "Iteration 6972 Loss: 1.1040983200073242\n",
      "Iteration 6973 Loss: 1.3155492544174194\n",
      "Iteration 6974 Loss: 1.4466230869293213\n",
      "Iteration 6975 Loss: 1.4589766263961792\n",
      "Iteration 6976 Loss: 1.953913927078247\n",
      "Iteration 6977 Loss: 1.2794382572174072\n",
      "Iteration 6978 Loss: 1.0933984518051147\n",
      "Iteration 6979 Loss: 1.6342780590057373\n",
      "Iteration 6979 Loss: 1.381376028060913\n",
      "Iteration 6980 Loss: 1.3268687725067139\n",
      "Iteration 6981 Loss: 1.7003135681152344\n",
      "Iteration 6982 Loss: 1.2251771688461304\n",
      "Iteration 6983 Loss: 1.4794236421585083\n",
      "Iteration 6984 Loss: 1.5867139101028442\n",
      "Iteration 6985 Loss: 1.6283345222473145\n",
      "Iteration 6986 Loss: 1.11826491355896\n",
      "Iteration 6987 Loss: 1.6347383260726929\n",
      "Iteration 6988 Loss: 1.4526057243347168\n",
      "Iteration 6989 Loss: 1.3563058376312256\n",
      "Iteration 6989 Loss: 1.4508745670318604\n",
      "Iteration 6990 Loss: 2.233015298843384\n",
      "Iteration 6991 Loss: 1.872263789176941\n",
      "Iteration 6992 Loss: 0.8956508040428162\n",
      "Iteration 6993 Loss: 1.6871581077575684\n",
      "Iteration 6994 Loss: 1.4917653799057007\n",
      "Iteration 6995 Loss: 1.290727138519287\n",
      "Iteration 6996 Loss: 1.2734489440917969\n",
      "Iteration 6997 Loss: 1.552626371383667\n",
      "Iteration 6998 Loss: 1.3669928312301636\n",
      "Iteration 6999 Loss: 1.5814250707626343\n",
      "Iteration 6999 Loss: 1.5245074033737183\n",
      "Iteration 7000 Loss: 1.761965274810791\n",
      "Iteration 7001 Loss: 1.2281582355499268\n",
      "Iteration 7002 Loss: 1.4822355508804321\n",
      "Iteration 7003 Loss: 1.4297447204589844\n",
      "Iteration 7004 Loss: 1.4388982057571411\n",
      "Iteration 7005 Loss: 1.4812418222427368\n",
      "Iteration 7006 Loss: 1.2813308238983154\n",
      "Iteration 7007 Loss: 1.199520230293274\n",
      "Iteration 7008 Loss: 1.711084008216858\n",
      "Iteration 7009 Loss: 1.3824046850204468\n",
      "Iteration 7009 Loss: 1.4396584033966064\n",
      "Iteration 7010 Loss: 1.285268783569336\n",
      "Iteration 7011 Loss: 1.2859230041503906\n",
      "Iteration 7012 Loss: 1.1936447620391846\n",
      "Iteration 7013 Loss: 1.2864408493041992\n",
      "Iteration 7014 Loss: 1.6846333742141724\n",
      "Iteration 7015 Loss: 1.4308292865753174\n",
      "Iteration 7016 Loss: 1.6853781938552856\n",
      "Iteration 7017 Loss: 1.2153840065002441\n",
      "Iteration 7018 Loss: 1.2559294700622559\n",
      "Iteration 7019 Loss: 1.503360390663147\n",
      "Iteration 7019 Loss: 1.3826792240142822\n",
      "Iteration 7020 Loss: 1.6857779026031494\n",
      "Iteration 7021 Loss: 2.014514923095703\n",
      "Iteration 7022 Loss: 1.3206883668899536\n",
      "Iteration 7023 Loss: 1.887642741203308\n",
      "Iteration 7024 Loss: 1.5779497623443604\n",
      "Iteration 7025 Loss: 1.3408859968185425\n",
      "Iteration 7026 Loss: 1.6408978700637817\n",
      "Iteration 7027 Loss: 1.318191409111023\n",
      "Iteration 7028 Loss: 1.6266340017318726\n",
      "Iteration 7029 Loss: 1.4393970966339111\n",
      "Iteration 7029 Loss: 1.5852580070495605\n",
      "Iteration 7030 Loss: 1.3309704065322876\n",
      "Iteration 7031 Loss: 1.867596983909607\n",
      "Iteration 7032 Loss: 1.7169383764266968\n",
      "Iteration 7033 Loss: 1.2551534175872803\n",
      "Iteration 7034 Loss: 0.9993047714233398\n",
      "Iteration 7035 Loss: 1.516520380973816\n",
      "Iteration 7036 Loss: 1.6657602787017822\n",
      "Iteration 7037 Loss: 1.2540372610092163\n",
      "Iteration 7038 Loss: 2.0656018257141113\n",
      "Iteration 7039 Loss: 1.5503604412078857\n",
      "Iteration 7039 Loss: 1.5222244262695312\n",
      "Iteration 7040 Loss: 1.1742357015609741\n",
      "Iteration 7041 Loss: 1.6683051586151123\n",
      "Iteration 7042 Loss: 1.7213647365570068\n",
      "Iteration 7043 Loss: 1.2978025674819946\n",
      "Iteration 7044 Loss: 0.9336857199668884\n",
      "Iteration 7045 Loss: 1.4124447107315063\n",
      "Iteration 7046 Loss: 1.7559945583343506\n",
      "Iteration 7047 Loss: 1.3131836652755737\n",
      "Iteration 7048 Loss: 1.3774105310440063\n",
      "Iteration 7049 Loss: 1.5503785610198975\n",
      "Iteration 7049 Loss: 1.420480728149414\n",
      "Iteration 7050 Loss: 1.595266342163086\n",
      "Iteration 7051 Loss: 1.8049798011779785\n",
      "Iteration 7052 Loss: 1.6523175239562988\n",
      "Iteration 7053 Loss: 1.5791113376617432\n",
      "Iteration 7054 Loss: 1.39375901222229\n",
      "Iteration 7055 Loss: 1.8836286067962646\n",
      "Iteration 7056 Loss: 1.5387955904006958\n",
      "Iteration 7057 Loss: 1.5489791631698608\n",
      "Iteration 7058 Loss: 1.6358016729354858\n",
      "Iteration 7059 Loss: 1.9618613719940186\n",
      "Iteration 7059 Loss: 1.6594499349594116\n",
      "Iteration 7060 Loss: 1.12433660030365\n",
      "Iteration 7061 Loss: 1.6667699813842773\n",
      "Iteration 7062 Loss: 1.4982634782791138\n",
      "Iteration 7063 Loss: 1.3860738277435303\n",
      "Iteration 7064 Loss: 1.4193286895751953\n",
      "Iteration 7065 Loss: 1.9954255819320679\n",
      "Iteration 7066 Loss: 1.2745598554611206\n",
      "Iteration 7067 Loss: 1.0413830280303955\n",
      "Iteration 7068 Loss: 1.7009344100952148\n",
      "Iteration 7069 Loss: 1.3467532396316528\n",
      "Iteration 7069 Loss: 1.445382833480835\n",
      "Iteration 7070 Loss: 1.4495339393615723\n",
      "Iteration 7071 Loss: 1.5356664657592773\n",
      "Iteration 7072 Loss: 1.1038202047348022\n",
      "Iteration 7073 Loss: 1.9270447492599487\n",
      "Iteration 7074 Loss: 1.5375713109970093\n",
      "Iteration 7075 Loss: 1.155856728553772\n",
      "Iteration 7076 Loss: 1.817757248878479\n",
      "Iteration 7077 Loss: 1.6460034847259521\n",
      "Iteration 7078 Loss: 1.3084126710891724\n",
      "Iteration 7079 Loss: 1.6877015829086304\n",
      "Iteration 7079 Loss: 1.5169368982315063\n",
      "Iteration 7080 Loss: 1.6021127700805664\n",
      "Iteration 7081 Loss: 1.4039078950881958\n",
      "Iteration 7082 Loss: 1.6361480951309204\n",
      "Iteration 7083 Loss: 1.2328671216964722\n",
      "Iteration 7084 Loss: 0.9391833543777466\n",
      "Iteration 7085 Loss: 0.8588321805000305\n",
      "Iteration 7086 Loss: 1.566339373588562\n",
      "Iteration 7087 Loss: 1.049919843673706\n",
      "Iteration 7088 Loss: 0.5923135876655579\n",
      "Iteration 7089 Loss: 1.411260724067688\n",
      "Iteration 7089 Loss: 1.2292885780334473\n",
      "Iteration 7090 Loss: 1.4601787328720093\n",
      "Iteration 7091 Loss: 1.1841232776641846\n",
      "Iteration 7092 Loss: 1.203197956085205\n",
      "Iteration 7093 Loss: 1.6397641897201538\n",
      "Iteration 7094 Loss: 1.2903196811676025\n",
      "Iteration 7095 Loss: 1.5260264873504639\n",
      "Iteration 7096 Loss: 1.4837541580200195\n",
      "Iteration 7097 Loss: 1.5525174140930176\n",
      "Iteration 7098 Loss: 1.084345817565918\n",
      "Iteration 7099 Loss: 1.8976291418075562\n",
      "Iteration 7099 Loss: 1.4321857690811157\n",
      "Iteration 7100 Loss: 1.1235707998275757\n",
      "Iteration 7101 Loss: 1.483139991760254\n",
      "Iteration 7102 Loss: 1.2841449975967407\n",
      "Iteration 7103 Loss: 0.9511544108390808\n",
      "Iteration 7104 Loss: 1.160067081451416\n",
      "Iteration 7105 Loss: 1.536738395690918\n",
      "Iteration 7106 Loss: 2.0823590755462646\n",
      "Iteration 7107 Loss: 0.8975640535354614\n",
      "Iteration 7108 Loss: 1.1252498626708984\n",
      "Iteration 7109 Loss: 1.185990810394287\n",
      "Iteration 7109 Loss: 1.282997965812683\n",
      "Iteration 7110 Loss: 1.2866593599319458\n",
      "Iteration 7111 Loss: 1.0415836572647095\n",
      "Iteration 7112 Loss: 1.6399739980697632\n",
      "Iteration 7113 Loss: 1.5817981958389282\n",
      "Iteration 7114 Loss: 1.0926871299743652\n",
      "Iteration 7115 Loss: 1.2403799295425415\n",
      "Iteration 7116 Loss: 1.3079278469085693\n",
      "Iteration 7117 Loss: 1.6811271905899048\n",
      "Iteration 7118 Loss: 1.4670312404632568\n",
      "Iteration 7119 Loss: 1.2654485702514648\n",
      "Iteration 7119 Loss: 1.360461950302124\n",
      "Iteration 7120 Loss: 1.3426426649093628\n",
      "Iteration 7121 Loss: 1.3601402044296265\n",
      "Iteration 7122 Loss: 1.293350338935852\n",
      "Iteration 7123 Loss: 1.187596082687378\n",
      "Iteration 7124 Loss: 1.4243587255477905\n",
      "Iteration 7125 Loss: 1.3185783624649048\n",
      "Iteration 7126 Loss: 1.5391314029693604\n",
      "Iteration 7127 Loss: 1.1795744895935059\n",
      "Iteration 7128 Loss: 1.6377424001693726\n",
      "Iteration 7129 Loss: 1.1866405010223389\n",
      "Iteration 7129 Loss: 1.346975564956665\n",
      "Iteration 7130 Loss: 1.79941725730896\n",
      "Iteration 7131 Loss: 1.6016602516174316\n",
      "Iteration 7132 Loss: 1.0852837562561035\n",
      "Iteration 7133 Loss: 1.5061739683151245\n",
      "Iteration 7134 Loss: 1.5899646282196045\n",
      "Iteration 7135 Loss: 1.5963627099990845\n",
      "Iteration 7136 Loss: 1.4319533109664917\n",
      "Iteration 7137 Loss: 1.3767704963684082\n",
      "Iteration 7138 Loss: 1.2362542152404785\n",
      "Iteration 7139 Loss: 1.9018586874008179\n",
      "Iteration 7139 Loss: 1.5125700235366821\n",
      "Iteration 7140 Loss: 1.4435391426086426\n",
      "Iteration 7141 Loss: 1.2651257514953613\n",
      "Iteration 7142 Loss: 1.7098909616470337\n",
      "Iteration 7143 Loss: 1.0435230731964111\n",
      "Iteration 7144 Loss: 1.9258626699447632\n",
      "Iteration 7145 Loss: 1.6153569221496582\n",
      "Iteration 7146 Loss: 1.0318716764450073\n",
      "Iteration 7147 Loss: 1.4061496257781982\n",
      "Iteration 7148 Loss: 1.4906858205795288\n",
      "Iteration 7149 Loss: 1.6723624467849731\n",
      "Iteration 7149 Loss: 1.4604367017745972\n",
      "Iteration 7150 Loss: 1.2775678634643555\n",
      "Iteration 7151 Loss: 1.4455746412277222\n",
      "Iteration 7152 Loss: 1.5563746690750122\n",
      "Iteration 7153 Loss: 1.8231945037841797\n",
      "Iteration 7154 Loss: 1.7243270874023438\n",
      "Iteration 7155 Loss: 1.4355610609054565\n",
      "Iteration 7156 Loss: 1.9812437295913696\n",
      "Iteration 7157 Loss: 1.2637470960617065\n",
      "Iteration 7158 Loss: 1.793066382408142\n",
      "Iteration 7159 Loss: 1.2055965662002563\n",
      "Iteration 7159 Loss: 1.5506254434585571\n",
      "Iteration 7160 Loss: 1.1330417394638062\n",
      "Iteration 7161 Loss: 1.4771740436553955\n",
      "Iteration 7162 Loss: 1.190137267112732\n",
      "Iteration 7163 Loss: 1.4181249141693115\n",
      "Iteration 7164 Loss: 1.3148449659347534\n",
      "Iteration 7165 Loss: 1.4601117372512817\n",
      "Iteration 7166 Loss: 1.8599858283996582\n",
      "Iteration 7167 Loss: 1.0981597900390625\n",
      "Iteration 7168 Loss: 1.5634721517562866\n",
      "Iteration 7169 Loss: 1.3438740968704224\n",
      "Iteration 7169 Loss: 1.385892629623413\n",
      "Iteration 7170 Loss: 1.5177083015441895\n",
      "Iteration 7171 Loss: 0.8817159533500671\n",
      "Iteration 7172 Loss: 1.4071063995361328\n",
      "Iteration 7173 Loss: 1.117003321647644\n",
      "Iteration 7174 Loss: 1.689853310585022\n",
      "Iteration 7175 Loss: 1.2635589838027954\n",
      "Iteration 7176 Loss: 2.065556287765503\n",
      "Iteration 7177 Loss: 1.9066675901412964\n",
      "Iteration 7178 Loss: 1.57395339012146\n",
      "Iteration 7179 Loss: 1.3815120458602905\n",
      "Iteration 7179 Loss: 1.4804636240005493\n",
      "Iteration 7180 Loss: 1.2612191438674927\n",
      "Iteration 7181 Loss: 1.6399625539779663\n",
      "Iteration 7182 Loss: 1.4778571128845215\n",
      "Iteration 7183 Loss: 1.0228753089904785\n",
      "Iteration 7184 Loss: 1.7196688652038574\n",
      "Iteration 7185 Loss: 1.2280470132827759\n",
      "Iteration 7186 Loss: 1.7847965955734253\n",
      "Iteration 7187 Loss: 1.2921937704086304\n",
      "Iteration 7188 Loss: 1.2346820831298828\n",
      "Iteration 7189 Loss: 1.2275710105895996\n",
      "Iteration 7189 Loss: 1.3888872861862183\n",
      "Iteration 7190 Loss: 0.8778373003005981\n",
      "Iteration 7191 Loss: 1.4806314706802368\n",
      "Iteration 7192 Loss: 2.0068047046661377\n",
      "Iteration 7193 Loss: 1.5253652334213257\n",
      "Iteration 7194 Loss: 1.5956836938858032\n",
      "Iteration 7195 Loss: 1.6159648895263672\n",
      "Iteration 7196 Loss: 1.6937851905822754\n",
      "Iteration 7197 Loss: 1.7518030405044556\n",
      "Iteration 7198 Loss: 1.5149182081222534\n",
      "Iteration 7199 Loss: 1.705165982246399\n",
      "Iteration 7199 Loss: 1.576796054840088\n",
      "Iteration 7200 Loss: 1.7439945936203003\n",
      "Iteration 7201 Loss: 1.5419456958770752\n",
      "Iteration 7202 Loss: 1.442740559577942\n",
      "Iteration 7203 Loss: 2.0959396362304688\n",
      "Iteration 7204 Loss: 1.1673812866210938\n",
      "Iteration 7205 Loss: 1.6776502132415771\n",
      "Iteration 7206 Loss: 1.4284601211547852\n",
      "Iteration 7207 Loss: 1.4099212884902954\n",
      "Iteration 7208 Loss: 1.7787268161773682\n",
      "Iteration 7209 Loss: 1.5126413106918335\n",
      "Iteration 7209 Loss: 1.5799401998519897\n",
      "Iteration 7210 Loss: 1.9833098649978638\n",
      "Iteration 7211 Loss: 1.813028335571289\n",
      "Iteration 7212 Loss: 1.8444658517837524\n",
      "Iteration 7213 Loss: 1.4861135482788086\n",
      "Iteration 7214 Loss: 1.2451088428497314\n",
      "Iteration 7215 Loss: 0.895138144493103\n",
      "Iteration 7216 Loss: 1.9805302619934082\n",
      "Iteration 7217 Loss: 1.4928345680236816\n",
      "Iteration 7218 Loss: 1.7414350509643555\n",
      "Iteration 7219 Loss: 1.0847042798995972\n",
      "Iteration 7219 Loss: 1.5566667318344116\n",
      "Iteration 7220 Loss: 1.144799828529358\n",
      "Iteration 7221 Loss: 1.2405582666397095\n",
      "Iteration 7222 Loss: 1.8050148487091064\n",
      "Iteration 7223 Loss: 1.744009256362915\n",
      "Iteration 7224 Loss: 1.4000967741012573\n",
      "Iteration 7225 Loss: 1.4646601676940918\n",
      "Iteration 7226 Loss: 1.795438528060913\n",
      "Iteration 7227 Loss: 1.2786468267440796\n",
      "Iteration 7228 Loss: 1.8752460479736328\n",
      "Iteration 7229 Loss: 1.5249085426330566\n",
      "Iteration 7229 Loss: 1.5273377895355225\n",
      "Iteration 7230 Loss: 1.0515412092208862\n",
      "Iteration 7231 Loss: 1.3926047086715698\n",
      "Iteration 7232 Loss: 1.5263913869857788\n",
      "Iteration 7233 Loss: 2.0049262046813965\n",
      "Iteration 7234 Loss: 1.5362145900726318\n",
      "Iteration 7235 Loss: 1.7634646892547607\n",
      "Iteration 7236 Loss: 1.8266282081604004\n",
      "Iteration 7237 Loss: 1.3520872592926025\n",
      "Iteration 7238 Loss: 1.3934308290481567\n",
      "Iteration 7239 Loss: 1.1044100522994995\n",
      "Iteration 7239 Loss: 1.495169997215271\n",
      "Iteration 7240 Loss: 1.3065437078475952\n",
      "Iteration 7241 Loss: 1.587778091430664\n",
      "Iteration 7242 Loss: 1.2429280281066895\n",
      "Iteration 7243 Loss: 1.436397910118103\n",
      "Iteration 7244 Loss: 1.2416921854019165\n",
      "Iteration 7245 Loss: 1.7246229648590088\n",
      "Iteration 7246 Loss: 1.5052311420440674\n",
      "Iteration 7247 Loss: 1.8902335166931152\n",
      "Iteration 7248 Loss: 1.4883843660354614\n",
      "Iteration 7249 Loss: 1.7178112268447876\n",
      "Iteration 7249 Loss: 1.514162302017212\n",
      "Iteration 7250 Loss: 1.9739346504211426\n",
      "Iteration 7251 Loss: 1.5236995220184326\n",
      "Iteration 7252 Loss: 1.2465488910675049\n",
      "Iteration 7253 Loss: 1.0433931350708008\n",
      "Iteration 7254 Loss: 1.4760069847106934\n",
      "Iteration 7255 Loss: 1.6043944358825684\n",
      "Iteration 7256 Loss: 1.0678343772888184\n",
      "Iteration 7257 Loss: 1.8665159940719604\n",
      "Iteration 7258 Loss: 0.9858770370483398\n",
      "Iteration 7259 Loss: 1.0066001415252686\n",
      "Iteration 7259 Loss: 1.3794806003570557\n",
      "Iteration 7260 Loss: 1.3549367189407349\n",
      "Iteration 7261 Loss: 1.4710222482681274\n",
      "Iteration 7262 Loss: 1.3871238231658936\n",
      "Iteration 7263 Loss: 1.3988157510757446\n",
      "Iteration 7264 Loss: 1.0197303295135498\n",
      "Iteration 7265 Loss: 1.8125544786453247\n",
      "Iteration 7266 Loss: 1.409557580947876\n",
      "Iteration 7267 Loss: 1.3387057781219482\n",
      "Iteration 7268 Loss: 1.788270354270935\n",
      "Iteration 7269 Loss: 1.703250765800476\n",
      "Iteration 7269 Loss: 1.4683969020843506\n",
      "Iteration 7270 Loss: 1.8322890996932983\n",
      "Iteration 7271 Loss: 2.109327554702759\n",
      "Iteration 7272 Loss: 1.6395522356033325\n",
      "Iteration 7273 Loss: 1.2753753662109375\n",
      "Iteration 7274 Loss: 1.8543047904968262\n",
      "Iteration 7275 Loss: 1.0262126922607422\n",
      "Iteration 7276 Loss: 1.4662386178970337\n",
      "Iteration 7277 Loss: 1.7985432147979736\n",
      "Iteration 7278 Loss: 1.7948806285858154\n",
      "Iteration 7279 Loss: 1.273069143295288\n",
      "Iteration 7279 Loss: 1.6069793701171875\n",
      "Iteration 7280 Loss: 1.479817271232605\n",
      "Iteration 7281 Loss: 1.2451200485229492\n",
      "Iteration 7282 Loss: 1.368872046470642\n",
      "Iteration 7283 Loss: 1.4860153198242188\n",
      "Iteration 7284 Loss: 1.9015953540802002\n",
      "Iteration 7285 Loss: 1.3722212314605713\n",
      "Iteration 7286 Loss: 1.5100129842758179\n",
      "Iteration 7287 Loss: 1.5056002140045166\n",
      "Iteration 7288 Loss: 1.2166657447814941\n",
      "Iteration 7289 Loss: 1.3791307210922241\n",
      "Iteration 7289 Loss: 1.4465049505233765\n",
      "Iteration 7290 Loss: 1.4023739099502563\n",
      "Iteration 7291 Loss: 1.852538824081421\n",
      "Iteration 7292 Loss: 1.6308101415634155\n",
      "Iteration 7293 Loss: 1.1039974689483643\n",
      "Iteration 7294 Loss: 1.687150001525879\n",
      "Iteration 7295 Loss: 1.3966175317764282\n",
      "Iteration 7296 Loss: 1.0688960552215576\n",
      "Iteration 7297 Loss: 0.7367013096809387\n",
      "Iteration 7298 Loss: 1.422117829322815\n",
      "Iteration 7299 Loss: 1.542295217514038\n",
      "Iteration 7299 Loss: 1.3843499422073364\n",
      "Iteration 7300 Loss: 1.8932031393051147\n",
      "Iteration 7301 Loss: 1.5976190567016602\n",
      "Iteration 7302 Loss: 1.335047960281372\n",
      "Iteration 7303 Loss: 1.7961571216583252\n",
      "Iteration 7304 Loss: 1.6245287656784058\n",
      "Iteration 7305 Loss: 1.2728592157363892\n",
      "Iteration 7306 Loss: 1.249815821647644\n",
      "Iteration 7307 Loss: 1.3050405979156494\n",
      "Iteration 7308 Loss: 1.1631485223770142\n",
      "Iteration 7309 Loss: 0.896811306476593\n",
      "Iteration 7309 Loss: 1.4134231805801392\n",
      "Iteration 7310 Loss: 1.8369929790496826\n",
      "Iteration 7311 Loss: 2.1318485736846924\n",
      "Iteration 7312 Loss: 1.3629688024520874\n",
      "Iteration 7313 Loss: 1.2163349390029907\n",
      "Iteration 7314 Loss: 1.3103256225585938\n",
      "Iteration 7315 Loss: 1.1954569816589355\n",
      "Iteration 7316 Loss: 1.3428696393966675\n",
      "Iteration 7317 Loss: 1.7792001962661743\n",
      "Iteration 7318 Loss: 1.3053401708602905\n",
      "Iteration 7319 Loss: 1.5870964527130127\n",
      "Iteration 7319 Loss: 1.5068433284759521\n",
      "Iteration 7320 Loss: 1.3518259525299072\n",
      "Iteration 7321 Loss: 1.4161027669906616\n",
      "Iteration 7322 Loss: 1.3257553577423096\n",
      "Iteration 7323 Loss: 1.7768691778182983\n",
      "Iteration 7324 Loss: 1.4619877338409424\n",
      "Iteration 7325 Loss: 1.5170884132385254\n",
      "Iteration 7326 Loss: 1.1190872192382812\n",
      "Iteration 7327 Loss: 1.1434900760650635\n",
      "Iteration 7328 Loss: 1.6137381792068481\n",
      "Iteration 7329 Loss: 1.2133033275604248\n",
      "Iteration 7329 Loss: 1.3939247131347656\n",
      "Iteration 7330 Loss: 1.5661580562591553\n",
      "Iteration 7331 Loss: 0.8924159407615662\n",
      "Iteration 7332 Loss: 1.608916997909546\n",
      "Iteration 7333 Loss: 1.6082251071929932\n",
      "Iteration 7334 Loss: 1.705125093460083\n",
      "Iteration 7335 Loss: 1.3642520904541016\n",
      "Iteration 7336 Loss: 1.3183140754699707\n",
      "Iteration 7337 Loss: 1.6213723421096802\n",
      "Iteration 7338 Loss: 1.7170697450637817\n",
      "Iteration 7339 Loss: 1.2899858951568604\n",
      "Iteration 7339 Loss: 1.4691835641860962\n",
      "Iteration 7340 Loss: 1.0434439182281494\n",
      "Iteration 7341 Loss: 1.656421422958374\n",
      "Iteration 7342 Loss: 1.0940773487091064\n",
      "Iteration 7343 Loss: 1.8876711130142212\n",
      "Iteration 7344 Loss: 1.5621248483657837\n",
      "Iteration 7345 Loss: 1.3018420934677124\n",
      "Iteration 7346 Loss: 0.9420366883277893\n",
      "Iteration 7347 Loss: 1.3435087203979492\n",
      "Iteration 7348 Loss: 1.1701323986053467\n",
      "Iteration 7349 Loss: 1.4949218034744263\n",
      "Iteration 7349 Loss: 1.3496180772781372\n",
      "Iteration 7350 Loss: 1.3497506380081177\n",
      "Iteration 7351 Loss: 1.255637764930725\n",
      "Iteration 7352 Loss: 1.7140144109725952\n",
      "Iteration 7353 Loss: 1.5206143856048584\n",
      "Iteration 7354 Loss: 1.2966405153274536\n",
      "Iteration 7355 Loss: 1.8139588832855225\n",
      "Iteration 7356 Loss: 1.5305179357528687\n",
      "Iteration 7357 Loss: 1.6554685831069946\n",
      "Iteration 7358 Loss: 1.587709903717041\n",
      "Iteration 7359 Loss: 1.3701910972595215\n",
      "Iteration 7359 Loss: 1.5094504356384277\n",
      "Iteration 7360 Loss: 1.3696062564849854\n",
      "Iteration 7361 Loss: 0.9761502742767334\n",
      "Iteration 7362 Loss: 1.0968761444091797\n",
      "Iteration 7363 Loss: 1.5334970951080322\n",
      "Iteration 7364 Loss: 1.2279653549194336\n",
      "Iteration 7365 Loss: 1.7328941822052002\n",
      "Iteration 7366 Loss: 1.0641045570373535\n",
      "Iteration 7367 Loss: 1.5340867042541504\n",
      "Iteration 7368 Loss: 1.2741713523864746\n",
      "Iteration 7369 Loss: 1.6472502946853638\n",
      "Iteration 7369 Loss: 1.3456602096557617\n",
      "Iteration 7370 Loss: 1.9008630514144897\n",
      "Iteration 7371 Loss: 1.661097526550293\n",
      "Iteration 7372 Loss: 1.5893170833587646\n",
      "Iteration 7373 Loss: 1.2498242855072021\n",
      "Iteration 7374 Loss: 1.725901484489441\n",
      "Iteration 7375 Loss: 1.394524097442627\n",
      "Iteration 7376 Loss: 1.8011035919189453\n",
      "Iteration 7377 Loss: 1.7574522495269775\n",
      "Iteration 7378 Loss: 1.3435983657836914\n",
      "Iteration 7379 Loss: 1.3180274963378906\n",
      "Iteration 7379 Loss: 1.5741708278656006\n",
      "Iteration 7380 Loss: 1.3327693939208984\n",
      "Iteration 7381 Loss: 1.2630491256713867\n",
      "Iteration 7382 Loss: 1.8712364435195923\n",
      "Iteration 7383 Loss: 1.0436946153640747\n",
      "Iteration 7384 Loss: 1.1909968852996826\n",
      "Iteration 7385 Loss: 1.321389079093933\n",
      "Iteration 7386 Loss: 1.62753427028656\n",
      "Iteration 7387 Loss: 1.513198971748352\n",
      "Iteration 7388 Loss: 1.363767385482788\n",
      "Iteration 7389 Loss: 1.5473605394363403\n",
      "Iteration 7389 Loss: 1.4074996709823608\n",
      "Iteration 7390 Loss: 1.4588561058044434\n",
      "Iteration 7391 Loss: 1.8558094501495361\n",
      "Iteration 7392 Loss: 0.9517284631729126\n",
      "Iteration 7393 Loss: 2.123098611831665\n",
      "Iteration 7394 Loss: 1.2828983068466187\n",
      "Iteration 7395 Loss: 1.0830786228179932\n",
      "Iteration 7396 Loss: 1.9699010848999023\n",
      "Iteration 7397 Loss: 1.2859629392623901\n",
      "Iteration 7398 Loss: 1.1428271532058716\n",
      "Iteration 7399 Loss: 0.838192343711853\n",
      "Iteration 7399 Loss: 1.3992352485656738\n",
      "Iteration 7400 Loss: 0.8758162260055542\n",
      "Iteration 7401 Loss: 1.2429178953170776\n",
      "Iteration 7402 Loss: 1.0770487785339355\n",
      "Iteration 7403 Loss: 1.7458328008651733\n",
      "Iteration 7404 Loss: 1.1942840814590454\n",
      "Iteration 7405 Loss: 1.3765439987182617\n",
      "Iteration 7406 Loss: 1.5231752395629883\n",
      "Iteration 7407 Loss: 1.5369807481765747\n",
      "Iteration 7408 Loss: 1.8624966144561768\n",
      "Iteration 7409 Loss: 1.2399625778198242\n",
      "Iteration 7409 Loss: 1.3675059080123901\n",
      "Iteration 7410 Loss: 1.7201006412506104\n",
      "Iteration 7411 Loss: 1.5828496217727661\n",
      "Iteration 7412 Loss: 1.701826810836792\n",
      "Iteration 7413 Loss: 1.552811622619629\n",
      "Iteration 7414 Loss: 1.1658967733383179\n",
      "Iteration 7415 Loss: 1.618836522102356\n",
      "Iteration 7416 Loss: 1.7045763731002808\n",
      "Iteration 7417 Loss: 1.0383498668670654\n",
      "Iteration 7418 Loss: 1.0530726909637451\n",
      "Iteration 7419 Loss: 1.6829904317855835\n",
      "Iteration 7419 Loss: 1.4821311235427856\n",
      "Iteration 7420 Loss: 1.3484891653060913\n",
      "Iteration 7421 Loss: 1.1855114698410034\n",
      "Iteration 7422 Loss: 1.739303708076477\n",
      "Iteration 7423 Loss: 1.1067836284637451\n",
      "Iteration 7424 Loss: 1.8414580821990967\n",
      "Iteration 7425 Loss: 1.3535513877868652\n",
      "Iteration 7426 Loss: 1.0323542356491089\n",
      "Iteration 7427 Loss: 1.6378363370895386\n",
      "Iteration 7428 Loss: 1.941401720046997\n",
      "Iteration 7429 Loss: 1.3959711790084839\n",
      "Iteration 7429 Loss: 1.458266019821167\n",
      "Iteration 7430 Loss: 1.277359962463379\n",
      "Iteration 7431 Loss: 1.6075836420059204\n",
      "Iteration 7432 Loss: 1.3614929914474487\n",
      "Iteration 7433 Loss: 1.5231993198394775\n",
      "Iteration 7434 Loss: 1.2768497467041016\n",
      "Iteration 7435 Loss: 1.2517180442810059\n",
      "Iteration 7436 Loss: 2.0248889923095703\n",
      "Iteration 7437 Loss: 1.7350082397460938\n",
      "Iteration 7438 Loss: 1.722591519355774\n",
      "Iteration 7439 Loss: 1.7969131469726562\n",
      "Iteration 7439 Loss: 1.5577605962753296\n",
      "Iteration 7440 Loss: 1.182839274406433\n",
      "Iteration 7441 Loss: 1.0399059057235718\n",
      "Iteration 7442 Loss: 1.6198723316192627\n",
      "Iteration 7443 Loss: 1.0587623119354248\n",
      "Iteration 7444 Loss: 1.3731603622436523\n",
      "Iteration 7445 Loss: 1.4663681983947754\n",
      "Iteration 7446 Loss: 1.3706252574920654\n",
      "Iteration 7447 Loss: 1.739875078201294\n",
      "Iteration 7448 Loss: 1.5581916570663452\n",
      "Iteration 7449 Loss: 1.5806143283843994\n",
      "Iteration 7449 Loss: 1.3990215063095093\n",
      "Iteration 7450 Loss: 1.3577826023101807\n",
      "Iteration 7451 Loss: 1.1629263162612915\n",
      "Iteration 7452 Loss: 1.5535527467727661\n",
      "Iteration 7453 Loss: 1.6253734827041626\n",
      "Iteration 7454 Loss: 1.5574971437454224\n",
      "Iteration 7455 Loss: 1.5527987480163574\n",
      "Iteration 7456 Loss: 1.0297746658325195\n",
      "Iteration 7457 Loss: 1.5855165719985962\n",
      "Iteration 7458 Loss: 1.478568196296692\n",
      "Iteration 7459 Loss: 1.2773590087890625\n",
      "Iteration 7459 Loss: 1.4181149005889893\n",
      "Iteration 7460 Loss: 1.7659175395965576\n",
      "Iteration 7461 Loss: 1.7121809720993042\n",
      "Iteration 7462 Loss: 1.380823016166687\n",
      "Iteration 7463 Loss: 1.2256118059158325\n",
      "Iteration 7464 Loss: 1.323989987373352\n",
      "Iteration 7465 Loss: 1.7187269926071167\n",
      "Iteration 7466 Loss: 1.3138182163238525\n",
      "Iteration 7467 Loss: 1.531951665878296\n",
      "Iteration 7468 Loss: 1.7029898166656494\n",
      "Iteration 7469 Loss: 1.6212078332901\n",
      "Iteration 7469 Loss: 1.5297218561172485\n",
      "Iteration 7470 Loss: 1.5544371604919434\n",
      "Iteration 7471 Loss: 1.3110111951828003\n",
      "Iteration 7472 Loss: 1.5673974752426147\n",
      "Iteration 7473 Loss: 1.217310905456543\n",
      "Iteration 7474 Loss: 1.129494071006775\n",
      "Iteration 7475 Loss: 1.0472294092178345\n",
      "Iteration 7476 Loss: 1.2714935541152954\n",
      "Iteration 7477 Loss: 1.759285807609558\n",
      "Iteration 7478 Loss: 1.484761357307434\n",
      "Iteration 7479 Loss: 1.4078913927078247\n",
      "Iteration 7479 Loss: 1.3750313520431519\n",
      "Iteration 7480 Loss: 1.2890249490737915\n",
      "Iteration 7481 Loss: 1.5055372714996338\n",
      "Iteration 7482 Loss: 1.4454004764556885\n",
      "Iteration 7483 Loss: 1.2003573179244995\n",
      "Iteration 7484 Loss: 1.6934341192245483\n",
      "Iteration 7485 Loss: 1.6725536584854126\n",
      "Iteration 7486 Loss: 1.6772693395614624\n",
      "Iteration 7487 Loss: 1.5201352834701538\n",
      "Iteration 7488 Loss: 1.3184523582458496\n",
      "Iteration 7489 Loss: 1.0734953880310059\n",
      "Iteration 7489 Loss: 1.439565896987915\n",
      "Iteration 7490 Loss: 1.1510728597640991\n",
      "Iteration 7491 Loss: 1.7054994106292725\n",
      "Iteration 7492 Loss: 1.5555367469787598\n",
      "Iteration 7493 Loss: 1.5155185461044312\n",
      "Iteration 7494 Loss: 1.3977550268173218\n",
      "Iteration 7495 Loss: 1.6420278549194336\n",
      "Iteration 7496 Loss: 1.1251107454299927\n",
      "Iteration 7497 Loss: 1.357820987701416\n",
      "Iteration 7498 Loss: 1.1106454133987427\n",
      "Iteration 7499 Loss: 1.8253828287124634\n",
      "Iteration 7499 Loss: 1.4386370182037354\n",
      "Iteration 7500 Loss: 1.7761625051498413\n",
      "Iteration 7501 Loss: 1.4367690086364746\n",
      "Iteration 7502 Loss: 1.4090111255645752\n",
      "Iteration 7503 Loss: 1.3983256816864014\n",
      "Iteration 7504 Loss: 1.6219418048858643\n",
      "Iteration 7505 Loss: 1.2541773319244385\n",
      "Iteration 7506 Loss: 1.4833672046661377\n",
      "Iteration 7507 Loss: 1.4860762357711792\n",
      "Iteration 7508 Loss: 1.198287010192871\n",
      "Iteration 7509 Loss: 1.526310682296753\n",
      "Iteration 7509 Loss: 1.4590427875518799\n",
      "Iteration 7510 Loss: 1.2627477645874023\n",
      "Iteration 7511 Loss: 1.0021892786026\n",
      "Iteration 7512 Loss: 1.8068993091583252\n",
      "Iteration 7513 Loss: 1.486236572265625\n",
      "Iteration 7514 Loss: 1.6254842281341553\n",
      "Iteration 7515 Loss: 1.09896719455719\n",
      "Iteration 7516 Loss: 1.4380488395690918\n",
      "Iteration 7517 Loss: 1.5839637517929077\n",
      "Iteration 7518 Loss: 1.516176462173462\n",
      "Iteration 7519 Loss: 1.6088188886642456\n",
      "Iteration 7519 Loss: 1.44295334815979\n",
      "Iteration 7520 Loss: 1.5594537258148193\n",
      "Iteration 7521 Loss: 1.2710708379745483\n",
      "Iteration 7522 Loss: 1.7976264953613281\n",
      "Iteration 7523 Loss: 1.7543030977249146\n",
      "Iteration 7524 Loss: 1.79043447971344\n",
      "Iteration 7525 Loss: 1.3862804174423218\n",
      "Iteration 7526 Loss: 1.5199878215789795\n",
      "Iteration 7527 Loss: 1.335568904876709\n",
      "Iteration 7528 Loss: 1.40364408493042\n",
      "Iteration 7529 Loss: 2.0363452434539795\n",
      "Iteration 7529 Loss: 1.585471510887146\n",
      "Iteration 7530 Loss: 1.2358379364013672\n",
      "Iteration 7531 Loss: 1.3545942306518555\n",
      "Iteration 7532 Loss: 1.52531898021698\n",
      "Iteration 7533 Loss: 1.3236044645309448\n",
      "Iteration 7534 Loss: 1.4025143384933472\n",
      "Iteration 7535 Loss: 1.3688721656799316\n",
      "Iteration 7536 Loss: 1.4188787937164307\n",
      "Iteration 7537 Loss: 1.433607578277588\n",
      "Iteration 7538 Loss: 1.2385551929473877\n",
      "Iteration 7539 Loss: 1.1702899932861328\n",
      "Iteration 7539 Loss: 1.3472073078155518\n",
      "Iteration 7540 Loss: 1.7491554021835327\n",
      "Iteration 7541 Loss: 1.2313165664672852\n",
      "Iteration 7542 Loss: 1.7476030588150024\n",
      "Iteration 7543 Loss: 0.8554303646087646\n",
      "Iteration 7544 Loss: 1.6346746683120728\n",
      "Iteration 7545 Loss: 1.5166077613830566\n",
      "Iteration 7546 Loss: 1.8080403804779053\n",
      "Iteration 7547 Loss: 1.2678773403167725\n",
      "Iteration 7548 Loss: 1.0087658166885376\n",
      "Iteration 7549 Loss: 1.575014591217041\n",
      "Iteration 7549 Loss: 1.4394487142562866\n",
      "Iteration 7550 Loss: 1.5230191946029663\n",
      "Iteration 7551 Loss: 1.4737049341201782\n",
      "Iteration 7552 Loss: 1.3516602516174316\n",
      "Iteration 7553 Loss: 1.5572068691253662\n",
      "Iteration 7554 Loss: 1.6092127561569214\n",
      "Iteration 7555 Loss: 1.555645227432251\n",
      "Iteration 7556 Loss: 1.4160284996032715\n",
      "Iteration 7557 Loss: 1.4376075267791748\n",
      "Iteration 7558 Loss: 1.5328192710876465\n",
      "Iteration 7559 Loss: 1.3173848390579224\n",
      "Iteration 7559 Loss: 1.477428913116455\n",
      "Iteration 7560 Loss: 1.4165599346160889\n",
      "Iteration 7561 Loss: 1.6707490682601929\n",
      "Iteration 7562 Loss: 0.947446346282959\n",
      "Iteration 7563 Loss: 1.4292426109313965\n",
      "Iteration 7564 Loss: 1.8273322582244873\n",
      "Iteration 7565 Loss: 1.1262812614440918\n",
      "Iteration 7566 Loss: 1.4594627618789673\n",
      "Iteration 7567 Loss: 1.4103574752807617\n",
      "Iteration 7568 Loss: 1.2789582014083862\n",
      "Iteration 7569 Loss: 1.300981879234314\n",
      "Iteration 7569 Loss: 1.3867371082305908\n",
      "Iteration 7570 Loss: 1.8899255990982056\n",
      "Iteration 7571 Loss: 1.720603585243225\n",
      "Iteration 7572 Loss: 1.4612586498260498\n",
      "Iteration 7573 Loss: 1.8138052225112915\n",
      "Iteration 7574 Loss: 1.210746169090271\n",
      "Iteration 7575 Loss: 1.4435884952545166\n",
      "Iteration 7576 Loss: 2.1053359508514404\n",
      "Iteration 7577 Loss: 1.5530096292495728\n",
      "Iteration 7578 Loss: 1.0433908700942993\n",
      "Iteration 7579 Loss: 1.6606974601745605\n",
      "Iteration 7579 Loss: 1.5902361869812012\n",
      "Iteration 7580 Loss: 0.9112312197685242\n",
      "Iteration 7581 Loss: 1.8337692022323608\n",
      "Iteration 7582 Loss: 1.5274332761764526\n",
      "Iteration 7583 Loss: 1.3510242700576782\n",
      "Iteration 7584 Loss: 1.2526819705963135\n",
      "Iteration 7585 Loss: 1.2542489767074585\n",
      "Iteration 7586 Loss: 1.2297828197479248\n",
      "Iteration 7587 Loss: 1.6836833953857422\n",
      "Iteration 7588 Loss: 1.5714932680130005\n",
      "Iteration 7589 Loss: 0.9839816689491272\n",
      "Iteration 7589 Loss: 1.3599330186843872\n",
      "Iteration 7590 Loss: 1.346804141998291\n",
      "Iteration 7591 Loss: 1.181056022644043\n",
      "Iteration 7592 Loss: 1.1029279232025146\n",
      "Iteration 7593 Loss: 1.032910943031311\n",
      "Iteration 7594 Loss: 1.3970195055007935\n",
      "Iteration 7595 Loss: 1.070611596107483\n",
      "Iteration 7596 Loss: 1.6739274263381958\n",
      "Iteration 7597 Loss: 1.6947553157806396\n",
      "Iteration 7598 Loss: 1.315685749053955\n",
      "Iteration 7599 Loss: 1.7620515823364258\n",
      "Iteration 7599 Loss: 1.357775092124939\n",
      "Iteration 7600 Loss: 1.5111030340194702\n",
      "Iteration 7601 Loss: 0.9989101886749268\n",
      "Iteration 7602 Loss: 2.147857427597046\n",
      "Iteration 7603 Loss: 1.2904105186462402\n",
      "Iteration 7604 Loss: 1.34396493434906\n",
      "Iteration 7605 Loss: 1.855320930480957\n",
      "Iteration 7606 Loss: 1.6880378723144531\n",
      "Iteration 7607 Loss: 1.2862467765808105\n",
      "Iteration 7608 Loss: 1.3916165828704834\n",
      "Iteration 7609 Loss: 0.8243085741996765\n",
      "Iteration 7609 Loss: 1.4337778091430664\n",
      "Iteration 7610 Loss: 1.3601216077804565\n",
      "Iteration 7611 Loss: 1.1252670288085938\n",
      "Iteration 7612 Loss: 1.2211692333221436\n",
      "Iteration 7613 Loss: 1.4814163446426392\n",
      "Iteration 7614 Loss: 0.9618763327598572\n",
      "Iteration 7615 Loss: 1.4882608652114868\n",
      "Iteration 7616 Loss: 1.2243845462799072\n",
      "Iteration 7617 Loss: 1.6374233961105347\n",
      "Iteration 7618 Loss: 1.324156641960144\n",
      "Iteration 7619 Loss: 1.5302435159683228\n",
      "Iteration 7619 Loss: 1.3354319334030151\n",
      "Iteration 7620 Loss: 1.9497368335723877\n",
      "Iteration 7621 Loss: 1.3420323133468628\n",
      "Iteration 7622 Loss: 1.3374632596969604\n",
      "Iteration 7623 Loss: 1.3145818710327148\n",
      "Iteration 7624 Loss: 1.4268670082092285\n",
      "Iteration 7625 Loss: 1.3595616817474365\n",
      "Iteration 7626 Loss: 1.3031140565872192\n",
      "Iteration 7627 Loss: 1.7310916185379028\n",
      "Iteration 7628 Loss: 1.429111123085022\n",
      "Iteration 7629 Loss: 0.7380326986312866\n",
      "Iteration 7629 Loss: 1.3931591510772705\n",
      "Iteration 7630 Loss: 2.246584892272949\n",
      "Iteration 7631 Loss: 1.1876935958862305\n",
      "Iteration 7632 Loss: 1.1273506879806519\n",
      "Iteration 7633 Loss: 1.8097484111785889\n",
      "Iteration 7634 Loss: 1.4114696979522705\n",
      "Iteration 7635 Loss: 1.1774883270263672\n",
      "Iteration 7636 Loss: 0.9579724073410034\n",
      "Iteration 7637 Loss: 1.1413683891296387\n",
      "Iteration 7638 Loss: 1.7329397201538086\n",
      "Iteration 7639 Loss: 1.3412529230117798\n",
      "Iteration 7639 Loss: 1.4133869409561157\n",
      "Iteration 7640 Loss: 1.2669143676757812\n",
      "Iteration 7641 Loss: 1.5773752927780151\n",
      "Iteration 7642 Loss: 1.4552180767059326\n",
      "Iteration 7643 Loss: 1.4837067127227783\n",
      "Iteration 7644 Loss: 2.1780760288238525\n",
      "Iteration 7645 Loss: 1.397147297859192\n",
      "Iteration 7646 Loss: 1.57984459400177\n",
      "Iteration 7647 Loss: 1.6223688125610352\n",
      "Iteration 7648 Loss: 1.7577252388000488\n",
      "Iteration 7649 Loss: 1.3588175773620605\n",
      "Iteration 7649 Loss: 1.5677193403244019\n",
      "Iteration 7650 Loss: 1.67923903465271\n",
      "Iteration 7651 Loss: 2.112499713897705\n",
      "Iteration 7652 Loss: 1.666939377784729\n",
      "Iteration 7653 Loss: 1.3854185342788696\n",
      "Iteration 7654 Loss: 2.1504569053649902\n",
      "Iteration 7655 Loss: 0.9537242650985718\n",
      "Iteration 7656 Loss: 1.2919447422027588\n",
      "Iteration 7657 Loss: 1.7004127502441406\n",
      "Iteration 7658 Loss: 1.0898523330688477\n",
      "Iteration 7659 Loss: 1.9506999254226685\n",
      "Iteration 7659 Loss: 1.598118782043457\n",
      "Iteration 7660 Loss: 1.686544418334961\n",
      "Iteration 7661 Loss: 1.5103740692138672\n",
      "Iteration 7662 Loss: 0.951879620552063\n",
      "Iteration 7663 Loss: 0.9874756932258606\n",
      "Iteration 7664 Loss: 1.214280128479004\n",
      "Iteration 7665 Loss: 1.1548442840576172\n",
      "Iteration 7666 Loss: 1.3003935813903809\n",
      "Iteration 7667 Loss: 1.1445605754852295\n",
      "Iteration 7668 Loss: 1.318915843963623\n",
      "Iteration 7669 Loss: 0.9469399452209473\n",
      "Iteration 7669 Loss: 1.221620798110962\n",
      "Iteration 7670 Loss: 1.2826988697052002\n",
      "Iteration 7671 Loss: 1.5759990215301514\n",
      "Iteration 7672 Loss: 1.757108211517334\n",
      "Iteration 7673 Loss: 1.4246623516082764\n",
      "Iteration 7674 Loss: 0.9205554723739624\n",
      "Iteration 7675 Loss: 1.5543173551559448\n",
      "Iteration 7676 Loss: 1.6920241117477417\n",
      "Iteration 7677 Loss: 1.6949574947357178\n",
      "Iteration 7678 Loss: 1.2639974355697632\n",
      "Iteration 7679 Loss: 1.462327480316162\n",
      "Iteration 7679 Loss: 1.4628647565841675\n",
      "Iteration 7680 Loss: 1.3640202283859253\n",
      "Iteration 7681 Loss: 1.3891241550445557\n",
      "Iteration 7682 Loss: 1.199325680732727\n",
      "Iteration 7683 Loss: 1.4725525379180908\n",
      "Iteration 7684 Loss: 1.6551190614700317\n",
      "Iteration 7685 Loss: 1.4133474826812744\n",
      "Iteration 7686 Loss: 1.684495449066162\n",
      "Iteration 7687 Loss: 1.8044601678848267\n",
      "Iteration 7688 Loss: 1.690807580947876\n",
      "Iteration 7689 Loss: 1.5060533285140991\n",
      "Iteration 7689 Loss: 1.5179306268692017\n",
      "Iteration 7690 Loss: 1.4499136209487915\n",
      "Iteration 7691 Loss: 1.4859206676483154\n",
      "Iteration 7692 Loss: 1.562690019607544\n",
      "Iteration 7693 Loss: 1.4573320150375366\n",
      "Iteration 7694 Loss: 1.4863150119781494\n",
      "Iteration 7695 Loss: 1.6636182069778442\n",
      "Iteration 7696 Loss: 1.4987504482269287\n",
      "Iteration 7697 Loss: 0.9714625477790833\n",
      "Iteration 7698 Loss: 1.1260380744934082\n",
      "Iteration 7699 Loss: 1.1611542701721191\n",
      "Iteration 7699 Loss: 1.3863193988800049\n",
      "Iteration 7700 Loss: 1.3825608491897583\n",
      "Iteration 7701 Loss: 1.394091248512268\n",
      "Iteration 7702 Loss: 1.1320222616195679\n",
      "Iteration 7703 Loss: 1.2971668243408203\n",
      "Iteration 7704 Loss: 1.5428390502929688\n",
      "Iteration 7705 Loss: 1.5278347730636597\n",
      "Iteration 7706 Loss: 1.731890082359314\n",
      "Iteration 7707 Loss: 1.6174917221069336\n",
      "Iteration 7708 Loss: 1.6365617513656616\n",
      "Iteration 7709 Loss: 1.197965383529663\n",
      "Iteration 7709 Loss: 1.4460422992706299\n",
      "Iteration 7710 Loss: 1.6617597341537476\n",
      "Iteration 7711 Loss: 1.6159982681274414\n",
      "Iteration 7712 Loss: 1.3519161939620972\n",
      "Iteration 7713 Loss: 1.5057941675186157\n",
      "Iteration 7714 Loss: 1.5232583284378052\n",
      "Iteration 7715 Loss: 1.3603588342666626\n",
      "Iteration 7716 Loss: 1.5088436603546143\n",
      "Iteration 7717 Loss: 1.5014111995697021\n",
      "Iteration 7718 Loss: 1.2428139448165894\n",
      "Iteration 7719 Loss: 1.3325122594833374\n",
      "Iteration 7719 Loss: 1.4604666233062744\n",
      "Iteration 7720 Loss: 1.3905500173568726\n",
      "Iteration 7721 Loss: 1.3522593975067139\n",
      "Iteration 7722 Loss: 1.3550748825073242\n",
      "Iteration 7723 Loss: 1.4102380275726318\n",
      "Iteration 7724 Loss: 1.5481104850769043\n",
      "Iteration 7725 Loss: 1.1928397417068481\n",
      "Iteration 7726 Loss: 1.5556286573410034\n",
      "Iteration 7727 Loss: 0.9010190963745117\n",
      "Iteration 7728 Loss: 1.7686192989349365\n",
      "Iteration 7729 Loss: 1.4408029317855835\n",
      "Iteration 7729 Loss: 1.3915143013000488\n",
      "Iteration 7730 Loss: 1.2863147258758545\n",
      "Iteration 7731 Loss: 1.6826214790344238\n",
      "Iteration 7732 Loss: 1.3487802743911743\n",
      "Iteration 7733 Loss: 1.2277230024337769\n",
      "Iteration 7734 Loss: 1.23387610912323\n",
      "Iteration 7735 Loss: 2.159297466278076\n",
      "Iteration 7736 Loss: 1.5481898784637451\n",
      "Iteration 7737 Loss: 1.0977928638458252\n",
      "Iteration 7738 Loss: 1.54021155834198\n",
      "Iteration 7739 Loss: 1.1441847085952759\n",
      "Iteration 7739 Loss: 1.4268991947174072\n",
      "Iteration 7740 Loss: 1.8214069604873657\n",
      "Iteration 7741 Loss: 1.3607696294784546\n",
      "Iteration 7742 Loss: 1.476133942604065\n",
      "Iteration 7743 Loss: 1.5326638221740723\n",
      "Iteration 7744 Loss: 1.4574612379074097\n",
      "Iteration 7745 Loss: 1.7291053533554077\n",
      "Iteration 7746 Loss: 1.2214184999465942\n",
      "Iteration 7747 Loss: 1.3757555484771729\n",
      "Iteration 7748 Loss: 1.6925711631774902\n",
      "Iteration 7749 Loss: 0.9905268549919128\n",
      "Iteration 7749 Loss: 1.465781331062317\n",
      "Iteration 7750 Loss: 1.2082680463790894\n",
      "Iteration 7751 Loss: 1.076688289642334\n",
      "Iteration 7752 Loss: 1.4493601322174072\n",
      "Iteration 7753 Loss: 1.0664499998092651\n",
      "Iteration 7754 Loss: 1.7297950983047485\n",
      "Iteration 7755 Loss: 1.7547156810760498\n",
      "Iteration 7756 Loss: 1.4482113122940063\n",
      "Iteration 7757 Loss: 1.5117743015289307\n",
      "Iteration 7758 Loss: 1.7834446430206299\n",
      "Iteration 7759 Loss: 1.6912355422973633\n",
      "Iteration 7759 Loss: 1.471994400024414\n",
      "Iteration 7760 Loss: 1.3350846767425537\n",
      "Iteration 7761 Loss: 1.6854002475738525\n",
      "Iteration 7762 Loss: 1.8724265098571777\n",
      "Iteration 7763 Loss: 1.795179843902588\n",
      "Iteration 7764 Loss: 1.3512145280838013\n",
      "Iteration 7765 Loss: 1.166062831878662\n",
      "Iteration 7766 Loss: 1.4289034605026245\n",
      "Iteration 7767 Loss: 1.541833758354187\n",
      "Iteration 7768 Loss: 1.120769739151001\n",
      "Iteration 7769 Loss: 1.3516724109649658\n",
      "Iteration 7769 Loss: 1.4648548364639282\n",
      "Iteration 7770 Loss: 1.5359749794006348\n",
      "Iteration 7771 Loss: 1.5006041526794434\n",
      "Iteration 7772 Loss: 1.3155367374420166\n",
      "Iteration 7773 Loss: 1.5595537424087524\n",
      "Iteration 7774 Loss: 1.7414696216583252\n",
      "Iteration 7775 Loss: 1.5877748727798462\n",
      "Iteration 7776 Loss: 1.3659287691116333\n",
      "Iteration 7777 Loss: 1.2842012643814087\n",
      "Iteration 7778 Loss: 1.4045498371124268\n",
      "Iteration 7779 Loss: 1.2557533979415894\n",
      "Iteration 7779 Loss: 1.4551347494125366\n",
      "Iteration 7780 Loss: 0.7771614193916321\n",
      "Iteration 7781 Loss: 1.0292725563049316\n",
      "Iteration 7782 Loss: 1.3016541004180908\n",
      "Iteration 7783 Loss: 1.6260578632354736\n",
      "Iteration 7784 Loss: 1.25575852394104\n",
      "Iteration 7785 Loss: 1.5085620880126953\n",
      "Iteration 7786 Loss: 1.3583662509918213\n",
      "Iteration 7787 Loss: 1.1107245683670044\n",
      "Iteration 7788 Loss: 1.2847198247909546\n",
      "Iteration 7789 Loss: 1.308440089225769\n",
      "Iteration 7789 Loss: 1.25607168674469\n",
      "Iteration 7790 Loss: 1.339726448059082\n",
      "Iteration 7791 Loss: 1.2812403440475464\n",
      "Iteration 7792 Loss: 1.3533751964569092\n",
      "Iteration 7793 Loss: 1.4122223854064941\n",
      "Iteration 7794 Loss: 1.5059479475021362\n",
      "Iteration 7795 Loss: 1.5384806394577026\n",
      "Iteration 7796 Loss: 1.691543459892273\n",
      "Iteration 7797 Loss: 1.2840980291366577\n",
      "Iteration 7798 Loss: 1.5958528518676758\n",
      "Iteration 7799 Loss: 1.947198510169983\n",
      "Iteration 7799 Loss: 1.4949685335159302\n",
      "Iteration 7800 Loss: 1.6076496839523315\n",
      "Iteration 7801 Loss: 1.4409878253936768\n",
      "Iteration 7802 Loss: 1.3746227025985718\n",
      "Iteration 7803 Loss: 1.406063437461853\n",
      "Iteration 7804 Loss: 2.045233964920044\n",
      "Iteration 7805 Loss: 1.2331918478012085\n",
      "Iteration 7806 Loss: 1.6083897352218628\n",
      "Iteration 7807 Loss: 1.6870689392089844\n",
      "Iteration 7808 Loss: 1.4300786256790161\n",
      "Iteration 7809 Loss: 1.59810209274292\n",
      "Iteration 7809 Loss: 1.543138861656189\n",
      "Iteration 7810 Loss: 1.1690402030944824\n",
      "Iteration 7811 Loss: 1.3092234134674072\n",
      "Iteration 7812 Loss: 0.9579470157623291\n",
      "Iteration 7813 Loss: 1.7018139362335205\n",
      "Iteration 7814 Loss: 1.1409281492233276\n",
      "Iteration 7815 Loss: 1.434200644493103\n",
      "Iteration 7816 Loss: 1.3133304119110107\n",
      "Iteration 7817 Loss: 1.7914735078811646\n",
      "Iteration 7818 Loss: 1.3929070234298706\n",
      "Iteration 7819 Loss: 1.6700416803359985\n",
      "Iteration 7819 Loss: 1.3880904912948608\n",
      "Iteration 7820 Loss: 1.2334314584732056\n",
      "Iteration 7821 Loss: 1.4500161409378052\n",
      "Iteration 7822 Loss: 1.709230899810791\n",
      "Iteration 7823 Loss: 1.742472529411316\n",
      "Iteration 7824 Loss: 1.2955645322799683\n",
      "Iteration 7825 Loss: 1.3965938091278076\n",
      "Iteration 7826 Loss: 2.059006929397583\n",
      "Iteration 7827 Loss: 1.2798409461975098\n",
      "Iteration 7828 Loss: 1.9119627475738525\n",
      "Iteration 7829 Loss: 1.3552799224853516\n",
      "Iteration 7829 Loss: 1.5433399677276611\n",
      "Iteration 7830 Loss: 1.0668227672576904\n",
      "Iteration 7831 Loss: 1.172033429145813\n",
      "Iteration 7832 Loss: 1.3384833335876465\n",
      "Iteration 7833 Loss: 1.612467885017395\n",
      "Iteration 7834 Loss: 1.867393136024475\n",
      "Iteration 7835 Loss: 1.0739538669586182\n",
      "Iteration 7836 Loss: 1.3638248443603516\n",
      "Iteration 7837 Loss: 1.4348101615905762\n",
      "Iteration 7838 Loss: 1.0408700704574585\n",
      "Iteration 7839 Loss: 1.5580533742904663\n",
      "Iteration 7839 Loss: 1.352871298789978\n",
      "Iteration 7840 Loss: 1.0816808938980103\n",
      "Iteration 7841 Loss: 0.6899805665016174\n",
      "Iteration 7842 Loss: 1.2100473642349243\n",
      "Iteration 7843 Loss: 1.2354092597961426\n",
      "Iteration 7844 Loss: 1.2050224542617798\n",
      "Iteration 7845 Loss: 1.4459559917449951\n",
      "Iteration 7846 Loss: 1.6052145957946777\n",
      "Iteration 7847 Loss: 1.3147189617156982\n",
      "Iteration 7848 Loss: 1.3434429168701172\n",
      "Iteration 7849 Loss: 1.828826904296875\n",
      "Iteration 7849 Loss: 1.2960299253463745\n",
      "Iteration 7850 Loss: 1.219645619392395\n",
      "Iteration 7851 Loss: 1.3513206243515015\n",
      "Iteration 7852 Loss: 1.2514032125473022\n",
      "Iteration 7853 Loss: 1.3255302906036377\n",
      "Iteration 7854 Loss: 1.4959728717803955\n",
      "Iteration 7855 Loss: 1.778402328491211\n",
      "Iteration 7856 Loss: 1.502289056777954\n",
      "Iteration 7857 Loss: 1.6432363986968994\n",
      "Iteration 7858 Loss: 1.917234182357788\n",
      "Iteration 7859 Loss: 1.6693648099899292\n",
      "Iteration 7859 Loss: 1.5154398679733276\n",
      "Iteration 7860 Loss: 1.3869988918304443\n",
      "Iteration 7861 Loss: 1.3787975311279297\n",
      "Iteration 7862 Loss: 1.4063150882720947\n",
      "Iteration 7863 Loss: 1.354705572128296\n",
      "Iteration 7864 Loss: 1.7812669277191162\n",
      "Iteration 7865 Loss: 1.197545051574707\n",
      "Iteration 7866 Loss: 0.532768964767456\n",
      "Iteration 7867 Loss: 1.5060003995895386\n",
      "Iteration 7868 Loss: 1.1685311794281006\n",
      "Iteration 7869 Loss: 1.4648798704147339\n",
      "Iteration 7869 Loss: 1.3177809715270996\n",
      "Iteration 7870 Loss: 1.3361046314239502\n",
      "Iteration 7871 Loss: 1.4486761093139648\n",
      "Iteration 7872 Loss: 1.598917841911316\n",
      "Iteration 7873 Loss: 1.1804916858673096\n",
      "Iteration 7874 Loss: 1.4691320657730103\n",
      "Iteration 7875 Loss: 1.7940189838409424\n",
      "Iteration 7876 Loss: 1.8016544580459595\n",
      "Iteration 7877 Loss: 1.2936078310012817\n",
      "Iteration 7878 Loss: 1.5133088827133179\n",
      "Iteration 7879 Loss: 1.3722273111343384\n",
      "Iteration 7879 Loss: 1.480813980102539\n",
      "Iteration 7880 Loss: 1.0815870761871338\n",
      "Iteration 7881 Loss: 1.5141870975494385\n",
      "Iteration 7882 Loss: 1.4784013032913208\n",
      "Iteration 7883 Loss: 1.4190646409988403\n",
      "Iteration 7884 Loss: 1.6708407402038574\n",
      "Iteration 7885 Loss: 1.5280321836471558\n",
      "Iteration 7886 Loss: 1.422608494758606\n",
      "Iteration 7887 Loss: 1.5871214866638184\n",
      "Iteration 7888 Loss: 1.3330382108688354\n",
      "Iteration 7889 Loss: 1.4326049089431763\n",
      "Iteration 7889 Loss: 1.4467484951019287\n",
      "Iteration 7890 Loss: 1.1740283966064453\n",
      "Iteration 7891 Loss: 1.6857140064239502\n",
      "Iteration 7892 Loss: 1.039692997932434\n",
      "Iteration 7893 Loss: 1.1689863204956055\n",
      "Iteration 7894 Loss: 1.3138891458511353\n",
      "Iteration 7895 Loss: 1.3269679546356201\n",
      "Iteration 7896 Loss: 1.3036980628967285\n",
      "Iteration 7897 Loss: 1.4484620094299316\n",
      "Iteration 7898 Loss: 1.0093766450881958\n",
      "Iteration 7899 Loss: 1.1252281665802002\n",
      "Iteration 7899 Loss: 1.2596044540405273\n",
      "Iteration 7900 Loss: 1.2114428281784058\n",
      "Iteration 7901 Loss: 1.3364423513412476\n",
      "Iteration 7902 Loss: 1.6225757598876953\n",
      "Iteration 7903 Loss: 1.19294011592865\n",
      "Iteration 7904 Loss: 1.5082472562789917\n",
      "Iteration 7905 Loss: 1.2491497993469238\n",
      "Iteration 7906 Loss: 1.827523946762085\n",
      "Iteration 7907 Loss: 1.0129832029342651\n",
      "Iteration 7908 Loss: 1.5228668451309204\n",
      "Iteration 7909 Loss: 1.7421249151229858\n",
      "Iteration 7909 Loss: 1.422629714012146\n",
      "Iteration 7910 Loss: 0.9035176634788513\n",
      "Iteration 7911 Loss: 1.197912335395813\n",
      "Iteration 7912 Loss: 1.4167722463607788\n",
      "Iteration 7913 Loss: 1.4741941690444946\n",
      "Iteration 7914 Loss: 1.342882513999939\n",
      "Iteration 7915 Loss: 1.4380210638046265\n",
      "Iteration 7916 Loss: 1.3615891933441162\n",
      "Iteration 7917 Loss: 1.253290057182312\n",
      "Iteration 7918 Loss: 1.3261362314224243\n",
      "Iteration 7919 Loss: 1.560498833656311\n",
      "Iteration 7919 Loss: 1.3274815082550049\n",
      "Iteration 7920 Loss: 1.779614806175232\n",
      "Iteration 7921 Loss: 1.1044718027114868\n",
      "Iteration 7922 Loss: 1.2186481952667236\n",
      "Iteration 7923 Loss: 1.672732949256897\n",
      "Iteration 7924 Loss: 1.630495309829712\n",
      "Iteration 7925 Loss: 0.9252321720123291\n",
      "Iteration 7926 Loss: 1.1699309349060059\n",
      "Iteration 7927 Loss: 1.481756329536438\n",
      "Iteration 7928 Loss: 1.5192052125930786\n",
      "Iteration 7929 Loss: 1.2077549695968628\n",
      "Iteration 7929 Loss: 1.3709843158721924\n",
      "Iteration 7930 Loss: 1.7159353494644165\n",
      "Iteration 7931 Loss: 1.793002963066101\n",
      "Iteration 7932 Loss: 1.6264715194702148\n",
      "Iteration 7933 Loss: 0.7697624564170837\n",
      "Iteration 7934 Loss: 1.4531501531600952\n",
      "Iteration 7935 Loss: 2.0446219444274902\n",
      "Iteration 7936 Loss: 1.5014333724975586\n",
      "Iteration 7937 Loss: 1.7601929903030396\n",
      "Iteration 7938 Loss: 1.739671230316162\n",
      "Iteration 7939 Loss: 1.753676414489746\n",
      "Iteration 7939 Loss: 1.615791916847229\n",
      "Iteration 7940 Loss: 1.0461455583572388\n",
      "Iteration 7941 Loss: 1.9095395803451538\n",
      "Iteration 7942 Loss: 1.4616270065307617\n",
      "Iteration 7943 Loss: 1.408002257347107\n",
      "Iteration 7944 Loss: 1.3936049938201904\n",
      "Iteration 7945 Loss: 1.2600187063217163\n",
      "Iteration 7946 Loss: 0.9946306347846985\n",
      "Iteration 7947 Loss: 1.2894964218139648\n",
      "Iteration 7948 Loss: 1.1931147575378418\n",
      "Iteration 7949 Loss: 1.5873005390167236\n",
      "Iteration 7949 Loss: 1.354348063468933\n",
      "Iteration 7950 Loss: 1.5173072814941406\n",
      "Iteration 7951 Loss: 1.3580076694488525\n",
      "Iteration 7952 Loss: 1.4259159564971924\n",
      "Iteration 7953 Loss: 1.4702357053756714\n",
      "Iteration 7954 Loss: 1.4144688844680786\n",
      "Iteration 7955 Loss: 1.3020333051681519\n",
      "Iteration 7956 Loss: 1.1568094491958618\n",
      "Iteration 7957 Loss: 1.4221234321594238\n",
      "Iteration 7958 Loss: 1.7730767726898193\n",
      "Iteration 7959 Loss: 1.1838868856430054\n",
      "Iteration 7959 Loss: 1.4023864269256592\n",
      "Iteration 7960 Loss: 1.443191409111023\n",
      "Iteration 7961 Loss: 1.359688639640808\n",
      "Iteration 7962 Loss: 1.3249934911727905\n",
      "Iteration 7963 Loss: 2.0390336513519287\n",
      "Iteration 7964 Loss: 1.3518407344818115\n",
      "Iteration 7965 Loss: 1.185685157775879\n",
      "Iteration 7966 Loss: 1.4771614074707031\n",
      "Iteration 7967 Loss: 1.3501216173171997\n",
      "Iteration 7968 Loss: 1.2225168943405151\n",
      "Iteration 7969 Loss: 1.307820200920105\n",
      "Iteration 7969 Loss: 1.406205415725708\n",
      "Iteration 7970 Loss: 1.564765214920044\n",
      "Iteration 7971 Loss: 1.4210047721862793\n",
      "Iteration 7972 Loss: 1.5265071392059326\n",
      "Iteration 7973 Loss: 1.6274749040603638\n",
      "Iteration 7974 Loss: 1.4329431056976318\n",
      "Iteration 7975 Loss: 1.274900197982788\n",
      "Iteration 7976 Loss: 1.1018916368484497\n",
      "Iteration 7977 Loss: 1.4248093366622925\n",
      "Iteration 7978 Loss: 1.4641045331954956\n",
      "Iteration 7979 Loss: 1.220383882522583\n",
      "Iteration 7979 Loss: 1.4058784246444702\n",
      "Iteration 7980 Loss: 1.839115858078003\n",
      "Iteration 7981 Loss: 1.4848579168319702\n",
      "Iteration 7982 Loss: 1.0468788146972656\n",
      "Iteration 7983 Loss: 2.092355251312256\n",
      "Iteration 7984 Loss: 1.6637955904006958\n",
      "Iteration 7985 Loss: 1.532918095588684\n",
      "Iteration 7986 Loss: 2.218116283416748\n",
      "Iteration 7987 Loss: 1.303199052810669\n",
      "Iteration 7988 Loss: 1.1474474668502808\n",
      "Iteration 7989 Loss: 1.4120804071426392\n",
      "Iteration 7989 Loss: 1.5740764141082764\n",
      "Iteration 7990 Loss: 1.4682412147521973\n",
      "Iteration 7991 Loss: 1.473426342010498\n",
      "Iteration 7992 Loss: 1.2088640928268433\n",
      "Iteration 7993 Loss: 1.46476149559021\n",
      "Iteration 7994 Loss: 1.6232954263687134\n",
      "Iteration 7995 Loss: 1.6892573833465576\n",
      "Iteration 7996 Loss: 1.319949984550476\n",
      "Iteration 7997 Loss: 1.568480134010315\n",
      "Iteration 7998 Loss: 1.9370174407958984\n",
      "Iteration 7999 Loss: 1.4987467527389526\n",
      "Iteration 7999 Loss: 1.5252041816711426\n",
      "Iteration 8000 Loss: 1.9531211853027344\n",
      "Iteration 8001 Loss: 1.2695385217666626\n",
      "Iteration 8002 Loss: 1.3982231616973877\n",
      "Iteration 8003 Loss: 1.1696524620056152\n",
      "Iteration 8004 Loss: 1.785784125328064\n",
      "Iteration 8005 Loss: 1.2739450931549072\n",
      "Iteration 8006 Loss: 1.4868860244750977\n",
      "Iteration 8007 Loss: 1.2057180404663086\n",
      "Iteration 8008 Loss: 1.3289811611175537\n",
      "Iteration 8009 Loss: 1.629624366760254\n",
      "Iteration 8009 Loss: 1.4501473903656006\n",
      "Iteration 8010 Loss: 1.7167339324951172\n",
      "Iteration 8011 Loss: 1.879608392715454\n",
      "Iteration 8012 Loss: 0.983564019203186\n",
      "Iteration 8013 Loss: 1.3042213916778564\n",
      "Iteration 8014 Loss: 1.250355839729309\n",
      "Iteration 8015 Loss: 1.3365941047668457\n",
      "Iteration 8016 Loss: 1.200196623802185\n",
      "Iteration 8017 Loss: 2.1517977714538574\n",
      "Iteration 8018 Loss: 1.3647857904434204\n",
      "Iteration 8019 Loss: 1.3635410070419312\n",
      "Iteration 8019 Loss: 1.4551397562026978\n",
      "Iteration 8020 Loss: 1.5281805992126465\n",
      "Iteration 8021 Loss: 1.7024102210998535\n",
      "Iteration 8022 Loss: 1.0989497900009155\n",
      "Iteration 8023 Loss: 1.9607843160629272\n",
      "Iteration 8024 Loss: 1.7208354473114014\n",
      "Iteration 8025 Loss: 1.7478971481323242\n",
      "Iteration 8026 Loss: 1.389450192451477\n",
      "Iteration 8027 Loss: 1.2388521432876587\n",
      "Iteration 8028 Loss: 1.3491102457046509\n",
      "Iteration 8029 Loss: 1.4773035049438477\n",
      "Iteration 8029 Loss: 1.5213773250579834\n",
      "Iteration 8030 Loss: 1.2962145805358887\n",
      "Iteration 8031 Loss: 1.516080617904663\n",
      "Iteration 8032 Loss: 1.9038984775543213\n",
      "Iteration 8033 Loss: 1.4535993337631226\n",
      "Iteration 8034 Loss: 1.3151260614395142\n",
      "Iteration 8035 Loss: 1.6161216497421265\n",
      "Iteration 8036 Loss: 0.8379592299461365\n",
      "Iteration 8037 Loss: 1.7859219312667847\n",
      "Iteration 8038 Loss: 1.0628689527511597\n",
      "Iteration 8039 Loss: 1.3813539743423462\n",
      "Iteration 8039 Loss: 1.416914463043213\n",
      "Iteration 8040 Loss: 1.1315542459487915\n",
      "Iteration 8041 Loss: 1.126658320426941\n",
      "Iteration 8042 Loss: 1.1067124605178833\n",
      "Iteration 8043 Loss: 1.4772461652755737\n",
      "Iteration 8044 Loss: 1.6208049058914185\n",
      "Iteration 8045 Loss: 1.5715887546539307\n",
      "Iteration 8046 Loss: 1.3333467245101929\n",
      "Iteration 8047 Loss: 1.0431150197982788\n",
      "Iteration 8048 Loss: 1.160842776298523\n",
      "Iteration 8049 Loss: 1.2850873470306396\n",
      "Iteration 8049 Loss: 1.2856955528259277\n",
      "Iteration 8050 Loss: 1.2700536251068115\n",
      "Iteration 8051 Loss: 1.5807075500488281\n",
      "Iteration 8052 Loss: 0.9961610436439514\n",
      "Iteration 8053 Loss: 1.2067081928253174\n",
      "Iteration 8054 Loss: 0.9608051180839539\n",
      "Iteration 8055 Loss: 1.3084449768066406\n",
      "Iteration 8056 Loss: 0.9263074398040771\n",
      "Iteration 8057 Loss: 1.25220787525177\n",
      "Iteration 8058 Loss: 1.6416800022125244\n",
      "Iteration 8059 Loss: 1.195372223854065\n",
      "Iteration 8059 Loss: 1.2338448762893677\n",
      "Iteration 8060 Loss: 1.5303733348846436\n",
      "Iteration 8061 Loss: 1.3201141357421875\n",
      "Iteration 8062 Loss: 0.9402180314064026\n",
      "Iteration 8063 Loss: 1.6120097637176514\n",
      "Iteration 8064 Loss: 1.3099478483200073\n",
      "Iteration 8065 Loss: 0.9992449283599854\n",
      "Iteration 8066 Loss: 1.319740891456604\n",
      "Iteration 8067 Loss: 1.1458042860031128\n",
      "Iteration 8068 Loss: 1.7035926580429077\n",
      "Iteration 8069 Loss: 1.8188793659210205\n",
      "Iteration 8069 Loss: 1.3699924945831299\n",
      "Iteration 8070 Loss: 1.7689768075942993\n",
      "Iteration 8071 Loss: 1.6001039743423462\n",
      "Iteration 8072 Loss: 1.5972579717636108\n",
      "Iteration 8073 Loss: 1.6047072410583496\n",
      "Iteration 8074 Loss: 1.7393641471862793\n",
      "Iteration 8075 Loss: 1.239067792892456\n",
      "Iteration 8076 Loss: 1.1018500328063965\n",
      "Iteration 8077 Loss: 1.3619325160980225\n",
      "Iteration 8078 Loss: 1.7624015808105469\n",
      "Iteration 8079 Loss: 1.6238107681274414\n",
      "Iteration 8079 Loss: 1.539947271347046\n",
      "Iteration 8080 Loss: 1.8447511196136475\n",
      "Iteration 8081 Loss: 1.3322161436080933\n",
      "Iteration 8082 Loss: 1.6385564804077148\n",
      "Iteration 8083 Loss: 1.217810869216919\n",
      "Iteration 8084 Loss: 2.0646450519561768\n",
      "Iteration 8085 Loss: 1.254960536956787\n",
      "Iteration 8086 Loss: 1.3226449489593506\n",
      "Iteration 8087 Loss: 1.6597988605499268\n",
      "Iteration 8088 Loss: 1.098637342453003\n",
      "Iteration 8089 Loss: 1.5316177606582642\n",
      "Iteration 8089 Loss: 1.4965637922286987\n",
      "Iteration 8090 Loss: 1.4040688276290894\n",
      "Iteration 8091 Loss: 1.3785110712051392\n",
      "Iteration 8092 Loss: 1.4554589986801147\n",
      "Iteration 8093 Loss: 1.1163958311080933\n",
      "Iteration 8094 Loss: 1.7480417490005493\n",
      "Iteration 8095 Loss: 1.2325788736343384\n",
      "Iteration 8096 Loss: 1.314501404762268\n",
      "Iteration 8097 Loss: 1.6021156311035156\n",
      "Iteration 8098 Loss: 0.797243058681488\n",
      "Iteration 8099 Loss: 1.3922616243362427\n",
      "Iteration 8099 Loss: 1.3441178798675537\n",
      "Iteration 8100 Loss: 1.3926494121551514\n",
      "Iteration 8101 Loss: 1.7751168012619019\n",
      "Iteration 8102 Loss: 1.5168225765228271\n",
      "Iteration 8103 Loss: 1.4125922918319702\n",
      "Iteration 8104 Loss: 1.3092745542526245\n",
      "Iteration 8105 Loss: 1.610154628753662\n",
      "Iteration 8106 Loss: 1.6126527786254883\n",
      "Iteration 8107 Loss: 1.7663758993148804\n",
      "Iteration 8108 Loss: 1.3673194646835327\n",
      "Iteration 8109 Loss: 1.240371823310852\n",
      "Iteration 8109 Loss: 1.5003330707550049\n",
      "Iteration 8110 Loss: 1.3525810241699219\n",
      "Iteration 8111 Loss: 1.6019161939620972\n",
      "Iteration 8112 Loss: 1.3088335990905762\n",
      "Iteration 8113 Loss: 1.1510353088378906\n",
      "Iteration 8114 Loss: 1.1963638067245483\n",
      "Iteration 8115 Loss: 1.3097063302993774\n",
      "Iteration 8116 Loss: 1.1655373573303223\n",
      "Iteration 8117 Loss: 1.649328589439392\n",
      "Iteration 8118 Loss: 1.653059720993042\n",
      "Iteration 8119 Loss: 1.6998738050460815\n",
      "Iteration 8119 Loss: 1.4088236093521118\n",
      "Iteration 8120 Loss: 1.3567253351211548\n",
      "Iteration 8121 Loss: 1.128914713859558\n",
      "Iteration 8122 Loss: 1.4809914827346802\n",
      "Iteration 8123 Loss: 1.7356998920440674\n",
      "Iteration 8124 Loss: 1.6451051235198975\n",
      "Iteration 8125 Loss: 1.318375825881958\n",
      "Iteration 8126 Loss: 1.2743518352508545\n",
      "Iteration 8127 Loss: 1.3222776651382446\n",
      "Iteration 8128 Loss: 1.5935907363891602\n",
      "Iteration 8129 Loss: 1.669771671295166\n",
      "Iteration 8129 Loss: 1.452580451965332\n",
      "Iteration 8130 Loss: 1.337052583694458\n",
      "Iteration 8131 Loss: 1.5677391290664673\n",
      "Iteration 8132 Loss: 1.7682273387908936\n",
      "Iteration 8133 Loss: 1.5125483274459839\n",
      "Iteration 8134 Loss: 1.1082532405853271\n",
      "Iteration 8135 Loss: 1.0841257572174072\n",
      "Iteration 8136 Loss: 0.9708465933799744\n",
      "Iteration 8137 Loss: 1.5570050477981567\n",
      "Iteration 8138 Loss: 1.0431653261184692\n",
      "Iteration 8139 Loss: 0.9771141409873962\n",
      "Iteration 8139 Loss: 1.2926076650619507\n",
      "Iteration 8140 Loss: 1.6396738290786743\n",
      "Iteration 8141 Loss: 1.3518141508102417\n",
      "Iteration 8142 Loss: 1.3709511756896973\n",
      "Iteration 8143 Loss: 1.8704994916915894\n",
      "Iteration 8144 Loss: 1.708174467086792\n",
      "Iteration 8145 Loss: 1.2468174695968628\n",
      "Iteration 8146 Loss: 1.1680930852890015\n",
      "Iteration 8147 Loss: 1.453398585319519\n",
      "Iteration 8148 Loss: 1.9227278232574463\n",
      "Iteration 8149 Loss: 1.2744154930114746\n",
      "Iteration 8149 Loss: 1.5006566047668457\n",
      "Iteration 8150 Loss: 1.0681310892105103\n",
      "Iteration 8151 Loss: 1.3257397413253784\n",
      "Iteration 8152 Loss: 1.4020013809204102\n",
      "Iteration 8153 Loss: 1.632650375366211\n",
      "Iteration 8154 Loss: 1.135190486907959\n",
      "Iteration 8155 Loss: 1.2917364835739136\n",
      "Iteration 8156 Loss: 0.9375041127204895\n",
      "Iteration 8157 Loss: 2.2824440002441406\n",
      "Iteration 8158 Loss: 1.7414705753326416\n",
      "Iteration 8159 Loss: 1.1458327770233154\n",
      "Iteration 8159 Loss: 1.3962701559066772\n",
      "Iteration 8160 Loss: 1.512349247932434\n",
      "Iteration 8161 Loss: 1.1716514825820923\n",
      "Iteration 8162 Loss: 1.5107489824295044\n",
      "Iteration 8163 Loss: 1.5990149974822998\n",
      "Iteration 8164 Loss: 1.4025465250015259\n",
      "Iteration 8165 Loss: 1.537074327468872\n",
      "Iteration 8166 Loss: 1.3451478481292725\n",
      "Iteration 8167 Loss: 1.5248932838439941\n",
      "Iteration 8168 Loss: 1.4198269844055176\n",
      "Iteration 8169 Loss: 1.1774640083312988\n",
      "Iteration 8169 Loss: 1.4200718402862549\n",
      "Iteration 8170 Loss: 1.052817463874817\n",
      "Iteration 8171 Loss: 0.9650025367736816\n",
      "Iteration 8172 Loss: 1.3642189502716064\n",
      "Iteration 8173 Loss: 1.388707160949707\n",
      "Iteration 8174 Loss: 1.543244481086731\n",
      "Iteration 8175 Loss: 1.5958863496780396\n",
      "Iteration 8176 Loss: 1.4961233139038086\n",
      "Iteration 8177 Loss: 1.0546717643737793\n",
      "Iteration 8178 Loss: 1.2011712789535522\n",
      "Iteration 8179 Loss: 1.270534873008728\n",
      "Iteration 8179 Loss: 1.2932379245758057\n",
      "Iteration 8180 Loss: 1.132749319076538\n",
      "Iteration 8181 Loss: 1.2908276319503784\n",
      "Iteration 8182 Loss: 1.8192286491394043\n",
      "Iteration 8183 Loss: 1.5796376466751099\n",
      "Iteration 8184 Loss: 1.6179713010787964\n",
      "Iteration 8185 Loss: 1.5051562786102295\n",
      "Iteration 8186 Loss: 1.607564926147461\n",
      "Iteration 8187 Loss: 1.393552303314209\n",
      "Iteration 8188 Loss: 1.4191895723342896\n",
      "Iteration 8189 Loss: 1.4325400590896606\n",
      "Iteration 8189 Loss: 1.4798418283462524\n",
      "Iteration 8190 Loss: 1.1398662328720093\n",
      "Iteration 8191 Loss: 0.9213321805000305\n",
      "Iteration 8192 Loss: 1.630208134651184\n",
      "Iteration 8193 Loss: 1.222157597541809\n",
      "Iteration 8194 Loss: 1.4491997957229614\n",
      "Iteration 8195 Loss: 1.6634578704833984\n",
      "Iteration 8196 Loss: 1.68263840675354\n",
      "Iteration 8197 Loss: 1.4517872333526611\n",
      "Iteration 8198 Loss: 1.2323623895645142\n",
      "Iteration 8199 Loss: 1.401780366897583\n",
      "Iteration 8199 Loss: 1.3794790506362915\n",
      "Iteration 8200 Loss: 1.8328073024749756\n",
      "Iteration 8201 Loss: 1.0978847742080688\n",
      "Iteration 8202 Loss: 1.7878016233444214\n",
      "Iteration 8203 Loss: 1.4248160123825073\n",
      "Iteration 8204 Loss: 1.5993090867996216\n",
      "Iteration 8205 Loss: 1.5727218389511108\n",
      "Iteration 8206 Loss: 1.769103765487671\n",
      "Iteration 8207 Loss: 0.92265385389328\n",
      "Iteration 8208 Loss: 1.4820064306259155\n",
      "Iteration 8209 Loss: 1.7580640316009521\n",
      "Iteration 8209 Loss: 1.524716854095459\n",
      "Iteration 8210 Loss: 1.4297641515731812\n",
      "Iteration 8211 Loss: 0.9031158089637756\n",
      "Iteration 8212 Loss: 1.943333625793457\n",
      "Iteration 8213 Loss: 1.543924331665039\n",
      "Iteration 8214 Loss: 1.4713813066482544\n",
      "Iteration 8215 Loss: 1.3906176090240479\n",
      "Iteration 8216 Loss: 1.0316065549850464\n",
      "Iteration 8217 Loss: 1.277024269104004\n",
      "Iteration 8218 Loss: 1.424066424369812\n",
      "Iteration 8219 Loss: 1.2638548612594604\n",
      "Iteration 8219 Loss: 1.3678687810897827\n",
      "Iteration 8220 Loss: 1.3195291757583618\n",
      "Iteration 8221 Loss: 1.5934526920318604\n",
      "Iteration 8222 Loss: 1.8158316612243652\n",
      "Iteration 8223 Loss: 1.6684914827346802\n",
      "Iteration 8224 Loss: 1.3848763704299927\n",
      "Iteration 8225 Loss: 1.1825976371765137\n",
      "Iteration 8226 Loss: 0.9341617822647095\n",
      "Iteration 8227 Loss: 1.3752670288085938\n",
      "Iteration 8228 Loss: 1.4714348316192627\n",
      "Iteration 8229 Loss: 1.4651830196380615\n",
      "Iteration 8229 Loss: 1.4210827350616455\n",
      "Iteration 8230 Loss: 1.2757452726364136\n",
      "Iteration 8231 Loss: 1.4824934005737305\n",
      "Iteration 8232 Loss: 1.3144959211349487\n",
      "Iteration 8233 Loss: 1.8530341386795044\n",
      "Iteration 8234 Loss: 1.1169732809066772\n",
      "Iteration 8235 Loss: 1.6238024234771729\n",
      "Iteration 8236 Loss: 1.176543951034546\n",
      "Iteration 8237 Loss: 1.582168459892273\n",
      "Iteration 8238 Loss: 1.3704358339309692\n",
      "Iteration 8239 Loss: 1.3730002641677856\n",
      "Iteration 8239 Loss: 1.4168692827224731\n",
      "Iteration 8240 Loss: 1.4480416774749756\n",
      "Iteration 8241 Loss: 2.036783218383789\n",
      "Iteration 8242 Loss: 1.4993764162063599\n",
      "Iteration 8243 Loss: 1.3650442361831665\n",
      "Iteration 8244 Loss: 1.395444393157959\n",
      "Iteration 8245 Loss: 1.8658767938613892\n",
      "Iteration 8246 Loss: 1.274956226348877\n",
      "Iteration 8247 Loss: 2.0546317100524902\n",
      "Iteration 8248 Loss: 1.0414369106292725\n",
      "Iteration 8249 Loss: 1.0099109411239624\n",
      "Iteration 8249 Loss: 1.499150276184082\n",
      "Iteration 8250 Loss: 1.6582067012786865\n",
      "Iteration 8251 Loss: 1.2625606060028076\n",
      "Iteration 8252 Loss: 0.9571821093559265\n",
      "Iteration 8253 Loss: 1.801785945892334\n",
      "Iteration 8254 Loss: 1.7367395162582397\n",
      "Iteration 8255 Loss: 1.7804388999938965\n",
      "Iteration 8256 Loss: 1.3280572891235352\n",
      "Iteration 8257 Loss: 1.797109603881836\n",
      "Iteration 8258 Loss: 1.442969799041748\n",
      "Iteration 8259 Loss: 1.1847285032272339\n",
      "Iteration 8259 Loss: 1.4949778318405151\n",
      "Iteration 8260 Loss: 1.3681297302246094\n",
      "Iteration 8261 Loss: 1.6233967542648315\n",
      "Iteration 8262 Loss: 1.1998982429504395\n",
      "Iteration 8263 Loss: 1.0934841632843018\n",
      "Iteration 8264 Loss: 1.304929256439209\n",
      "Iteration 8265 Loss: 1.1279326677322388\n",
      "Iteration 8266 Loss: 1.8245902061462402\n",
      "Iteration 8267 Loss: 1.5199285745620728\n",
      "Iteration 8268 Loss: 1.8610637187957764\n",
      "Iteration 8269 Loss: 1.3127117156982422\n",
      "Iteration 8269 Loss: 1.4236066341400146\n",
      "Iteration 8270 Loss: 1.6945146322250366\n",
      "Iteration 8271 Loss: 1.4437165260314941\n",
      "Iteration 8272 Loss: 1.781112790107727\n",
      "Iteration 8273 Loss: 1.598118543624878\n",
      "Iteration 8274 Loss: 1.7335327863693237\n",
      "Iteration 8275 Loss: 1.5840497016906738\n",
      "Iteration 8276 Loss: 1.041019082069397\n",
      "Iteration 8277 Loss: 1.7066020965576172\n",
      "Iteration 8278 Loss: 1.4764933586120605\n",
      "Iteration 8279 Loss: 1.185840129852295\n",
      "Iteration 8279 Loss: 1.5245001316070557\n",
      "Iteration 8280 Loss: 1.340171456336975\n",
      "Iteration 8281 Loss: 1.3680572509765625\n",
      "Iteration 8282 Loss: 1.2030611038208008\n",
      "Iteration 8283 Loss: 1.5361618995666504\n",
      "Iteration 8284 Loss: 1.1898527145385742\n",
      "Iteration 8285 Loss: 1.3996049165725708\n",
      "Iteration 8286 Loss: 1.027525544166565\n",
      "Iteration 8287 Loss: 1.814603567123413\n",
      "Iteration 8288 Loss: 1.70061457157135\n",
      "Iteration 8289 Loss: 1.2641422748565674\n",
      "Iteration 8289 Loss: 1.3843796253204346\n",
      "Iteration 8290 Loss: 1.059356451034546\n",
      "Iteration 8291 Loss: 1.1917767524719238\n",
      "Iteration 8292 Loss: 1.392747163772583\n",
      "Iteration 8293 Loss: 1.3355273008346558\n",
      "Iteration 8294 Loss: 1.9201339483261108\n",
      "Iteration 8295 Loss: 1.5800907611846924\n",
      "Iteration 8296 Loss: 1.5873750448226929\n",
      "Iteration 8297 Loss: 1.649891972541809\n",
      "Iteration 8298 Loss: 1.1952766180038452\n",
      "Iteration 8299 Loss: 1.327021837234497\n",
      "Iteration 8299 Loss: 1.423919677734375\n",
      "Iteration 8300 Loss: 2.08638596534729\n",
      "Iteration 8301 Loss: 1.2186059951782227\n",
      "Iteration 8302 Loss: 1.380204439163208\n",
      "Iteration 8303 Loss: 1.2986562252044678\n",
      "Iteration 8304 Loss: 1.6993486881256104\n",
      "Iteration 8305 Loss: 1.8846631050109863\n",
      "Iteration 8306 Loss: 1.3258434534072876\n",
      "Iteration 8307 Loss: 1.5907182693481445\n",
      "Iteration 8308 Loss: 1.2811071872711182\n",
      "Iteration 8309 Loss: 1.796817660331726\n",
      "Iteration 8309 Loss: 1.5562350749969482\n",
      "Iteration 8310 Loss: 1.4074956178665161\n",
      "Iteration 8311 Loss: 1.5474414825439453\n",
      "Iteration 8312 Loss: 1.3104138374328613\n",
      "Iteration 8313 Loss: 1.3071484565734863\n",
      "Iteration 8314 Loss: 1.3009132146835327\n",
      "Iteration 8315 Loss: 1.514899730682373\n",
      "Iteration 8316 Loss: 1.720703363418579\n",
      "Iteration 8317 Loss: 1.3797154426574707\n",
      "Iteration 8318 Loss: 1.141758918762207\n",
      "Iteration 8319 Loss: 1.5097471475601196\n",
      "Iteration 8319 Loss: 1.4140236377716064\n",
      "Iteration 8320 Loss: 1.2456326484680176\n",
      "Iteration 8321 Loss: 1.105467438697815\n",
      "Iteration 8322 Loss: 1.4720921516418457\n",
      "Iteration 8323 Loss: 1.120490550994873\n",
      "Iteration 8324 Loss: 1.6564584970474243\n",
      "Iteration 8325 Loss: 1.116072416305542\n",
      "Iteration 8326 Loss: 1.1251177787780762\n",
      "Iteration 8327 Loss: 1.520068883895874\n",
      "Iteration 8328 Loss: 0.9670886397361755\n",
      "Iteration 8329 Loss: 1.5264394283294678\n",
      "Iteration 8329 Loss: 1.2854928970336914\n",
      "Iteration 8330 Loss: 1.4770313501358032\n",
      "Iteration 8331 Loss: 1.7840464115142822\n",
      "Iteration 8332 Loss: 1.234229326248169\n",
      "Iteration 8333 Loss: 1.4247021675109863\n",
      "Iteration 8334 Loss: 1.869296669960022\n",
      "Iteration 8335 Loss: 1.4601775407791138\n",
      "Iteration 8336 Loss: 1.3720083236694336\n",
      "Iteration 8337 Loss: 1.0253057479858398\n",
      "Iteration 8338 Loss: 1.2013897895812988\n",
      "Iteration 8339 Loss: 1.2256592512130737\n",
      "Iteration 8339 Loss: 1.4073846340179443\n",
      "Iteration 8340 Loss: 1.43718683719635\n",
      "Iteration 8341 Loss: 1.6625632047653198\n",
      "Iteration 8342 Loss: 1.7444391250610352\n",
      "Iteration 8343 Loss: 1.4942713975906372\n",
      "Iteration 8344 Loss: 1.0514018535614014\n",
      "Iteration 8345 Loss: 1.699412226676941\n",
      "Iteration 8346 Loss: 1.3688251972198486\n",
      "Iteration 8347 Loss: 1.2695891857147217\n",
      "Iteration 8348 Loss: 1.5640144348144531\n",
      "Iteration 8349 Loss: 0.6858201622962952\n",
      "Iteration 8349 Loss: 1.3977524042129517\n",
      "Iteration 8350 Loss: 1.6388949155807495\n",
      "Iteration 8351 Loss: 1.736470103263855\n",
      "Iteration 8352 Loss: 1.5238747596740723\n",
      "Iteration 8353 Loss: 1.581070065498352\n",
      "Iteration 8354 Loss: 1.179434895515442\n",
      "Iteration 8355 Loss: 1.5178120136260986\n",
      "Iteration 8356 Loss: 1.2069711685180664\n",
      "Iteration 8357 Loss: 1.4131755828857422\n",
      "Iteration 8358 Loss: 1.517452597618103\n",
      "Iteration 8359 Loss: 1.6959248781204224\n",
      "Iteration 8359 Loss: 1.5011080503463745\n",
      "Iteration 8360 Loss: 1.5518712997436523\n",
      "Iteration 8361 Loss: 1.439144253730774\n",
      "Iteration 8362 Loss: 1.475071907043457\n",
      "Iteration 8363 Loss: 1.1718640327453613\n",
      "Iteration 8364 Loss: 1.6395108699798584\n",
      "Iteration 8365 Loss: 1.4582891464233398\n",
      "Iteration 8366 Loss: 1.074336051940918\n",
      "Iteration 8367 Loss: 1.3170362710952759\n",
      "Iteration 8368 Loss: 1.2488712072372437\n",
      "Iteration 8369 Loss: 1.4612661600112915\n",
      "Iteration 8369 Loss: 1.3837262392044067\n",
      "Iteration 8370 Loss: 1.296164870262146\n",
      "Iteration 8371 Loss: 1.3921785354614258\n",
      "Iteration 8372 Loss: 1.6822400093078613\n",
      "Iteration 8373 Loss: 1.3613497018814087\n",
      "Iteration 8374 Loss: 1.2045531272888184\n",
      "Iteration 8375 Loss: 1.3373185396194458\n",
      "Iteration 8376 Loss: 1.6845521926879883\n",
      "Iteration 8377 Loss: 1.4494454860687256\n",
      "Iteration 8378 Loss: 1.589226245880127\n",
      "Iteration 8379 Loss: 1.2081406116485596\n",
      "Iteration 8379 Loss: 1.420516848564148\n",
      "Iteration 8380 Loss: 1.6285501718521118\n",
      "Iteration 8381 Loss: 1.3557833433151245\n",
      "Iteration 8382 Loss: 1.2836475372314453\n",
      "Iteration 8383 Loss: 1.3914722204208374\n",
      "Iteration 8384 Loss: 1.5079046487808228\n",
      "Iteration 8385 Loss: 1.536256194114685\n",
      "Iteration 8386 Loss: 1.4362812042236328\n",
      "Iteration 8387 Loss: 1.227433443069458\n",
      "Iteration 8388 Loss: 1.257981300354004\n",
      "Iteration 8389 Loss: 1.506619930267334\n",
      "Iteration 8389 Loss: 1.4131929874420166\n",
      "Iteration 8390 Loss: 1.4998278617858887\n",
      "Iteration 8391 Loss: 1.642965316772461\n",
      "Iteration 8392 Loss: 1.5702835321426392\n",
      "Iteration 8393 Loss: 1.2809890508651733\n",
      "Iteration 8394 Loss: 1.486954927444458\n",
      "Iteration 8395 Loss: 1.5603559017181396\n",
      "Iteration 8396 Loss: 1.751093864440918\n",
      "Iteration 8397 Loss: 0.9833933711051941\n",
      "Iteration 8398 Loss: 1.2456306219100952\n",
      "Iteration 8399 Loss: 1.6345574855804443\n",
      "Iteration 8399 Loss: 1.4656051397323608\n",
      "Iteration 8400 Loss: 1.036896824836731\n",
      "Iteration 8401 Loss: 1.4955347776412964\n",
      "Iteration 8402 Loss: 1.7278178930282593\n",
      "Iteration 8403 Loss: 1.5533008575439453\n",
      "Iteration 8404 Loss: 1.2649694681167603\n",
      "Iteration 8405 Loss: 0.8435329794883728\n",
      "Iteration 8406 Loss: 0.9382467269897461\n",
      "Iteration 8407 Loss: 1.623223066329956\n",
      "Iteration 8408 Loss: 1.5265452861785889\n",
      "Iteration 8409 Loss: 1.0413095951080322\n",
      "Iteration 8409 Loss: 1.3051377534866333\n",
      "Iteration 8410 Loss: 1.2841687202453613\n",
      "Iteration 8411 Loss: 1.253050446510315\n",
      "Iteration 8412 Loss: 1.793555498123169\n",
      "Iteration 8413 Loss: 1.2190693616867065\n",
      "Iteration 8414 Loss: 1.1643638610839844\n",
      "Iteration 8415 Loss: 1.739445686340332\n",
      "Iteration 8416 Loss: 1.7364784479141235\n",
      "Iteration 8417 Loss: 1.1818231344223022\n",
      "Iteration 8418 Loss: 1.358495831489563\n",
      "Iteration 8419 Loss: 0.895832896232605\n",
      "Iteration 8419 Loss: 1.3626283407211304\n",
      "Iteration 8420 Loss: 1.130311369895935\n",
      "Iteration 8421 Loss: 1.5852528810501099\n",
      "Iteration 8422 Loss: 1.212348222732544\n",
      "Iteration 8423 Loss: 1.4306762218475342\n",
      "Iteration 8424 Loss: 1.049679160118103\n",
      "Iteration 8425 Loss: 1.627867341041565\n",
      "Iteration 8426 Loss: 1.6624482870101929\n",
      "Iteration 8427 Loss: 1.7662999629974365\n",
      "Iteration 8428 Loss: 1.9666579961776733\n",
      "Iteration 8429 Loss: 1.4677774906158447\n",
      "Iteration 8429 Loss: 1.4899318218231201\n",
      "Iteration 8430 Loss: 1.304476261138916\n",
      "Iteration 8431 Loss: 1.2677358388900757\n",
      "Iteration 8432 Loss: 1.3468393087387085\n",
      "Iteration 8433 Loss: 1.397834062576294\n",
      "Iteration 8434 Loss: 1.227373719215393\n",
      "Iteration 8435 Loss: 2.087894916534424\n",
      "Iteration 8436 Loss: 1.8055874109268188\n",
      "Iteration 8437 Loss: 1.190388560295105\n",
      "Iteration 8438 Loss: 1.1591435670852661\n",
      "Iteration 8439 Loss: 1.7087491750717163\n",
      "Iteration 8439 Loss: 1.4496023654937744\n",
      "Iteration 8440 Loss: 1.1995306015014648\n",
      "Iteration 8441 Loss: 1.4137217998504639\n",
      "Iteration 8442 Loss: 1.2575758695602417\n",
      "Iteration 8443 Loss: 1.13206946849823\n",
      "Iteration 8444 Loss: 1.6034901142120361\n",
      "Iteration 8445 Loss: 1.1445584297180176\n",
      "Iteration 8446 Loss: 1.915489912033081\n",
      "Iteration 8447 Loss: 1.319286823272705\n",
      "Iteration 8448 Loss: 1.5197820663452148\n",
      "Iteration 8449 Loss: 1.4170256853103638\n",
      "Iteration 8449 Loss: 1.3922531604766846\n",
      "Iteration 8450 Loss: 1.2799034118652344\n",
      "Iteration 8451 Loss: 1.1394288539886475\n",
      "Iteration 8452 Loss: 1.7897756099700928\n",
      "Iteration 8453 Loss: 1.8115386962890625\n",
      "Iteration 8454 Loss: 1.2892637252807617\n",
      "Iteration 8455 Loss: 1.5880013704299927\n",
      "Iteration 8456 Loss: 1.5686782598495483\n",
      "Iteration 8457 Loss: 2.3518543243408203\n",
      "Iteration 8458 Loss: 1.436703085899353\n",
      "Iteration 8459 Loss: 1.12773597240448\n",
      "Iteration 8459 Loss: 1.5382883548736572\n",
      "Iteration 8460 Loss: 1.6488659381866455\n",
      "Iteration 8461 Loss: 1.2888776063919067\n",
      "Iteration 8462 Loss: 1.1535321474075317\n",
      "Iteration 8463 Loss: 1.334962248802185\n",
      "Iteration 8464 Loss: 1.1828303337097168\n",
      "Iteration 8465 Loss: 1.6023614406585693\n",
      "Iteration 8466 Loss: 1.3161160945892334\n",
      "Iteration 8467 Loss: 1.8159981966018677\n",
      "Iteration 8468 Loss: 1.3803527355194092\n",
      "Iteration 8469 Loss: 1.3276493549346924\n",
      "Iteration 8469 Loss: 1.4051545858383179\n",
      "Iteration 8470 Loss: 1.8472400903701782\n",
      "Iteration 8471 Loss: 1.2357161045074463\n",
      "Iteration 8472 Loss: 1.5056145191192627\n",
      "Iteration 8473 Loss: 1.6532633304595947\n",
      "Iteration 8474 Loss: 1.735015630722046\n",
      "Iteration 8475 Loss: 1.0147130489349365\n",
      "Iteration 8476 Loss: 1.5410491228103638\n",
      "Iteration 8477 Loss: 1.4107623100280762\n",
      "Iteration 8478 Loss: 1.29277503490448\n",
      "Iteration 8479 Loss: 1.122272253036499\n",
      "Iteration 8479 Loss: 1.4358421564102173\n",
      "Iteration 8480 Loss: 1.5094331502914429\n",
      "Iteration 8481 Loss: 1.374691128730774\n",
      "Iteration 8482 Loss: 1.3404382467269897\n",
      "Iteration 8483 Loss: 1.9422855377197266\n",
      "Iteration 8484 Loss: 1.504313588142395\n",
      "Iteration 8485 Loss: 1.404747724533081\n",
      "Iteration 8486 Loss: 1.10503089427948\n",
      "Iteration 8487 Loss: 1.614688754081726\n",
      "Iteration 8488 Loss: 1.4172568321228027\n",
      "Iteration 8489 Loss: 1.1079301834106445\n",
      "Iteration 8489 Loss: 1.432081699371338\n",
      "Iteration 8490 Loss: 1.101596474647522\n",
      "Iteration 8491 Loss: 1.5164388418197632\n",
      "Iteration 8492 Loss: 1.5767602920532227\n",
      "Iteration 8493 Loss: 1.1501219272613525\n",
      "Iteration 8494 Loss: 1.5234136581420898\n",
      "Iteration 8495 Loss: 1.5177273750305176\n",
      "Iteration 8496 Loss: 1.2783269882202148\n",
      "Iteration 8497 Loss: 1.424027919769287\n",
      "Iteration 8498 Loss: 1.7672839164733887\n",
      "Iteration 8499 Loss: 1.4590660333633423\n",
      "Iteration 8499 Loss: 1.4314762353897095\n",
      "Iteration 8500 Loss: 1.249728798866272\n",
      "Iteration 8501 Loss: 1.3389723300933838\n",
      "Iteration 8502 Loss: 0.9742677807807922\n",
      "Iteration 8503 Loss: 1.068107008934021\n",
      "Iteration 8504 Loss: 1.7463948726654053\n",
      "Iteration 8505 Loss: 1.3729815483093262\n",
      "Iteration 8506 Loss: 1.5311897993087769\n",
      "Iteration 8507 Loss: 1.4994432926177979\n",
      "Iteration 8508 Loss: 1.3788999319076538\n",
      "Iteration 8509 Loss: 1.5354079008102417\n",
      "Iteration 8509 Loss: 1.3695393800735474\n",
      "Iteration 8510 Loss: 1.5507367849349976\n",
      "Iteration 8511 Loss: 1.2553038597106934\n",
      "Iteration 8512 Loss: 1.179708480834961\n",
      "Iteration 8513 Loss: 1.482132911682129\n",
      "Iteration 8514 Loss: 1.5219357013702393\n",
      "Iteration 8515 Loss: 1.108535885810852\n",
      "Iteration 8516 Loss: 1.6763962507247925\n",
      "Iteration 8517 Loss: 1.469063401222229\n",
      "Iteration 8518 Loss: 1.3743349313735962\n",
      "Iteration 8519 Loss: 1.2008228302001953\n",
      "Iteration 8519 Loss: 1.3818970918655396\n",
      "Iteration 8520 Loss: 1.2227050065994263\n",
      "Iteration 8521 Loss: 1.5691794157028198\n",
      "Iteration 8522 Loss: 1.540848970413208\n",
      "Iteration 8523 Loss: 1.2969249486923218\n",
      "Iteration 8524 Loss: 1.7010066509246826\n",
      "Iteration 8525 Loss: 1.5204899311065674\n",
      "Iteration 8526 Loss: 1.5816391706466675\n",
      "Iteration 8527 Loss: 1.3120951652526855\n",
      "Iteration 8528 Loss: 1.542531967163086\n",
      "Iteration 8529 Loss: 1.1644045114517212\n",
      "Iteration 8529 Loss: 1.4451825618743896\n",
      "Iteration 8530 Loss: 1.1903164386749268\n",
      "Iteration 8531 Loss: 1.072904348373413\n",
      "Iteration 8532 Loss: 1.5246317386627197\n",
      "Iteration 8533 Loss: 1.6078369617462158\n",
      "Iteration 8534 Loss: 1.514635443687439\n",
      "Iteration 8535 Loss: 1.1177774667739868\n",
      "Iteration 8536 Loss: 1.5723892450332642\n",
      "Iteration 8537 Loss: 1.3463373184204102\n",
      "Iteration 8538 Loss: 1.4164358377456665\n",
      "Iteration 8539 Loss: 1.4525952339172363\n",
      "Iteration 8539 Loss: 1.381585955619812\n",
      "Iteration 8540 Loss: 1.0756149291992188\n",
      "Iteration 8541 Loss: 1.527069330215454\n",
      "Iteration 8542 Loss: 1.7116451263427734\n",
      "Iteration 8543 Loss: 1.2296305894851685\n",
      "Iteration 8544 Loss: 1.0504766702651978\n",
      "Iteration 8545 Loss: 1.2672746181488037\n",
      "Iteration 8546 Loss: 1.24946129322052\n",
      "Iteration 8547 Loss: 1.7032992839813232\n",
      "Iteration 8548 Loss: 1.1836754083633423\n",
      "Iteration 8549 Loss: 1.305134654045105\n",
      "Iteration 8549 Loss: 1.3303282260894775\n",
      "Iteration 8550 Loss: 1.045487403869629\n",
      "Iteration 8551 Loss: 1.0510417222976685\n",
      "Iteration 8552 Loss: 1.623840570449829\n",
      "Iteration 8553 Loss: 1.4273552894592285\n",
      "Iteration 8554 Loss: 1.1703935861587524\n",
      "Iteration 8555 Loss: 1.0760856866836548\n",
      "Iteration 8556 Loss: 1.3288079500198364\n",
      "Iteration 8557 Loss: 1.4220980405807495\n",
      "Iteration 8558 Loss: 1.4533445835113525\n",
      "Iteration 8559 Loss: 1.5545306205749512\n",
      "Iteration 8559 Loss: 1.3152985572814941\n",
      "Iteration 8560 Loss: 1.143607497215271\n",
      "Iteration 8561 Loss: 1.6070886850357056\n",
      "Iteration 8562 Loss: 1.757140874862671\n",
      "Iteration 8563 Loss: 1.2117842435836792\n",
      "Iteration 8564 Loss: 1.2826203107833862\n",
      "Iteration 8565 Loss: 1.3006536960601807\n",
      "Iteration 8566 Loss: 1.5502889156341553\n",
      "Iteration 8567 Loss: 1.2810115814208984\n",
      "Iteration 8568 Loss: 1.3348416090011597\n",
      "Iteration 8569 Loss: 1.5810356140136719\n",
      "Iteration 8569 Loss: 1.4050073623657227\n",
      "Iteration 8570 Loss: 1.7844758033752441\n",
      "Iteration 8571 Loss: 1.9017964601516724\n",
      "Iteration 8572 Loss: 1.4894540309906006\n",
      "Iteration 8573 Loss: 1.9502652883529663\n",
      "Iteration 8574 Loss: 1.2836060523986816\n",
      "Iteration 8575 Loss: 1.4963539838790894\n",
      "Iteration 8576 Loss: 1.6146403551101685\n",
      "Iteration 8577 Loss: 1.3311505317687988\n",
      "Iteration 8578 Loss: 1.768660545349121\n",
      "Iteration 8579 Loss: 1.1080412864685059\n",
      "Iteration 8579 Loss: 1.5728442668914795\n",
      "Iteration 8580 Loss: 1.32185697555542\n",
      "Iteration 8581 Loss: 1.6139050722122192\n",
      "Iteration 8582 Loss: 1.349265217781067\n",
      "Iteration 8583 Loss: 1.1808054447174072\n",
      "Iteration 8584 Loss: 1.1400742530822754\n",
      "Iteration 8585 Loss: 1.3669867515563965\n",
      "Iteration 8586 Loss: 1.859025001525879\n",
      "Iteration 8587 Loss: 1.8495326042175293\n",
      "Iteration 8588 Loss: 1.356924295425415\n",
      "Iteration 8589 Loss: 1.4770894050598145\n",
      "Iteration 8589 Loss: 1.4515464305877686\n",
      "Iteration 8590 Loss: 1.5072424411773682\n",
      "Iteration 8591 Loss: 1.3706634044647217\n",
      "Iteration 8592 Loss: 1.3042223453521729\n",
      "Iteration 8593 Loss: 1.8823270797729492\n",
      "Iteration 8594 Loss: 1.178652286529541\n",
      "Iteration 8595 Loss: 1.4485032558441162\n",
      "Iteration 8596 Loss: 1.6176949739456177\n",
      "Iteration 8597 Loss: 1.4015215635299683\n",
      "Iteration 8598 Loss: 1.5592312812805176\n",
      "Iteration 8599 Loss: 1.300966739654541\n",
      "Iteration 8599 Loss: 1.4571025371551514\n",
      "Iteration 8600 Loss: 1.434136152267456\n",
      "Iteration 8601 Loss: 1.6237775087356567\n",
      "Iteration 8602 Loss: 1.2483731508255005\n",
      "Iteration 8603 Loss: 1.2694073915481567\n",
      "Iteration 8604 Loss: 1.2378321886062622\n",
      "Iteration 8605 Loss: 1.2183635234832764\n",
      "Iteration 8606 Loss: 1.38176429271698\n",
      "Iteration 8607 Loss: 1.0808706283569336\n",
      "Iteration 8608 Loss: 1.3735361099243164\n",
      "Iteration 8609 Loss: 1.3074009418487549\n",
      "Iteration 8609 Loss: 1.3175461292266846\n",
      "Iteration 8610 Loss: 1.332624077796936\n",
      "Iteration 8611 Loss: 1.3743880987167358\n",
      "Iteration 8612 Loss: 1.3675439357757568\n",
      "Iteration 8613 Loss: 0.8933555483818054\n",
      "Iteration 8614 Loss: 1.0266473293304443\n",
      "Iteration 8615 Loss: 1.0286380052566528\n",
      "Iteration 8616 Loss: 1.4225109815597534\n",
      "Iteration 8617 Loss: 1.2126694917678833\n",
      "Iteration 8618 Loss: 1.724014163017273\n",
      "Iteration 8619 Loss: 1.6358813047409058\n",
      "Iteration 8619 Loss: 1.301827311515808\n",
      "Iteration 8620 Loss: 1.7562367916107178\n",
      "Iteration 8621 Loss: 1.3551557064056396\n",
      "Iteration 8622 Loss: 1.0913031101226807\n",
      "Iteration 8623 Loss: 1.5388493537902832\n",
      "Iteration 8624 Loss: 1.03446626663208\n",
      "Iteration 8625 Loss: 1.813986897468567\n",
      "Iteration 8626 Loss: 1.3031409978866577\n",
      "Iteration 8627 Loss: 1.330141544342041\n",
      "Iteration 8628 Loss: 1.6173292398452759\n",
      "Iteration 8629 Loss: 1.3668975830078125\n",
      "Iteration 8629 Loss: 1.420750617980957\n",
      "Iteration 8630 Loss: 1.2544375658035278\n",
      "Iteration 8631 Loss: 1.3612903356552124\n",
      "Iteration 8632 Loss: 1.4055534601211548\n",
      "Iteration 8633 Loss: 1.3267403841018677\n",
      "Iteration 8634 Loss: 1.3609609603881836\n",
      "Iteration 8635 Loss: 1.3183469772338867\n",
      "Iteration 8636 Loss: 1.030504822731018\n",
      "Iteration 8637 Loss: 1.0328879356384277\n",
      "Iteration 8638 Loss: 1.5348196029663086\n",
      "Iteration 8639 Loss: 1.2555586099624634\n",
      "Iteration 8639 Loss: 1.2881101369857788\n",
      "Iteration 8640 Loss: 1.5631860494613647\n",
      "Iteration 8641 Loss: 1.354637861251831\n",
      "Iteration 8642 Loss: 0.8912651538848877\n",
      "Iteration 8643 Loss: 1.081096887588501\n",
      "Iteration 8644 Loss: 0.9298416972160339\n",
      "Iteration 8645 Loss: 1.7158663272857666\n",
      "Iteration 8646 Loss: 1.229910135269165\n",
      "Iteration 8647 Loss: 1.0472038984298706\n",
      "Iteration 8648 Loss: 1.6415480375289917\n",
      "Iteration 8649 Loss: 0.7268690466880798\n",
      "Iteration 8649 Loss: 1.2181423902511597\n",
      "Iteration 8650 Loss: 1.2383190393447876\n",
      "Iteration 8651 Loss: 1.562922477722168\n",
      "Iteration 8652 Loss: 1.6574190855026245\n",
      "Iteration 8653 Loss: 1.6589642763137817\n",
      "Iteration 8654 Loss: 1.4507074356079102\n",
      "Iteration 8655 Loss: 1.4122740030288696\n",
      "Iteration 8656 Loss: 1.0496435165405273\n",
      "Iteration 8657 Loss: 1.4994795322418213\n",
      "Iteration 8658 Loss: 1.1384294033050537\n",
      "Iteration 8659 Loss: 1.3450418710708618\n",
      "Iteration 8659 Loss: 1.4013200998306274\n",
      "Iteration 8660 Loss: 1.4126570224761963\n",
      "Iteration 8661 Loss: 1.2972427606582642\n",
      "Iteration 8662 Loss: 1.4237526655197144\n",
      "Iteration 8663 Loss: 1.1413273811340332\n",
      "Iteration 8664 Loss: 1.6840184926986694\n",
      "Iteration 8665 Loss: 1.6133673191070557\n",
      "Iteration 8666 Loss: 1.0546799898147583\n",
      "Iteration 8667 Loss: 1.680000901222229\n",
      "Iteration 8668 Loss: 1.2483657598495483\n",
      "Iteration 8669 Loss: 1.6834462881088257\n",
      "Iteration 8669 Loss: 1.4238858222961426\n",
      "Iteration 8670 Loss: 1.2209429740905762\n",
      "Iteration 8671 Loss: 2.0868043899536133\n",
      "Iteration 8672 Loss: 1.6566628217697144\n",
      "Iteration 8673 Loss: 1.2563544511795044\n",
      "Iteration 8674 Loss: 1.1239099502563477\n",
      "Iteration 8675 Loss: 1.1429219245910645\n",
      "Iteration 8676 Loss: 1.0255091190338135\n",
      "Iteration 8677 Loss: 1.2592583894729614\n",
      "Iteration 8678 Loss: 1.8201062679290771\n",
      "Iteration 8679 Loss: 1.23526132106781\n",
      "Iteration 8679 Loss: 1.3827730417251587\n",
      "Iteration 8680 Loss: 1.208890676498413\n",
      "Iteration 8681 Loss: 1.365249752998352\n",
      "Iteration 8682 Loss: 1.5621219873428345\n",
      "Iteration 8683 Loss: 1.1811336278915405\n",
      "Iteration 8684 Loss: 1.6289762258529663\n",
      "Iteration 8685 Loss: 1.0450645685195923\n",
      "Iteration 8686 Loss: 1.3122841119766235\n",
      "Iteration 8687 Loss: 0.9801706075668335\n",
      "Iteration 8688 Loss: 1.3362393379211426\n",
      "Iteration 8689 Loss: 1.5792063474655151\n",
      "Iteration 8689 Loss: 1.3199336528778076\n",
      "Iteration 8690 Loss: 1.4091815948486328\n",
      "Iteration 8691 Loss: 1.2910159826278687\n",
      "Iteration 8692 Loss: 1.2170294523239136\n",
      "Iteration 8693 Loss: 1.378125786781311\n",
      "Iteration 8694 Loss: 1.3630677461624146\n",
      "Iteration 8695 Loss: 1.160692811012268\n",
      "Iteration 8696 Loss: 1.2298321723937988\n",
      "Iteration 8697 Loss: 1.494544267654419\n",
      "Iteration 8698 Loss: 1.6314507722854614\n",
      "Iteration 8699 Loss: 1.4881945848464966\n",
      "Iteration 8699 Loss: 1.3663134574890137\n",
      "Iteration 8700 Loss: 1.5022159814834595\n",
      "Iteration 8701 Loss: 1.9148240089416504\n",
      "Iteration 8702 Loss: 1.541348934173584\n",
      "Iteration 8703 Loss: 1.1890759468078613\n",
      "Iteration 8704 Loss: 1.2482914924621582\n",
      "Iteration 8705 Loss: 1.6164557933807373\n",
      "Iteration 8706 Loss: 1.3561792373657227\n",
      "Iteration 8707 Loss: 1.0300239324569702\n",
      "Iteration 8708 Loss: 1.268994927406311\n",
      "Iteration 8709 Loss: 0.9045065641403198\n",
      "Iteration 8709 Loss: 1.3571916818618774\n",
      "Iteration 8710 Loss: 2.0186686515808105\n",
      "Iteration 8711 Loss: 1.4696016311645508\n",
      "Iteration 8712 Loss: 1.3252811431884766\n",
      "Iteration 8713 Loss: 1.453131914138794\n",
      "Iteration 8714 Loss: 1.449078917503357\n",
      "Iteration 8715 Loss: 1.5293571949005127\n",
      "Iteration 8716 Loss: 1.5035529136657715\n",
      "Iteration 8717 Loss: 1.7461647987365723\n",
      "Iteration 8718 Loss: 1.4835795164108276\n",
      "Iteration 8719 Loss: 1.3324730396270752\n",
      "Iteration 8719 Loss: 1.5310888290405273\n",
      "Iteration 8720 Loss: 1.3969776630401611\n",
      "Iteration 8721 Loss: 1.4065830707550049\n",
      "Iteration 8722 Loss: 1.8194410800933838\n",
      "Iteration 8723 Loss: 1.2926281690597534\n",
      "Iteration 8724 Loss: 1.6727874279022217\n",
      "Iteration 8725 Loss: 1.027198076248169\n",
      "Iteration 8726 Loss: 1.4934492111206055\n",
      "Iteration 8727 Loss: 1.3665142059326172\n",
      "Iteration 8728 Loss: 1.6008789539337158\n",
      "Iteration 8729 Loss: 1.7010339498519897\n",
      "Iteration 8729 Loss: 1.4777491092681885\n",
      "Iteration 8730 Loss: 1.4804049730300903\n",
      "Iteration 8731 Loss: 1.4989980459213257\n",
      "Iteration 8732 Loss: 1.4527652263641357\n",
      "Iteration 8733 Loss: 1.1263247728347778\n",
      "Iteration 8734 Loss: 1.3297383785247803\n",
      "Iteration 8735 Loss: 1.701373815536499\n",
      "Iteration 8736 Loss: 1.345871090888977\n",
      "Iteration 8737 Loss: 1.191575050354004\n",
      "Iteration 8738 Loss: 1.3658487796783447\n",
      "Iteration 8739 Loss: 1.3048759698867798\n",
      "Iteration 8739 Loss: 1.3797776699066162\n",
      "Iteration 8740 Loss: 1.1205148696899414\n",
      "Iteration 8741 Loss: 1.1590477228164673\n",
      "Iteration 8742 Loss: 1.4297809600830078\n",
      "Iteration 8743 Loss: 1.4829543828964233\n",
      "Iteration 8744 Loss: 1.1667646169662476\n",
      "Iteration 8745 Loss: 1.245928406715393\n",
      "Iteration 8746 Loss: 1.2708430290222168\n",
      "Iteration 8747 Loss: 1.7044422626495361\n",
      "Iteration 8748 Loss: 0.947102963924408\n",
      "Iteration 8749 Loss: 1.5027610063552856\n",
      "Iteration 8749 Loss: 1.3030140399932861\n",
      "Iteration 8750 Loss: 1.345818281173706\n",
      "Iteration 8751 Loss: 1.8183985948562622\n",
      "Iteration 8752 Loss: 1.6646782159805298\n",
      "Iteration 8753 Loss: 1.1255812644958496\n",
      "Iteration 8754 Loss: 1.7755205631256104\n",
      "Iteration 8755 Loss: 1.3141870498657227\n",
      "Iteration 8756 Loss: 1.109001874923706\n",
      "Iteration 8757 Loss: 1.2825291156768799\n",
      "Iteration 8758 Loss: 1.235548973083496\n",
      "Iteration 8759 Loss: 1.3313956260681152\n",
      "Iteration 8759 Loss: 1.4002659320831299\n",
      "Iteration 8760 Loss: 1.1648210287094116\n",
      "Iteration 8761 Loss: 1.0395835638046265\n",
      "Iteration 8762 Loss: 0.9345647096633911\n",
      "Iteration 8763 Loss: 1.1876038312911987\n",
      "Iteration 8764 Loss: 1.2351412773132324\n",
      "Iteration 8765 Loss: 1.6978800296783447\n",
      "Iteration 8766 Loss: 1.418746829032898\n",
      "Iteration 8767 Loss: 1.424017310142517\n",
      "Iteration 8768 Loss: 1.5085726976394653\n",
      "Iteration 8769 Loss: 1.5621377229690552\n",
      "Iteration 8769 Loss: 1.3173068761825562\n",
      "Iteration 8770 Loss: 1.5209211111068726\n",
      "Iteration 8771 Loss: 1.5283814668655396\n",
      "Iteration 8772 Loss: 1.1551003456115723\n",
      "Iteration 8773 Loss: 1.253258466720581\n",
      "Iteration 8774 Loss: 1.277972936630249\n",
      "Iteration 8775 Loss: 1.5716103315353394\n",
      "Iteration 8776 Loss: 1.4527013301849365\n",
      "Iteration 8777 Loss: 1.4194114208221436\n",
      "Iteration 8778 Loss: 1.7265228033065796\n",
      "Iteration 8779 Loss: 1.2962276935577393\n",
      "Iteration 8779 Loss: 1.420210838317871\n",
      "Iteration 8780 Loss: 1.1892788410186768\n",
      "Iteration 8781 Loss: 1.4351837635040283\n",
      "Iteration 8782 Loss: 1.7943038940429688\n",
      "Iteration 8783 Loss: 1.6200042963027954\n",
      "Iteration 8784 Loss: 1.2728019952774048\n",
      "Iteration 8785 Loss: 1.2946499586105347\n",
      "Iteration 8786 Loss: 1.2494736909866333\n",
      "Iteration 8787 Loss: 1.3146512508392334\n",
      "Iteration 8788 Loss: 0.9148381948471069\n",
      "Iteration 8789 Loss: 1.672305941581726\n",
      "Iteration 8789 Loss: 1.3757493495941162\n",
      "Iteration 8790 Loss: 1.7337141036987305\n",
      "Iteration 8791 Loss: 1.5763336420059204\n",
      "Iteration 8792 Loss: 1.333742380142212\n",
      "Iteration 8793 Loss: 1.2890983819961548\n",
      "Iteration 8794 Loss: 1.107007622718811\n",
      "Iteration 8795 Loss: 0.8955508470535278\n",
      "Iteration 8796 Loss: 1.2454410791397095\n",
      "Iteration 8797 Loss: 1.2678313255310059\n",
      "Iteration 8798 Loss: 1.6998251676559448\n",
      "Iteration 8799 Loss: 1.243557095527649\n",
      "Iteration 8799 Loss: 1.3392102718353271\n",
      "Iteration 8800 Loss: 1.3892531394958496\n",
      "Iteration 8801 Loss: 1.9935282468795776\n",
      "Iteration 8802 Loss: 1.0105195045471191\n",
      "Iteration 8803 Loss: 0.796272873878479\n",
      "Iteration 8804 Loss: 1.2197986841201782\n",
      "Iteration 8805 Loss: 1.3568260669708252\n",
      "Iteration 8806 Loss: 1.1374467611312866\n",
      "Iteration 8807 Loss: 0.9008259177207947\n",
      "Iteration 8808 Loss: 1.4494739770889282\n",
      "Iteration 8809 Loss: 1.57832932472229\n",
      "Iteration 8809 Loss: 1.2832274436950684\n",
      "Iteration 8810 Loss: 1.559600591659546\n",
      "Iteration 8811 Loss: 1.209679365158081\n",
      "Iteration 8812 Loss: 1.0504344701766968\n",
      "Iteration 8813 Loss: 1.5986263751983643\n",
      "Iteration 8814 Loss: 1.1823832988739014\n",
      "Iteration 8815 Loss: 1.5603549480438232\n",
      "Iteration 8816 Loss: 1.6359169483184814\n",
      "Iteration 8817 Loss: 1.1732569932937622\n",
      "Iteration 8818 Loss: 1.2290071249008179\n",
      "Iteration 8819 Loss: 1.5221800804138184\n",
      "Iteration 8819 Loss: 1.372144103050232\n",
      "Iteration 8820 Loss: 1.3506903648376465\n",
      "Iteration 8821 Loss: 1.4642977714538574\n",
      "Iteration 8822 Loss: 1.3298369646072388\n",
      "Iteration 8823 Loss: 0.720172643661499\n",
      "Iteration 8824 Loss: 1.4178802967071533\n",
      "Iteration 8825 Loss: 1.3242648839950562\n",
      "Iteration 8826 Loss: 1.8032095432281494\n",
      "Iteration 8827 Loss: 1.6471234560012817\n",
      "Iteration 8828 Loss: 1.5400786399841309\n",
      "Iteration 8829 Loss: 1.278485894203186\n",
      "Iteration 8829 Loss: 1.387603998184204\n",
      "Iteration 8830 Loss: 1.786952257156372\n",
      "Iteration 8831 Loss: 1.545120358467102\n",
      "Iteration 8832 Loss: 1.2442690134048462\n",
      "Iteration 8833 Loss: 1.5969014167785645\n",
      "Iteration 8834 Loss: 1.3802143335342407\n",
      "Iteration 8835 Loss: 1.4337278604507446\n",
      "Iteration 8836 Loss: 1.3312586545944214\n",
      "Iteration 8837 Loss: 1.184249997138977\n",
      "Iteration 8838 Loss: 1.395898699760437\n",
      "Iteration 8839 Loss: 1.647183895111084\n",
      "Iteration 8839 Loss: 1.4545776844024658\n",
      "Iteration 8840 Loss: 0.9936387538909912\n",
      "Iteration 8841 Loss: 1.3326733112335205\n",
      "Iteration 8842 Loss: 1.9923877716064453\n",
      "Iteration 8843 Loss: 1.3966599702835083\n",
      "Iteration 8844 Loss: 1.3214852809906006\n",
      "Iteration 8845 Loss: 0.7932885885238647\n",
      "Iteration 8846 Loss: 1.2646604776382446\n",
      "Iteration 8847 Loss: 0.8285289406776428\n",
      "Iteration 8848 Loss: 1.2507438659667969\n",
      "Iteration 8849 Loss: 1.1280570030212402\n",
      "Iteration 8849 Loss: 1.2302124500274658\n",
      "Iteration 8850 Loss: 1.5463933944702148\n",
      "Iteration 8851 Loss: 1.4272009134292603\n",
      "Iteration 8852 Loss: 1.2026498317718506\n",
      "Iteration 8853 Loss: 1.579581618309021\n",
      "Iteration 8854 Loss: 1.443562626838684\n",
      "Iteration 8855 Loss: 1.3260170221328735\n",
      "Iteration 8856 Loss: 1.9232416152954102\n",
      "Iteration 8857 Loss: 1.741385579109192\n",
      "Iteration 8858 Loss: 1.5725960731506348\n",
      "Iteration 8859 Loss: 1.0300731658935547\n",
      "Iteration 8859 Loss: 1.4792702198028564\n",
      "Iteration 8860 Loss: 1.499077320098877\n",
      "Iteration 8861 Loss: 1.485907793045044\n",
      "Iteration 8862 Loss: 1.0694445371627808\n",
      "Iteration 8863 Loss: 1.4021447896957397\n",
      "Iteration 8864 Loss: 0.9526681900024414\n",
      "Iteration 8865 Loss: 1.791988730430603\n",
      "Iteration 8866 Loss: 2.0034730434417725\n",
      "Iteration 8867 Loss: 1.102730393409729\n",
      "Iteration 8868 Loss: 1.3936222791671753\n",
      "Iteration 8869 Loss: 1.4042826890945435\n",
      "Iteration 8869 Loss: 1.4105340242385864\n",
      "Iteration 8870 Loss: 1.1568204164505005\n",
      "Iteration 8871 Loss: 1.3330689668655396\n",
      "Iteration 8872 Loss: 1.7054979801177979\n",
      "Iteration 8873 Loss: 1.2183960676193237\n",
      "Iteration 8874 Loss: 1.5284868478775024\n",
      "Iteration 8875 Loss: 2.144490957260132\n",
      "Iteration 8876 Loss: 1.4109315872192383\n",
      "Iteration 8877 Loss: 1.1122626066207886\n",
      "Iteration 8878 Loss: 1.421690583229065\n",
      "Iteration 8879 Loss: 1.3313708305358887\n",
      "Iteration 8879 Loss: 1.4363017082214355\n",
      "Iteration 8880 Loss: 1.71248197555542\n",
      "Iteration 8881 Loss: 1.7409738302230835\n",
      "Iteration 8882 Loss: 1.6370818614959717\n",
      "Iteration 8883 Loss: 0.9833483695983887\n",
      "Iteration 8884 Loss: 1.4139217138290405\n",
      "Iteration 8885 Loss: 1.7628711462020874\n",
      "Iteration 8886 Loss: 1.0990804433822632\n",
      "Iteration 8887 Loss: 1.3472756147384644\n",
      "Iteration 8888 Loss: 1.3748161792755127\n",
      "Iteration 8889 Loss: 1.682946801185608\n",
      "Iteration 8889 Loss: 1.4754797220230103\n",
      "Iteration 8890 Loss: 1.470677375793457\n",
      "Iteration 8891 Loss: 1.7890973091125488\n",
      "Iteration 8892 Loss: 1.5915791988372803\n",
      "Iteration 8893 Loss: 1.6297377347946167\n",
      "Iteration 8894 Loss: 0.9799726009368896\n",
      "Iteration 8895 Loss: 1.432005524635315\n",
      "Iteration 8896 Loss: 1.0934340953826904\n",
      "Iteration 8897 Loss: 1.279442310333252\n",
      "Iteration 8898 Loss: 1.5447872877120972\n",
      "Iteration 8899 Loss: 1.6186150312423706\n",
      "Iteration 8899 Loss: 1.4429349899291992\n",
      "Iteration 8900 Loss: 1.5776770114898682\n",
      "Iteration 8901 Loss: 1.6900030374526978\n",
      "Iteration 8902 Loss: 1.2862341403961182\n",
      "Iteration 8903 Loss: 1.5165318250656128\n",
      "Iteration 8904 Loss: 1.4794865846633911\n",
      "Iteration 8905 Loss: 1.9844753742218018\n",
      "Iteration 8906 Loss: 1.8692680597305298\n",
      "Iteration 8907 Loss: 1.3511741161346436\n",
      "Iteration 8908 Loss: 1.2024157047271729\n",
      "Iteration 8909 Loss: 1.1341336965560913\n",
      "Iteration 8909 Loss: 1.5091400146484375\n",
      "Iteration 8910 Loss: 1.620048999786377\n",
      "Iteration 8911 Loss: 0.7966575622558594\n",
      "Iteration 8912 Loss: 1.1128909587860107\n",
      "Iteration 8913 Loss: 1.8600257635116577\n",
      "Iteration 8914 Loss: 1.0135471820831299\n",
      "Iteration 8915 Loss: 1.5673282146453857\n",
      "Iteration 8916 Loss: 1.270047664642334\n",
      "Iteration 8917 Loss: 1.3872698545455933\n",
      "Iteration 8918 Loss: 1.3703662157058716\n",
      "Iteration 8919 Loss: 1.1319124698638916\n",
      "Iteration 8919 Loss: 1.3130096197128296\n",
      "Iteration 8920 Loss: 1.0984647274017334\n",
      "Iteration 8921 Loss: 1.3563902378082275\n",
      "Iteration 8922 Loss: 1.0472201108932495\n",
      "Iteration 8923 Loss: 1.2785969972610474\n",
      "Iteration 8924 Loss: 1.412151575088501\n",
      "Iteration 8925 Loss: 1.525974154472351\n",
      "Iteration 8926 Loss: 1.5803747177124023\n",
      "Iteration 8927 Loss: 1.708695411682129\n",
      "Iteration 8928 Loss: 1.4860843420028687\n",
      "Iteration 8929 Loss: 1.4391868114471436\n",
      "Iteration 8929 Loss: 1.3933138847351074\n",
      "Iteration 8930 Loss: 1.446028470993042\n",
      "Iteration 8931 Loss: 1.2996586561203003\n",
      "Iteration 8932 Loss: 0.9854748845100403\n",
      "Iteration 8933 Loss: 1.4045833349227905\n",
      "Iteration 8934 Loss: 1.5925359725952148\n",
      "Iteration 8935 Loss: 1.426141381263733\n",
      "Iteration 8936 Loss: 1.265200138092041\n",
      "Iteration 8937 Loss: 1.344639539718628\n",
      "Iteration 8938 Loss: 1.3994591236114502\n",
      "Iteration 8939 Loss: 0.6279198527336121\n",
      "Iteration 8939 Loss: 1.27916419506073\n",
      "Iteration 8940 Loss: 1.5127067565917969\n",
      "Iteration 8941 Loss: 1.3322843313217163\n",
      "Iteration 8942 Loss: 1.5422637462615967\n",
      "Iteration 8943 Loss: 1.810481309890747\n",
      "Iteration 8944 Loss: 0.960589587688446\n",
      "Iteration 8945 Loss: 1.2571237087249756\n",
      "Iteration 8946 Loss: 1.1791313886642456\n",
      "Iteration 8947 Loss: 1.691398024559021\n",
      "Iteration 8948 Loss: 1.374779462814331\n",
      "Iteration 8949 Loss: 1.4691838026046753\n",
      "Iteration 8949 Loss: 1.412994146347046\n",
      "Iteration 8950 Loss: 1.315772294998169\n",
      "Iteration 8951 Loss: 1.0753583908081055\n",
      "Iteration 8952 Loss: 1.4911035299301147\n",
      "Iteration 8953 Loss: 1.3308802843093872\n",
      "Iteration 8954 Loss: 1.2140103578567505\n",
      "Iteration 8955 Loss: 1.4833054542541504\n",
      "Iteration 8956 Loss: 1.5133305788040161\n",
      "Iteration 8957 Loss: 1.485853910446167\n",
      "Iteration 8958 Loss: 1.4581551551818848\n",
      "Iteration 8959 Loss: 1.3852250576019287\n",
      "Iteration 8959 Loss: 1.3752995729446411\n",
      "Iteration 8960 Loss: 1.3758803606033325\n",
      "Iteration 8961 Loss: 1.1693438291549683\n",
      "Iteration 8962 Loss: 1.5606532096862793\n",
      "Iteration 8963 Loss: 1.470029592514038\n",
      "Iteration 8964 Loss: 1.34251868724823\n",
      "Iteration 8965 Loss: 1.1812478303909302\n",
      "Iteration 8966 Loss: 1.5321190357208252\n",
      "Iteration 8967 Loss: 1.3242731094360352\n",
      "Iteration 8968 Loss: 1.5995779037475586\n",
      "Iteration 8969 Loss: 1.2458336353302002\n",
      "Iteration 8969 Loss: 1.3801476955413818\n",
      "Iteration 8970 Loss: 1.4203027486801147\n",
      "Iteration 8971 Loss: 1.616174340248108\n",
      "Iteration 8972 Loss: 1.2647203207015991\n",
      "Iteration 8973 Loss: 1.5190300941467285\n",
      "Iteration 8974 Loss: 1.4617091417312622\n",
      "Iteration 8975 Loss: 1.5269420146942139\n",
      "Iteration 8976 Loss: 1.3082386255264282\n",
      "Iteration 8977 Loss: 1.3940114974975586\n",
      "Iteration 8978 Loss: 1.2774397134780884\n",
      "Iteration 8979 Loss: 1.2415181398391724\n",
      "Iteration 8979 Loss: 1.4030086994171143\n",
      "Iteration 8980 Loss: 1.2060915231704712\n",
      "Iteration 8981 Loss: 1.4205628633499146\n",
      "Iteration 8982 Loss: 1.9157099723815918\n",
      "Iteration 8983 Loss: 1.2001503705978394\n",
      "Iteration 8984 Loss: 1.7478384971618652\n",
      "Iteration 8985 Loss: 1.6144150495529175\n",
      "Iteration 8986 Loss: 1.881286382675171\n",
      "Iteration 8987 Loss: 1.171785831451416\n",
      "Iteration 8988 Loss: 1.1103553771972656\n",
      "Iteration 8989 Loss: 1.2249069213867188\n",
      "Iteration 8989 Loss: 1.449310302734375\n",
      "Iteration 8990 Loss: 1.153923511505127\n",
      "Iteration 8991 Loss: 1.4396729469299316\n",
      "Iteration 8992 Loss: 1.2329984903335571\n",
      "Iteration 8993 Loss: 1.552117109298706\n",
      "Iteration 8994 Loss: 1.3250678777694702\n",
      "Iteration 8995 Loss: 1.5790666341781616\n",
      "Iteration 8996 Loss: 1.6516114473342896\n",
      "Iteration 8997 Loss: 1.5594249963760376\n",
      "Iteration 8998 Loss: 1.61956787109375\n",
      "Iteration 8999 Loss: 1.3947194814682007\n",
      "Iteration 8999 Loss: 1.4508169889450073\n",
      "Iteration 9000 Loss: 1.0876837968826294\n",
      "Iteration 9001 Loss: 1.35735285282135\n",
      "Iteration 9002 Loss: 0.8857687711715698\n",
      "Iteration 9003 Loss: 1.2968895435333252\n",
      "Iteration 9004 Loss: 1.594281792640686\n",
      "Iteration 9005 Loss: 1.6599844694137573\n",
      "Iteration 9006 Loss: 1.1946475505828857\n",
      "Iteration 9007 Loss: 1.6279418468475342\n",
      "Iteration 9008 Loss: 1.1565742492675781\n",
      "Iteration 9009 Loss: 1.0033715963363647\n",
      "Iteration 9009 Loss: 1.286449670791626\n",
      "Iteration 9010 Loss: 1.1819552183151245\n",
      "Iteration 9011 Loss: 1.6097222566604614\n",
      "Iteration 9012 Loss: 1.4779932498931885\n",
      "Iteration 9013 Loss: 0.8979582786560059\n",
      "Iteration 9014 Loss: 1.5402790307998657\n",
      "Iteration 9015 Loss: 1.4769752025604248\n",
      "Iteration 9016 Loss: 1.731028437614441\n",
      "Iteration 9017 Loss: 2.0373613834381104\n",
      "Iteration 9018 Loss: 1.2782783508300781\n",
      "Iteration 9019 Loss: 1.2900594472885132\n",
      "Iteration 9019 Loss: 1.4521610736846924\n",
      "Iteration 9020 Loss: 1.4979913234710693\n",
      "Iteration 9021 Loss: 1.1650320291519165\n",
      "Iteration 9022 Loss: 1.4780436754226685\n",
      "Iteration 9023 Loss: 0.7244496941566467\n",
      "Iteration 9024 Loss: 1.2476447820663452\n",
      "Iteration 9025 Loss: 1.7269933223724365\n",
      "Iteration 9026 Loss: 1.328911542892456\n",
      "Iteration 9027 Loss: 1.0889651775360107\n",
      "Iteration 9028 Loss: 1.492118000984192\n",
      "Iteration 9029 Loss: 1.5546940565109253\n",
      "Iteration 9029 Loss: 1.330484390258789\n",
      "Iteration 9030 Loss: 1.358851432800293\n",
      "Iteration 9031 Loss: 1.3475351333618164\n",
      "Iteration 9032 Loss: 1.2474068403244019\n",
      "Iteration 9033 Loss: 1.0503027439117432\n",
      "Iteration 9034 Loss: 1.550956130027771\n",
      "Iteration 9035 Loss: 1.5468025207519531\n",
      "Iteration 9036 Loss: 1.404557704925537\n",
      "Iteration 9037 Loss: 1.1551830768585205\n",
      "Iteration 9038 Loss: 1.2788848876953125\n",
      "Iteration 9039 Loss: 1.9187129735946655\n",
      "Iteration 9039 Loss: 1.3859193325042725\n",
      "Iteration 9040 Loss: 1.21714448928833\n",
      "Iteration 9041 Loss: 1.6317754983901978\n",
      "Iteration 9042 Loss: 1.0236002206802368\n",
      "Iteration 9043 Loss: 1.1580270528793335\n",
      "Iteration 9044 Loss: 1.8466672897338867\n",
      "Iteration 9045 Loss: 1.779915452003479\n",
      "Iteration 9046 Loss: 1.3752628564834595\n",
      "Iteration 9047 Loss: 1.0709766149520874\n",
      "Iteration 9048 Loss: 1.3698569536209106\n",
      "Iteration 9049 Loss: 1.6086806058883667\n",
      "Iteration 9049 Loss: 1.4081907272338867\n",
      "Iteration 9050 Loss: 1.4291332960128784\n",
      "Iteration 9051 Loss: 1.7263941764831543\n",
      "Iteration 9052 Loss: 1.6506444215774536\n",
      "Iteration 9053 Loss: 1.2753074169158936\n",
      "Iteration 9054 Loss: 1.3793692588806152\n",
      "Iteration 9055 Loss: 1.5788061618804932\n",
      "Iteration 9056 Loss: 1.2648926973342896\n",
      "Iteration 9057 Loss: 1.3671175241470337\n",
      "Iteration 9058 Loss: 1.3662272691726685\n",
      "Iteration 9059 Loss: 1.0058016777038574\n",
      "Iteration 9059 Loss: 1.4043694734573364\n",
      "Iteration 9060 Loss: 1.4711393117904663\n",
      "Iteration 9061 Loss: 0.9269198775291443\n",
      "Iteration 9062 Loss: 1.2558403015136719\n",
      "Iteration 9063 Loss: 1.6133009195327759\n",
      "Iteration 9064 Loss: 1.558996319770813\n",
      "Iteration 9065 Loss: 1.6653493642807007\n",
      "Iteration 9066 Loss: 1.1930633783340454\n",
      "Iteration 9067 Loss: 1.4416134357452393\n",
      "Iteration 9068 Loss: 1.9247488975524902\n",
      "Iteration 9069 Loss: 1.2650877237319946\n",
      "Iteration 9069 Loss: 1.4316059350967407\n",
      "Iteration 9070 Loss: 0.8492951393127441\n",
      "Iteration 9071 Loss: 1.3711574077606201\n",
      "Iteration 9072 Loss: 1.1574949026107788\n",
      "Iteration 9073 Loss: 1.409686803817749\n",
      "Iteration 9074 Loss: 1.4828612804412842\n",
      "Iteration 9075 Loss: 1.7818071842193604\n",
      "Iteration 9076 Loss: 1.8147034645080566\n",
      "Iteration 9077 Loss: 1.5892908573150635\n",
      "Iteration 9078 Loss: 1.300563097000122\n",
      "Iteration 9079 Loss: 1.2993961572647095\n",
      "Iteration 9079 Loss: 1.405625581741333\n",
      "Iteration 9080 Loss: 1.6844598054885864\n",
      "Iteration 9081 Loss: 1.1143516302108765\n",
      "Iteration 9082 Loss: 1.6362113952636719\n",
      "Iteration 9083 Loss: 1.582712173461914\n",
      "Iteration 9084 Loss: 1.1550885438919067\n",
      "Iteration 9085 Loss: 1.8636629581451416\n",
      "Iteration 9086 Loss: 1.4704043865203857\n",
      "Iteration 9087 Loss: 1.2282406091690063\n",
      "Iteration 9088 Loss: 1.3462568521499634\n",
      "Iteration 9089 Loss: 0.8241165280342102\n",
      "Iteration 9089 Loss: 1.3905504941940308\n",
      "Iteration 9090 Loss: 1.161637544631958\n",
      "Iteration 9091 Loss: 1.2351949214935303\n",
      "Iteration 9092 Loss: 1.2711423635482788\n",
      "Iteration 9093 Loss: 1.662637710571289\n",
      "Iteration 9094 Loss: 1.5180814266204834\n",
      "Iteration 9095 Loss: 1.440209984779358\n",
      "Iteration 9096 Loss: 1.5307276248931885\n",
      "Iteration 9097 Loss: 1.8896692991256714\n",
      "Iteration 9098 Loss: 1.015902042388916\n",
      "Iteration 9099 Loss: 1.0552374124526978\n",
      "Iteration 9099 Loss: 1.3780440092086792\n",
      "Iteration 9100 Loss: 1.2780746221542358\n",
      "Iteration 9101 Loss: 1.1454341411590576\n",
      "Iteration 9102 Loss: 1.7336130142211914\n",
      "Iteration 9103 Loss: 1.109186053276062\n",
      "Iteration 9104 Loss: 1.1808466911315918\n",
      "Iteration 9105 Loss: 1.6821397542953491\n",
      "Iteration 9106 Loss: 1.6618189811706543\n",
      "Iteration 9107 Loss: 1.7137740850448608\n",
      "Iteration 9108 Loss: 1.2434529066085815\n",
      "Iteration 9109 Loss: 1.6398693323135376\n",
      "Iteration 9109 Loss: 1.4388208389282227\n",
      "Iteration 9110 Loss: 1.2611253261566162\n",
      "Iteration 9111 Loss: 1.647204041481018\n",
      "Iteration 9112 Loss: 1.6255875825881958\n",
      "Iteration 9113 Loss: 1.1157463788986206\n",
      "Iteration 9114 Loss: 2.0079658031463623\n",
      "Iteration 9115 Loss: 1.7956818342208862\n",
      "Iteration 9116 Loss: 1.441380262374878\n",
      "Iteration 9117 Loss: 1.4264931678771973\n",
      "Iteration 9118 Loss: 1.141351342201233\n",
      "Iteration 9119 Loss: 0.9657310843467712\n",
      "Iteration 9119 Loss: 1.4428266286849976\n",
      "Iteration 9120 Loss: 1.5253329277038574\n",
      "Iteration 9121 Loss: 1.8311854600906372\n",
      "Iteration 9122 Loss: 1.1765189170837402\n",
      "Iteration 9123 Loss: 1.177873969078064\n",
      "Iteration 9124 Loss: 1.1832685470581055\n",
      "Iteration 9125 Loss: 1.7006049156188965\n",
      "Iteration 9126 Loss: 1.696901798248291\n",
      "Iteration 9127 Loss: 1.716036081314087\n",
      "Iteration 9128 Loss: 1.3039836883544922\n",
      "Iteration 9129 Loss: 1.1616889238357544\n",
      "Iteration 9129 Loss: 1.447339415550232\n",
      "Iteration 9130 Loss: 1.11748468875885\n",
      "Iteration 9131 Loss: 1.633461356163025\n",
      "Iteration 9132 Loss: 1.468040943145752\n",
      "Iteration 9133 Loss: 1.3814926147460938\n",
      "Iteration 9134 Loss: 1.5007445812225342\n",
      "Iteration 9135 Loss: 0.9371802806854248\n",
      "Iteration 9136 Loss: 1.1775323152542114\n",
      "Iteration 9137 Loss: 1.2400133609771729\n",
      "Iteration 9138 Loss: 1.5704033374786377\n",
      "Iteration 9139 Loss: 1.3019102811813354\n",
      "Iteration 9139 Loss: 1.3328263759613037\n",
      "Iteration 9140 Loss: 1.7767159938812256\n",
      "Iteration 9141 Loss: 1.0962042808532715\n",
      "Iteration 9142 Loss: 1.306820034980774\n",
      "Iteration 9143 Loss: 1.192413330078125\n",
      "Iteration 9144 Loss: 1.4110727310180664\n",
      "Iteration 9145 Loss: 1.2167854309082031\n",
      "Iteration 9146 Loss: 1.3028459548950195\n",
      "Iteration 9147 Loss: 1.8024967908859253\n",
      "Iteration 9148 Loss: 1.2400469779968262\n",
      "Iteration 9149 Loss: 1.1858078241348267\n",
      "Iteration 9149 Loss: 1.353121042251587\n",
      "Iteration 9150 Loss: 1.5786614418029785\n",
      "Iteration 9151 Loss: 1.9543697834014893\n",
      "Iteration 9152 Loss: 1.6577296257019043\n",
      "Iteration 9153 Loss: 1.5128587484359741\n",
      "Iteration 9154 Loss: 1.515484094619751\n",
      "Iteration 9155 Loss: 1.22115957736969\n",
      "Iteration 9156 Loss: 1.0567151308059692\n",
      "Iteration 9157 Loss: 1.2842775583267212\n",
      "Iteration 9158 Loss: 1.4029171466827393\n",
      "Iteration 9159 Loss: 1.862236738204956\n",
      "Iteration 9159 Loss: 1.504641056060791\n",
      "Iteration 9160 Loss: 1.2078020572662354\n",
      "Iteration 9161 Loss: 1.2554970979690552\n",
      "Iteration 9162 Loss: 1.400976538658142\n",
      "Iteration 9163 Loss: 1.5098942518234253\n",
      "Iteration 9164 Loss: 1.4603835344314575\n",
      "Iteration 9165 Loss: 1.4040806293487549\n",
      "Iteration 9166 Loss: 1.302642583847046\n",
      "Iteration 9167 Loss: 1.6742606163024902\n",
      "Iteration 9168 Loss: 0.9735367298126221\n",
      "Iteration 9169 Loss: 1.4374330043792725\n",
      "Iteration 9169 Loss: 1.3626506328582764\n",
      "Iteration 9170 Loss: 1.4336040019989014\n",
      "Iteration 9171 Loss: 1.170116901397705\n",
      "Iteration 9172 Loss: 1.3091883659362793\n",
      "Iteration 9173 Loss: 1.422661542892456\n",
      "Iteration 9174 Loss: 1.7227299213409424\n",
      "Iteration 9175 Loss: 1.3190279006958008\n",
      "Iteration 9176 Loss: 1.3406260013580322\n",
      "Iteration 9177 Loss: 1.6309751272201538\n",
      "Iteration 9178 Loss: 1.6392780542373657\n",
      "Iteration 9179 Loss: 1.2159630060195923\n",
      "Iteration 9179 Loss: 1.420417070388794\n",
      "Iteration 9180 Loss: 1.3264250755310059\n",
      "Iteration 9181 Loss: 1.344365119934082\n",
      "Iteration 9182 Loss: 1.3324459791183472\n",
      "Iteration 9183 Loss: 1.472936749458313\n",
      "Iteration 9184 Loss: 1.4852656126022339\n",
      "Iteration 9185 Loss: 1.0965933799743652\n",
      "Iteration 9186 Loss: 1.0625478029251099\n",
      "Iteration 9187 Loss: 1.2648987770080566\n",
      "Iteration 9188 Loss: 1.7581082582473755\n",
      "Iteration 9189 Loss: 1.6469236612319946\n",
      "Iteration 9189 Loss: 1.3790509700775146\n",
      "Iteration 9190 Loss: 1.4316288232803345\n",
      "Iteration 9191 Loss: 1.3172606229782104\n",
      "Iteration 9192 Loss: 1.5143409967422485\n",
      "Iteration 9193 Loss: 1.3936102390289307\n",
      "Iteration 9194 Loss: 1.0265922546386719\n",
      "Iteration 9195 Loss: 1.2339247465133667\n",
      "Iteration 9196 Loss: 1.4318242073059082\n",
      "Iteration 9197 Loss: 1.7223390340805054\n",
      "Iteration 9198 Loss: 1.1969846487045288\n",
      "Iteration 9199 Loss: 1.447860836982727\n",
      "Iteration 9199 Loss: 1.3716366291046143\n",
      "Iteration 9200 Loss: 1.167231798171997\n",
      "Iteration 9201 Loss: 1.759609341621399\n",
      "Iteration 9202 Loss: 1.1472548246383667\n",
      "Iteration 9203 Loss: 1.1762715578079224\n",
      "Iteration 9204 Loss: 0.9838493466377258\n",
      "Iteration 9205 Loss: 1.3780040740966797\n",
      "Iteration 9206 Loss: 0.9585219621658325\n",
      "Iteration 9207 Loss: 1.9028323888778687\n",
      "Iteration 9208 Loss: 1.086426019668579\n",
      "Iteration 9209 Loss: 1.6419479846954346\n",
      "Iteration 9209 Loss: 1.320194959640503\n",
      "Iteration 9210 Loss: 1.229907512664795\n",
      "Iteration 9211 Loss: 1.5966551303863525\n",
      "Iteration 9212 Loss: 1.3451032638549805\n",
      "Iteration 9213 Loss: 1.4838045835494995\n",
      "Iteration 9214 Loss: 1.1280381679534912\n",
      "Iteration 9215 Loss: 1.426754117012024\n",
      "Iteration 9216 Loss: 1.473338007926941\n",
      "Iteration 9217 Loss: 1.632407307624817\n",
      "Iteration 9218 Loss: 1.3052829504013062\n",
      "Iteration 9219 Loss: 1.3814352750778198\n",
      "Iteration 9219 Loss: 1.4002726078033447\n",
      "Iteration 9220 Loss: 1.2505054473876953\n",
      "Iteration 9221 Loss: 1.24203360080719\n",
      "Iteration 9222 Loss: 1.5859581232070923\n",
      "Iteration 9223 Loss: 0.8899073004722595\n",
      "Iteration 9224 Loss: 1.1945767402648926\n",
      "Iteration 9225 Loss: 1.4730010032653809\n",
      "Iteration 9226 Loss: 1.982441782951355\n",
      "Iteration 9227 Loss: 1.553852915763855\n",
      "Iteration 9228 Loss: 1.5751968622207642\n",
      "Iteration 9229 Loss: 1.0705807209014893\n",
      "Iteration 9229 Loss: 1.381805419921875\n",
      "Iteration 9230 Loss: 1.4060704708099365\n",
      "Iteration 9231 Loss: 1.4211987257003784\n",
      "Iteration 9232 Loss: 1.4946948289871216\n",
      "Iteration 9233 Loss: 1.1423792839050293\n",
      "Iteration 9234 Loss: 1.25007164478302\n",
      "Iteration 9235 Loss: 1.180405616760254\n",
      "Iteration 9236 Loss: 1.533633828163147\n",
      "Iteration 9237 Loss: 0.9289623498916626\n",
      "Iteration 9238 Loss: 1.6282684803009033\n",
      "Iteration 9239 Loss: 1.3666572570800781\n",
      "Iteration 9239 Loss: 1.33523428440094\n",
      "Iteration 9240 Loss: 1.2562834024429321\n",
      "Iteration 9241 Loss: 1.6732412576675415\n",
      "Iteration 9242 Loss: 1.1686443090438843\n",
      "Iteration 9243 Loss: 1.13157057762146\n",
      "Iteration 9244 Loss: 1.6553335189819336\n",
      "Iteration 9245 Loss: 0.9918494820594788\n",
      "Iteration 9246 Loss: 1.5793944597244263\n",
      "Iteration 9247 Loss: 1.6074419021606445\n",
      "Iteration 9248 Loss: 1.4277139902114868\n",
      "Iteration 9249 Loss: 1.1665414571762085\n",
      "Iteration 9249 Loss: 1.365801453590393\n",
      "Iteration 9250 Loss: 1.4053308963775635\n",
      "Iteration 9251 Loss: 1.9794037342071533\n",
      "Iteration 9252 Loss: 1.6931334733963013\n",
      "Iteration 9253 Loss: 1.470502495765686\n",
      "Iteration 9254 Loss: 1.4509027004241943\n",
      "Iteration 9255 Loss: 1.692351222038269\n",
      "Iteration 9256 Loss: 1.2407889366149902\n",
      "Iteration 9257 Loss: 1.8575156927108765\n",
      "Iteration 9258 Loss: 0.8236886262893677\n",
      "Iteration 9259 Loss: 1.380624532699585\n",
      "Iteration 9259 Loss: 1.4994242191314697\n",
      "Iteration 9260 Loss: 1.1492085456848145\n",
      "Iteration 9261 Loss: 1.4753191471099854\n",
      "Iteration 9262 Loss: 1.3970454931259155\n",
      "Iteration 9263 Loss: 1.379257321357727\n",
      "Iteration 9264 Loss: 1.5541950464248657\n",
      "Iteration 9265 Loss: 1.528079867362976\n",
      "Iteration 9266 Loss: 0.9849883317947388\n",
      "Iteration 9267 Loss: 1.2859338521957397\n",
      "Iteration 9268 Loss: 0.803214430809021\n",
      "Iteration 9269 Loss: 1.8419452905654907\n",
      "Iteration 9269 Loss: 1.339918851852417\n",
      "Iteration 9270 Loss: 1.4339909553527832\n",
      "Iteration 9271 Loss: 1.240736722946167\n",
      "Iteration 9272 Loss: 0.9229519963264465\n",
      "Iteration 9273 Loss: 1.7297276258468628\n",
      "Iteration 9274 Loss: 1.583857536315918\n",
      "Iteration 9275 Loss: 1.4323457479476929\n",
      "Iteration 9276 Loss: 1.4304627180099487\n",
      "Iteration 9277 Loss: 0.9223136901855469\n",
      "Iteration 9278 Loss: 1.4051471948623657\n",
      "Iteration 9279 Loss: 1.012412428855896\n",
      "Iteration 9279 Loss: 1.3113946914672852\n",
      "Iteration 9280 Loss: 1.074775218963623\n",
      "Iteration 9281 Loss: 1.4294252395629883\n",
      "Iteration 9282 Loss: 1.216802716255188\n",
      "Iteration 9283 Loss: 1.007807970046997\n",
      "Iteration 9284 Loss: 1.441902995109558\n",
      "Iteration 9285 Loss: 1.2102590799331665\n",
      "Iteration 9286 Loss: 1.100697636604309\n",
      "Iteration 9287 Loss: 1.2712781429290771\n",
      "Iteration 9288 Loss: 0.6759645938873291\n",
      "Iteration 9289 Loss: 1.4827744960784912\n",
      "Iteration 9289 Loss: 1.1911689043045044\n",
      "Iteration 9290 Loss: 1.1614493131637573\n",
      "Iteration 9291 Loss: 1.3424546718597412\n",
      "Iteration 9292 Loss: 1.4210745096206665\n",
      "Iteration 9293 Loss: 1.297928810119629\n",
      "Iteration 9294 Loss: 1.6011601686477661\n",
      "Iteration 9295 Loss: 1.2623741626739502\n",
      "Iteration 9296 Loss: 1.5703974962234497\n",
      "Iteration 9297 Loss: 1.5625981092453003\n",
      "Iteration 9298 Loss: 1.388414740562439\n",
      "Iteration 9299 Loss: 1.4980422258377075\n",
      "Iteration 9299 Loss: 1.410589337348938\n",
      "Iteration 9300 Loss: 1.4063867330551147\n",
      "Iteration 9301 Loss: 1.1528900861740112\n",
      "Iteration 9302 Loss: 1.586641788482666\n",
      "Iteration 9303 Loss: 1.2198444604873657\n",
      "Iteration 9304 Loss: 1.518126368522644\n",
      "Iteration 9305 Loss: 1.2685705423355103\n",
      "Iteration 9306 Loss: 1.7150423526763916\n",
      "Iteration 9307 Loss: 1.8560633659362793\n",
      "Iteration 9308 Loss: 1.2063937187194824\n",
      "Iteration 9309 Loss: 1.1650316715240479\n",
      "Iteration 9309 Loss: 1.409499168395996\n",
      "Iteration 9310 Loss: 1.4048432111740112\n",
      "Iteration 9311 Loss: 1.7358959913253784\n",
      "Iteration 9312 Loss: 1.388597011566162\n",
      "Iteration 9313 Loss: 1.6723557710647583\n",
      "Iteration 9314 Loss: 1.3830982446670532\n",
      "Iteration 9315 Loss: 1.3502612113952637\n",
      "Iteration 9316 Loss: 1.1931384801864624\n",
      "Iteration 9317 Loss: 1.3516660928726196\n",
      "Iteration 9318 Loss: 0.992219090461731\n",
      "Iteration 9319 Loss: 1.0812703371047974\n",
      "Iteration 9319 Loss: 1.3553345203399658\n",
      "Iteration 9320 Loss: 1.8509509563446045\n",
      "Iteration 9321 Loss: 0.9504950642585754\n",
      "Iteration 9322 Loss: 1.937169075012207\n",
      "Iteration 9323 Loss: 1.3529653549194336\n",
      "Iteration 9324 Loss: 1.0158956050872803\n",
      "Iteration 9325 Loss: 1.87393319606781\n",
      "Iteration 9326 Loss: 1.3468260765075684\n",
      "Iteration 9327 Loss: 1.0834827423095703\n",
      "Iteration 9328 Loss: 0.919067919254303\n",
      "Iteration 9329 Loss: 1.015953540802002\n",
      "Iteration 9329 Loss: 1.3346738815307617\n",
      "Iteration 9330 Loss: 1.7902634143829346\n",
      "Iteration 9331 Loss: 1.127677083015442\n",
      "Iteration 9332 Loss: 1.3949588537216187\n",
      "Iteration 9333 Loss: 1.0815846920013428\n",
      "Iteration 9334 Loss: 1.3140337467193604\n",
      "Iteration 9335 Loss: 1.6417551040649414\n",
      "Iteration 9336 Loss: 1.7289775609970093\n",
      "Iteration 9337 Loss: 1.2455358505249023\n",
      "Iteration 9338 Loss: 1.351647973060608\n",
      "Iteration 9339 Loss: 0.8320198655128479\n",
      "Iteration 9339 Loss: 1.3508453369140625\n",
      "Iteration 9340 Loss: 1.3647406101226807\n",
      "Iteration 9341 Loss: 1.3787996768951416\n",
      "Iteration 9342 Loss: 1.5203622579574585\n",
      "Iteration 9343 Loss: 1.1745171546936035\n",
      "Iteration 9344 Loss: 1.422386646270752\n",
      "Iteration 9345 Loss: 0.9007250070571899\n",
      "Iteration 9346 Loss: 1.060479760169983\n",
      "Iteration 9347 Loss: 1.325242280960083\n",
      "Iteration 9348 Loss: 1.343817114830017\n",
      "Iteration 9349 Loss: 1.361328363418579\n",
      "Iteration 9349 Loss: 1.2852399349212646\n",
      "Iteration 9350 Loss: 1.1565001010894775\n",
      "Iteration 9351 Loss: 1.3471598625183105\n",
      "Iteration 9352 Loss: 1.250608205795288\n",
      "Iteration 9353 Loss: 1.356374979019165\n",
      "Iteration 9354 Loss: 1.0194475650787354\n",
      "Iteration 9355 Loss: 1.0971989631652832\n",
      "Iteration 9356 Loss: 1.4868128299713135\n",
      "Iteration 9357 Loss: 1.1117358207702637\n",
      "Iteration 9358 Loss: 1.8181982040405273\n",
      "Iteration 9359 Loss: 1.2524898052215576\n",
      "Iteration 9359 Loss: 1.2896525859832764\n",
      "Iteration 9360 Loss: 1.2473771572113037\n",
      "Iteration 9361 Loss: 1.529715895652771\n",
      "Iteration 9362 Loss: 1.387833595275879\n",
      "Iteration 9363 Loss: 1.408488154411316\n",
      "Iteration 9364 Loss: 1.3071606159210205\n",
      "Iteration 9365 Loss: 1.2477636337280273\n",
      "Iteration 9366 Loss: 1.3506098985671997\n",
      "Iteration 9367 Loss: 1.8402211666107178\n",
      "Iteration 9368 Loss: 1.7165310382843018\n",
      "Iteration 9369 Loss: 1.425696849822998\n",
      "Iteration 9369 Loss: 1.4461398124694824\n",
      "Iteration 9370 Loss: 1.502718210220337\n",
      "Iteration 9371 Loss: 1.1298694610595703\n",
      "Iteration 9372 Loss: 1.6308672428131104\n",
      "Iteration 9373 Loss: 1.446180820465088\n",
      "Iteration 9374 Loss: 1.653772234916687\n",
      "Iteration 9375 Loss: 1.0852230787277222\n",
      "Iteration 9376 Loss: 1.1521623134613037\n",
      "Iteration 9377 Loss: 1.2603049278259277\n",
      "Iteration 9378 Loss: 1.5432451963424683\n",
      "Iteration 9379 Loss: 1.3929195404052734\n",
      "Iteration 9379 Loss: 1.3797261714935303\n",
      "Iteration 9380 Loss: 1.5957211256027222\n",
      "Iteration 9381 Loss: 1.2448101043701172\n",
      "Iteration 9382 Loss: 1.1338032484054565\n",
      "Iteration 9383 Loss: 1.269351840019226\n",
      "Iteration 9384 Loss: 1.3001619577407837\n",
      "Iteration 9385 Loss: 1.6587616205215454\n",
      "Iteration 9386 Loss: 1.0003162622451782\n",
      "Iteration 9387 Loss: 1.2382134199142456\n",
      "Iteration 9388 Loss: 1.8162078857421875\n",
      "Iteration 9389 Loss: 1.3728077411651611\n",
      "Iteration 9389 Loss: 1.3630156517028809\n",
      "Iteration 9390 Loss: 1.1645594835281372\n",
      "Iteration 9391 Loss: 1.4544751644134521\n",
      "Iteration 9392 Loss: 1.3066580295562744\n",
      "Iteration 9393 Loss: 0.9941173791885376\n",
      "Iteration 9394 Loss: 0.9323592185974121\n",
      "Iteration 9395 Loss: 1.8434144258499146\n",
      "Iteration 9396 Loss: 1.508170485496521\n",
      "Iteration 9397 Loss: 1.5235573053359985\n",
      "Iteration 9398 Loss: 1.3868308067321777\n",
      "Iteration 9399 Loss: 1.5651307106018066\n",
      "Iteration 9399 Loss: 1.3679273128509521\n",
      "Iteration 9400 Loss: 1.333117961883545\n",
      "Iteration 9401 Loss: 1.1145561933517456\n",
      "Iteration 9402 Loss: 1.6932097673416138\n",
      "Iteration 9403 Loss: 1.5390942096710205\n",
      "Iteration 9404 Loss: 1.3432655334472656\n",
      "Iteration 9405 Loss: 1.0236659049987793\n",
      "Iteration 9406 Loss: 1.4156194925308228\n",
      "Iteration 9407 Loss: 1.701361894607544\n",
      "Iteration 9408 Loss: 1.461219072341919\n",
      "Iteration 9409 Loss: 1.4864757061004639\n",
      "Iteration 9409 Loss: 1.411158561706543\n",
      "Iteration 9410 Loss: 1.6240973472595215\n",
      "Iteration 9411 Loss: 1.4451953172683716\n",
      "Iteration 9412 Loss: 1.3536614179611206\n",
      "Iteration 9413 Loss: 1.5630290508270264\n",
      "Iteration 9414 Loss: 0.8924615383148193\n",
      "Iteration 9415 Loss: 1.5023988485336304\n",
      "Iteration 9416 Loss: 1.1054701805114746\n",
      "Iteration 9417 Loss: 1.3151803016662598\n",
      "Iteration 9418 Loss: 1.8015064001083374\n",
      "Iteration 9419 Loss: 0.9793398976325989\n",
      "Iteration 9419 Loss: 1.3582340478897095\n",
      "Iteration 9420 Loss: 1.0381629467010498\n",
      "Iteration 9421 Loss: 1.5392684936523438\n",
      "Iteration 9422 Loss: 1.4922735691070557\n",
      "Iteration 9423 Loss: 1.4590582847595215\n",
      "Iteration 9424 Loss: 1.1320445537567139\n",
      "Iteration 9425 Loss: 1.6975467205047607\n",
      "Iteration 9426 Loss: 1.3452216386795044\n",
      "Iteration 9427 Loss: 1.2075386047363281\n",
      "Iteration 9428 Loss: 1.4371278285980225\n",
      "Iteration 9429 Loss: 1.287293791770935\n",
      "Iteration 9429 Loss: 1.363553762435913\n",
      "Iteration 9430 Loss: 0.8970876336097717\n",
      "Iteration 9431 Loss: 1.4491242170333862\n",
      "Iteration 9432 Loss: 1.4701521396636963\n",
      "Iteration 9433 Loss: 1.4595166444778442\n",
      "Iteration 9434 Loss: 1.2612913846969604\n",
      "Iteration 9435 Loss: 1.2271558046340942\n",
      "Iteration 9436 Loss: 1.3900492191314697\n",
      "Iteration 9437 Loss: 1.3093193769454956\n",
      "Iteration 9438 Loss: 1.0028635263442993\n",
      "Iteration 9439 Loss: 1.568642020225525\n",
      "Iteration 9439 Loss: 1.3035202026367188\n",
      "Iteration 9440 Loss: 1.14675772190094\n",
      "Iteration 9441 Loss: 1.4018186330795288\n",
      "Iteration 9442 Loss: 1.1884300708770752\n",
      "Iteration 9443 Loss: 1.1063830852508545\n",
      "Iteration 9444 Loss: 1.022329330444336\n",
      "Iteration 9445 Loss: 1.0082334280014038\n",
      "Iteration 9446 Loss: 1.176965594291687\n",
      "Iteration 9447 Loss: 1.1537244319915771\n",
      "Iteration 9448 Loss: 1.2469542026519775\n",
      "Iteration 9449 Loss: 1.2999379634857178\n",
      "Iteration 9449 Loss: 1.1751534938812256\n",
      "Iteration 9450 Loss: 1.6930996179580688\n",
      "Iteration 9451 Loss: 1.591928482055664\n",
      "Iteration 9452 Loss: 1.3595647811889648\n",
      "Iteration 9453 Loss: 1.323079228401184\n",
      "Iteration 9454 Loss: 1.430208683013916\n",
      "Iteration 9455 Loss: 1.0991742610931396\n",
      "Iteration 9456 Loss: 1.5347996950149536\n",
      "Iteration 9457 Loss: 1.6883652210235596\n",
      "Iteration 9458 Loss: 0.621467113494873\n",
      "Iteration 9459 Loss: 1.2202345132827759\n",
      "Iteration 9459 Loss: 1.3561921119689941\n",
      "Iteration 9460 Loss: 1.425736904144287\n",
      "Iteration 9461 Loss: 1.5776500701904297\n",
      "Iteration 9462 Loss: 1.2429214715957642\n",
      "Iteration 9463 Loss: 1.3373761177062988\n",
      "Iteration 9464 Loss: 1.361026644706726\n",
      "Iteration 9465 Loss: 1.2398250102996826\n",
      "Iteration 9466 Loss: 1.1165447235107422\n",
      "Iteration 9467 Loss: 1.0218219757080078\n",
      "Iteration 9468 Loss: 0.9344359636306763\n",
      "Iteration 9469 Loss: 1.5936925411224365\n",
      "Iteration 9469 Loss: 1.2851030826568604\n",
      "Iteration 9470 Loss: 1.8039532899856567\n",
      "Iteration 9471 Loss: 1.3771781921386719\n",
      "Iteration 9472 Loss: 1.0799998044967651\n",
      "Iteration 9473 Loss: 1.4819540977478027\n",
      "Iteration 9474 Loss: 1.1831684112548828\n",
      "Iteration 9475 Loss: 1.1416542530059814\n",
      "Iteration 9476 Loss: 1.860744595527649\n",
      "Iteration 9477 Loss: 1.7842134237289429\n",
      "Iteration 9478 Loss: 1.1333554983139038\n",
      "Iteration 9479 Loss: 1.460314154624939\n",
      "Iteration 9479 Loss: 1.4306535720825195\n",
      "Iteration 9480 Loss: 1.4579744338989258\n",
      "Iteration 9481 Loss: 0.995779275894165\n",
      "Iteration 9482 Loss: 1.3618220090866089\n",
      "Iteration 9483 Loss: 1.0175584554672241\n",
      "Iteration 9484 Loss: 1.0173195600509644\n",
      "Iteration 9485 Loss: 1.5590167045593262\n",
      "Iteration 9486 Loss: 1.750091314315796\n",
      "Iteration 9487 Loss: 0.9973996877670288\n",
      "Iteration 9488 Loss: 1.136049509048462\n",
      "Iteration 9489 Loss: 1.2515366077423096\n",
      "Iteration 9489 Loss: 1.2544547319412231\n",
      "Iteration 9490 Loss: 1.1147816181182861\n",
      "Iteration 9491 Loss: 1.40286123752594\n",
      "Iteration 9492 Loss: 1.5875645875930786\n",
      "Iteration 9493 Loss: 1.0880974531173706\n",
      "Iteration 9494 Loss: 1.5052052736282349\n",
      "Iteration 9495 Loss: 1.109495759010315\n",
      "Iteration 9496 Loss: 0.7867074608802795\n",
      "Iteration 9497 Loss: 1.0559836626052856\n",
      "Iteration 9498 Loss: 1.0359513759613037\n",
      "Iteration 9499 Loss: 1.5095716714859009\n",
      "Iteration 9499 Loss: 1.2196221351623535\n",
      "Iteration 9500 Loss: 1.0903897285461426\n",
      "Iteration 9501 Loss: 1.2745236158370972\n",
      "Iteration 9502 Loss: 1.281288981437683\n",
      "Iteration 9503 Loss: 1.5655401945114136\n",
      "Iteration 9504 Loss: 1.4625649452209473\n",
      "Iteration 9505 Loss: 0.9319313764572144\n",
      "Iteration 9506 Loss: 1.5041753053665161\n",
      "Iteration 9507 Loss: 1.1011766195297241\n",
      "Iteration 9508 Loss: 1.430924415588379\n",
      "Iteration 9509 Loss: 1.6473087072372437\n",
      "Iteration 9509 Loss: 1.3289823532104492\n",
      "Iteration 9510 Loss: 1.2740583419799805\n",
      "Iteration 9511 Loss: 1.5706579685211182\n",
      "Iteration 9512 Loss: 1.9500778913497925\n",
      "Iteration 9513 Loss: 1.188501000404358\n",
      "Iteration 9514 Loss: 0.9693782925605774\n",
      "Iteration 9515 Loss: 1.5951718091964722\n",
      "Iteration 9516 Loss: 1.1702473163604736\n",
      "Iteration 9517 Loss: 1.5623081922531128\n",
      "Iteration 9518 Loss: 1.519877314567566\n",
      "Iteration 9519 Loss: 1.1129194498062134\n",
      "Iteration 9519 Loss: 1.3913198709487915\n",
      "Iteration 9520 Loss: 1.249776840209961\n",
      "Iteration 9521 Loss: 1.4850749969482422\n",
      "Iteration 9522 Loss: 1.1469922065734863\n",
      "Iteration 9523 Loss: 1.512445330619812\n",
      "Iteration 9524 Loss: 1.5463708639144897\n",
      "Iteration 9525 Loss: 1.597845196723938\n",
      "Iteration 9526 Loss: 1.2058120965957642\n",
      "Iteration 9527 Loss: 1.2929847240447998\n",
      "Iteration 9528 Loss: 1.2675610780715942\n",
      "Iteration 9529 Loss: 1.7982473373413086\n",
      "Iteration 9529 Loss: 1.410310983657837\n",
      "Iteration 9530 Loss: 1.048204779624939\n",
      "Iteration 9531 Loss: 1.497986078262329\n",
      "Iteration 9532 Loss: 1.1622525453567505\n",
      "Iteration 9533 Loss: 1.3537721633911133\n",
      "Iteration 9534 Loss: 1.0832557678222656\n",
      "Iteration 9535 Loss: 1.4630917310714722\n",
      "Iteration 9536 Loss: 1.232755184173584\n",
      "Iteration 9537 Loss: 1.3709666728973389\n",
      "Iteration 9538 Loss: 1.7912791967391968\n",
      "Iteration 9539 Loss: 0.7435644268989563\n",
      "Iteration 9539 Loss: 1.2747128009796143\n",
      "Iteration 9540 Loss: 1.140791654586792\n",
      "Iteration 9541 Loss: 1.4102890491485596\n",
      "Iteration 9542 Loss: 1.0658652782440186\n",
      "Iteration 9543 Loss: 1.608083724975586\n",
      "Iteration 9544 Loss: 1.4897710084915161\n",
      "Iteration 9545 Loss: 1.3376843929290771\n",
      "Iteration 9546 Loss: 1.028956413269043\n",
      "Iteration 9547 Loss: 0.9168380498886108\n",
      "Iteration 9548 Loss: 1.397408366203308\n",
      "Iteration 9549 Loss: 1.3465768098831177\n",
      "Iteration 9549 Loss: 1.274226427078247\n",
      "Iteration 9550 Loss: 1.4681843519210815\n",
      "Iteration 9551 Loss: 1.5676108598709106\n",
      "Iteration 9552 Loss: 1.17518949508667\n",
      "Iteration 9553 Loss: 1.6193879842758179\n",
      "Iteration 9554 Loss: 0.8455684185028076\n",
      "Iteration 9555 Loss: 1.4449067115783691\n",
      "Iteration 9556 Loss: 1.4882841110229492\n",
      "Iteration 9557 Loss: 1.3926382064819336\n",
      "Iteration 9558 Loss: 1.503859281539917\n",
      "Iteration 9559 Loss: 1.3183420896530151\n",
      "Iteration 9559 Loss: 1.3823970556259155\n",
      "Iteration 9560 Loss: 1.5509145259857178\n",
      "Iteration 9561 Loss: 1.6728367805480957\n",
      "Iteration 9562 Loss: 1.0402659177780151\n",
      "Iteration 9563 Loss: 1.5557289123535156\n",
      "Iteration 9564 Loss: 1.6136378049850464\n",
      "Iteration 9565 Loss: 1.4857555627822876\n",
      "Iteration 9566 Loss: 1.767562747001648\n",
      "Iteration 9567 Loss: 1.8686659336090088\n",
      "Iteration 9568 Loss: 1.543055534362793\n",
      "Iteration 9569 Loss: 1.2244917154312134\n",
      "Iteration 9569 Loss: 1.5322916507720947\n",
      "Iteration 9570 Loss: 1.1923969984054565\n",
      "Iteration 9571 Loss: 1.1849385499954224\n",
      "Iteration 9572 Loss: 1.5638712644577026\n",
      "Iteration 9573 Loss: 1.2253682613372803\n",
      "Iteration 9574 Loss: 1.3726794719696045\n",
      "Iteration 9575 Loss: 1.9447842836380005\n",
      "Iteration 9576 Loss: 1.6828707456588745\n",
      "Iteration 9577 Loss: 1.2001993656158447\n",
      "Iteration 9578 Loss: 1.1987025737762451\n",
      "Iteration 9579 Loss: 1.3912904262542725\n",
      "Iteration 9579 Loss: 1.3957102298736572\n",
      "Iteration 9580 Loss: 1.487921118736267\n",
      "Iteration 9581 Loss: 1.2588335275650024\n",
      "Iteration 9582 Loss: 1.3959814310073853\n",
      "Iteration 9583 Loss: 0.8634400367736816\n",
      "Iteration 9584 Loss: 1.2315043210983276\n",
      "Iteration 9585 Loss: 1.1216223239898682\n",
      "Iteration 9586 Loss: 1.2358992099761963\n",
      "Iteration 9587 Loss: 1.5730911493301392\n",
      "Iteration 9588 Loss: 1.232947826385498\n",
      "Iteration 9589 Loss: 1.5105626583099365\n",
      "Iteration 9589 Loss: 1.2911803722381592\n",
      "Iteration 9590 Loss: 1.5657892227172852\n",
      "Iteration 9591 Loss: 1.0442525148391724\n",
      "Iteration 9592 Loss: 1.4719029664993286\n",
      "Iteration 9593 Loss: 1.3352265357971191\n",
      "Iteration 9594 Loss: 1.4315036535263062\n",
      "Iteration 9595 Loss: 1.4638137817382812\n",
      "Iteration 9596 Loss: 1.1523728370666504\n",
      "Iteration 9597 Loss: 1.682811975479126\n",
      "Iteration 9598 Loss: 1.127464771270752\n",
      "Iteration 9599 Loss: 1.6087768077850342\n",
      "Iteration 9599 Loss: 1.3883912563323975\n",
      "Iteration 9600 Loss: 1.1137582063674927\n",
      "Iteration 9601 Loss: 1.4731967449188232\n",
      "Iteration 9602 Loss: 1.2117085456848145\n",
      "Iteration 9603 Loss: 1.3964076042175293\n",
      "Iteration 9604 Loss: 0.6319683790206909\n",
      "Iteration 9605 Loss: 1.6832642555236816\n",
      "Iteration 9606 Loss: 1.345986008644104\n",
      "Iteration 9607 Loss: 1.5100613832473755\n",
      "Iteration 9608 Loss: 1.2005724906921387\n",
      "Iteration 9609 Loss: 1.1562445163726807\n",
      "Iteration 9609 Loss: 1.272316813468933\n",
      "Iteration 9610 Loss: 1.6377540826797485\n",
      "Iteration 9611 Loss: 1.1670552492141724\n",
      "Iteration 9612 Loss: 0.9090127348899841\n",
      "Iteration 9613 Loss: 1.124848484992981\n",
      "Iteration 9614 Loss: 1.1812478303909302\n",
      "Iteration 9615 Loss: 1.4731643199920654\n",
      "Iteration 9616 Loss: 1.021429181098938\n",
      "Iteration 9617 Loss: 1.7067610025405884\n",
      "Iteration 9618 Loss: 1.147786021232605\n",
      "Iteration 9619 Loss: 1.296573519706726\n",
      "Iteration 9619 Loss: 1.2665632963180542\n",
      "Iteration 9620 Loss: 1.5389896631240845\n",
      "Iteration 9621 Loss: 1.0126672983169556\n",
      "Iteration 9622 Loss: 1.5611997842788696\n",
      "Iteration 9623 Loss: 1.5126266479492188\n",
      "Iteration 9624 Loss: 1.3777977228164673\n",
      "Iteration 9625 Loss: 1.2233388423919678\n",
      "Iteration 9626 Loss: 0.9704424738883972\n",
      "Iteration 9627 Loss: 1.235387921333313\n",
      "Iteration 9628 Loss: 1.3019185066223145\n",
      "Iteration 9629 Loss: 1.3510769605636597\n",
      "Iteration 9629 Loss: 1.308544635772705\n",
      "Iteration 9630 Loss: 1.4782875776290894\n",
      "Iteration 9631 Loss: 1.4123903512954712\n",
      "Iteration 9632 Loss: 0.8977051973342896\n",
      "Iteration 9633 Loss: 1.5892153978347778\n",
      "Iteration 9634 Loss: 1.2341700792312622\n",
      "Iteration 9635 Loss: 1.2990803718566895\n",
      "Iteration 9636 Loss: 1.6299160718917847\n",
      "Iteration 9637 Loss: 1.5827292203903198\n",
      "Iteration 9638 Loss: 1.1876963376998901\n",
      "Iteration 9639 Loss: 1.453174114227295\n",
      "Iteration 9639 Loss: 1.376436471939087\n",
      "Iteration 9640 Loss: 1.5896745920181274\n",
      "Iteration 9641 Loss: 1.2296645641326904\n",
      "Iteration 9642 Loss: 1.620218276977539\n",
      "Iteration 9643 Loss: 1.4498369693756104\n",
      "Iteration 9644 Loss: 1.4256126880645752\n",
      "Iteration 9645 Loss: 1.0683375597000122\n",
      "Iteration 9646 Loss: 1.1920093297958374\n",
      "Iteration 9647 Loss: 1.5184110403060913\n",
      "Iteration 9648 Loss: 1.3286361694335938\n",
      "Iteration 9649 Loss: 1.2547900676727295\n",
      "Iteration 9649 Loss: 1.3677189350128174\n",
      "Iteration 9650 Loss: 1.2633328437805176\n",
      "Iteration 9651 Loss: 1.5365567207336426\n",
      "Iteration 9652 Loss: 1.0930180549621582\n",
      "Iteration 9653 Loss: 1.423012614250183\n",
      "Iteration 9654 Loss: 1.0239274501800537\n",
      "Iteration 9655 Loss: 1.7895292043685913\n",
      "Iteration 9656 Loss: 1.6244176626205444\n",
      "Iteration 9657 Loss: 1.521938681602478\n",
      "Iteration 9658 Loss: 0.8445470333099365\n",
      "Iteration 9659 Loss: 1.919427514076233\n",
      "Iteration 9659 Loss: 1.403970718383789\n",
      "Iteration 9660 Loss: 1.5970184803009033\n",
      "Iteration 9661 Loss: 1.4306049346923828\n",
      "Iteration 9662 Loss: 1.4750555753707886\n",
      "Iteration 9663 Loss: 1.3883461952209473\n",
      "Iteration 9664 Loss: 1.2203876972198486\n",
      "Iteration 9665 Loss: 1.149378776550293\n",
      "Iteration 9666 Loss: 1.3909518718719482\n",
      "Iteration 9667 Loss: 1.33705472946167\n",
      "Iteration 9668 Loss: 0.8808352947235107\n",
      "Iteration 9669 Loss: 1.4060509204864502\n",
      "Iteration 9669 Loss: 1.3275684118270874\n",
      "Iteration 9670 Loss: 1.2412965297698975\n",
      "Iteration 9671 Loss: 1.7888572216033936\n",
      "Iteration 9672 Loss: 1.4475648403167725\n",
      "Iteration 9673 Loss: 1.6143759489059448\n",
      "Iteration 9674 Loss: 1.1920595169067383\n",
      "Iteration 9675 Loss: 1.1478949785232544\n",
      "Iteration 9676 Loss: 1.535430669784546\n",
      "Iteration 9677 Loss: 1.7457315921783447\n",
      "Iteration 9678 Loss: 0.8811205625534058\n",
      "Iteration 9679 Loss: 1.2909650802612305\n",
      "Iteration 9679 Loss: 1.388529658317566\n",
      "Iteration 9680 Loss: 1.7414917945861816\n",
      "Iteration 9681 Loss: 0.8146466612815857\n",
      "Iteration 9682 Loss: 1.7756999731063843\n",
      "Iteration 9683 Loss: 1.5923088788986206\n",
      "Iteration 9684 Loss: 1.5685667991638184\n",
      "Iteration 9685 Loss: 0.9621323347091675\n",
      "Iteration 9686 Loss: 1.2009963989257812\n",
      "Iteration 9687 Loss: 1.1985646486282349\n",
      "Iteration 9688 Loss: 1.5645705461502075\n",
      "Iteration 9689 Loss: 1.4351392984390259\n",
      "Iteration 9689 Loss: 1.3854117393493652\n",
      "Iteration 9690 Loss: 1.2278306484222412\n",
      "Iteration 9691 Loss: 1.4092704057693481\n",
      "Iteration 9692 Loss: 1.2189610004425049\n",
      "Iteration 9693 Loss: 1.297914743423462\n",
      "Iteration 9694 Loss: 1.5260802507400513\n",
      "Iteration 9695 Loss: 1.3439100980758667\n",
      "Iteration 9696 Loss: 1.2558099031448364\n",
      "Iteration 9697 Loss: 1.6234592199325562\n",
      "Iteration 9698 Loss: 1.1437541246414185\n",
      "Iteration 9699 Loss: 1.1415200233459473\n",
      "Iteration 9699 Loss: 1.3188509941101074\n",
      "Iteration 9700 Loss: 1.438249111175537\n",
      "Iteration 9701 Loss: 1.4648690223693848\n",
      "Iteration 9702 Loss: 1.124758243560791\n",
      "Iteration 9703 Loss: 1.4454975128173828\n",
      "Iteration 9704 Loss: 1.49307119846344\n",
      "Iteration 9705 Loss: 1.0517258644104004\n",
      "Iteration 9706 Loss: 1.2900665998458862\n",
      "Iteration 9707 Loss: 1.7275718450546265\n",
      "Iteration 9708 Loss: 1.1491843461990356\n",
      "Iteration 9709 Loss: 1.430415391921997\n",
      "Iteration 9709 Loss: 1.3615409135818481\n",
      "Iteration 9710 Loss: 1.2857496738433838\n",
      "Iteration 9711 Loss: 1.5406262874603271\n",
      "Iteration 9712 Loss: 1.7881780862808228\n",
      "Iteration 9713 Loss: 1.3546011447906494\n",
      "Iteration 9714 Loss: 1.2395603656768799\n",
      "Iteration 9715 Loss: 1.3095747232437134\n",
      "Iteration 9716 Loss: 1.3094565868377686\n",
      "Iteration 9717 Loss: 1.5958306789398193\n",
      "Iteration 9718 Loss: 1.6247614622116089\n",
      "Iteration 9719 Loss: 1.3866764307022095\n",
      "Iteration 9719 Loss: 1.4435017108917236\n",
      "Iteration 9720 Loss: 1.7193061113357544\n",
      "Iteration 9721 Loss: 1.4594581127166748\n",
      "Iteration 9722 Loss: 1.431859016418457\n",
      "Iteration 9723 Loss: 1.4219385385513306\n",
      "Iteration 9724 Loss: 0.9878206253051758\n",
      "Iteration 9725 Loss: 1.0369868278503418\n",
      "Iteration 9726 Loss: 0.838527500629425\n",
      "Iteration 9727 Loss: 1.389541506767273\n",
      "Iteration 9728 Loss: 1.415544033050537\n",
      "Iteration 9729 Loss: 1.5448459386825562\n",
      "Iteration 9729 Loss: 1.324582815170288\n",
      "Iteration 9730 Loss: 1.033402681350708\n",
      "Iteration 9731 Loss: 1.2315113544464111\n",
      "Iteration 9732 Loss: 1.7711029052734375\n",
      "Iteration 9733 Loss: 1.6943861246109009\n",
      "Iteration 9734 Loss: 1.0301439762115479\n",
      "Iteration 9735 Loss: 1.58585786819458\n",
      "Iteration 9736 Loss: 0.8652247190475464\n",
      "Iteration 9737 Loss: 1.4700578451156616\n",
      "Iteration 9738 Loss: 1.4797741174697876\n",
      "Iteration 9739 Loss: 1.4669510126113892\n",
      "Iteration 9739 Loss: 1.3628413677215576\n",
      "Iteration 9740 Loss: 1.277801275253296\n",
      "Iteration 9741 Loss: 1.4316439628601074\n",
      "Iteration 9742 Loss: 1.3443095684051514\n",
      "Iteration 9743 Loss: 1.6088650226593018\n",
      "Iteration 9744 Loss: 1.099400281906128\n",
      "Iteration 9745 Loss: 1.053949236869812\n",
      "Iteration 9746 Loss: 1.4925761222839355\n",
      "Iteration 9747 Loss: 1.7273094654083252\n",
      "Iteration 9748 Loss: 1.5274609327316284\n",
      "Iteration 9749 Loss: 1.42279851436615\n",
      "Iteration 9749 Loss: 1.398611307144165\n",
      "Iteration 9750 Loss: 1.3812013864517212\n",
      "Iteration 9751 Loss: 1.4434572458267212\n",
      "Iteration 9752 Loss: 1.652094841003418\n",
      "Iteration 9753 Loss: 1.1814708709716797\n",
      "Iteration 9754 Loss: 1.3275755643844604\n",
      "Iteration 9755 Loss: 1.0188877582550049\n",
      "Iteration 9756 Loss: 1.3902130126953125\n",
      "Iteration 9757 Loss: 1.4990705251693726\n",
      "Iteration 9758 Loss: 1.582924723625183\n",
      "Iteration 9759 Loss: 1.2650541067123413\n",
      "Iteration 9759 Loss: 1.374194860458374\n",
      "Iteration 9760 Loss: 1.5525758266448975\n",
      "Iteration 9761 Loss: 1.0763508081436157\n",
      "Iteration 9762 Loss: 1.2594449520111084\n",
      "Iteration 9763 Loss: 0.9943003058433533\n",
      "Iteration 9764 Loss: 1.8021208047866821\n",
      "Iteration 9765 Loss: 1.8333423137664795\n",
      "Iteration 9766 Loss: 1.2386101484298706\n",
      "Iteration 9767 Loss: 1.4537436962127686\n",
      "Iteration 9768 Loss: 1.0743955373764038\n",
      "Iteration 9769 Loss: 1.4460997581481934\n",
      "Iteration 9769 Loss: 1.3730984926223755\n",
      "Iteration 9770 Loss: 1.513440489768982\n",
      "Iteration 9771 Loss: 1.4137606620788574\n",
      "Iteration 9772 Loss: 0.947522759437561\n",
      "Iteration 9773 Loss: 1.6853718757629395\n",
      "Iteration 9774 Loss: 1.020462155342102\n",
      "Iteration 9775 Loss: 1.4714664220809937\n",
      "Iteration 9776 Loss: 1.3753901720046997\n",
      "Iteration 9777 Loss: 1.5619467496871948\n",
      "Iteration 9778 Loss: 1.2350811958312988\n",
      "Iteration 9779 Loss: 1.442732810974121\n",
      "Iteration 9779 Loss: 1.3667174577713013\n",
      "Iteration 9780 Loss: 1.4738420248031616\n",
      "Iteration 9781 Loss: 1.4405438899993896\n",
      "Iteration 9782 Loss: 1.1532787084579468\n",
      "Iteration 9783 Loss: 1.4015543460845947\n",
      "Iteration 9784 Loss: 0.8985086679458618\n",
      "Iteration 9785 Loss: 1.5030412673950195\n",
      "Iteration 9786 Loss: 1.5126065015792847\n",
      "Iteration 9787 Loss: 1.29805588722229\n",
      "Iteration 9788 Loss: 1.3832404613494873\n",
      "Iteration 9789 Loss: 1.3897219896316528\n",
      "Iteration 9789 Loss: 1.3454394340515137\n",
      "Iteration 9790 Loss: 1.4250893592834473\n",
      "Iteration 9791 Loss: 1.1695640087127686\n",
      "Iteration 9792 Loss: 1.566726565361023\n",
      "Iteration 9793 Loss: 1.5938341617584229\n",
      "Iteration 9794 Loss: 2.0483694076538086\n",
      "Iteration 9795 Loss: 1.585587739944458\n",
      "Iteration 9796 Loss: 1.018778920173645\n",
      "Iteration 9797 Loss: 1.7109326124191284\n",
      "Iteration 9798 Loss: 1.6899375915527344\n",
      "Iteration 9799 Loss: 1.472129225730896\n",
      "Iteration 9799 Loss: 1.528095006942749\n",
      "Iteration 9800 Loss: 1.4466753005981445\n",
      "Iteration 9801 Loss: 1.3297650814056396\n",
      "Iteration 9802 Loss: 1.29766047000885\n",
      "Iteration 9803 Loss: 1.506569266319275\n",
      "Iteration 9804 Loss: 1.6173821687698364\n",
      "Iteration 9805 Loss: 1.5072073936462402\n",
      "Iteration 9806 Loss: 1.6134508848190308\n",
      "Iteration 9807 Loss: 0.9383182525634766\n",
      "Iteration 9808 Loss: 1.1921377182006836\n",
      "Iteration 9809 Loss: 0.9205602407455444\n",
      "Iteration 9809 Loss: 1.3369725942611694\n",
      "Iteration 9810 Loss: 1.272779941558838\n",
      "Iteration 9811 Loss: 1.4259858131408691\n",
      "Iteration 9812 Loss: 1.4588263034820557\n",
      "Iteration 9813 Loss: 1.466554045677185\n",
      "Iteration 9814 Loss: 1.1088268756866455\n",
      "Iteration 9815 Loss: 1.3097827434539795\n",
      "Iteration 9816 Loss: 1.0351216793060303\n",
      "Iteration 9817 Loss: 1.4798258543014526\n",
      "Iteration 9818 Loss: 1.3342370986938477\n",
      "Iteration 9819 Loss: 1.2771525382995605\n",
      "Iteration 9819 Loss: 1.3169093132019043\n",
      "Iteration 9820 Loss: 0.9312708377838135\n",
      "Iteration 9821 Loss: 0.9797414541244507\n",
      "Iteration 9822 Loss: 1.2254446744918823\n",
      "Iteration 9823 Loss: 1.3245652914047241\n",
      "Iteration 9824 Loss: 1.6298972368240356\n",
      "Iteration 9825 Loss: 1.4707156419754028\n",
      "Iteration 9826 Loss: 1.416371464729309\n",
      "Iteration 9827 Loss: 1.4542734622955322\n",
      "Iteration 9828 Loss: 1.6971594095230103\n",
      "Iteration 9829 Loss: 1.1808234453201294\n",
      "Iteration 9829 Loss: 1.331026315689087\n",
      "Iteration 9830 Loss: 0.7486595511436462\n",
      "Iteration 9831 Loss: 1.1958239078521729\n",
      "Iteration 9832 Loss: 1.3609542846679688\n",
      "Iteration 9833 Loss: 1.1244832277297974\n",
      "Iteration 9834 Loss: 1.6281812191009521\n",
      "Iteration 9835 Loss: 1.4231536388397217\n",
      "Iteration 9836 Loss: 1.2488675117492676\n",
      "Iteration 9837 Loss: 1.5370036363601685\n",
      "Iteration 9838 Loss: 1.3558484315872192\n",
      "Iteration 9839 Loss: 1.331917405128479\n",
      "Iteration 9839 Loss: 1.2954891920089722\n",
      "Iteration 9840 Loss: 1.4688133001327515\n",
      "Iteration 9841 Loss: 1.8277112245559692\n",
      "Iteration 9842 Loss: 1.6407434940338135\n",
      "Iteration 9843 Loss: 1.8313552141189575\n",
      "Iteration 9844 Loss: 1.2550448179244995\n",
      "Iteration 9845 Loss: 1.062416672706604\n",
      "Iteration 9846 Loss: 1.3643587827682495\n",
      "Iteration 9847 Loss: 1.2089639902114868\n",
      "Iteration 9848 Loss: 1.6838290691375732\n",
      "Iteration 9849 Loss: 1.2468293905258179\n",
      "Iteration 9849 Loss: 1.459006667137146\n",
      "Iteration 9850 Loss: 1.3304181098937988\n",
      "Iteration 9851 Loss: 1.4259941577911377\n",
      "Iteration 9852 Loss: 1.0275264978408813\n",
      "Iteration 9853 Loss: 1.6991032361984253\n",
      "Iteration 9854 Loss: 1.7169122695922852\n",
      "Iteration 9855 Loss: 1.358957052230835\n",
      "Iteration 9856 Loss: 1.184523344039917\n",
      "Iteration 9857 Loss: 1.5321093797683716\n",
      "Iteration 9858 Loss: 1.5517820119857788\n",
      "Iteration 9859 Loss: 1.319300889968872\n",
      "Iteration 9859 Loss: 1.4146625995635986\n",
      "Iteration 9860 Loss: 1.489335298538208\n",
      "Iteration 9861 Loss: 1.4348050355911255\n",
      "Iteration 9862 Loss: 1.4199018478393555\n",
      "Iteration 9863 Loss: 1.6373183727264404\n",
      "Iteration 9864 Loss: 1.4048352241516113\n",
      "Iteration 9865 Loss: 1.7371374368667603\n",
      "Iteration 9866 Loss: 1.351289987564087\n",
      "Iteration 9867 Loss: 1.5340917110443115\n",
      "Iteration 9868 Loss: 1.5084880590438843\n",
      "Iteration 9869 Loss: 1.1801905632019043\n",
      "Iteration 9869 Loss: 1.4697394371032715\n",
      "Iteration 9870 Loss: 0.898622453212738\n",
      "Iteration 9871 Loss: 1.3232688903808594\n",
      "Iteration 9872 Loss: 1.8406733274459839\n",
      "Iteration 9873 Loss: 1.3403464555740356\n",
      "Iteration 9874 Loss: 1.6624798774719238\n",
      "Iteration 9875 Loss: 1.5594322681427002\n",
      "Iteration 9876 Loss: 1.4462602138519287\n",
      "Iteration 9877 Loss: 1.3241512775421143\n",
      "Iteration 9878 Loss: 1.0466172695159912\n",
      "Iteration 9879 Loss: 1.458426833152771\n",
      "Iteration 9879 Loss: 1.3900277614593506\n",
      "Iteration 9880 Loss: 0.6653927564620972\n",
      "Iteration 9881 Loss: 0.8677897453308105\n",
      "Iteration 9882 Loss: 1.2127468585968018\n",
      "Iteration 9883 Loss: 1.4986461400985718\n",
      "Iteration 9884 Loss: 1.2669448852539062\n",
      "Iteration 9885 Loss: 1.0143400430679321\n",
      "Iteration 9886 Loss: 1.4802913665771484\n",
      "Iteration 9887 Loss: 1.5636298656463623\n",
      "Iteration 9888 Loss: 1.2151508331298828\n",
      "Iteration 9889 Loss: 1.4773856401443481\n",
      "Iteration 9889 Loss: 1.2262319326400757\n",
      "Iteration 9890 Loss: 1.2685189247131348\n",
      "Iteration 9891 Loss: 1.3807971477508545\n",
      "Iteration 9892 Loss: 1.0232627391815186\n",
      "Iteration 9893 Loss: 1.058085560798645\n",
      "Iteration 9894 Loss: 1.636671543121338\n",
      "Iteration 9895 Loss: 1.934158444404602\n",
      "Iteration 9896 Loss: 1.0564937591552734\n",
      "Iteration 9897 Loss: 1.067964792251587\n",
      "Iteration 9898 Loss: 1.6280438899993896\n",
      "Iteration 9899 Loss: 1.390031099319458\n",
      "Iteration 9899 Loss: 1.3444026708602905\n",
      "Iteration 9900 Loss: 1.0360890626907349\n",
      "Iteration 9901 Loss: 1.4906835556030273\n",
      "Iteration 9902 Loss: 1.2940318584442139\n",
      "Iteration 9903 Loss: 1.5362287759780884\n",
      "Iteration 9904 Loss: 1.3688582181930542\n",
      "Iteration 9905 Loss: 1.1508275270462036\n",
      "Iteration 9906 Loss: 1.8751165866851807\n",
      "Iteration 9907 Loss: 1.5153145790100098\n",
      "Iteration 9908 Loss: 1.5394612550735474\n",
      "Iteration 9909 Loss: 1.5007530450820923\n",
      "Iteration 9909 Loss: 1.4307365417480469\n",
      "Iteration 9910 Loss: 1.6458964347839355\n",
      "Iteration 9911 Loss: 1.269281268119812\n",
      "Iteration 9912 Loss: 1.5443313121795654\n",
      "Iteration 9913 Loss: 1.4809134006500244\n",
      "Iteration 9914 Loss: 1.454189658164978\n",
      "Iteration 9915 Loss: 1.5318057537078857\n",
      "Iteration 9916 Loss: 1.4657176733016968\n",
      "Iteration 9917 Loss: 1.4652321338653564\n",
      "Iteration 9918 Loss: 1.1115003824234009\n",
      "Iteration 9919 Loss: 1.5564473867416382\n",
      "Iteration 9919 Loss: 1.4525314569473267\n",
      "Iteration 9920 Loss: 1.170974612236023\n",
      "Iteration 9921 Loss: 1.8504421710968018\n",
      "Iteration 9922 Loss: 1.431520700454712\n",
      "Iteration 9923 Loss: 1.1929627656936646\n",
      "Iteration 9924 Loss: 1.4397718906402588\n",
      "Iteration 9925 Loss: 1.399476408958435\n",
      "Iteration 9926 Loss: 1.2405017614364624\n",
      "Iteration 9927 Loss: 1.1278905868530273\n",
      "Iteration 9928 Loss: 1.2047886848449707\n",
      "Iteration 9929 Loss: 1.3946807384490967\n",
      "Iteration 9929 Loss: 1.3453009128570557\n",
      "Iteration 9930 Loss: 1.3581207990646362\n",
      "Iteration 9931 Loss: 0.5567089319229126\n",
      "Iteration 9932 Loss: 1.6958365440368652\n",
      "Iteration 9933 Loss: 1.5189317464828491\n",
      "Iteration 9934 Loss: 1.1840038299560547\n",
      "Iteration 9935 Loss: 1.4368318319320679\n",
      "Iteration 9936 Loss: 1.6440447568893433\n",
      "Iteration 9937 Loss: 1.679813027381897\n",
      "Iteration 9938 Loss: 1.246411919593811\n",
      "Iteration 9939 Loss: 1.0682251453399658\n",
      "Iteration 9939 Loss: 1.3388928174972534\n",
      "Iteration 9940 Loss: 1.2352931499481201\n",
      "Iteration 9941 Loss: 1.5138920545578003\n",
      "Iteration 9942 Loss: 1.7346901893615723\n",
      "Iteration 9943 Loss: 1.4118075370788574\n",
      "Iteration 9944 Loss: 1.2550674676895142\n",
      "Iteration 9945 Loss: 1.3336107730865479\n",
      "Iteration 9946 Loss: 0.9430356025695801\n",
      "Iteration 9947 Loss: 1.5168554782867432\n",
      "Iteration 9948 Loss: 1.4436334371566772\n",
      "Iteration 9949 Loss: 0.8590430021286011\n",
      "Iteration 9949 Loss: 1.324692964553833\n",
      "Iteration 9950 Loss: 1.2460558414459229\n",
      "Iteration 9951 Loss: 1.269436001777649\n",
      "Iteration 9952 Loss: 1.4026559591293335\n",
      "Iteration 9953 Loss: 1.5698955059051514\n",
      "Iteration 9954 Loss: 1.3247779607772827\n",
      "Iteration 9955 Loss: 0.9603200554847717\n",
      "Iteration 9956 Loss: 1.5094057321548462\n",
      "Iteration 9957 Loss: 1.725991129875183\n",
      "Iteration 9958 Loss: 0.8409503698348999\n",
      "Iteration 9959 Loss: 1.209069848060608\n",
      "Iteration 9959 Loss: 1.3058559894561768\n",
      "Iteration 9960 Loss: 1.3360530138015747\n",
      "Iteration 9961 Loss: 1.5566508769989014\n",
      "Iteration 9962 Loss: 1.43197500705719\n",
      "Iteration 9963 Loss: 1.3423879146575928\n",
      "Iteration 9964 Loss: 1.5643786191940308\n",
      "Iteration 9965 Loss: 1.198197603225708\n",
      "Iteration 9966 Loss: 1.0605212450027466\n",
      "Iteration 9967 Loss: 1.8567177057266235\n",
      "Iteration 9968 Loss: 1.2635372877120972\n",
      "Iteration 9969 Loss: 1.2217365503311157\n",
      "Iteration 9969 Loss: 1.3832155466079712\n",
      "Iteration 9970 Loss: 1.2291313409805298\n",
      "Iteration 9971 Loss: 1.168837070465088\n",
      "Iteration 9972 Loss: 0.8518281579017639\n",
      "Iteration 9973 Loss: 1.1697171926498413\n",
      "Iteration 9974 Loss: 1.7556328773498535\n",
      "Iteration 9975 Loss: 1.411537528038025\n",
      "Iteration 9976 Loss: 1.7902491092681885\n",
      "Iteration 9977 Loss: 1.3201547861099243\n",
      "Iteration 9978 Loss: 1.463948369026184\n",
      "Iteration 9979 Loss: 1.3009123802185059\n",
      "Iteration 9979 Loss: 1.346194863319397\n",
      "Iteration 9980 Loss: 1.2125985622406006\n",
      "Iteration 9981 Loss: 1.5518850088119507\n",
      "Iteration 9982 Loss: 1.5358209609985352\n",
      "Iteration 9983 Loss: 1.0707067251205444\n",
      "Iteration 9984 Loss: 1.184438943862915\n",
      "Iteration 9985 Loss: 1.5775498151779175\n",
      "Iteration 9986 Loss: 1.4208756685256958\n",
      "Iteration 9987 Loss: 1.2875161170959473\n",
      "Iteration 9988 Loss: 1.4375288486480713\n",
      "Iteration 9989 Loss: 1.6127647161483765\n",
      "Iteration 9989 Loss: 1.3891685009002686\n",
      "Iteration 9990 Loss: 1.451622724533081\n",
      "Iteration 9991 Loss: 1.2426819801330566\n",
      "Iteration 9992 Loss: 0.9963012933731079\n",
      "Iteration 9993 Loss: 1.214866042137146\n",
      "Iteration 9994 Loss: 1.5791592597961426\n",
      "Iteration 9995 Loss: 1.109789252281189\n",
      "Iteration 9996 Loss: 0.9391690492630005\n",
      "Iteration 9997 Loss: 1.1098848581314087\n",
      "Iteration 9998 Loss: 1.4075024127960205\n",
      "Iteration 9999 Loss: 1.4856501817703247\n",
      "Iteration 9999 Loss: 1.2536627054214478\n",
      "Iteration 10000 Loss: 0.9088545441627502\n",
      "Iteration 10001 Loss: 1.7558749914169312\n",
      "Iteration 10002 Loss: 1.3739662170410156\n",
      "Iteration 10003 Loss: 1.3078521490097046\n",
      "Iteration 10004 Loss: 1.2300297021865845\n",
      "Iteration 10005 Loss: 1.4067448377609253\n",
      "Iteration 10006 Loss: 1.6275076866149902\n",
      "Iteration 10007 Loss: 1.3349186182022095\n",
      "Iteration 10008 Loss: 1.332377552986145\n",
      "Iteration 10009 Loss: 1.6223032474517822\n",
      "Iteration 10009 Loss: 1.390043020248413\n",
      "Iteration 10010 Loss: 1.5638158321380615\n",
      "Iteration 10011 Loss: 1.2384968996047974\n",
      "Iteration 10012 Loss: 1.2191874980926514\n",
      "Iteration 10013 Loss: 1.1296882629394531\n",
      "Iteration 10014 Loss: 0.875054121017456\n",
      "Iteration 10015 Loss: 1.398730993270874\n",
      "Iteration 10016 Loss: 1.619786262512207\n",
      "Iteration 10017 Loss: 1.1960591077804565\n",
      "Iteration 10018 Loss: 1.4046039581298828\n",
      "Iteration 10019 Loss: 1.8139967918395996\n",
      "Iteration 10019 Loss: 1.3459420204162598\n",
      "Iteration 10020 Loss: 1.5477638244628906\n",
      "Iteration 10021 Loss: 1.3138010501861572\n",
      "Iteration 10022 Loss: 1.6219168901443481\n",
      "Iteration 10023 Loss: 1.7885082960128784\n",
      "Iteration 10024 Loss: 1.0108627080917358\n",
      "Iteration 10025 Loss: 1.6697969436645508\n",
      "Iteration 10026 Loss: 1.20978581905365\n",
      "Iteration 10027 Loss: 1.4023524522781372\n",
      "Iteration 10028 Loss: 1.3119133710861206\n",
      "Iteration 10029 Loss: 1.687862753868103\n",
      "Iteration 10029 Loss: 1.4564563035964966\n",
      "Iteration 10030 Loss: 1.6679565906524658\n",
      "Iteration 10031 Loss: 1.2096500396728516\n",
      "Iteration 10032 Loss: 1.35956871509552\n",
      "Iteration 10033 Loss: 1.0086073875427246\n",
      "Iteration 10034 Loss: 1.2141516208648682\n",
      "Iteration 10035 Loss: 1.3210818767547607\n",
      "Iteration 10036 Loss: 1.2739256620407104\n",
      "Iteration 10037 Loss: 1.4260618686676025\n",
      "Iteration 10038 Loss: 1.6526923179626465\n",
      "Iteration 10039 Loss: 1.4294556379318237\n",
      "Iteration 10039 Loss: 1.3563151359558105\n",
      "Iteration 10040 Loss: 1.3023842573165894\n",
      "Iteration 10041 Loss: 1.2243694067001343\n",
      "Iteration 10042 Loss: 1.2676677703857422\n",
      "Iteration 10043 Loss: 1.0716472864151\n",
      "Iteration 10044 Loss: 1.1471525430679321\n",
      "Iteration 10045 Loss: 1.4637565612792969\n",
      "Iteration 10046 Loss: 1.90928053855896\n",
      "Iteration 10047 Loss: 1.0059540271759033\n",
      "Iteration 10048 Loss: 1.4992374181747437\n",
      "Iteration 10049 Loss: 1.3900644779205322\n",
      "Iteration 10049 Loss: 1.3281514644622803\n",
      "Iteration 10050 Loss: 1.2334779500961304\n",
      "Iteration 10051 Loss: 1.504150152206421\n",
      "Iteration 10052 Loss: 1.5828626155853271\n",
      "Iteration 10053 Loss: 1.4326200485229492\n",
      "Iteration 10054 Loss: 1.0539255142211914\n",
      "Iteration 10055 Loss: 1.1894267797470093\n",
      "Iteration 10056 Loss: 0.9999411702156067\n",
      "Iteration 10057 Loss: 1.2271037101745605\n",
      "Iteration 10058 Loss: 1.430906057357788\n",
      "Iteration 10059 Loss: 1.6649385690689087\n",
      "Iteration 10059 Loss: 1.3319352865219116\n",
      "Iteration 10060 Loss: 1.0008705854415894\n",
      "Iteration 10061 Loss: 1.1525311470031738\n",
      "Iteration 10062 Loss: 1.5566589832305908\n",
      "Iteration 10063 Loss: 1.5368123054504395\n",
      "Iteration 10064 Loss: 0.8350977897644043\n",
      "Iteration 10065 Loss: 1.443314790725708\n",
      "Iteration 10066 Loss: 1.2183018922805786\n",
      "Iteration 10067 Loss: 1.247158169746399\n",
      "Iteration 10068 Loss: 0.9255581498146057\n",
      "Iteration 10069 Loss: 1.392915964126587\n",
      "Iteration 10069 Loss: 1.230921983718872\n",
      "Iteration 10070 Loss: 0.9712361097335815\n",
      "Iteration 10071 Loss: 1.4232970476150513\n",
      "Iteration 10072 Loss: 1.5002503395080566\n",
      "Iteration 10073 Loss: 1.2003048658370972\n",
      "Iteration 10074 Loss: 1.4225225448608398\n",
      "Iteration 10075 Loss: 1.5804827213287354\n",
      "Iteration 10076 Loss: 1.2719053030014038\n",
      "Iteration 10077 Loss: 1.2386682033538818\n",
      "Iteration 10078 Loss: 1.634207844734192\n",
      "Iteration 10079 Loss: 1.3224718570709229\n",
      "Iteration 10079 Loss: 1.356534719467163\n",
      "Iteration 10080 Loss: 0.9060375094413757\n",
      "Iteration 10081 Loss: 1.4616116285324097\n",
      "Iteration 10082 Loss: 1.4823375940322876\n",
      "Iteration 10083 Loss: 1.4966049194335938\n",
      "Iteration 10084 Loss: 1.2538278102874756\n",
      "Iteration 10085 Loss: 1.5735644102096558\n",
      "Iteration 10086 Loss: 1.4489516019821167\n",
      "Iteration 10087 Loss: 1.4334522485733032\n",
      "Iteration 10088 Loss: 1.5438084602355957\n",
      "Iteration 10089 Loss: 0.8998633623123169\n",
      "Iteration 10089 Loss: 1.3500059843063354\n",
      "Iteration 10090 Loss: 1.315665364265442\n",
      "Iteration 10091 Loss: 1.7419441938400269\n",
      "Iteration 10092 Loss: 1.4064867496490479\n",
      "Iteration 10093 Loss: 1.2448935508728027\n",
      "Iteration 10094 Loss: 1.2405427694320679\n",
      "Iteration 10095 Loss: 1.7506811618804932\n",
      "Iteration 10096 Loss: 1.190818428993225\n",
      "Iteration 10097 Loss: 1.4896191358566284\n",
      "Iteration 10098 Loss: 1.4224978685379028\n",
      "Iteration 10099 Loss: 1.2942829132080078\n",
      "Iteration 10099 Loss: 1.409743070602417\n",
      "Iteration 10100 Loss: 1.186586856842041\n",
      "Iteration 10101 Loss: 1.5495128631591797\n",
      "Iteration 10102 Loss: 1.132304310798645\n",
      "Iteration 10103 Loss: 1.5933340787887573\n",
      "Iteration 10104 Loss: 1.1750988960266113\n",
      "Iteration 10105 Loss: 1.998786211013794\n",
      "Iteration 10106 Loss: 1.3203123807907104\n",
      "Iteration 10107 Loss: 1.3313881158828735\n",
      "Iteration 10108 Loss: 1.5664445161819458\n",
      "Iteration 10109 Loss: 1.2278262376785278\n",
      "Iteration 10109 Loss: 1.4081594944000244\n",
      "Iteration 10110 Loss: 1.2944175004959106\n",
      "Iteration 10111 Loss: 1.424728512763977\n",
      "Iteration 10112 Loss: 1.7095011472702026\n",
      "Iteration 10113 Loss: 1.6300866603851318\n",
      "Iteration 10114 Loss: 1.7517740726470947\n",
      "Iteration 10115 Loss: 1.4073325395584106\n",
      "Iteration 10116 Loss: 1.4470276832580566\n",
      "Iteration 10117 Loss: 1.0247303247451782\n",
      "Iteration 10118 Loss: 1.354364275932312\n",
      "Iteration 10119 Loss: 1.0784977674484253\n",
      "Iteration 10119 Loss: 1.4122461080551147\n",
      "Iteration 10120 Loss: 1.5560405254364014\n",
      "Iteration 10121 Loss: 1.4981886148452759\n",
      "Iteration 10122 Loss: 1.8720594644546509\n",
      "Iteration 10123 Loss: 1.5839821100234985\n",
      "Iteration 10124 Loss: 2.0314571857452393\n",
      "Iteration 10125 Loss: 1.3867924213409424\n",
      "Iteration 10126 Loss: 0.9409311413764954\n",
      "Iteration 10127 Loss: 1.6444534063339233\n",
      "Iteration 10128 Loss: 1.0743026733398438\n",
      "Iteration 10129 Loss: 1.5602339506149292\n",
      "Iteration 10129 Loss: 1.5148441791534424\n",
      "Iteration 10130 Loss: 1.4147077798843384\n",
      "Iteration 10131 Loss: 1.3360445499420166\n",
      "Iteration 10132 Loss: 1.0993471145629883\n",
      "Iteration 10133 Loss: 0.9522721171379089\n",
      "Iteration 10134 Loss: 1.6071058511734009\n",
      "Iteration 10135 Loss: 0.8545718193054199\n",
      "Iteration 10136 Loss: 0.9406841397285461\n",
      "Iteration 10137 Loss: 1.282218098640442\n",
      "Iteration 10138 Loss: 1.1484768390655518\n",
      "Iteration 10139 Loss: 1.0410487651824951\n",
      "Iteration 10139 Loss: 1.1676477193832397\n",
      "Iteration 10140 Loss: 1.3030332326889038\n",
      "Iteration 10141 Loss: 1.3372315168380737\n",
      "Iteration 10142 Loss: 1.5018455982208252\n",
      "Iteration 10143 Loss: 1.4415743350982666\n",
      "Iteration 10144 Loss: 1.715824842453003\n",
      "Iteration 10145 Loss: 1.392282485961914\n",
      "Iteration 10146 Loss: 1.0434625148773193\n",
      "Iteration 10147 Loss: 1.2418051958084106\n",
      "Iteration 10148 Loss: 1.1995341777801514\n",
      "Iteration 10149 Loss: 1.2368422746658325\n",
      "Iteration 10149 Loss: 1.341343641281128\n",
      "Iteration 10150 Loss: 1.346101999282837\n",
      "Iteration 10151 Loss: 0.9602717161178589\n",
      "Iteration 10152 Loss: 1.3600938320159912\n",
      "Iteration 10153 Loss: 1.002896785736084\n",
      "Iteration 10154 Loss: 1.2922698259353638\n",
      "Iteration 10155 Loss: 1.2374012470245361\n",
      "Iteration 10156 Loss: 1.4261523485183716\n",
      "Iteration 10157 Loss: 1.393182635307312\n",
      "Iteration 10158 Loss: 1.1872389316558838\n",
      "Iteration 10159 Loss: 1.6300779581069946\n",
      "Iteration 10159 Loss: 1.2835687398910522\n",
      "Iteration 10160 Loss: 1.7163468599319458\n",
      "Iteration 10161 Loss: 1.5055067539215088\n",
      "Iteration 10162 Loss: 1.0969343185424805\n",
      "Iteration 10163 Loss: 1.2655515670776367\n",
      "Iteration 10164 Loss: 1.5425856113433838\n",
      "Iteration 10165 Loss: 1.3447891473770142\n",
      "Iteration 10166 Loss: 1.1553514003753662\n",
      "Iteration 10167 Loss: 1.092055320739746\n",
      "Iteration 10168 Loss: 1.1504372358322144\n",
      "Iteration 10169 Loss: 1.3663464784622192\n",
      "Iteration 10169 Loss: 1.3235905170440674\n",
      "Iteration 10170 Loss: 1.4926528930664062\n",
      "Iteration 10171 Loss: 1.60062575340271\n",
      "Iteration 10172 Loss: 1.2585949897766113\n",
      "Iteration 10173 Loss: 1.368459939956665\n",
      "Iteration 10174 Loss: 0.9183018803596497\n",
      "Iteration 10175 Loss: 1.413546085357666\n",
      "Iteration 10176 Loss: 1.1289345026016235\n",
      "Iteration 10177 Loss: 1.682841181755066\n",
      "Iteration 10178 Loss: 0.9678133726119995\n",
      "Iteration 10179 Loss: 1.1081565618515015\n",
      "Iteration 10179 Loss: 1.2939927577972412\n",
      "Iteration 10180 Loss: 1.4898334741592407\n",
      "Iteration 10181 Loss: 0.9500598311424255\n",
      "Iteration 10182 Loss: 1.7195688486099243\n",
      "Iteration 10183 Loss: 1.4215977191925049\n",
      "Iteration 10184 Loss: 1.531546950340271\n",
      "Iteration 10185 Loss: 1.5174086093902588\n",
      "Iteration 10186 Loss: 1.6831514835357666\n",
      "Iteration 10187 Loss: 0.7648568153381348\n",
      "Iteration 10188 Loss: 1.0516623258590698\n",
      "Iteration 10189 Loss: 1.3315478563308716\n",
      "Iteration 10189 Loss: 1.3461233377456665\n",
      "Iteration 10190 Loss: 1.4766530990600586\n",
      "Iteration 10191 Loss: 1.5451946258544922\n",
      "Iteration 10192 Loss: 1.3992040157318115\n",
      "Iteration 10193 Loss: 1.3297861814498901\n",
      "Iteration 10194 Loss: 1.6836528778076172\n",
      "Iteration 10195 Loss: 1.2340853214263916\n",
      "Iteration 10196 Loss: 1.6342602968215942\n",
      "Iteration 10197 Loss: 1.0582764148712158\n",
      "Iteration 10198 Loss: 1.4111244678497314\n",
      "Iteration 10199 Loss: 1.2856062650680542\n",
      "Iteration 10199 Loss: 1.4057843685150146\n",
      "Iteration 10200 Loss: 1.2712242603302002\n",
      "Iteration 10201 Loss: 0.9323049187660217\n",
      "Iteration 10202 Loss: 0.9498736262321472\n",
      "Iteration 10203 Loss: 1.2351363897323608\n",
      "Iteration 10204 Loss: 1.1862517595291138\n",
      "Iteration 10205 Loss: 0.8695340156555176\n",
      "Iteration 10206 Loss: 1.5883890390396118\n",
      "Iteration 10207 Loss: 1.316692590713501\n",
      "Iteration 10208 Loss: 1.1360182762145996\n",
      "Iteration 10209 Loss: 1.4560600519180298\n",
      "Iteration 10209 Loss: 1.1941485404968262\n",
      "Iteration 10210 Loss: 1.1174730062484741\n",
      "Iteration 10211 Loss: 1.2444130182266235\n",
      "Iteration 10212 Loss: 1.166319489479065\n",
      "Iteration 10213 Loss: 1.062201976776123\n",
      "Iteration 10214 Loss: 1.3609095811843872\n",
      "Iteration 10215 Loss: 1.7796121835708618\n",
      "Iteration 10216 Loss: 1.5294830799102783\n",
      "Iteration 10217 Loss: 1.3012597560882568\n",
      "Iteration 10218 Loss: 0.8583353757858276\n",
      "Iteration 10219 Loss: 1.5148981809616089\n",
      "Iteration 10219 Loss: 1.2934906482696533\n",
      "Iteration 10220 Loss: 1.1887285709381104\n",
      "Iteration 10221 Loss: 1.0743569135665894\n",
      "Iteration 10222 Loss: 1.5358083248138428\n",
      "Iteration 10223 Loss: 1.540976881980896\n",
      "Iteration 10224 Loss: 1.5823767185211182\n",
      "Iteration 10225 Loss: 1.1440412998199463\n",
      "Iteration 10226 Loss: 1.0362603664398193\n",
      "Iteration 10227 Loss: 1.6089718341827393\n",
      "Iteration 10228 Loss: 1.2090834379196167\n",
      "Iteration 10229 Loss: 1.0700130462646484\n",
      "Iteration 10229 Loss: 1.2990617752075195\n",
      "Iteration 10230 Loss: 1.6910706758499146\n",
      "Iteration 10231 Loss: 1.3890589475631714\n",
      "Iteration 10232 Loss: 1.5490198135375977\n",
      "Iteration 10233 Loss: 1.6993061304092407\n",
      "Iteration 10234 Loss: 1.1925561428070068\n",
      "Iteration 10235 Loss: 1.4381767511367798\n",
      "Iteration 10236 Loss: 1.1681811809539795\n",
      "Iteration 10237 Loss: 1.3596303462982178\n",
      "Iteration 10238 Loss: 1.7018449306488037\n",
      "Iteration 10239 Loss: 1.369737148284912\n",
      "Iteration 10239 Loss: 1.4558583498001099\n",
      "Iteration 10240 Loss: 1.057822346687317\n",
      "Iteration 10241 Loss: 1.375150203704834\n",
      "Iteration 10242 Loss: 1.5995368957519531\n",
      "Iteration 10243 Loss: 2.0062971115112305\n",
      "Iteration 10244 Loss: 1.4794212579727173\n",
      "Iteration 10245 Loss: 1.3045436143875122\n",
      "Iteration 10246 Loss: 1.684688687324524\n",
      "Iteration 10247 Loss: 1.4539830684661865\n",
      "Iteration 10248 Loss: 1.2395200729370117\n",
      "Iteration 10249 Loss: 1.3833814859390259\n",
      "Iteration 10249 Loss: 1.4584344625473022\n",
      "Iteration 10250 Loss: 1.3267102241516113\n",
      "Iteration 10251 Loss: 1.3605104684829712\n",
      "Iteration 10252 Loss: 1.4569759368896484\n",
      "Iteration 10253 Loss: 0.9823837280273438\n",
      "Iteration 10254 Loss: 1.461329460144043\n",
      "Iteration 10255 Loss: 1.3662569522857666\n",
      "Iteration 10256 Loss: 1.3888964653015137\n",
      "Iteration 10257 Loss: 1.4471373558044434\n",
      "Iteration 10258 Loss: 1.507121205329895\n",
      "Iteration 10259 Loss: 1.4856828451156616\n",
      "Iteration 10259 Loss: 1.378300428390503\n",
      "Iteration 10260 Loss: 1.7300784587860107\n",
      "Iteration 10261 Loss: 1.62488853931427\n",
      "Iteration 10262 Loss: 1.5676543712615967\n",
      "Iteration 10263 Loss: 1.1446141004562378\n",
      "Iteration 10264 Loss: 1.1270582675933838\n",
      "Iteration 10265 Loss: 0.99663245677948\n",
      "Iteration 10266 Loss: 1.3760271072387695\n",
      "Iteration 10267 Loss: 1.6238094568252563\n",
      "Iteration 10268 Loss: 1.3622419834136963\n",
      "Iteration 10269 Loss: 1.4075579643249512\n",
      "Iteration 10269 Loss: 1.3960562944412231\n",
      "Iteration 10270 Loss: 1.1476571559906006\n",
      "Iteration 10271 Loss: 1.3010179996490479\n",
      "Iteration 10272 Loss: 1.6089483499526978\n",
      "Iteration 10273 Loss: 1.2027406692504883\n",
      "Iteration 10274 Loss: 1.4030283689498901\n",
      "Iteration 10275 Loss: 1.2549301385879517\n",
      "Iteration 10276 Loss: 1.3209120035171509\n",
      "Iteration 10277 Loss: 1.5875966548919678\n",
      "Iteration 10278 Loss: 1.1588784456253052\n",
      "Iteration 10279 Loss: 1.193841576576233\n",
      "Iteration 10279 Loss: 1.3179552555084229\n",
      "Iteration 10280 Loss: 1.497882604598999\n",
      "Iteration 10281 Loss: 1.3599967956542969\n",
      "Iteration 10282 Loss: 1.323783040046692\n",
      "Iteration 10283 Loss: 0.9861849546432495\n",
      "Iteration 10284 Loss: 1.4375308752059937\n",
      "Iteration 10285 Loss: 0.9925125241279602\n",
      "Iteration 10286 Loss: 1.4999494552612305\n",
      "Iteration 10287 Loss: 1.0906485319137573\n",
      "Iteration 10288 Loss: 1.5143492221832275\n",
      "Iteration 10289 Loss: 1.2535512447357178\n",
      "Iteration 10289 Loss: 1.295638918876648\n",
      "Iteration 10290 Loss: 1.0026081800460815\n",
      "Iteration 10291 Loss: 0.9297569394111633\n",
      "Iteration 10292 Loss: 1.3974385261535645\n",
      "Iteration 10293 Loss: 1.3351120948791504\n",
      "Iteration 10294 Loss: 2.0574231147766113\n",
      "Iteration 10295 Loss: 1.3327322006225586\n",
      "Iteration 10296 Loss: 1.4550693035125732\n",
      "Iteration 10297 Loss: 1.345394253730774\n",
      "Iteration 10298 Loss: 1.0802947282791138\n",
      "Iteration 10299 Loss: 1.1430128812789917\n",
      "Iteration 10299 Loss: 1.3078842163085938\n",
      "Iteration 10300 Loss: 1.843064308166504\n",
      "Iteration 10301 Loss: 1.4795832633972168\n",
      "Iteration 10302 Loss: 1.7140406370162964\n",
      "Iteration 10303 Loss: 1.1014444828033447\n",
      "Iteration 10304 Loss: 1.2560392618179321\n",
      "Iteration 10305 Loss: 1.4762117862701416\n",
      "Iteration 10306 Loss: 1.3196543455123901\n",
      "Iteration 10307 Loss: 1.0260882377624512\n",
      "Iteration 10308 Loss: 1.5793076753616333\n",
      "Iteration 10309 Loss: 1.4848514795303345\n",
      "Iteration 10309 Loss: 1.4280284643173218\n",
      "Iteration 10310 Loss: 1.057187795639038\n",
      "Iteration 10311 Loss: 1.5688128471374512\n",
      "Iteration 10312 Loss: 1.1489416360855103\n",
      "Iteration 10313 Loss: 1.3428555727005005\n",
      "Iteration 10314 Loss: 0.9483718276023865\n",
      "Iteration 10315 Loss: 1.7190303802490234\n",
      "Iteration 10316 Loss: 1.0167335271835327\n",
      "Iteration 10317 Loss: 1.1976940631866455\n",
      "Iteration 10318 Loss: 1.3153648376464844\n",
      "Iteration 10319 Loss: 1.0504823923110962\n",
      "Iteration 10319 Loss: 1.2365473508834839\n",
      "Iteration 10320 Loss: 1.2105486392974854\n",
      "Iteration 10321 Loss: 1.176417350769043\n",
      "Iteration 10322 Loss: 1.7881908416748047\n",
      "Iteration 10323 Loss: 1.0620874166488647\n",
      "Iteration 10324 Loss: 1.8616230487823486\n",
      "Iteration 10325 Loss: 1.0034068822860718\n",
      "Iteration 10326 Loss: 1.295959711074829\n",
      "Iteration 10327 Loss: 1.1170787811279297\n",
      "Iteration 10328 Loss: 1.2765066623687744\n",
      "Iteration 10329 Loss: 1.1817322969436646\n",
      "Iteration 10329 Loss: 1.2973551750183105\n",
      "Iteration 10330 Loss: 1.0612130165100098\n",
      "Iteration 10331 Loss: 1.1597278118133545\n",
      "Iteration 10332 Loss: 1.0913747549057007\n",
      "Iteration 10333 Loss: 1.4000426530838013\n",
      "Iteration 10334 Loss: 1.7827593088150024\n",
      "Iteration 10335 Loss: 1.155199408531189\n",
      "Iteration 10336 Loss: 1.6496376991271973\n",
      "Iteration 10337 Loss: 1.3314487934112549\n",
      "Iteration 10338 Loss: 1.2335010766983032\n",
      "Iteration 10339 Loss: 1.1945842504501343\n",
      "Iteration 10339 Loss: 1.3059487342834473\n",
      "Iteration 10340 Loss: 1.4164924621582031\n",
      "Iteration 10341 Loss: 1.181222915649414\n",
      "Iteration 10342 Loss: 1.5851924419403076\n",
      "Iteration 10343 Loss: 1.3342589139938354\n",
      "Iteration 10344 Loss: 1.53780198097229\n",
      "Iteration 10345 Loss: 1.224267840385437\n",
      "Iteration 10346 Loss: 1.239592432975769\n",
      "Iteration 10347 Loss: 1.4291260242462158\n",
      "Iteration 10348 Loss: 1.2218133211135864\n",
      "Iteration 10349 Loss: 1.6845988035202026\n",
      "Iteration 10349 Loss: 1.385436773300171\n",
      "Iteration 10350 Loss: 1.3035917282104492\n",
      "Iteration 10351 Loss: 1.4002437591552734\n",
      "Iteration 10352 Loss: 1.0450942516326904\n",
      "Iteration 10353 Loss: 1.3315892219543457\n",
      "Iteration 10354 Loss: 1.634844183921814\n",
      "Iteration 10355 Loss: 1.353857398033142\n",
      "Iteration 10356 Loss: 1.1832793951034546\n",
      "Iteration 10357 Loss: 1.1711994409561157\n",
      "Iteration 10358 Loss: 1.311704397201538\n",
      "Iteration 10359 Loss: 1.268272876739502\n",
      "Iteration 10359 Loss: 1.3003675937652588\n",
      "Iteration 10360 Loss: 1.2780100107192993\n",
      "Iteration 10361 Loss: 1.4828639030456543\n",
      "Iteration 10362 Loss: 1.7645366191864014\n",
      "Iteration 10363 Loss: 1.1053080558776855\n",
      "Iteration 10364 Loss: 1.6352940797805786\n",
      "Iteration 10365 Loss: 1.674045205116272\n",
      "Iteration 10366 Loss: 1.4554353952407837\n",
      "Iteration 10367 Loss: 1.7943121194839478\n",
      "Iteration 10368 Loss: 1.2327715158462524\n",
      "Iteration 10369 Loss: 1.0703779458999634\n",
      "Iteration 10369 Loss: 1.4492956399917603\n",
      "Iteration 10370 Loss: 1.84722101688385\n",
      "Iteration 10371 Loss: 1.2224161624908447\n",
      "Iteration 10372 Loss: 0.8421405553817749\n",
      "Iteration 10373 Loss: 1.7323793172836304\n",
      "Iteration 10374 Loss: 1.3742321729660034\n",
      "Iteration 10375 Loss: 1.386174201965332\n",
      "Iteration 10376 Loss: 1.1578242778778076\n",
      "Iteration 10377 Loss: 1.2466903924942017\n",
      "Iteration 10378 Loss: 1.4269450902938843\n",
      "Iteration 10379 Loss: 1.2652690410614014\n",
      "Iteration 10379 Loss: 1.3501293659210205\n",
      "Iteration 10380 Loss: 1.4718669652938843\n",
      "Iteration 10381 Loss: 1.501094937324524\n",
      "Iteration 10382 Loss: 1.2553629875183105\n",
      "Iteration 10383 Loss: 0.8277930021286011\n",
      "Iteration 10384 Loss: 1.2494806051254272\n",
      "Iteration 10385 Loss: 1.0264562368392944\n",
      "Iteration 10386 Loss: 1.3327903747558594\n",
      "Iteration 10387 Loss: 1.5184414386749268\n",
      "Iteration 10388 Loss: 1.6272128820419312\n",
      "Iteration 10389 Loss: 1.3019236326217651\n",
      "Iteration 10389 Loss: 1.3112422227859497\n",
      "Iteration 10390 Loss: 1.3616920709609985\n",
      "Iteration 10391 Loss: 1.3104525804519653\n",
      "Iteration 10392 Loss: 1.3368629217147827\n",
      "Iteration 10393 Loss: 1.3257081508636475\n",
      "Iteration 10394 Loss: 0.9212194681167603\n",
      "Iteration 10395 Loss: 0.8369468450546265\n",
      "Iteration 10396 Loss: 1.097153902053833\n",
      "Iteration 10397 Loss: 1.2895348072052002\n",
      "Iteration 10398 Loss: 1.751888632774353\n",
      "Iteration 10399 Loss: 0.9176648259162903\n",
      "Iteration 10399 Loss: 1.2149124145507812\n",
      "Iteration 10400 Loss: 1.2767293453216553\n",
      "Iteration 10401 Loss: 1.5058428049087524\n",
      "Iteration 10402 Loss: 1.8265986442565918\n",
      "Iteration 10403 Loss: 1.5652801990509033\n",
      "Iteration 10404 Loss: 1.4039517641067505\n",
      "Iteration 10405 Loss: 1.4387006759643555\n",
      "Iteration 10406 Loss: 1.7677253484725952\n",
      "Iteration 10407 Loss: 1.1090954542160034\n",
      "Iteration 10408 Loss: 1.3921937942504883\n",
      "Iteration 10409 Loss: 1.3939850330352783\n",
      "Iteration 10409 Loss: 1.4680101871490479\n",
      "Iteration 10410 Loss: 0.9768718481063843\n",
      "Iteration 10411 Loss: 1.3631113767623901\n",
      "Iteration 10412 Loss: 1.63922119140625\n",
      "Iteration 10413 Loss: 1.3823493719100952\n",
      "Iteration 10414 Loss: 1.1098750829696655\n",
      "Iteration 10415 Loss: 0.985474169254303\n",
      "Iteration 10416 Loss: 1.8214563131332397\n",
      "Iteration 10417 Loss: 1.0336759090423584\n",
      "Iteration 10418 Loss: 1.934161901473999\n",
      "Iteration 10419 Loss: 1.5300171375274658\n",
      "Iteration 10419 Loss: 1.3776214122772217\n",
      "Iteration 10420 Loss: 1.2560418844223022\n",
      "Iteration 10421 Loss: 1.2155542373657227\n",
      "Iteration 10422 Loss: 1.0654358863830566\n",
      "Iteration 10423 Loss: 1.3163297176361084\n",
      "Iteration 10424 Loss: 1.436207890510559\n",
      "Iteration 10425 Loss: 1.2365591526031494\n",
      "Iteration 10426 Loss: 1.3710439205169678\n",
      "Iteration 10427 Loss: 1.5559625625610352\n",
      "Iteration 10428 Loss: 1.1466413736343384\n",
      "Iteration 10429 Loss: 1.1220152378082275\n",
      "Iteration 10429 Loss: 1.2721792459487915\n",
      "Iteration 10430 Loss: 1.0072426795959473\n",
      "Iteration 10431 Loss: 1.306530475616455\n",
      "Iteration 10432 Loss: 1.6546727418899536\n",
      "Iteration 10433 Loss: 1.4791005849838257\n",
      "Iteration 10434 Loss: 1.6543084383010864\n",
      "Iteration 10435 Loss: 1.1534749269485474\n",
      "Iteration 10436 Loss: 1.1171107292175293\n",
      "Iteration 10437 Loss: 1.1609845161437988\n",
      "Iteration 10438 Loss: 1.2083849906921387\n",
      "Iteration 10439 Loss: 1.7049983739852905\n",
      "Iteration 10439 Loss: 1.3446807861328125\n",
      "Iteration 10440 Loss: 1.4874802827835083\n",
      "Iteration 10441 Loss: 1.088452935218811\n",
      "Iteration 10442 Loss: 1.3728400468826294\n",
      "Iteration 10443 Loss: 1.4048304557800293\n",
      "Iteration 10444 Loss: 0.9173001050949097\n",
      "Iteration 10445 Loss: 1.5600488185882568\n",
      "Iteration 10446 Loss: 1.654313325881958\n",
      "Iteration 10447 Loss: 1.1876827478408813\n",
      "Iteration 10448 Loss: 1.6201999187469482\n",
      "Iteration 10449 Loss: 0.674964189529419\n",
      "Iteration 10449 Loss: 1.2968113422393799\n",
      "Iteration 10450 Loss: 1.1840379238128662\n",
      "Iteration 10451 Loss: 1.643365740776062\n",
      "Iteration 10452 Loss: 1.0002224445343018\n",
      "Iteration 10453 Loss: 1.343666672706604\n",
      "Iteration 10454 Loss: 1.402852177619934\n",
      "Iteration 10455 Loss: 1.233032464981079\n",
      "Iteration 10456 Loss: 1.9433801174163818\n",
      "Iteration 10457 Loss: 1.2051658630371094\n",
      "Iteration 10458 Loss: 1.2096666097640991\n",
      "Iteration 10459 Loss: 1.8163385391235352\n",
      "Iteration 10459 Loss: 1.3981728553771973\n",
      "Iteration 10460 Loss: 1.400362253189087\n",
      "Iteration 10461 Loss: 1.5514825582504272\n",
      "Iteration 10462 Loss: 1.2451019287109375\n",
      "Iteration 10463 Loss: 1.2189058065414429\n",
      "Iteration 10464 Loss: 1.2294615507125854\n",
      "Iteration 10465 Loss: 1.1230578422546387\n",
      "Iteration 10466 Loss: 1.1225003004074097\n",
      "Iteration 10467 Loss: 0.7691469192504883\n",
      "Iteration 10468 Loss: 1.4171236753463745\n",
      "Iteration 10469 Loss: 1.8462320566177368\n",
      "Iteration 10469 Loss: 1.2923376560211182\n",
      "Iteration 10470 Loss: 1.3716124296188354\n",
      "Iteration 10471 Loss: 1.3997446298599243\n",
      "Iteration 10472 Loss: 1.1784770488739014\n",
      "Iteration 10473 Loss: 1.3249012231826782\n",
      "Iteration 10474 Loss: 1.2134838104248047\n",
      "Iteration 10475 Loss: 1.748857021331787\n",
      "Iteration 10476 Loss: 1.8779500722885132\n",
      "Iteration 10477 Loss: 1.502285122871399\n",
      "Iteration 10478 Loss: 1.4210560321807861\n",
      "Iteration 10479 Loss: 1.0133438110351562\n",
      "Iteration 10479 Loss: 1.4051711559295654\n",
      "Iteration 10480 Loss: 1.2789480686187744\n",
      "Iteration 10481 Loss: 1.325593113899231\n",
      "Iteration 10482 Loss: 1.5469027757644653\n",
      "Iteration 10483 Loss: 1.4924664497375488\n",
      "Iteration 10484 Loss: 1.709691047668457\n",
      "Iteration 10485 Loss: 1.321001410484314\n",
      "Iteration 10486 Loss: 0.9122603535652161\n",
      "Iteration 10487 Loss: 1.1915309429168701\n",
      "Iteration 10488 Loss: 1.7112927436828613\n",
      "Iteration 10489 Loss: 1.3451869487762451\n",
      "Iteration 10489 Loss: 1.383487343788147\n",
      "Iteration 10490 Loss: 1.0962003469467163\n",
      "Iteration 10491 Loss: 1.1728485822677612\n",
      "Iteration 10492 Loss: 1.4466383457183838\n",
      "Iteration 10493 Loss: 0.9251396656036377\n",
      "Iteration 10494 Loss: 1.8734068870544434\n",
      "Iteration 10495 Loss: 1.1515297889709473\n",
      "Iteration 10496 Loss: 1.561967372894287\n",
      "Iteration 10497 Loss: 1.1995508670806885\n",
      "Iteration 10498 Loss: 1.2901897430419922\n",
      "Iteration 10499 Loss: 1.4833085536956787\n",
      "Iteration 10499 Loss: 1.320077896118164\n",
      "Iteration 10500 Loss: 1.284383773803711\n",
      "Iteration 10501 Loss: 1.5730632543563843\n",
      "Iteration 10502 Loss: 1.0335811376571655\n",
      "Iteration 10503 Loss: 0.9886309504508972\n",
      "Iteration 10504 Loss: 1.322186827659607\n",
      "Iteration 10505 Loss: 1.4142783880233765\n",
      "Iteration 10506 Loss: 1.4629509449005127\n",
      "Iteration 10507 Loss: 0.9648767709732056\n",
      "Iteration 10508 Loss: 1.2460217475891113\n",
      "Iteration 10509 Loss: 1.6518906354904175\n",
      "Iteration 10509 Loss: 1.2941863536834717\n",
      "Iteration 10510 Loss: 1.5358558893203735\n",
      "Iteration 10511 Loss: 1.4603118896484375\n",
      "Iteration 10512 Loss: 1.0473990440368652\n",
      "Iteration 10513 Loss: 1.047466516494751\n",
      "Iteration 10514 Loss: 1.1853039264678955\n",
      "Iteration 10515 Loss: 1.1538845300674438\n",
      "Iteration 10516 Loss: 1.767960548400879\n",
      "Iteration 10517 Loss: 1.5047760009765625\n",
      "Iteration 10518 Loss: 1.2501215934753418\n",
      "Iteration 10519 Loss: 1.2923808097839355\n",
      "Iteration 10519 Loss: 1.3245460987091064\n",
      "Iteration 10520 Loss: 1.5396132469177246\n",
      "Iteration 10521 Loss: 1.3340703248977661\n",
      "Iteration 10522 Loss: 0.9254239797592163\n",
      "Iteration 10523 Loss: 1.4860409498214722\n",
      "Iteration 10524 Loss: 1.029808759689331\n",
      "Iteration 10525 Loss: 1.0345512628555298\n",
      "Iteration 10526 Loss: 1.5843886137008667\n",
      "Iteration 10527 Loss: 1.1786704063415527\n",
      "Iteration 10528 Loss: 1.3698155879974365\n",
      "Iteration 10529 Loss: 0.7702640295028687\n",
      "Iteration 10529 Loss: 1.2252647876739502\n",
      "Iteration 10530 Loss: 1.3678653240203857\n",
      "Iteration 10531 Loss: 1.7099679708480835\n",
      "Iteration 10532 Loss: 0.8452212810516357\n",
      "Iteration 10533 Loss: 1.3271979093551636\n",
      "Iteration 10534 Loss: 1.4006755352020264\n",
      "Iteration 10535 Loss: 1.4383745193481445\n",
      "Iteration 10536 Loss: 1.1456190347671509\n",
      "Iteration 10537 Loss: 1.129046082496643\n",
      "Iteration 10538 Loss: 1.662542462348938\n",
      "Iteration 10539 Loss: 1.3542108535766602\n",
      "Iteration 10539 Loss: 1.3380721807479858\n",
      "Iteration 10540 Loss: 1.4496114253997803\n",
      "Iteration 10541 Loss: 1.134522557258606\n",
      "Iteration 10542 Loss: 1.3963847160339355\n",
      "Iteration 10543 Loss: 0.7826435565948486\n",
      "Iteration 10544 Loss: 1.3847589492797852\n",
      "Iteration 10545 Loss: 1.2983567714691162\n",
      "Iteration 10546 Loss: 1.5132887363433838\n",
      "Iteration 10547 Loss: 1.1629389524459839\n",
      "Iteration 10548 Loss: 1.463889479637146\n",
      "Iteration 10549 Loss: 1.7522729635238647\n",
      "Iteration 10549 Loss: 1.333866834640503\n",
      "Iteration 10550 Loss: 1.2416460514068604\n",
      "Iteration 10551 Loss: 1.0902900695800781\n",
      "Iteration 10552 Loss: 1.2564247846603394\n",
      "Iteration 10553 Loss: 1.2164980173110962\n",
      "Iteration 10554 Loss: 1.357704997062683\n",
      "Iteration 10555 Loss: 1.5497199296951294\n",
      "Iteration 10556 Loss: 1.5241986513137817\n",
      "Iteration 10557 Loss: 1.5140019655227661\n",
      "Iteration 10558 Loss: 1.0991957187652588\n",
      "Iteration 10559 Loss: 1.4016642570495605\n",
      "Iteration 10559 Loss: 1.3251343965530396\n",
      "Iteration 10560 Loss: 1.2572567462921143\n",
      "Iteration 10561 Loss: 1.1737297773361206\n",
      "Iteration 10562 Loss: 1.4431146383285522\n",
      "Iteration 10563 Loss: 1.4313496351242065\n",
      "Iteration 10564 Loss: 1.2959001064300537\n",
      "Iteration 10565 Loss: 1.3681743144989014\n",
      "Iteration 10566 Loss: 1.3068804740905762\n",
      "Iteration 10567 Loss: 1.5323837995529175\n",
      "Iteration 10568 Loss: 1.7776966094970703\n",
      "Iteration 10569 Loss: 1.4353415966033936\n",
      "Iteration 10569 Loss: 1.4021828174591064\n",
      "Iteration 10570 Loss: 1.3557816743850708\n",
      "Iteration 10571 Loss: 1.546332836151123\n",
      "Iteration 10572 Loss: 1.1274334192276\n",
      "Iteration 10573 Loss: 1.243283748626709\n",
      "Iteration 10574 Loss: 1.367974042892456\n",
      "Iteration 10575 Loss: 1.5099397897720337\n",
      "Iteration 10576 Loss: 1.714264988899231\n",
      "Iteration 10577 Loss: 1.7639180421829224\n",
      "Iteration 10578 Loss: 1.3661210536956787\n",
      "Iteration 10579 Loss: 0.7527033090591431\n",
      "Iteration 10579 Loss: 1.3747752904891968\n",
      "Iteration 10580 Loss: 1.3164790868759155\n",
      "Iteration 10581 Loss: 1.1290572881698608\n",
      "Iteration 10582 Loss: 1.2908413410186768\n",
      "Iteration 10583 Loss: 1.351714849472046\n",
      "Iteration 10584 Loss: 1.3317193984985352\n",
      "Iteration 10585 Loss: 1.3921970129013062\n",
      "Iteration 10586 Loss: 1.3376281261444092\n",
      "Iteration 10587 Loss: 0.9732001423835754\n",
      "Iteration 10588 Loss: 1.1160333156585693\n",
      "Iteration 10589 Loss: 1.52166748046875\n",
      "Iteration 10589 Loss: 1.276053786277771\n",
      "Iteration 10590 Loss: 1.3876557350158691\n",
      "Iteration 10591 Loss: 1.8838589191436768\n",
      "Iteration 10592 Loss: 0.6820188164710999\n",
      "Iteration 10593 Loss: 1.2261059284210205\n",
      "Iteration 10594 Loss: 1.5824565887451172\n",
      "Iteration 10595 Loss: 1.4810943603515625\n",
      "Iteration 10596 Loss: 1.3292529582977295\n",
      "Iteration 10597 Loss: 1.1625430583953857\n",
      "Iteration 10598 Loss: 1.7587213516235352\n",
      "Iteration 10599 Loss: 1.0154922008514404\n",
      "Iteration 10599 Loss: 1.3509199619293213\n",
      "Iteration 10600 Loss: 0.9118313789367676\n",
      "Iteration 10601 Loss: 1.4955095052719116\n",
      "Iteration 10602 Loss: 1.5762931108474731\n",
      "Iteration 10603 Loss: 1.283260703086853\n",
      "Iteration 10604 Loss: 1.2305349111557007\n",
      "Iteration 10605 Loss: 1.3568840026855469\n",
      "Iteration 10606 Loss: 1.4952757358551025\n",
      "Iteration 10607 Loss: 0.9947379231452942\n",
      "Iteration 10608 Loss: 1.3810670375823975\n",
      "Iteration 10609 Loss: 1.3352240324020386\n",
      "Iteration 10609 Loss: 1.3060617446899414\n",
      "Iteration 10610 Loss: 1.0880111455917358\n",
      "Iteration 10611 Loss: 1.216968297958374\n",
      "Iteration 10612 Loss: 1.6266201734542847\n",
      "Iteration 10613 Loss: 1.5177773237228394\n",
      "Iteration 10614 Loss: 1.6757009029388428\n",
      "Iteration 10615 Loss: 1.3867095708847046\n",
      "Iteration 10616 Loss: 1.6661239862442017\n",
      "Iteration 10617 Loss: 1.327778935432434\n",
      "Iteration 10618 Loss: 1.0688165426254272\n",
      "Iteration 10619 Loss: 1.0746649503707886\n",
      "Iteration 10619 Loss: 1.3649171590805054\n",
      "Iteration 10620 Loss: 0.9558883309364319\n",
      "Iteration 10621 Loss: 1.4212989807128906\n",
      "Iteration 10622 Loss: 1.4283722639083862\n",
      "Iteration 10623 Loss: 1.0146918296813965\n",
      "Iteration 10624 Loss: 1.446103572845459\n",
      "Iteration 10625 Loss: 1.611098289489746\n",
      "Iteration 10626 Loss: 1.171849012374878\n",
      "Iteration 10627 Loss: 1.7555240392684937\n",
      "Iteration 10628 Loss: 1.1366441249847412\n",
      "Iteration 10629 Loss: 1.5655019283294678\n",
      "Iteration 10629 Loss: 1.3506972789764404\n",
      "Iteration 10630 Loss: 1.3958919048309326\n",
      "Iteration 10631 Loss: 1.2919491529464722\n",
      "Iteration 10632 Loss: 1.4210478067398071\n",
      "Iteration 10633 Loss: 1.6399857997894287\n",
      "Iteration 10634 Loss: 1.2832659482955933\n",
      "Iteration 10635 Loss: 1.320705771446228\n",
      "Iteration 10636 Loss: 0.8956741094589233\n",
      "Iteration 10637 Loss: 1.2191553115844727\n",
      "Iteration 10638 Loss: 1.6176992654800415\n",
      "Iteration 10639 Loss: 1.1610199213027954\n",
      "Iteration 10639 Loss: 1.3246394395828247\n",
      "Iteration 10640 Loss: 1.3210251331329346\n",
      "Iteration 10641 Loss: 1.6985751390457153\n",
      "Iteration 10642 Loss: 0.5522643327713013\n",
      "Iteration 10643 Loss: 1.0895509719848633\n",
      "Iteration 10644 Loss: 0.8567947149276733\n",
      "Iteration 10645 Loss: 1.2739694118499756\n",
      "Iteration 10646 Loss: 1.3749620914459229\n",
      "Iteration 10647 Loss: 1.4136749505996704\n",
      "Iteration 10648 Loss: 1.5785213708877563\n",
      "Iteration 10649 Loss: 1.5755800008773804\n",
      "Iteration 10649 Loss: 1.2734917402267456\n",
      "Iteration 10650 Loss: 1.2974573373794556\n",
      "Iteration 10651 Loss: 1.1036659479141235\n",
      "Iteration 10652 Loss: 1.0992391109466553\n",
      "Iteration 10653 Loss: 1.2611223459243774\n",
      "Iteration 10654 Loss: 1.1803545951843262\n",
      "Iteration 10655 Loss: 1.1789071559906006\n",
      "Iteration 10656 Loss: 1.0052289962768555\n",
      "Iteration 10657 Loss: 1.5167582035064697\n",
      "Iteration 10658 Loss: 1.087824821472168\n",
      "Iteration 10659 Loss: 0.8116905689239502\n",
      "Iteration 10659 Loss: 1.1542248725891113\n",
      "Iteration 10660 Loss: 0.927614152431488\n",
      "Iteration 10661 Loss: 1.392434000968933\n",
      "Iteration 10662 Loss: 1.4601647853851318\n",
      "Iteration 10663 Loss: 1.3229809999465942\n",
      "Iteration 10664 Loss: 1.4689662456512451\n",
      "Iteration 10665 Loss: 0.8932435512542725\n",
      "Iteration 10666 Loss: 1.1375724077224731\n",
      "Iteration 10667 Loss: 0.9592574238777161\n",
      "Iteration 10668 Loss: 0.8890198469161987\n",
      "Iteration 10669 Loss: 1.1390374898910522\n",
      "Iteration 10669 Loss: 1.1590291261672974\n",
      "Iteration 10670 Loss: 1.1199736595153809\n",
      "Iteration 10671 Loss: 1.454176902770996\n",
      "Iteration 10672 Loss: 1.35625159740448\n",
      "Iteration 10673 Loss: 1.5080138444900513\n",
      "Iteration 10674 Loss: 1.4928234815597534\n",
      "Iteration 10675 Loss: 1.5741854906082153\n",
      "Iteration 10676 Loss: 1.154687523841858\n",
      "Iteration 10677 Loss: 1.5791630744934082\n",
      "Iteration 10678 Loss: 1.1425609588623047\n",
      "Iteration 10679 Loss: 1.3749282360076904\n",
      "Iteration 10679 Loss: 1.3756765127182007\n",
      "Iteration 10680 Loss: 1.214678406715393\n",
      "Iteration 10681 Loss: 1.5481868982315063\n",
      "Iteration 10682 Loss: 1.5682297945022583\n",
      "Iteration 10683 Loss: 1.1841003894805908\n",
      "Iteration 10684 Loss: 1.2024084329605103\n",
      "Iteration 10685 Loss: 1.3381166458129883\n",
      "Iteration 10686 Loss: 1.225594162940979\n",
      "Iteration 10687 Loss: 1.5032449960708618\n",
      "Iteration 10688 Loss: 1.1229199171066284\n",
      "Iteration 10689 Loss: 1.5327996015548706\n",
      "Iteration 10689 Loss: 1.3440279960632324\n",
      "Iteration 10690 Loss: 1.747381329536438\n",
      "Iteration 10691 Loss: 2.00970458984375\n",
      "Iteration 10692 Loss: 1.3901903629302979\n",
      "Iteration 10693 Loss: 1.5287489891052246\n",
      "Iteration 10694 Loss: 1.0480760335922241\n",
      "Iteration 10695 Loss: 1.4467575550079346\n",
      "Iteration 10696 Loss: 1.1781820058822632\n",
      "Iteration 10697 Loss: 1.1383440494537354\n",
      "Iteration 10698 Loss: 1.309407114982605\n",
      "Iteration 10699 Loss: 1.3585063219070435\n",
      "Iteration 10699 Loss: 1.415529727935791\n",
      "Iteration 10700 Loss: 1.711884617805481\n",
      "Iteration 10701 Loss: 1.4463189840316772\n",
      "Iteration 10702 Loss: 1.7070943117141724\n",
      "Iteration 10703 Loss: 1.4618974924087524\n",
      "Iteration 10704 Loss: 1.288476586341858\n",
      "Iteration 10705 Loss: 1.2190591096878052\n",
      "Iteration 10706 Loss: 1.456675410270691\n",
      "Iteration 10707 Loss: 1.3441743850708008\n",
      "Iteration 10708 Loss: 1.5276398658752441\n",
      "Iteration 10709 Loss: 1.3454885482788086\n",
      "Iteration 10709 Loss: 1.4508709907531738\n",
      "Iteration 10710 Loss: 1.537607192993164\n",
      "Iteration 10711 Loss: 1.4149563312530518\n",
      "Iteration 10712 Loss: 1.0913794040679932\n",
      "Iteration 10713 Loss: 1.2640767097473145\n",
      "Iteration 10714 Loss: 1.733670949935913\n",
      "Iteration 10715 Loss: 1.4067041873931885\n",
      "Iteration 10716 Loss: 1.1589566469192505\n",
      "Iteration 10717 Loss: 1.3077369928359985\n",
      "Iteration 10718 Loss: 1.6615941524505615\n",
      "Iteration 10719 Loss: 1.378321886062622\n",
      "Iteration 10719 Loss: 1.3955004215240479\n",
      "Iteration 10720 Loss: 1.4562633037567139\n",
      "Iteration 10721 Loss: 1.003097653388977\n",
      "Iteration 10722 Loss: 1.1213266849517822\n",
      "Iteration 10723 Loss: 1.6336915493011475\n",
      "Iteration 10724 Loss: 0.9552890658378601\n",
      "Iteration 10725 Loss: 1.2388029098510742\n",
      "Iteration 10726 Loss: 1.3537830114364624\n",
      "Iteration 10727 Loss: 1.6560955047607422\n",
      "Iteration 10728 Loss: 1.9649033546447754\n",
      "Iteration 10729 Loss: 0.9329321384429932\n",
      "Iteration 10729 Loss: 1.3316185474395752\n",
      "Iteration 10730 Loss: 1.5518407821655273\n",
      "Iteration 10731 Loss: 1.6723880767822266\n",
      "Iteration 10732 Loss: 1.3399016857147217\n",
      "Iteration 10733 Loss: 1.650287389755249\n",
      "Iteration 10734 Loss: 1.21169114112854\n",
      "Iteration 10735 Loss: 1.622173547744751\n",
      "Iteration 10736 Loss: 1.3374887704849243\n",
      "Iteration 10737 Loss: 1.3419307470321655\n",
      "Iteration 10738 Loss: 1.4714946746826172\n",
      "Iteration 10739 Loss: 1.519040584564209\n",
      "Iteration 10739 Loss: 1.4718236923217773\n",
      "Iteration 10740 Loss: 1.2528762817382812\n",
      "Iteration 10741 Loss: 1.521286129951477\n",
      "Iteration 10742 Loss: 1.6200716495513916\n",
      "Iteration 10743 Loss: 1.266570806503296\n",
      "Iteration 10744 Loss: 1.328957200050354\n",
      "Iteration 10745 Loss: 1.097245216369629\n",
      "Iteration 10746 Loss: 1.4321815967559814\n",
      "Iteration 10747 Loss: 1.1433461904525757\n",
      "Iteration 10748 Loss: 1.3082252740859985\n",
      "Iteration 10749 Loss: 1.4330559968948364\n",
      "Iteration 10749 Loss: 1.3403816223144531\n",
      "Iteration 10750 Loss: 1.4338568449020386\n",
      "Iteration 10751 Loss: 1.3731749057769775\n",
      "Iteration 10752 Loss: 1.0653926134109497\n",
      "Iteration 10753 Loss: 1.6006702184677124\n",
      "Iteration 10754 Loss: 1.4395374059677124\n",
      "Iteration 10755 Loss: 1.2015366554260254\n",
      "Iteration 10756 Loss: 1.5375072956085205\n",
      "Iteration 10757 Loss: 1.4239733219146729\n",
      "Iteration 10758 Loss: 2.1119892597198486\n",
      "Iteration 10759 Loss: 1.1573083400726318\n",
      "Iteration 10759 Loss: 1.4344944953918457\n",
      "Iteration 10760 Loss: 1.4830023050308228\n",
      "Iteration 10761 Loss: 1.4442963600158691\n",
      "Iteration 10762 Loss: 1.34050452709198\n",
      "Iteration 10763 Loss: 0.8823962807655334\n",
      "Iteration 10764 Loss: 1.1161625385284424\n",
      "Iteration 10765 Loss: 1.1658935546875\n",
      "Iteration 10766 Loss: 1.2269400358200073\n",
      "Iteration 10767 Loss: 1.0868144035339355\n",
      "Iteration 10768 Loss: 1.6694259643554688\n",
      "Iteration 10769 Loss: 1.3139861822128296\n",
      "Iteration 10769 Loss: 1.2729421854019165\n",
      "Iteration 10770 Loss: 1.3428322076797485\n",
      "Iteration 10771 Loss: 1.1945074796676636\n",
      "Iteration 10772 Loss: 1.111607551574707\n",
      "Iteration 10773 Loss: 1.179878830909729\n",
      "Iteration 10774 Loss: 1.522035837173462\n",
      "Iteration 10775 Loss: 1.149963617324829\n",
      "Iteration 10776 Loss: 0.7539951801300049\n",
      "Iteration 10777 Loss: 1.1825320720672607\n",
      "Iteration 10778 Loss: 1.2412077188491821\n",
      "Iteration 10779 Loss: 1.1766358613967896\n",
      "Iteration 10779 Loss: 1.1855195760726929\n",
      "Iteration 10780 Loss: 1.2346237897872925\n",
      "Iteration 10781 Loss: 1.6423910856246948\n",
      "Iteration 10782 Loss: 1.240248680114746\n",
      "Iteration 10783 Loss: 1.0279521942138672\n",
      "Iteration 10784 Loss: 0.9785246253013611\n",
      "Iteration 10785 Loss: 1.4380556344985962\n",
      "Iteration 10786 Loss: 1.2566492557525635\n",
      "Iteration 10787 Loss: 1.5347156524658203\n",
      "Iteration 10788 Loss: 1.616986870765686\n",
      "Iteration 10789 Loss: 1.2638300657272339\n",
      "Iteration 10789 Loss: 1.3233978748321533\n",
      "Iteration 10790 Loss: 1.2693395614624023\n",
      "Iteration 10791 Loss: 1.4418244361877441\n",
      "Iteration 10792 Loss: 1.4770845174789429\n",
      "Iteration 10793 Loss: 1.3714221715927124\n",
      "Iteration 10794 Loss: 1.479171872138977\n",
      "Iteration 10795 Loss: 1.1101466417312622\n",
      "Iteration 10796 Loss: 1.2350653409957886\n",
      "Iteration 10797 Loss: 1.4863736629486084\n",
      "Iteration 10798 Loss: 1.349768042564392\n",
      "Iteration 10799 Loss: 0.7782572507858276\n",
      "Iteration 10799 Loss: 1.2998454570770264\n",
      "Iteration 10800 Loss: 1.4139313697814941\n",
      "Iteration 10801 Loss: 1.231952428817749\n",
      "Iteration 10802 Loss: 1.0974738597869873\n",
      "Iteration 10803 Loss: 1.5438064336776733\n",
      "Iteration 10804 Loss: 1.2381842136383057\n",
      "Iteration 10805 Loss: 1.1496596336364746\n",
      "Iteration 10806 Loss: 0.9107159376144409\n",
      "Iteration 10807 Loss: 1.6102920770645142\n",
      "Iteration 10808 Loss: 1.2618155479431152\n",
      "Iteration 10809 Loss: 1.8022420406341553\n",
      "Iteration 10809 Loss: 1.32600736618042\n",
      "Iteration 10810 Loss: 1.8061902523040771\n",
      "Iteration 10811 Loss: 0.7443472146987915\n",
      "Iteration 10812 Loss: 1.1112682819366455\n",
      "Iteration 10813 Loss: 1.0845856666564941\n",
      "Iteration 10814 Loss: 1.0461758375167847\n",
      "Iteration 10815 Loss: 1.0608190298080444\n",
      "Iteration 10816 Loss: 1.8349826335906982\n",
      "Iteration 10817 Loss: 1.2553579807281494\n",
      "Iteration 10818 Loss: 1.430991291999817\n",
      "Iteration 10819 Loss: 1.5147545337677002\n",
      "Iteration 10819 Loss: 1.2889472246170044\n",
      "Iteration 10820 Loss: 1.570970058441162\n",
      "Iteration 10821 Loss: 0.8437901735305786\n",
      "Iteration 10822 Loss: 0.827427864074707\n",
      "Iteration 10823 Loss: 1.1187447309494019\n",
      "Iteration 10824 Loss: 1.2090468406677246\n",
      "Iteration 10825 Loss: 1.262699007987976\n",
      "Iteration 10826 Loss: 1.470803141593933\n",
      "Iteration 10827 Loss: 1.4500712156295776\n",
      "Iteration 10828 Loss: 1.6158100366592407\n",
      "Iteration 10829 Loss: 1.2612876892089844\n",
      "Iteration 10829 Loss: 1.2630650997161865\n",
      "Iteration 10830 Loss: 1.5316710472106934\n",
      "Iteration 10831 Loss: 1.3016408681869507\n",
      "Iteration 10832 Loss: 1.4391566514968872\n",
      "Iteration 10833 Loss: 1.639255166053772\n",
      "Iteration 10834 Loss: 1.0192244052886963\n",
      "Iteration 10835 Loss: 1.4513715505599976\n",
      "Iteration 10836 Loss: 1.3369293212890625\n",
      "Iteration 10837 Loss: 1.2424890995025635\n",
      "Iteration 10838 Loss: 1.4184504747390747\n",
      "Iteration 10839 Loss: 1.4840835332870483\n",
      "Iteration 10839 Loss: 1.3864271640777588\n",
      "Iteration 10840 Loss: 1.1336860656738281\n",
      "Iteration 10841 Loss: 1.212999939918518\n",
      "Iteration 10842 Loss: 1.1180535554885864\n",
      "Iteration 10843 Loss: 1.4439668655395508\n",
      "Iteration 10844 Loss: 1.0729323625564575\n",
      "Iteration 10845 Loss: 1.5145511627197266\n",
      "Iteration 10846 Loss: 0.9584711790084839\n",
      "Iteration 10847 Loss: 1.4323722124099731\n",
      "Iteration 10848 Loss: 1.6234709024429321\n",
      "Iteration 10849 Loss: 0.8966696262359619\n",
      "Iteration 10849 Loss: 1.2407174110412598\n",
      "Iteration 10850 Loss: 1.077314853668213\n",
      "Iteration 10851 Loss: 1.1736162900924683\n",
      "Iteration 10852 Loss: 1.3147307634353638\n",
      "Iteration 10853 Loss: 1.1785863637924194\n",
      "Iteration 10854 Loss: 0.8848608732223511\n",
      "Iteration 10855 Loss: 1.7051620483398438\n",
      "Iteration 10856 Loss: 1.2359628677368164\n",
      "Iteration 10857 Loss: 1.3617219924926758\n",
      "Iteration 10858 Loss: 1.2219995260238647\n",
      "Iteration 10859 Loss: 1.813866138458252\n",
      "Iteration 10859 Loss: 1.2967822551727295\n",
      "Iteration 10860 Loss: 0.7343077063560486\n",
      "Iteration 10861 Loss: 1.5218712091445923\n",
      "Iteration 10862 Loss: 1.5645670890808105\n",
      "Iteration 10863 Loss: 1.2527978420257568\n",
      "Iteration 10864 Loss: 1.2445183992385864\n",
      "Iteration 10865 Loss: 1.3781723976135254\n",
      "Iteration 10866 Loss: 1.753495216369629\n",
      "Iteration 10867 Loss: 1.6214655637741089\n",
      "Iteration 10868 Loss: 1.7888221740722656\n",
      "Iteration 10869 Loss: 1.1295197010040283\n",
      "Iteration 10869 Loss: 1.3989536762237549\n",
      "Iteration 10870 Loss: 1.235168218612671\n",
      "Iteration 10871 Loss: 1.2811528444290161\n",
      "Iteration 10872 Loss: 1.1160684823989868\n",
      "Iteration 10873 Loss: 0.7469713687896729\n",
      "Iteration 10874 Loss: 1.3064333200454712\n",
      "Iteration 10875 Loss: 1.6762772798538208\n",
      "Iteration 10876 Loss: 1.1976577043533325\n",
      "Iteration 10877 Loss: 0.9809417128562927\n",
      "Iteration 10878 Loss: 0.9943392872810364\n",
      "Iteration 10879 Loss: 1.689858317375183\n",
      "Iteration 10879 Loss: 1.2224868535995483\n",
      "Iteration 10880 Loss: 1.04122793674469\n",
      "Iteration 10881 Loss: 0.8780602216720581\n",
      "Iteration 10882 Loss: 0.9875643253326416\n",
      "Iteration 10883 Loss: 1.185738444328308\n",
      "Iteration 10884 Loss: 0.9686433672904968\n",
      "Iteration 10885 Loss: 1.1547383069992065\n",
      "Iteration 10886 Loss: 0.8183049559593201\n",
      "Iteration 10887 Loss: 1.1569621562957764\n",
      "Iteration 10888 Loss: 1.439387559890747\n",
      "Iteration 10889 Loss: 1.3007264137268066\n",
      "Iteration 10889 Loss: 1.0931354761123657\n",
      "Iteration 10890 Loss: 1.6353623867034912\n",
      "Iteration 10891 Loss: 1.286228060722351\n",
      "Iteration 10892 Loss: 1.2135618925094604\n",
      "Iteration 10893 Loss: 1.3408317565917969\n",
      "Iteration 10894 Loss: 1.0404423475265503\n",
      "Iteration 10895 Loss: 1.5500249862670898\n",
      "Iteration 10896 Loss: 1.827072024345398\n",
      "Iteration 10897 Loss: 1.281526803970337\n",
      "Iteration 10898 Loss: 1.2158374786376953\n",
      "Iteration 10899 Loss: 1.2330266237258911\n",
      "Iteration 10899 Loss: 1.362391471862793\n",
      "Iteration 10900 Loss: 1.630334734916687\n",
      "Iteration 10901 Loss: 1.0126572847366333\n",
      "Iteration 10902 Loss: 1.1793278455734253\n",
      "Iteration 10903 Loss: 1.243146538734436\n",
      "Iteration 10904 Loss: 1.372353196144104\n",
      "Iteration 10905 Loss: 1.4150471687316895\n",
      "Iteration 10906 Loss: 0.9449268579483032\n",
      "Iteration 10907 Loss: 0.63740473985672\n",
      "Iteration 10908 Loss: 1.4467211961746216\n",
      "Iteration 10909 Loss: 1.334979772567749\n",
      "Iteration 10909 Loss: 1.2216899394989014\n",
      "Iteration 10910 Loss: 1.7429686784744263\n",
      "Iteration 10911 Loss: 1.057125210762024\n",
      "Iteration 10912 Loss: 1.1822654008865356\n",
      "Iteration 10913 Loss: 1.7447469234466553\n",
      "Iteration 10914 Loss: 1.1561386585235596\n",
      "Iteration 10915 Loss: 1.2429111003875732\n",
      "Iteration 10916 Loss: 1.1561479568481445\n",
      "Iteration 10917 Loss: 1.3755109310150146\n",
      "Iteration 10918 Loss: 0.5916478633880615\n",
      "Iteration 10919 Loss: 1.0508512258529663\n",
      "Iteration 10919 Loss: 1.2300313711166382\n",
      "Iteration 10920 Loss: 0.9590857625007629\n",
      "Iteration 10921 Loss: 1.216570496559143\n",
      "Iteration 10922 Loss: 1.3746980428695679\n",
      "Iteration 10923 Loss: 1.256923794746399\n",
      "Iteration 10924 Loss: 1.3651790618896484\n",
      "Iteration 10925 Loss: 1.3095020055770874\n",
      "Iteration 10926 Loss: 1.3647655248641968\n",
      "Iteration 10927 Loss: 1.3185639381408691\n",
      "Iteration 10928 Loss: 1.205378770828247\n",
      "Iteration 10929 Loss: 1.469523549079895\n",
      "Iteration 10929 Loss: 1.284019112586975\n",
      "Iteration 10930 Loss: 1.1631112098693848\n",
      "Iteration 10931 Loss: 1.0311102867126465\n",
      "Iteration 10932 Loss: 1.5725796222686768\n",
      "Iteration 10933 Loss: 0.9477695226669312\n",
      "Iteration 10934 Loss: 1.229512095451355\n",
      "Iteration 10935 Loss: 1.9297012090682983\n",
      "Iteration 10936 Loss: 1.0923335552215576\n",
      "Iteration 10937 Loss: 1.5158463716506958\n",
      "Iteration 10938 Loss: 1.4171953201293945\n",
      "Iteration 10939 Loss: 1.2951419353485107\n",
      "Iteration 10939 Loss: 1.3194301128387451\n",
      "Iteration 10940 Loss: 1.6741361618041992\n",
      "Iteration 10941 Loss: 1.3178224563598633\n",
      "Iteration 10942 Loss: 1.7026550769805908\n",
      "Iteration 10943 Loss: 1.4976495504379272\n",
      "Iteration 10944 Loss: 0.9954102039337158\n",
      "Iteration 10945 Loss: 1.449894666671753\n",
      "Iteration 10946 Loss: 1.5964722633361816\n",
      "Iteration 10947 Loss: 0.8663739562034607\n",
      "Iteration 10948 Loss: 1.8532782793045044\n",
      "Iteration 10949 Loss: 1.4819015264511108\n",
      "Iteration 10949 Loss: 1.4435594081878662\n",
      "Iteration 10950 Loss: 1.4116110801696777\n",
      "Iteration 10951 Loss: 1.6283334493637085\n",
      "Iteration 10952 Loss: 1.7626619338989258\n",
      "Iteration 10953 Loss: 0.9189414381980896\n",
      "Iteration 10954 Loss: 1.4499863386154175\n",
      "Iteration 10955 Loss: 1.0192127227783203\n",
      "Iteration 10956 Loss: 1.1962910890579224\n",
      "Iteration 10957 Loss: 1.215293288230896\n",
      "Iteration 10958 Loss: 1.9021234512329102\n",
      "Iteration 10959 Loss: 1.6068493127822876\n",
      "Iteration 10959 Loss: 1.4111303091049194\n",
      "Iteration 10960 Loss: 1.4249392747879028\n",
      "Iteration 10961 Loss: 1.8641159534454346\n",
      "Iteration 10962 Loss: 1.3966790437698364\n",
      "Iteration 10963 Loss: 2.0946426391601562\n",
      "Iteration 10964 Loss: 1.7138078212738037\n",
      "Iteration 10965 Loss: 1.4311527013778687\n",
      "Iteration 10966 Loss: 1.2242013216018677\n",
      "Iteration 10967 Loss: 1.3113031387329102\n",
      "Iteration 10968 Loss: 1.4804123640060425\n",
      "Iteration 10969 Loss: 1.1520105600357056\n",
      "Iteration 10969 Loss: 1.509326457977295\n",
      "Iteration 10970 Loss: 1.3228709697723389\n",
      "Iteration 10971 Loss: 1.1245396137237549\n",
      "Iteration 10972 Loss: 1.377630352973938\n",
      "Iteration 10973 Loss: 1.3018556833267212\n",
      "Iteration 10974 Loss: 1.278279185295105\n",
      "Iteration 10975 Loss: 1.2207350730895996\n",
      "Iteration 10976 Loss: 1.6033493280410767\n",
      "Iteration 10977 Loss: 1.1475423574447632\n",
      "Iteration 10978 Loss: 1.4834024906158447\n",
      "Iteration 10979 Loss: 1.4511891603469849\n",
      "Iteration 10979 Loss: 1.3311394453048706\n",
      "Iteration 10980 Loss: 1.5138933658599854\n",
      "Iteration 10981 Loss: 1.1967297792434692\n",
      "Iteration 10982 Loss: 1.1701568365097046\n",
      "Iteration 10983 Loss: 1.3894526958465576\n",
      "Iteration 10984 Loss: 1.176352858543396\n",
      "Iteration 10985 Loss: 1.0397062301635742\n",
      "Iteration 10986 Loss: 1.5673277378082275\n",
      "Iteration 10987 Loss: 1.2571650743484497\n",
      "Iteration 10988 Loss: 1.201647400856018\n",
      "Iteration 10989 Loss: 1.2000571489334106\n",
      "Iteration 10989 Loss: 1.2712488174438477\n",
      "Iteration 10990 Loss: 1.0414636135101318\n",
      "Iteration 10991 Loss: 1.212221384048462\n",
      "Iteration 10992 Loss: 1.1207329034805298\n",
      "Iteration 10993 Loss: 1.5167638063430786\n",
      "Iteration 10994 Loss: 1.253273367881775\n",
      "Iteration 10995 Loss: 1.3710535764694214\n",
      "Iteration 10996 Loss: 1.69983971118927\n",
      "Iteration 10997 Loss: 1.2583644390106201\n",
      "Iteration 10998 Loss: 1.1438151597976685\n",
      "Iteration 10999 Loss: 1.0796213150024414\n",
      "Iteration 10999 Loss: 1.2697149515151978\n",
      "Iteration 11000 Loss: 1.1975573301315308\n",
      "Iteration 11001 Loss: 1.531778335571289\n",
      "Iteration 11002 Loss: 1.2859524488449097\n",
      "Iteration 11003 Loss: 1.1953531503677368\n",
      "Iteration 11004 Loss: 1.6578433513641357\n",
      "Iteration 11005 Loss: 1.1112996339797974\n",
      "Iteration 11006 Loss: 1.1530619859695435\n",
      "Iteration 11007 Loss: 1.2348977327346802\n",
      "Iteration 11008 Loss: 1.2846739292144775\n",
      "Iteration 11009 Loss: 1.072774052619934\n",
      "Iteration 11009 Loss: 1.2725192308425903\n",
      "Iteration 11010 Loss: 1.6943941116333008\n",
      "Iteration 11011 Loss: 1.3162940740585327\n",
      "Iteration 11012 Loss: 1.6008528470993042\n",
      "Iteration 11013 Loss: 1.3443046808242798\n",
      "Iteration 11014 Loss: 1.0247882604599\n",
      "Iteration 11015 Loss: 1.1472340822219849\n",
      "Iteration 11016 Loss: 0.96884685754776\n",
      "Iteration 11017 Loss: 1.628058671951294\n",
      "Iteration 11018 Loss: 0.9005563855171204\n",
      "Iteration 11019 Loss: 1.5869190692901611\n",
      "Iteration 11019 Loss: 1.3212249279022217\n",
      "Iteration 11020 Loss: 0.7706997394561768\n",
      "Iteration 11021 Loss: 1.2117278575897217\n",
      "Iteration 11022 Loss: 1.2231098413467407\n",
      "Iteration 11023 Loss: 1.6840325593948364\n",
      "Iteration 11024 Loss: 1.4058421850204468\n",
      "Iteration 11025 Loss: 1.411877155303955\n",
      "Iteration 11026 Loss: 1.1738193035125732\n",
      "Iteration 11027 Loss: 1.3083221912384033\n",
      "Iteration 11028 Loss: 1.0279560089111328\n",
      "Iteration 11029 Loss: 1.4116398096084595\n",
      "Iteration 11029 Loss: 1.2629026174545288\n",
      "Iteration 11030 Loss: 1.3543734550476074\n",
      "Iteration 11031 Loss: 1.402374505996704\n",
      "Iteration 11032 Loss: 1.068170428276062\n",
      "Iteration 11033 Loss: 1.7185847759246826\n",
      "Iteration 11034 Loss: 1.3170571327209473\n",
      "Iteration 11035 Loss: 1.4430582523345947\n",
      "Iteration 11036 Loss: 1.1420283317565918\n",
      "Iteration 11037 Loss: 1.0063986778259277\n",
      "Iteration 11038 Loss: 0.9680392146110535\n",
      "Iteration 11039 Loss: 1.6106988191604614\n",
      "Iteration 11039 Loss: 1.3030784130096436\n",
      "Iteration 11040 Loss: 1.1267985105514526\n",
      "Iteration 11041 Loss: 1.397050142288208\n",
      "Iteration 11042 Loss: 1.5301666259765625\n",
      "Iteration 11043 Loss: 1.6672128438949585\n",
      "Iteration 11044 Loss: 0.9374012351036072\n",
      "Iteration 11045 Loss: 1.4815744161605835\n",
      "Iteration 11046 Loss: 1.1569453477859497\n",
      "Iteration 11047 Loss: 1.3717066049575806\n",
      "Iteration 11048 Loss: 1.3020598888397217\n",
      "Iteration 11049 Loss: 1.4713246822357178\n",
      "Iteration 11049 Loss: 1.344223976135254\n",
      "Iteration 11050 Loss: 1.8097494840621948\n",
      "Iteration 11051 Loss: 1.463574767112732\n",
      "Iteration 11052 Loss: 0.997753918170929\n",
      "Iteration 11053 Loss: 1.1732710599899292\n",
      "Iteration 11054 Loss: 1.3740323781967163\n",
      "Iteration 11055 Loss: 1.0441378355026245\n",
      "Iteration 11056 Loss: 1.2603349685668945\n",
      "Iteration 11057 Loss: 1.4045413732528687\n",
      "Iteration 11058 Loss: 1.1797188520431519\n",
      "Iteration 11059 Loss: 1.6811585426330566\n",
      "Iteration 11059 Loss: 1.33882737159729\n",
      "Iteration 11060 Loss: 1.7127045392990112\n",
      "Iteration 11061 Loss: 1.210479736328125\n",
      "Iteration 11062 Loss: 1.2272801399230957\n",
      "Iteration 11063 Loss: 1.223975419998169\n",
      "Iteration 11064 Loss: 1.8206874132156372\n",
      "Iteration 11065 Loss: 1.2032991647720337\n",
      "Iteration 11066 Loss: 1.523352861404419\n",
      "Iteration 11067 Loss: 1.5773673057556152\n",
      "Iteration 11068 Loss: 0.9512426257133484\n",
      "Iteration 11069 Loss: 1.6609565019607544\n",
      "Iteration 11069 Loss: 1.4111344814300537\n",
      "Iteration 11070 Loss: 1.5955045223236084\n",
      "Iteration 11071 Loss: 1.5592037439346313\n",
      "Iteration 11072 Loss: 1.1144754886627197\n",
      "Iteration 11073 Loss: 1.5188332796096802\n",
      "Iteration 11074 Loss: 1.6953916549682617\n",
      "Iteration 11075 Loss: 1.6670119762420654\n",
      "Iteration 11076 Loss: 0.8139187693595886\n",
      "Iteration 11077 Loss: 1.0757420063018799\n",
      "Iteration 11078 Loss: 1.1441619396209717\n",
      "Iteration 11079 Loss: 1.1540179252624512\n",
      "Iteration 11079 Loss: 1.3338261842727661\n",
      "Iteration 11080 Loss: 1.2978434562683105\n",
      "Iteration 11081 Loss: 1.539064645767212\n",
      "Iteration 11082 Loss: 1.2408194541931152\n",
      "Iteration 11083 Loss: 1.4147299528121948\n",
      "Iteration 11084 Loss: 1.6523609161376953\n",
      "Iteration 11085 Loss: 1.5789016485214233\n",
      "Iteration 11086 Loss: 0.9734508991241455\n",
      "Iteration 11087 Loss: 1.189089059829712\n",
      "Iteration 11088 Loss: 1.5097872018814087\n",
      "Iteration 11089 Loss: 1.371293306350708\n",
      "Iteration 11089 Loss: 1.3767340183258057\n",
      "Iteration 11090 Loss: 1.1921173334121704\n",
      "Iteration 11091 Loss: 1.260542869567871\n",
      "Iteration 11092 Loss: 1.5662283897399902\n",
      "Iteration 11093 Loss: 1.312418818473816\n",
      "Iteration 11094 Loss: 1.6370712518692017\n",
      "Iteration 11095 Loss: 1.7372642755508423\n",
      "Iteration 11096 Loss: 1.261256217956543\n",
      "Iteration 11097 Loss: 1.4896055459976196\n",
      "Iteration 11098 Loss: 1.2422573566436768\n",
      "Iteration 11099 Loss: 1.0664966106414795\n",
      "Iteration 11099 Loss: 1.3765259981155396\n",
      "Iteration 11100 Loss: 1.1967588663101196\n",
      "Iteration 11101 Loss: 1.3437132835388184\n",
      "Iteration 11102 Loss: 1.5742028951644897\n",
      "Iteration 11103 Loss: 1.8681159019470215\n",
      "Iteration 11104 Loss: 1.4220786094665527\n",
      "Iteration 11105 Loss: 1.6973289251327515\n",
      "Iteration 11106 Loss: 1.285146713256836\n",
      "Iteration 11107 Loss: 1.5290722846984863\n",
      "Iteration 11108 Loss: 1.6429297924041748\n",
      "Iteration 11109 Loss: 1.3782764673233032\n",
      "Iteration 11109 Loss: 1.4937623739242554\n",
      "Iteration 11110 Loss: 1.2826957702636719\n",
      "Iteration 11111 Loss: 1.334414005279541\n",
      "Iteration 11112 Loss: 1.4056981801986694\n",
      "Iteration 11113 Loss: 1.5019773244857788\n",
      "Iteration 11114 Loss: 1.462152361869812\n",
      "Iteration 11115 Loss: 1.4019839763641357\n",
      "Iteration 11116 Loss: 1.0395399332046509\n",
      "Iteration 11117 Loss: 1.8201818466186523\n",
      "Iteration 11118 Loss: 1.0235744714736938\n",
      "Iteration 11119 Loss: 1.5233774185180664\n",
      "Iteration 11119 Loss: 1.3795596361160278\n",
      "Iteration 11120 Loss: 1.6025464534759521\n",
      "Iteration 11121 Loss: 1.3115804195404053\n",
      "Iteration 11122 Loss: 0.891024112701416\n",
      "Iteration 11123 Loss: 1.6393309831619263\n",
      "Iteration 11124 Loss: 1.4226535558700562\n",
      "Iteration 11125 Loss: 1.0891895294189453\n",
      "Iteration 11126 Loss: 1.5962576866149902\n",
      "Iteration 11127 Loss: 1.171863317489624\n",
      "Iteration 11128 Loss: 1.084337830543518\n",
      "Iteration 11129 Loss: 1.2780455350875854\n",
      "Iteration 11129 Loss: 1.3086830377578735\n",
      "Iteration 11130 Loss: 1.8122539520263672\n",
      "Iteration 11131 Loss: 1.3172367811203003\n",
      "Iteration 11132 Loss: 1.4508671760559082\n",
      "Iteration 11133 Loss: 1.760475754737854\n",
      "Iteration 11134 Loss: 1.1661858558654785\n",
      "Iteration 11135 Loss: 1.5638482570648193\n",
      "Iteration 11136 Loss: 1.2575825452804565\n",
      "Iteration 11137 Loss: 1.515236258506775\n",
      "Iteration 11138 Loss: 1.0923173427581787\n",
      "Iteration 11139 Loss: 1.1398283243179321\n",
      "Iteration 11139 Loss: 1.4075833559036255\n",
      "Iteration 11140 Loss: 1.0509655475616455\n",
      "Iteration 11141 Loss: 1.3123929500579834\n",
      "Iteration 11142 Loss: 1.2318540811538696\n",
      "Iteration 11143 Loss: 1.2510616779327393\n",
      "Iteration 11144 Loss: 1.157231092453003\n",
      "Iteration 11145 Loss: 1.8603898286819458\n",
      "Iteration 11146 Loss: 1.1451994180679321\n",
      "Iteration 11147 Loss: 1.4622390270233154\n",
      "Iteration 11148 Loss: 1.3436199426651\n",
      "Iteration 11149 Loss: 1.1983263492584229\n",
      "Iteration 11149 Loss: 1.3013280630111694\n",
      "Iteration 11150 Loss: 1.3068785667419434\n",
      "Iteration 11151 Loss: 1.248952031135559\n",
      "Iteration 11152 Loss: 1.038466215133667\n",
      "Iteration 11153 Loss: 1.5203864574432373\n",
      "Iteration 11154 Loss: 0.8181851506233215\n",
      "Iteration 11155 Loss: 0.9926238059997559\n",
      "Iteration 11156 Loss: 1.1641334295272827\n",
      "Iteration 11157 Loss: 1.487236499786377\n",
      "Iteration 11158 Loss: 1.1311719417572021\n",
      "Iteration 11159 Loss: 1.9576056003570557\n",
      "Iteration 11159 Loss: 1.2665637731552124\n",
      "Iteration 11160 Loss: 1.5895178318023682\n",
      "Iteration 11161 Loss: 1.159049153327942\n",
      "Iteration 11162 Loss: 1.2792065143585205\n",
      "Iteration 11163 Loss: 1.3653401136398315\n",
      "Iteration 11164 Loss: 1.26102614402771\n",
      "Iteration 11165 Loss: 0.9117333292961121\n",
      "Iteration 11166 Loss: 0.9360313415527344\n",
      "Iteration 11167 Loss: 1.2705318927764893\n",
      "Iteration 11168 Loss: 1.257676124572754\n",
      "Iteration 11169 Loss: 1.6809645891189575\n",
      "Iteration 11169 Loss: 1.2711076736450195\n",
      "Iteration 11170 Loss: 1.4963274002075195\n",
      "Iteration 11171 Loss: 1.7037837505340576\n",
      "Iteration 11172 Loss: 1.3805075883865356\n",
      "Iteration 11173 Loss: 1.3575782775878906\n",
      "Iteration 11174 Loss: 1.1570433378219604\n",
      "Iteration 11175 Loss: 1.0243759155273438\n",
      "Iteration 11176 Loss: 1.0864439010620117\n",
      "Iteration 11177 Loss: 1.7651565074920654\n",
      "Iteration 11178 Loss: 0.989520251750946\n",
      "Iteration 11179 Loss: 1.221186876296997\n",
      "Iteration 11179 Loss: 1.3181923627853394\n",
      "Iteration 11180 Loss: 1.2190368175506592\n",
      "Iteration 11181 Loss: 1.5924046039581299\n",
      "Iteration 11182 Loss: 1.2007036209106445\n",
      "Iteration 11183 Loss: 1.000707745552063\n",
      "Iteration 11184 Loss: 1.1205047369003296\n",
      "Iteration 11185 Loss: 1.5178295373916626\n",
      "Iteration 11186 Loss: 1.3803614377975464\n",
      "Iteration 11187 Loss: 1.318648338317871\n",
      "Iteration 11188 Loss: 1.1318708658218384\n",
      "Iteration 11189 Loss: 1.3530035018920898\n",
      "Iteration 11189 Loss: 1.2835071086883545\n",
      "Iteration 11190 Loss: 1.74112069606781\n",
      "Iteration 11191 Loss: 1.427622675895691\n",
      "Iteration 11192 Loss: 1.3003578186035156\n",
      "Iteration 11193 Loss: 0.7272101640701294\n",
      "Iteration 11194 Loss: 1.4287208318710327\n",
      "Iteration 11195 Loss: 1.5108476877212524\n",
      "Iteration 11196 Loss: 1.0114343166351318\n",
      "Iteration 11197 Loss: 2.0184309482574463\n",
      "Iteration 11198 Loss: 1.4038100242614746\n",
      "Iteration 11199 Loss: 1.4243226051330566\n",
      "Iteration 11199 Loss: 1.3993878364562988\n",
      "Iteration 11200 Loss: 1.1957807540893555\n",
      "Iteration 11201 Loss: 1.531651258468628\n",
      "Iteration 11202 Loss: 1.4328958988189697\n",
      "Iteration 11203 Loss: 0.8238542079925537\n",
      "Iteration 11204 Loss: 1.548387050628662\n",
      "Iteration 11205 Loss: 1.4385771751403809\n",
      "Iteration 11206 Loss: 1.6304192543029785\n",
      "Iteration 11207 Loss: 1.3894104957580566\n",
      "Iteration 11208 Loss: 1.3391268253326416\n",
      "Iteration 11209 Loss: 1.4343767166137695\n",
      "Iteration 11209 Loss: 1.3764479160308838\n",
      "Iteration 11210 Loss: 1.163557767868042\n",
      "Iteration 11211 Loss: 0.9425580501556396\n",
      "Iteration 11212 Loss: 1.2925843000411987\n",
      "Iteration 11213 Loss: 0.9150806069374084\n",
      "Iteration 11214 Loss: 1.5337294340133667\n",
      "Iteration 11215 Loss: 1.5719921588897705\n",
      "Iteration 11216 Loss: 1.1956762075424194\n",
      "Iteration 11217 Loss: 0.86727374792099\n",
      "Iteration 11218 Loss: 1.2992607355117798\n",
      "Iteration 11219 Loss: 1.2023307085037231\n",
      "Iteration 11219 Loss: 1.1984041929244995\n",
      "Iteration 11220 Loss: 1.310281753540039\n",
      "Iteration 11221 Loss: 1.2792344093322754\n",
      "Iteration 11222 Loss: 1.292590618133545\n",
      "Iteration 11223 Loss: 1.318288803100586\n",
      "Iteration 11224 Loss: 1.3789236545562744\n",
      "Iteration 11225 Loss: 1.7281955480575562\n",
      "Iteration 11226 Loss: 1.1742576360702515\n",
      "Iteration 11227 Loss: 1.0478878021240234\n",
      "Iteration 11228 Loss: 1.5497307777404785\n",
      "Iteration 11229 Loss: 1.0842636823654175\n",
      "Iteration 11229 Loss: 1.3163654804229736\n",
      "Iteration 11230 Loss: 1.1395597457885742\n",
      "Iteration 11231 Loss: 1.3181970119476318\n",
      "Iteration 11232 Loss: 1.3079668283462524\n",
      "Iteration 11233 Loss: 1.5509724617004395\n",
      "Iteration 11234 Loss: 1.0856456756591797\n",
      "Iteration 11235 Loss: 1.0623970031738281\n",
      "Iteration 11236 Loss: 1.020086646080017\n",
      "Iteration 11237 Loss: 0.8919690251350403\n",
      "Iteration 11238 Loss: 0.9985873103141785\n",
      "Iteration 11239 Loss: 1.4758055210113525\n",
      "Iteration 11239 Loss: 1.1851186752319336\n",
      "Iteration 11240 Loss: 1.0566749572753906\n",
      "Iteration 11241 Loss: 1.713889241218567\n",
      "Iteration 11242 Loss: 1.5321980714797974\n",
      "Iteration 11243 Loss: 1.4058992862701416\n",
      "Iteration 11244 Loss: 1.3432055711746216\n",
      "Iteration 11245 Loss: 1.571305513381958\n",
      "Iteration 11246 Loss: 1.1006218194961548\n",
      "Iteration 11247 Loss: 1.3977915048599243\n",
      "Iteration 11248 Loss: 1.02632737159729\n",
      "Iteration 11249 Loss: 1.5955660343170166\n",
      "Iteration 11249 Loss: 1.3743479251861572\n",
      "Iteration 11250 Loss: 1.0551512241363525\n",
      "Iteration 11251 Loss: 1.0240410566329956\n",
      "Iteration 11252 Loss: 1.375159502029419\n",
      "Iteration 11253 Loss: 1.389837622642517\n",
      "Iteration 11254 Loss: 1.3099181652069092\n",
      "Iteration 11255 Loss: 1.1667540073394775\n",
      "Iteration 11256 Loss: 1.323107123374939\n",
      "Iteration 11257 Loss: 1.3720052242279053\n",
      "Iteration 11258 Loss: 1.3702446222305298\n",
      "Iteration 11259 Loss: 1.3017045259475708\n",
      "Iteration 11259 Loss: 1.2687922716140747\n",
      "Iteration 11260 Loss: 1.4100474119186401\n",
      "Iteration 11261 Loss: 0.9038722515106201\n",
      "Iteration 11262 Loss: 1.0684398412704468\n",
      "Iteration 11263 Loss: 0.8876684904098511\n",
      "Iteration 11264 Loss: 1.6060123443603516\n",
      "Iteration 11265 Loss: 1.5488964319229126\n",
      "Iteration 11266 Loss: 1.3136919736862183\n",
      "Iteration 11267 Loss: 1.719812035560608\n",
      "Iteration 11268 Loss: 1.576488971710205\n",
      "Iteration 11269 Loss: 1.6862281560897827\n",
      "Iteration 11269 Loss: 1.3721158504486084\n",
      "Iteration 11270 Loss: 1.7249172925949097\n",
      "Iteration 11271 Loss: 1.0052450895309448\n",
      "Iteration 11272 Loss: 1.3300206661224365\n",
      "Iteration 11273 Loss: 1.3849260807037354\n",
      "Iteration 11274 Loss: 1.15067720413208\n",
      "Iteration 11275 Loss: 1.3977655172348022\n",
      "Iteration 11276 Loss: 1.3912723064422607\n",
      "Iteration 11277 Loss: 1.1565630435943604\n",
      "Iteration 11278 Loss: 1.2884632349014282\n",
      "Iteration 11279 Loss: 1.8214211463928223\n",
      "Iteration 11279 Loss: 1.3651272058486938\n",
      "Iteration 11280 Loss: 1.3853610754013062\n",
      "Iteration 11281 Loss: 1.1802685260772705\n",
      "Iteration 11282 Loss: 1.550644874572754\n",
      "Iteration 11283 Loss: 1.0533549785614014\n",
      "Iteration 11284 Loss: 1.8884031772613525\n",
      "Iteration 11285 Loss: 1.7167097330093384\n",
      "Iteration 11286 Loss: 1.0587074756622314\n",
      "Iteration 11287 Loss: 1.5675939321517944\n",
      "Iteration 11288 Loss: 1.2231183052062988\n",
      "Iteration 11289 Loss: 1.6120789051055908\n",
      "Iteration 11289 Loss: 1.423624038696289\n",
      "Iteration 11290 Loss: 1.2043921947479248\n",
      "Iteration 11291 Loss: 1.6623014211654663\n",
      "Iteration 11292 Loss: 1.516788125038147\n",
      "Iteration 11293 Loss: 1.438779592514038\n",
      "Iteration 11294 Loss: 1.4690306186676025\n",
      "Iteration 11295 Loss: 1.3101215362548828\n",
      "Iteration 11296 Loss: 1.3248341083526611\n",
      "Iteration 11297 Loss: 1.7268413305282593\n",
      "Iteration 11298 Loss: 1.2676903009414673\n",
      "Iteration 11299 Loss: 1.571159839630127\n",
      "Iteration 11299 Loss: 1.4491938352584839\n",
      "Iteration 11300 Loss: 1.346254587173462\n",
      "Iteration 11301 Loss: 1.0725679397583008\n",
      "Iteration 11302 Loss: 1.6484780311584473\n",
      "Iteration 11303 Loss: 1.1191693544387817\n",
      "Iteration 11304 Loss: 1.3491129875183105\n",
      "Iteration 11305 Loss: 1.4417568445205688\n",
      "Iteration 11306 Loss: 0.7612746953964233\n",
      "Iteration 11307 Loss: 1.2052081823349\n",
      "Iteration 11308 Loss: 0.898899495601654\n",
      "Iteration 11309 Loss: 1.1138280630111694\n",
      "Iteration 11309 Loss: 1.1956549882888794\n",
      "Iteration 11310 Loss: 1.3981642723083496\n",
      "Iteration 11311 Loss: 1.6180756092071533\n",
      "Iteration 11312 Loss: 1.1339999437332153\n",
      "Iteration 11313 Loss: 1.1988178491592407\n",
      "Iteration 11314 Loss: 1.1003096103668213\n",
      "Iteration 11315 Loss: 1.1087974309921265\n",
      "Iteration 11316 Loss: 1.0426738262176514\n",
      "Iteration 11317 Loss: 1.2078890800476074\n",
      "Iteration 11318 Loss: 1.2452609539031982\n",
      "Iteration 11319 Loss: 0.7184082269668579\n",
      "Iteration 11319 Loss: 1.1772396564483643\n",
      "Iteration 11320 Loss: 1.6695512533187866\n",
      "Iteration 11321 Loss: 1.5970946550369263\n",
      "Iteration 11322 Loss: 1.6296173334121704\n",
      "Iteration 11323 Loss: 1.1242409944534302\n",
      "Iteration 11324 Loss: 1.3140830993652344\n",
      "Iteration 11325 Loss: 1.3062355518341064\n",
      "Iteration 11326 Loss: 1.8503684997558594\n",
      "Iteration 11327 Loss: 1.5905654430389404\n",
      "Iteration 11328 Loss: 1.3712502717971802\n",
      "Iteration 11329 Loss: 0.8249187469482422\n",
      "Iteration 11329 Loss: 1.4277925491333008\n",
      "Iteration 11330 Loss: 1.5139600038528442\n",
      "Iteration 11331 Loss: 1.20498526096344\n",
      "Iteration 11332 Loss: 1.7951972484588623\n",
      "Iteration 11333 Loss: 1.3673557043075562\n",
      "Iteration 11334 Loss: 1.6001594066619873\n",
      "Iteration 11335 Loss: 1.3190078735351562\n",
      "Iteration 11336 Loss: 1.8291888236999512\n",
      "Iteration 11337 Loss: 1.644063949584961\n",
      "Iteration 11338 Loss: 1.1030267477035522\n",
      "Iteration 11339 Loss: 1.3023077249526978\n",
      "Iteration 11339 Loss: 1.4679253101348877\n",
      "Iteration 11340 Loss: 1.4790209531784058\n",
      "Iteration 11341 Loss: 1.1499844789505005\n",
      "Iteration 11342 Loss: 1.9085217714309692\n",
      "Iteration 11343 Loss: 1.3385121822357178\n",
      "Iteration 11344 Loss: 1.330604076385498\n",
      "Iteration 11345 Loss: 1.752026081085205\n",
      "Iteration 11346 Loss: 1.2711750268936157\n",
      "Iteration 11347 Loss: 1.037087321281433\n",
      "Iteration 11348 Loss: 1.2201318740844727\n",
      "Iteration 11349 Loss: 1.2737493515014648\n",
      "Iteration 11349 Loss: 1.3760813474655151\n",
      "Iteration 11350 Loss: 1.300209879875183\n",
      "Iteration 11351 Loss: 0.8923757672309875\n",
      "Iteration 11352 Loss: 1.1640441417694092\n",
      "Iteration 11353 Loss: 1.2681738138198853\n",
      "Iteration 11354 Loss: 1.4959309101104736\n",
      "Iteration 11355 Loss: 1.8278478384017944\n",
      "Iteration 11356 Loss: 1.1362136602401733\n",
      "Iteration 11357 Loss: 1.3707756996154785\n",
      "Iteration 11358 Loss: 0.9288567900657654\n",
      "Iteration 11359 Loss: 1.1742784976959229\n",
      "Iteration 11359 Loss: 1.2558705806732178\n",
      "Iteration 11360 Loss: 1.0352901220321655\n",
      "Iteration 11361 Loss: 1.023264765739441\n",
      "Iteration 11362 Loss: 1.0673378705978394\n",
      "Iteration 11363 Loss: 1.4904251098632812\n",
      "Iteration 11364 Loss: 1.1854040622711182\n",
      "Iteration 11365 Loss: 1.1323760747909546\n",
      "Iteration 11366 Loss: 1.468888521194458\n",
      "Iteration 11367 Loss: 1.4379591941833496\n",
      "Iteration 11368 Loss: 1.3726227283477783\n",
      "Iteration 11369 Loss: 1.2733838558197021\n",
      "Iteration 11369 Loss: 1.2486951351165771\n",
      "Iteration 11370 Loss: 0.7481886744499207\n",
      "Iteration 11371 Loss: 1.5629782676696777\n",
      "Iteration 11372 Loss: 1.2028529644012451\n",
      "Iteration 11373 Loss: 1.257118821144104\n",
      "Iteration 11374 Loss: 1.3430386781692505\n",
      "Iteration 11375 Loss: 1.1047773361206055\n",
      "Iteration 11376 Loss: 1.488323450088501\n",
      "Iteration 11377 Loss: 1.5104529857635498\n",
      "Iteration 11378 Loss: 1.6053255796432495\n",
      "Iteration 11379 Loss: 1.4608432054519653\n",
      "Iteration 11379 Loss: 1.3283900022506714\n",
      "Iteration 11380 Loss: 1.339872121810913\n",
      "Iteration 11381 Loss: 0.9268888235092163\n",
      "Iteration 11382 Loss: 1.1155004501342773\n",
      "Iteration 11383 Loss: 1.4043785333633423\n",
      "Iteration 11384 Loss: 1.4007123708724976\n",
      "Iteration 11385 Loss: 1.0423182249069214\n",
      "Iteration 11386 Loss: 1.4002434015274048\n",
      "Iteration 11387 Loss: 1.1078705787658691\n",
      "Iteration 11388 Loss: 1.3658102750778198\n",
      "Iteration 11389 Loss: 0.9306039214134216\n",
      "Iteration 11389 Loss: 1.2034199237823486\n",
      "Iteration 11390 Loss: 1.0692059993743896\n",
      "Iteration 11391 Loss: 1.410542607307434\n",
      "Iteration 11392 Loss: 1.0707473754882812\n",
      "Iteration 11393 Loss: 1.442426085472107\n",
      "Iteration 11394 Loss: 1.4042080640792847\n",
      "Iteration 11395 Loss: 1.2622826099395752\n",
      "Iteration 11396 Loss: 1.746749997138977\n",
      "Iteration 11397 Loss: 1.7377394437789917\n",
      "Iteration 11398 Loss: 0.96241694688797\n",
      "Iteration 11399 Loss: 0.9908312559127808\n",
      "Iteration 11399 Loss: 1.3097150325775146\n",
      "Iteration 11400 Loss: 1.074548363685608\n",
      "Iteration 11401 Loss: 1.5756919384002686\n",
      "Iteration 11402 Loss: 1.4522426128387451\n",
      "Iteration 11403 Loss: 1.3349496126174927\n",
      "Iteration 11404 Loss: 1.2650511264801025\n",
      "Iteration 11405 Loss: 1.2911709547042847\n",
      "Iteration 11406 Loss: 1.0798401832580566\n",
      "Iteration 11407 Loss: 1.219010353088379\n",
      "Iteration 11408 Loss: 1.2682416439056396\n",
      "Iteration 11409 Loss: 1.199141502380371\n",
      "Iteration 11409 Loss: 1.2759888172149658\n",
      "Iteration 11410 Loss: 1.3006352186203003\n",
      "Iteration 11411 Loss: 1.0551446676254272\n",
      "Iteration 11412 Loss: 1.5228865146636963\n",
      "Iteration 11413 Loss: 1.3457850217819214\n",
      "Iteration 11414 Loss: 1.2273098230361938\n",
      "Iteration 11415 Loss: 1.1285887956619263\n",
      "Iteration 11416 Loss: 1.1859071254730225\n",
      "Iteration 11417 Loss: 1.1954175233840942\n",
      "Iteration 11418 Loss: 1.9957431554794312\n",
      "Iteration 11419 Loss: 1.022216796875\n",
      "Iteration 11419 Loss: 1.2979635000228882\n",
      "Iteration 11420 Loss: 1.0086967945098877\n",
      "Iteration 11421 Loss: 1.477327823638916\n",
      "Iteration 11422 Loss: 1.625062346458435\n",
      "Iteration 11423 Loss: 1.512412667274475\n",
      "Iteration 11424 Loss: 1.2355562448501587\n",
      "Iteration 11425 Loss: 1.2197400331497192\n",
      "Iteration 11426 Loss: 1.2800450325012207\n",
      "Iteration 11427 Loss: 1.806072473526001\n",
      "Iteration 11428 Loss: 1.0985418558120728\n",
      "Iteration 11429 Loss: 1.4419035911560059\n",
      "Iteration 11429 Loss: 1.3705358505249023\n",
      "Iteration 11430 Loss: 1.2286690473556519\n",
      "Iteration 11431 Loss: 1.229271411895752\n",
      "Iteration 11432 Loss: 1.222723364830017\n",
      "Iteration 11433 Loss: 1.3874305486679077\n",
      "Iteration 11434 Loss: 1.5157190561294556\n",
      "Iteration 11435 Loss: 1.2900224924087524\n",
      "Iteration 11436 Loss: 1.2675889730453491\n",
      "Iteration 11437 Loss: 1.1880425214767456\n",
      "Iteration 11438 Loss: 1.3174960613250732\n",
      "Iteration 11439 Loss: 1.2818362712860107\n",
      "Iteration 11439 Loss: 1.2928800582885742\n",
      "Iteration 11440 Loss: 1.2182190418243408\n",
      "Iteration 11441 Loss: 1.349004864692688\n",
      "Iteration 11442 Loss: 1.4551184177398682\n",
      "Iteration 11443 Loss: 1.219261884689331\n",
      "Iteration 11444 Loss: 1.4476782083511353\n",
      "Iteration 11445 Loss: 1.3563389778137207\n",
      "Iteration 11446 Loss: 1.3535224199295044\n",
      "Iteration 11447 Loss: 1.3761868476867676\n",
      "Iteration 11448 Loss: 1.5424494743347168\n",
      "Iteration 11449 Loss: 1.275359869003296\n",
      "Iteration 11449 Loss: 1.359314203262329\n",
      "Iteration 11450 Loss: 1.6510876417160034\n",
      "Iteration 11451 Loss: 1.1384400129318237\n",
      "Iteration 11452 Loss: 1.25652277469635\n",
      "Iteration 11453 Loss: 0.8392767310142517\n",
      "Iteration 11454 Loss: 1.1535682678222656\n",
      "Iteration 11455 Loss: 1.2014179229736328\n",
      "Iteration 11456 Loss: 1.3153854608535767\n",
      "Iteration 11457 Loss: 0.9955841302871704\n",
      "Iteration 11458 Loss: 1.3443429470062256\n",
      "Iteration 11459 Loss: 1.4422372579574585\n",
      "Iteration 11459 Loss: 1.2337863445281982\n",
      "Iteration 11460 Loss: 1.4653822183609009\n",
      "Iteration 11461 Loss: 1.5783021450042725\n",
      "Iteration 11462 Loss: 1.5418931245803833\n",
      "Iteration 11463 Loss: 1.2229093313217163\n",
      "Iteration 11464 Loss: 1.325933575630188\n",
      "Iteration 11465 Loss: 1.1375274658203125\n",
      "Iteration 11466 Loss: 1.7980955839157104\n",
      "Iteration 11467 Loss: 1.0848987102508545\n",
      "Iteration 11468 Loss: 1.4784601926803589\n",
      "Iteration 11469 Loss: 1.5131685733795166\n",
      "Iteration 11469 Loss: 1.4146569967269897\n",
      "Iteration 11470 Loss: 1.3232085704803467\n",
      "Iteration 11471 Loss: 1.0230401754379272\n",
      "Iteration 11472 Loss: 1.4346647262573242\n",
      "Iteration 11473 Loss: 1.6774616241455078\n",
      "Iteration 11474 Loss: 1.2480746507644653\n",
      "Iteration 11475 Loss: 2.0178329944610596\n",
      "Iteration 11476 Loss: 1.202138066291809\n",
      "Iteration 11477 Loss: 1.297229290008545\n",
      "Iteration 11478 Loss: 1.4661993980407715\n",
      "Iteration 11479 Loss: 0.9642472863197327\n",
      "Iteration 11479 Loss: 1.3654096126556396\n",
      "Iteration 11480 Loss: 1.1856006383895874\n",
      "Iteration 11481 Loss: 1.39411461353302\n",
      "Iteration 11482 Loss: 1.2548240423202515\n",
      "Iteration 11483 Loss: 1.5125027894973755\n",
      "Iteration 11484 Loss: 1.2443828582763672\n",
      "Iteration 11485 Loss: 1.251842975616455\n",
      "Iteration 11486 Loss: 1.450648546218872\n",
      "Iteration 11487 Loss: 1.315081000328064\n",
      "Iteration 11488 Loss: 1.1740829944610596\n",
      "Iteration 11489 Loss: 1.031787395477295\n",
      "Iteration 11489 Loss: 1.2814866304397583\n",
      "Iteration 11490 Loss: 1.4386844635009766\n",
      "Iteration 11491 Loss: 1.0399926900863647\n",
      "Iteration 11492 Loss: 1.3447901010513306\n",
      "Iteration 11493 Loss: 1.3278769254684448\n",
      "Iteration 11494 Loss: 1.8830715417861938\n",
      "Iteration 11495 Loss: 1.307183027267456\n",
      "Iteration 11496 Loss: 0.8253737688064575\n",
      "Iteration 11497 Loss: 1.1974635124206543\n",
      "Iteration 11498 Loss: 1.2538684606552124\n",
      "Iteration 11499 Loss: 1.2798370122909546\n",
      "Iteration 11499 Loss: 1.2898142337799072\n",
      "Iteration 11500 Loss: 1.468738317489624\n",
      "Iteration 11501 Loss: 1.524538516998291\n",
      "Iteration 11502 Loss: 1.2291899919509888\n",
      "Iteration 11503 Loss: 1.2703704833984375\n",
      "Iteration 11504 Loss: 1.431778073310852\n",
      "Iteration 11505 Loss: 1.579558253288269\n",
      "Iteration 11506 Loss: 1.0499821901321411\n",
      "Iteration 11507 Loss: 1.1749387979507446\n",
      "Iteration 11508 Loss: 1.1425598859786987\n",
      "Iteration 11509 Loss: 0.9227048754692078\n",
      "Iteration 11509 Loss: 1.2794358730316162\n",
      "Iteration 11510 Loss: 1.3490867614746094\n",
      "Iteration 11511 Loss: 1.3579938411712646\n",
      "Iteration 11512 Loss: 1.4683756828308105\n",
      "Iteration 11513 Loss: 1.0864927768707275\n",
      "Iteration 11514 Loss: 1.5211682319641113\n",
      "Iteration 11515 Loss: 1.1020095348358154\n",
      "Iteration 11516 Loss: 1.0866090059280396\n",
      "Iteration 11517 Loss: 1.4159140586853027\n",
      "Iteration 11518 Loss: 1.2757010459899902\n",
      "Iteration 11519 Loss: 1.0284515619277954\n",
      "Iteration 11519 Loss: 1.2691802978515625\n",
      "Iteration 11520 Loss: 1.4015326499938965\n",
      "Iteration 11521 Loss: 1.4705582857131958\n",
      "Iteration 11522 Loss: 1.2988373041152954\n",
      "Iteration 11523 Loss: 1.1131033897399902\n",
      "Iteration 11524 Loss: 1.7982326745986938\n",
      "Iteration 11525 Loss: 1.8332641124725342\n",
      "Iteration 11526 Loss: 1.0289855003356934\n",
      "Iteration 11527 Loss: 1.3663334846496582\n",
      "Iteration 11528 Loss: 1.28584885597229\n",
      "Iteration 11529 Loss: 1.3880823850631714\n",
      "Iteration 11529 Loss: 1.3984777927398682\n",
      "Iteration 11530 Loss: 1.2964986562728882\n",
      "Iteration 11531 Loss: 0.8716844916343689\n",
      "Iteration 11532 Loss: 1.0185339450836182\n",
      "Iteration 11533 Loss: 0.9775092005729675\n",
      "Iteration 11534 Loss: 0.9949082732200623\n",
      "Iteration 11535 Loss: 1.022690773010254\n",
      "Iteration 11536 Loss: 1.3211630582809448\n",
      "Iteration 11537 Loss: 1.1332199573516846\n",
      "Iteration 11538 Loss: 1.4183056354522705\n",
      "Iteration 11539 Loss: 1.5485869646072388\n",
      "Iteration 11539 Loss: 1.1603100299835205\n",
      "Iteration 11540 Loss: 1.7936776876449585\n",
      "Iteration 11541 Loss: 1.4357435703277588\n",
      "Iteration 11542 Loss: 1.5670802593231201\n",
      "Iteration 11543 Loss: 1.8006116151809692\n",
      "Iteration 11544 Loss: 1.1583307981491089\n",
      "Iteration 11545 Loss: 1.2694900035858154\n",
      "Iteration 11546 Loss: 1.3037183284759521\n",
      "Iteration 11547 Loss: 0.985334038734436\n",
      "Iteration 11548 Loss: 1.342602014541626\n",
      "Iteration 11549 Loss: 1.2007513046264648\n",
      "Iteration 11549 Loss: 1.3857340812683105\n",
      "Iteration 11550 Loss: 1.352520227432251\n",
      "Iteration 11551 Loss: 1.407005786895752\n",
      "Iteration 11552 Loss: 1.5597913265228271\n",
      "Iteration 11553 Loss: 1.2346116304397583\n",
      "Iteration 11554 Loss: 1.670722484588623\n",
      "Iteration 11555 Loss: 1.289781093597412\n",
      "Iteration 11556 Loss: 1.1392983198165894\n",
      "Iteration 11557 Loss: 1.6012107133865356\n",
      "Iteration 11558 Loss: 1.0189884901046753\n",
      "Iteration 11559 Loss: 1.4823012351989746\n",
      "Iteration 11559 Loss: 1.375623106956482\n",
      "Iteration 11560 Loss: 1.4201542139053345\n",
      "Iteration 11561 Loss: 1.40059494972229\n",
      "Iteration 11562 Loss: 1.1507190465927124\n",
      "Iteration 11563 Loss: 1.1647682189941406\n",
      "Iteration 11564 Loss: 1.18317711353302\n",
      "Iteration 11565 Loss: 1.340284824371338\n",
      "Iteration 11566 Loss: 1.0974586009979248\n",
      "Iteration 11567 Loss: 1.123373031616211\n",
      "Iteration 11568 Loss: 0.9122642874717712\n",
      "Iteration 11569 Loss: 1.1318840980529785\n",
      "Iteration 11569 Loss: 1.1924679279327393\n",
      "Iteration 11570 Loss: 1.4041978120803833\n",
      "Iteration 11571 Loss: 1.3042680025100708\n",
      "Iteration 11572 Loss: 1.4856767654418945\n",
      "Iteration 11573 Loss: 1.277613639831543\n",
      "Iteration 11574 Loss: 1.5440748929977417\n",
      "Iteration 11575 Loss: 1.3659870624542236\n",
      "Iteration 11576 Loss: 1.659627079963684\n",
      "Iteration 11577 Loss: 1.1427466869354248\n",
      "Iteration 11578 Loss: 1.3644076585769653\n",
      "Iteration 11579 Loss: 1.1333593130111694\n",
      "Iteration 11579 Loss: 1.3681957721710205\n",
      "Iteration 11580 Loss: 1.21269690990448\n",
      "Iteration 11581 Loss: 1.1068141460418701\n",
      "Iteration 11582 Loss: 0.7747275233268738\n",
      "Iteration 11583 Loss: 1.313993215560913\n",
      "Iteration 11584 Loss: 1.34226393699646\n",
      "Iteration 11585 Loss: 1.0322585105895996\n",
      "Iteration 11586 Loss: 1.2151249647140503\n",
      "Iteration 11587 Loss: 1.1961685419082642\n",
      "Iteration 11588 Loss: 1.6751306056976318\n",
      "Iteration 11589 Loss: 1.2634965181350708\n",
      "Iteration 11589 Loss: 1.2132675647735596\n",
      "Iteration 11590 Loss: 1.4434154033660889\n",
      "Iteration 11591 Loss: 1.928539514541626\n",
      "Iteration 11592 Loss: 1.1960901021957397\n",
      "Iteration 11593 Loss: 1.1205650568008423\n",
      "Iteration 11594 Loss: 1.7310084104537964\n",
      "Iteration 11595 Loss: 1.417035460472107\n",
      "Iteration 11596 Loss: 0.8885510563850403\n",
      "Iteration 11597 Loss: 1.127340316772461\n",
      "Iteration 11598 Loss: 1.6578712463378906\n",
      "Iteration 11599 Loss: 1.492370367050171\n",
      "Iteration 11599 Loss: 1.4002786874771118\n",
      "Iteration 11600 Loss: 1.3193519115447998\n",
      "Iteration 11601 Loss: 1.4935245513916016\n",
      "Iteration 11602 Loss: 1.3140679597854614\n",
      "Iteration 11603 Loss: 0.8219283819198608\n",
      "Iteration 11604 Loss: 1.038451910018921\n",
      "Iteration 11605 Loss: 1.9085999727249146\n",
      "Iteration 11606 Loss: 1.3633815050125122\n",
      "Iteration 11607 Loss: 1.6228713989257812\n",
      "Iteration 11608 Loss: 0.8027655482292175\n",
      "Iteration 11609 Loss: 1.4699656963348389\n",
      "Iteration 11609 Loss: 1.3154908418655396\n",
      "Iteration 11610 Loss: 1.2579894065856934\n",
      "Iteration 11611 Loss: 1.2262868881225586\n",
      "Iteration 11612 Loss: 1.1201614141464233\n",
      "Iteration 11613 Loss: 1.026452660560608\n",
      "Iteration 11614 Loss: 1.5881056785583496\n",
      "Iteration 11615 Loss: 1.4085607528686523\n",
      "Iteration 11616 Loss: 1.1406283378601074\n",
      "Iteration 11617 Loss: 1.2254338264465332\n",
      "Iteration 11618 Loss: 1.638391137123108\n",
      "Iteration 11619 Loss: 0.8886529207229614\n",
      "Iteration 11619 Loss: 1.2520662546157837\n",
      "Iteration 11620 Loss: 1.4072068929672241\n",
      "Iteration 11621 Loss: 0.9925981760025024\n",
      "Iteration 11622 Loss: 1.337074637413025\n",
      "Iteration 11623 Loss: 1.1731808185577393\n",
      "Iteration 11624 Loss: 1.0471181869506836\n",
      "Iteration 11625 Loss: 1.0845947265625\n",
      "Iteration 11626 Loss: 1.3888130187988281\n",
      "Iteration 11627 Loss: 1.2670509815216064\n",
      "Iteration 11628 Loss: 0.9686981439590454\n",
      "Iteration 11629 Loss: 1.0363949537277222\n",
      "Iteration 11629 Loss: 1.1702730655670166\n",
      "Iteration 11630 Loss: 1.446088194847107\n",
      "Iteration 11631 Loss: 1.3041460514068604\n",
      "Iteration 11632 Loss: 1.3324856758117676\n",
      "Iteration 11633 Loss: 1.3702203035354614\n",
      "Iteration 11634 Loss: 1.3375385999679565\n",
      "Iteration 11635 Loss: 2.018963098526001\n",
      "Iteration 11636 Loss: 0.9989384412765503\n",
      "Iteration 11637 Loss: 1.0138962268829346\n",
      "Iteration 11638 Loss: 1.1594676971435547\n",
      "Iteration 11639 Loss: 1.8871057033538818\n",
      "Iteration 11639 Loss: 1.3868849277496338\n",
      "Iteration 11640 Loss: 1.0061770677566528\n",
      "Iteration 11641 Loss: 1.187888741493225\n",
      "Iteration 11642 Loss: 1.3563356399536133\n",
      "Iteration 11643 Loss: 1.4001070261001587\n",
      "Iteration 11644 Loss: 1.339800238609314\n",
      "Iteration 11645 Loss: 0.6795067191123962\n",
      "Iteration 11646 Loss: 0.9819018840789795\n",
      "Iteration 11647 Loss: 1.4050922393798828\n",
      "Iteration 11648 Loss: 1.389359951019287\n",
      "Iteration 11649 Loss: 1.4473316669464111\n",
      "Iteration 11649 Loss: 1.2193500995635986\n",
      "Iteration 11650 Loss: 0.9139161109924316\n",
      "Iteration 11651 Loss: 1.5314549207687378\n",
      "Iteration 11652 Loss: 1.2407578229904175\n",
      "Iteration 11653 Loss: 1.0406798124313354\n",
      "Iteration 11654 Loss: 1.4192510843276978\n",
      "Iteration 11655 Loss: 1.0923680067062378\n",
      "Iteration 11656 Loss: 1.5146934986114502\n",
      "Iteration 11657 Loss: 1.486539602279663\n",
      "Iteration 11658 Loss: 1.2346878051757812\n",
      "Iteration 11659 Loss: 1.080538272857666\n",
      "Iteration 11659 Loss: 1.2554887533187866\n",
      "Iteration 11660 Loss: 1.5012729167938232\n",
      "Iteration 11661 Loss: 1.387904167175293\n",
      "Iteration 11662 Loss: 1.68693208694458\n",
      "Iteration 11663 Loss: 1.8155696392059326\n",
      "Iteration 11664 Loss: 1.2754963636398315\n",
      "Iteration 11665 Loss: 0.8423061370849609\n",
      "Iteration 11666 Loss: 1.2716615200042725\n",
      "Iteration 11667 Loss: 1.3918797969818115\n",
      "Iteration 11668 Loss: 1.8144276142120361\n",
      "Iteration 11669 Loss: 1.181713581085205\n",
      "Iteration 11669 Loss: 1.4169164896011353\n",
      "Iteration 11670 Loss: 1.4970015287399292\n",
      "Iteration 11671 Loss: 1.1479071378707886\n",
      "Iteration 11672 Loss: 0.8223281502723694\n",
      "Iteration 11673 Loss: 1.5872414112091064\n",
      "Iteration 11674 Loss: 1.408743143081665\n",
      "Iteration 11675 Loss: 1.2154382467269897\n",
      "Iteration 11676 Loss: 1.2828823328018188\n",
      "Iteration 11677 Loss: 0.9667636752128601\n",
      "Iteration 11678 Loss: 1.5779170989990234\n",
      "Iteration 11679 Loss: 1.3080953359603882\n",
      "Iteration 11679 Loss: 1.281431794166565\n",
      "Iteration 11680 Loss: 1.4394681453704834\n",
      "Iteration 11681 Loss: 1.1506612300872803\n",
      "Iteration 11682 Loss: 0.7295849919319153\n",
      "Iteration 11683 Loss: 1.038245439529419\n",
      "Iteration 11684 Loss: 0.8379499912261963\n",
      "Iteration 11685 Loss: 1.1257364749908447\n",
      "Iteration 11686 Loss: 1.1758038997650146\n",
      "Iteration 11687 Loss: 1.3195891380310059\n",
      "Iteration 11688 Loss: 1.4994906187057495\n",
      "Iteration 11689 Loss: 1.0049564838409424\n",
      "Iteration 11689 Loss: 1.1321485042572021\n",
      "Iteration 11690 Loss: 1.0023425817489624\n",
      "Iteration 11691 Loss: 1.546075701713562\n",
      "Iteration 11692 Loss: 1.170122742652893\n",
      "Iteration 11693 Loss: 1.4032371044158936\n",
      "Iteration 11694 Loss: 1.123097538948059\n",
      "Iteration 11695 Loss: 1.128653883934021\n",
      "Iteration 11696 Loss: 1.288589358329773\n",
      "Iteration 11697 Loss: 1.539785623550415\n",
      "Iteration 11698 Loss: 1.3791462182998657\n",
      "Iteration 11699 Loss: 1.2985509634017944\n",
      "Iteration 11699 Loss: 1.287960171699524\n",
      "Iteration 11700 Loss: 0.8581066131591797\n",
      "Iteration 11701 Loss: 1.0574740171432495\n",
      "Iteration 11702 Loss: 1.4684268236160278\n",
      "Iteration 11703 Loss: 0.5560322999954224\n",
      "Iteration 11704 Loss: 1.3255939483642578\n",
      "Iteration 11705 Loss: 1.4826663732528687\n",
      "Iteration 11706 Loss: 1.0743257999420166\n",
      "Iteration 11707 Loss: 1.5629992485046387\n",
      "Iteration 11708 Loss: 1.7857557535171509\n",
      "Iteration 11709 Loss: 1.2166005373001099\n",
      "Iteration 11709 Loss: 1.2387981414794922\n",
      "Iteration 11710 Loss: 1.5490589141845703\n",
      "Iteration 11711 Loss: 1.414915919303894\n",
      "Iteration 11712 Loss: 1.2111902236938477\n",
      "Iteration 11713 Loss: 1.4405293464660645\n",
      "Iteration 11714 Loss: 1.4397449493408203\n",
      "Iteration 11715 Loss: 1.4833794832229614\n",
      "Iteration 11716 Loss: 1.4636456966400146\n",
      "Iteration 11717 Loss: 1.5087497234344482\n",
      "Iteration 11718 Loss: 1.554043173789978\n",
      "Iteration 11719 Loss: 1.2418228387832642\n",
      "Iteration 11719 Loss: 1.4307080507278442\n",
      "Iteration 11720 Loss: 1.578893780708313\n",
      "Iteration 11721 Loss: 1.1414566040039062\n",
      "Iteration 11722 Loss: 1.1938315629959106\n",
      "Iteration 11723 Loss: 0.9542286396026611\n",
      "Iteration 11724 Loss: 1.0998016595840454\n",
      "Iteration 11725 Loss: 1.7614459991455078\n",
      "Iteration 11726 Loss: 1.1366181373596191\n",
      "Iteration 11727 Loss: 1.5635432004928589\n",
      "Iteration 11728 Loss: 1.185664415359497\n",
      "Iteration 11729 Loss: 0.803320586681366\n",
      "Iteration 11729 Loss: 1.2418804168701172\n",
      "Iteration 11730 Loss: 1.1676416397094727\n",
      "Iteration 11731 Loss: 1.037909984588623\n",
      "Iteration 11732 Loss: 1.1105960607528687\n",
      "Iteration 11733 Loss: 0.848562479019165\n",
      "Iteration 11734 Loss: 1.4454723596572876\n",
      "Iteration 11735 Loss: 0.922877311706543\n",
      "Iteration 11736 Loss: 1.1228642463684082\n",
      "Iteration 11737 Loss: 1.1752209663391113\n",
      "Iteration 11738 Loss: 1.6085689067840576\n",
      "Iteration 11739 Loss: 1.5449800491333008\n",
      "Iteration 11739 Loss: 1.1984694004058838\n",
      "Iteration 11740 Loss: 1.5786374807357788\n",
      "Iteration 11741 Loss: 1.3247054815292358\n",
      "Iteration 11742 Loss: 1.4040119647979736\n",
      "Iteration 11743 Loss: 0.943329930305481\n",
      "Iteration 11744 Loss: 1.7967993021011353\n",
      "Iteration 11745 Loss: 1.1215062141418457\n",
      "Iteration 11746 Loss: 1.233536720275879\n",
      "Iteration 11747 Loss: 1.0773041248321533\n",
      "Iteration 11748 Loss: 1.201021432876587\n",
      "Iteration 11749 Loss: 1.0254881381988525\n",
      "Iteration 11749 Loss: 1.2706340551376343\n",
      "Iteration 11750 Loss: 1.6659016609191895\n",
      "Iteration 11751 Loss: 1.305446982383728\n",
      "Iteration 11752 Loss: 1.3289761543273926\n",
      "Iteration 11753 Loss: 1.2873637676239014\n",
      "Iteration 11754 Loss: 1.2772310972213745\n",
      "Iteration 11755 Loss: 1.2020173072814941\n",
      "Iteration 11756 Loss: 1.156711220741272\n",
      "Iteration 11757 Loss: 0.756629467010498\n",
      "Iteration 11758 Loss: 0.950447142124176\n",
      "Iteration 11759 Loss: 1.2493077516555786\n",
      "Iteration 11759 Loss: 1.218003273010254\n",
      "Iteration 11760 Loss: 1.4971058368682861\n",
      "Iteration 11761 Loss: 1.5429118871688843\n",
      "Iteration 11762 Loss: 1.1691535711288452\n",
      "Iteration 11763 Loss: 1.414380431175232\n",
      "Iteration 11764 Loss: 1.1414520740509033\n",
      "Iteration 11765 Loss: 1.2665306329727173\n",
      "Iteration 11766 Loss: 1.0428236722946167\n",
      "Iteration 11767 Loss: 1.5853370428085327\n",
      "Iteration 11768 Loss: 1.5673348903656006\n",
      "Iteration 11769 Loss: 1.2298227548599243\n",
      "Iteration 11769 Loss: 1.3456852436065674\n",
      "Iteration 11770 Loss: 0.9648973345756531\n",
      "Iteration 11771 Loss: 1.3918843269348145\n",
      "Iteration 11772 Loss: 0.9984415769577026\n",
      "Iteration 11773 Loss: 0.7232955098152161\n",
      "Iteration 11774 Loss: 0.8032738566398621\n",
      "Iteration 11775 Loss: 1.5433019399642944\n",
      "Iteration 11776 Loss: 1.589572787284851\n",
      "Iteration 11777 Loss: 1.0664191246032715\n",
      "Iteration 11778 Loss: 1.3260433673858643\n",
      "Iteration 11779 Loss: 0.8501682281494141\n",
      "Iteration 11779 Loss: 1.1257297992706299\n",
      "Iteration 11780 Loss: 1.3225940465927124\n",
      "Iteration 11781 Loss: 1.339176058769226\n",
      "Iteration 11782 Loss: 1.756316065788269\n",
      "Iteration 11783 Loss: 0.8616847395896912\n",
      "Iteration 11784 Loss: 1.586708426475525\n",
      "Iteration 11785 Loss: 1.8365857601165771\n",
      "Iteration 11786 Loss: 1.3604103326797485\n",
      "Iteration 11787 Loss: 1.450226902961731\n",
      "Iteration 11788 Loss: 1.2143466472625732\n",
      "Iteration 11789 Loss: 0.8957832455635071\n",
      "Iteration 11789 Loss: 1.362383246421814\n",
      "Iteration 11790 Loss: 1.315073013305664\n",
      "Iteration 11791 Loss: 1.2527718544006348\n",
      "Iteration 11792 Loss: 0.9841204881668091\n",
      "Iteration 11793 Loss: 1.4389545917510986\n",
      "Iteration 11794 Loss: 1.4043500423431396\n",
      "Iteration 11795 Loss: 1.2230157852172852\n",
      "Iteration 11796 Loss: 1.3275113105773926\n",
      "Iteration 11797 Loss: 1.5959162712097168\n",
      "Iteration 11798 Loss: 1.1586180925369263\n",
      "Iteration 11799 Loss: 1.0163495540618896\n",
      "Iteration 11799 Loss: 1.2716681957244873\n",
      "Iteration 11800 Loss: 1.0090612173080444\n",
      "Iteration 11801 Loss: 1.1423985958099365\n",
      "Iteration 11802 Loss: 1.3455095291137695\n",
      "Iteration 11803 Loss: 1.1963139772415161\n",
      "Iteration 11804 Loss: 0.9605047702789307\n",
      "Iteration 11805 Loss: 1.4824246168136597\n",
      "Iteration 11806 Loss: 1.0436464548110962\n",
      "Iteration 11807 Loss: 1.1006395816802979\n",
      "Iteration 11808 Loss: 1.1421006917953491\n",
      "Iteration 11809 Loss: 1.4756786823272705\n",
      "Iteration 11809 Loss: 1.189827799797058\n",
      "Iteration 11810 Loss: 1.3062889575958252\n",
      "Iteration 11811 Loss: 1.1171101331710815\n",
      "Iteration 11812 Loss: 1.704835057258606\n",
      "Iteration 11813 Loss: 1.6913566589355469\n",
      "Iteration 11814 Loss: 1.6024081707000732\n",
      "Iteration 11815 Loss: 1.5102120637893677\n",
      "Iteration 11816 Loss: 1.1907737255096436\n",
      "Iteration 11817 Loss: 1.4830982685089111\n",
      "Iteration 11818 Loss: 1.3839327096939087\n",
      "Iteration 11819 Loss: 1.3819355964660645\n",
      "Iteration 11819 Loss: 1.437195062637329\n",
      "Iteration 11820 Loss: 1.1009126901626587\n",
      "Iteration 11821 Loss: 0.8733341097831726\n",
      "Iteration 11822 Loss: 1.2502003908157349\n",
      "Iteration 11823 Loss: 1.3266669511795044\n",
      "Iteration 11824 Loss: 1.7060081958770752\n",
      "Iteration 11825 Loss: 1.2418243885040283\n",
      "Iteration 11826 Loss: 1.3767883777618408\n",
      "Iteration 11827 Loss: 1.4210999011993408\n",
      "Iteration 11828 Loss: 1.381605625152588\n",
      "Iteration 11829 Loss: 1.3157849311828613\n",
      "Iteration 11829 Loss: 1.2994225025177002\n",
      "Iteration 11830 Loss: 1.1510945558547974\n",
      "Iteration 11831 Loss: 1.3335325717926025\n",
      "Iteration 11832 Loss: 0.7118143439292908\n",
      "Iteration 11833 Loss: 0.9757450222969055\n",
      "Iteration 11834 Loss: 1.1488064527511597\n",
      "Iteration 11835 Loss: 1.290658712387085\n",
      "Iteration 11836 Loss: 1.581580638885498\n",
      "Iteration 11837 Loss: 1.327364444732666\n",
      "Iteration 11838 Loss: 1.2321507930755615\n",
      "Iteration 11839 Loss: 1.499467134475708\n",
      "Iteration 11839 Loss: 1.2252213954925537\n",
      "Iteration 11840 Loss: 1.5268491506576538\n",
      "Iteration 11841 Loss: 1.4707937240600586\n",
      "Iteration 11842 Loss: 1.4042435884475708\n",
      "Iteration 11843 Loss: 1.1353503465652466\n",
      "Iteration 11844 Loss: 1.1644693613052368\n",
      "Iteration 11845 Loss: 1.0132144689559937\n",
      "Iteration 11846 Loss: 1.1672422885894775\n",
      "Iteration 11847 Loss: 1.0785738229751587\n",
      "Iteration 11848 Loss: 1.621483564376831\n",
      "Iteration 11849 Loss: 1.0917385816574097\n",
      "Iteration 11849 Loss: 1.2673958539962769\n",
      "Iteration 11850 Loss: 1.4020143747329712\n",
      "Iteration 11851 Loss: 1.2970178127288818\n",
      "Iteration 11852 Loss: 1.2672779560089111\n",
      "Iteration 11853 Loss: 1.7157725095748901\n",
      "Iteration 11854 Loss: 1.0709329843521118\n",
      "Iteration 11855 Loss: 1.5826083421707153\n",
      "Iteration 11856 Loss: 0.7119373083114624\n",
      "Iteration 11857 Loss: 1.4850825071334839\n",
      "Iteration 11858 Loss: 1.248034954071045\n",
      "Iteration 11859 Loss: 1.5329006910324097\n",
      "Iteration 11859 Loss: 1.3313579559326172\n",
      "Iteration 11860 Loss: 1.4883968830108643\n",
      "Iteration 11861 Loss: 1.021594524383545\n",
      "Iteration 11862 Loss: 1.6771719455718994\n",
      "Iteration 11863 Loss: 1.0633156299591064\n",
      "Iteration 11864 Loss: 1.4405800104141235\n",
      "Iteration 11865 Loss: 1.0696715116500854\n",
      "Iteration 11866 Loss: 0.985512375831604\n",
      "Iteration 11867 Loss: 1.5323230028152466\n",
      "Iteration 11868 Loss: 1.3460679054260254\n",
      "Iteration 11869 Loss: 1.297073483467102\n",
      "Iteration 11869 Loss: 1.292170763015747\n",
      "Iteration 11870 Loss: 1.0450719594955444\n",
      "Iteration 11871 Loss: 1.0473341941833496\n",
      "Iteration 11872 Loss: 1.367411732673645\n",
      "Iteration 11873 Loss: 1.3676296472549438\n",
      "Iteration 11874 Loss: 1.306565284729004\n",
      "Iteration 11875 Loss: 0.936244785785675\n",
      "Iteration 11876 Loss: 1.0190714597702026\n",
      "Iteration 11877 Loss: 1.0254312753677368\n",
      "Iteration 11878 Loss: 0.6222957372665405\n",
      "Iteration 11879 Loss: 1.5830904245376587\n",
      "Iteration 11879 Loss: 1.1320147514343262\n",
      "Iteration 11880 Loss: 1.0904959440231323\n",
      "Iteration 11881 Loss: 1.5339477062225342\n",
      "Iteration 11882 Loss: 1.0670902729034424\n",
      "Iteration 11883 Loss: 1.621258020401001\n",
      "Iteration 11884 Loss: 0.8535493612289429\n",
      "Iteration 11885 Loss: 1.363234281539917\n",
      "Iteration 11886 Loss: 1.147650957107544\n",
      "Iteration 11887 Loss: 1.1029269695281982\n",
      "Iteration 11888 Loss: 1.7200032472610474\n",
      "Iteration 11889 Loss: 1.2622069120407104\n",
      "Iteration 11889 Loss: 1.2762362957000732\n",
      "Iteration 11890 Loss: 1.353071689605713\n",
      "Iteration 11891 Loss: 1.358573079109192\n",
      "Iteration 11892 Loss: 1.4320560693740845\n",
      "Iteration 11893 Loss: 1.4355210065841675\n",
      "Iteration 11894 Loss: 1.5160855054855347\n",
      "Iteration 11895 Loss: 1.6807557344436646\n",
      "Iteration 11896 Loss: 1.2642320394515991\n",
      "Iteration 11897 Loss: 0.9340785145759583\n",
      "Iteration 11898 Loss: 1.4248634576797485\n",
      "Iteration 11899 Loss: 1.4244894981384277\n",
      "Iteration 11899 Loss: 1.3823726177215576\n",
      "Iteration 11900 Loss: 1.0356098413467407\n",
      "Iteration 11901 Loss: 1.551314115524292\n",
      "Iteration 11902 Loss: 1.4777277708053589\n",
      "Iteration 11903 Loss: 1.257401704788208\n",
      "Iteration 11904 Loss: 1.1812142133712769\n",
      "Iteration 11905 Loss: 1.522919774055481\n",
      "Iteration 11906 Loss: 1.1733136177062988\n",
      "Iteration 11907 Loss: 1.2436370849609375\n",
      "Iteration 11908 Loss: 1.3698768615722656\n",
      "Iteration 11909 Loss: 1.3073261976242065\n",
      "Iteration 11909 Loss: 1.312034010887146\n",
      "Iteration 11910 Loss: 1.2434695959091187\n",
      "Iteration 11911 Loss: 1.093666434288025\n",
      "Iteration 11912 Loss: 1.3650476932525635\n",
      "Iteration 11913 Loss: 1.1457099914550781\n",
      "Iteration 11914 Loss: 1.2892286777496338\n",
      "Iteration 11915 Loss: 1.6010093688964844\n",
      "Iteration 11916 Loss: 1.4203877449035645\n",
      "Iteration 11917 Loss: 1.8569813966751099\n",
      "Iteration 11918 Loss: 1.1444687843322754\n",
      "Iteration 11919 Loss: 1.0880978107452393\n",
      "Iteration 11919 Loss: 1.324806809425354\n",
      "Iteration 11920 Loss: 1.4070661067962646\n",
      "Iteration 11921 Loss: 1.6356979608535767\n",
      "Iteration 11922 Loss: 1.5058763027191162\n",
      "Iteration 11923 Loss: 1.3470652103424072\n",
      "Iteration 11924 Loss: 1.2016581296920776\n",
      "Iteration 11925 Loss: 1.2244471311569214\n",
      "Iteration 11926 Loss: 1.3803192377090454\n",
      "Iteration 11927 Loss: 0.9544922113418579\n",
      "Iteration 11928 Loss: 0.9758122563362122\n",
      "Iteration 11929 Loss: 1.1163870096206665\n",
      "Iteration 11929 Loss: 1.274882197380066\n",
      "Iteration 11930 Loss: 1.3617385625839233\n",
      "Iteration 11931 Loss: 1.0186470746994019\n",
      "Iteration 11932 Loss: 1.8020858764648438\n",
      "Iteration 11933 Loss: 1.8067511320114136\n",
      "Iteration 11934 Loss: 1.2794030904769897\n",
      "Iteration 11935 Loss: 0.8147218823432922\n",
      "Iteration 11936 Loss: 1.2982451915740967\n",
      "Iteration 11937 Loss: 1.5446830987930298\n",
      "Iteration 11938 Loss: 1.2309491634368896\n",
      "Iteration 11939 Loss: 1.325986385345459\n",
      "Iteration 11939 Loss: 1.3483211994171143\n",
      "Iteration 11940 Loss: 1.5038256645202637\n",
      "Iteration 11941 Loss: 1.5774364471435547\n",
      "Iteration 11942 Loss: 1.314404010772705\n",
      "Iteration 11943 Loss: 1.3370543718338013\n",
      "Iteration 11944 Loss: 0.783801257610321\n",
      "Iteration 11945 Loss: 1.2768101692199707\n",
      "Iteration 11946 Loss: 1.3818227052688599\n",
      "Iteration 11947 Loss: 1.2082797288894653\n",
      "Iteration 11948 Loss: 1.1983381509780884\n",
      "Iteration 11949 Loss: 0.7127833962440491\n",
      "Iteration 11949 Loss: 1.2294557094573975\n",
      "Iteration 11950 Loss: 1.5682560205459595\n",
      "Iteration 11951 Loss: 1.460952877998352\n",
      "Iteration 11952 Loss: 1.3892416954040527\n",
      "Iteration 11953 Loss: 1.6912108659744263\n",
      "Iteration 11954 Loss: 1.2390903234481812\n",
      "Iteration 11955 Loss: 1.7545925378799438\n",
      "Iteration 11956 Loss: 1.124246597290039\n",
      "Iteration 11957 Loss: 1.6523181200027466\n",
      "Iteration 11958 Loss: 1.631149172782898\n",
      "Iteration 11959 Loss: 1.4466509819030762\n",
      "Iteration 11959 Loss: 1.4957709312438965\n",
      "Iteration 11960 Loss: 1.3747689723968506\n",
      "Iteration 11961 Loss: 1.2909015417099\n",
      "Iteration 11962 Loss: 1.3933008909225464\n",
      "Iteration 11963 Loss: 1.1402819156646729\n",
      "Iteration 11964 Loss: 1.3728761672973633\n",
      "Iteration 11965 Loss: 1.222695231437683\n",
      "Iteration 11966 Loss: 1.057555913925171\n",
      "Iteration 11967 Loss: 1.097084641456604\n",
      "Iteration 11968 Loss: 1.31475830078125\n",
      "Iteration 11969 Loss: 1.509819507598877\n",
      "Iteration 11969 Loss: 1.2774044275283813\n",
      "Iteration 11970 Loss: 1.0793589353561401\n",
      "Iteration 11971 Loss: 1.1116896867752075\n",
      "Iteration 11972 Loss: 1.3107686042785645\n",
      "Iteration 11973 Loss: 1.341553807258606\n",
      "Iteration 11974 Loss: 1.1572575569152832\n",
      "Iteration 11975 Loss: 1.3391987085342407\n",
      "Iteration 11976 Loss: 1.1729415655136108\n",
      "Iteration 11977 Loss: 1.5409634113311768\n",
      "Iteration 11978 Loss: 0.9973360896110535\n",
      "Iteration 11979 Loss: 1.6488207578659058\n",
      "Iteration 11979 Loss: 1.2699888944625854\n",
      "Iteration 11980 Loss: 1.218082308769226\n",
      "Iteration 11981 Loss: 1.3438559770584106\n",
      "Iteration 11982 Loss: 1.2511614561080933\n",
      "Iteration 11983 Loss: 0.9489716291427612\n",
      "Iteration 11984 Loss: 1.11448073387146\n",
      "Iteration 11985 Loss: 1.5660837888717651\n",
      "Iteration 11986 Loss: 1.0138261318206787\n",
      "Iteration 11987 Loss: 1.272287130355835\n",
      "Iteration 11988 Loss: 1.5717804431915283\n",
      "Iteration 11989 Loss: 1.1503839492797852\n",
      "Iteration 11989 Loss: 1.245091438293457\n",
      "Iteration 11990 Loss: 1.0939213037490845\n",
      "Iteration 11991 Loss: 1.1168158054351807\n",
      "Iteration 11992 Loss: 1.6444605588912964\n",
      "Iteration 11993 Loss: 0.955049455165863\n",
      "Iteration 11994 Loss: 1.4419870376586914\n",
      "Iteration 11995 Loss: 1.1448466777801514\n",
      "Iteration 11996 Loss: 1.5651357173919678\n",
      "Iteration 11997 Loss: 1.2264314889907837\n",
      "Iteration 11998 Loss: 1.7237014770507812\n",
      "Iteration 11999 Loss: 1.0629887580871582\n",
      "Iteration 11999 Loss: 1.2975338697433472\n",
      "Iteration 12000 Loss: 0.9897753000259399\n",
      "Iteration 12001 Loss: 1.2433289289474487\n",
      "Iteration 12002 Loss: 1.1159132719039917\n",
      "Iteration 12003 Loss: 1.3440566062927246\n",
      "Iteration 12004 Loss: 1.419702172279358\n",
      "Iteration 12005 Loss: 1.0140372514724731\n",
      "Iteration 12006 Loss: 1.4143528938293457\n",
      "Iteration 12007 Loss: 1.093252182006836\n",
      "Iteration 12008 Loss: 1.2326946258544922\n",
      "Iteration 12009 Loss: 1.402181625366211\n",
      "Iteration 12009 Loss: 1.2269294261932373\n",
      "Iteration 12010 Loss: 1.2440530061721802\n",
      "Iteration 12011 Loss: 1.1376068592071533\n",
      "Iteration 12012 Loss: 1.2420127391815186\n",
      "Iteration 12013 Loss: 1.3776450157165527\n",
      "Iteration 12014 Loss: 1.1805639266967773\n",
      "Iteration 12015 Loss: 1.1941468715667725\n",
      "Iteration 12016 Loss: 1.2156128883361816\n",
      "Iteration 12017 Loss: 1.1692558526992798\n",
      "Iteration 12018 Loss: 1.3427122831344604\n",
      "Iteration 12019 Loss: 1.0989421606063843\n",
      "Iteration 12019 Loss: 1.2202552556991577\n",
      "Iteration 12020 Loss: 0.7196154594421387\n",
      "Iteration 12021 Loss: 1.5616239309310913\n",
      "Iteration 12022 Loss: 1.291390299797058\n",
      "Iteration 12023 Loss: 1.0788904428482056\n",
      "Iteration 12024 Loss: 1.3941946029663086\n",
      "Iteration 12025 Loss: 1.3547343015670776\n",
      "Iteration 12026 Loss: 1.1597648859024048\n",
      "Iteration 12027 Loss: 1.6410009860992432\n",
      "Iteration 12028 Loss: 1.0914182662963867\n",
      "Iteration 12029 Loss: 1.3898488283157349\n",
      "Iteration 12029 Loss: 1.268248200416565\n",
      "Iteration 12030 Loss: 1.3078845739364624\n",
      "Iteration 12031 Loss: 1.0825984477996826\n",
      "Iteration 12032 Loss: 1.6856930255889893\n",
      "Iteration 12033 Loss: 1.203083872795105\n",
      "Iteration 12034 Loss: 1.2668408155441284\n",
      "Iteration 12035 Loss: 1.264528751373291\n",
      "Iteration 12036 Loss: 0.9940711855888367\n",
      "Iteration 12037 Loss: 1.3307697772979736\n",
      "Iteration 12038 Loss: 1.154942274093628\n",
      "Iteration 12039 Loss: 0.7437087297439575\n",
      "Iteration 12039 Loss: 1.2034120559692383\n",
      "Iteration 12040 Loss: 1.4128942489624023\n",
      "Iteration 12041 Loss: 1.070697546005249\n",
      "Iteration 12042 Loss: 1.3904472589492798\n",
      "Iteration 12043 Loss: 1.3849117755889893\n",
      "Iteration 12044 Loss: 1.5572872161865234\n",
      "Iteration 12045 Loss: 1.2649188041687012\n",
      "Iteration 12046 Loss: 1.2318713665008545\n",
      "Iteration 12047 Loss: 1.1982293128967285\n",
      "Iteration 12048 Loss: 0.9551646709442139\n",
      "Iteration 12049 Loss: 1.4680808782577515\n",
      "Iteration 12049 Loss: 1.2934503555297852\n",
      "Iteration 12050 Loss: 1.3061244487762451\n",
      "Iteration 12051 Loss: 0.7133877873420715\n",
      "Iteration 12052 Loss: 1.5672080516815186\n",
      "Iteration 12053 Loss: 1.2539851665496826\n",
      "Iteration 12054 Loss: 1.7881242036819458\n",
      "Iteration 12055 Loss: 1.2503187656402588\n",
      "Iteration 12056 Loss: 1.6031414270401\n",
      "Iteration 12057 Loss: 1.336778998374939\n",
      "Iteration 12058 Loss: 1.7176697254180908\n",
      "Iteration 12059 Loss: 1.42921781539917\n",
      "Iteration 12059 Loss: 1.3965957164764404\n",
      "Iteration 12060 Loss: 1.2707583904266357\n",
      "Iteration 12061 Loss: 1.221340537071228\n",
      "Iteration 12062 Loss: 1.4668391942977905\n",
      "Iteration 12063 Loss: 1.271043062210083\n",
      "Iteration 12064 Loss: 1.3022040128707886\n",
      "Iteration 12065 Loss: 1.1287117004394531\n",
      "Iteration 12066 Loss: 1.0880177021026611\n",
      "Iteration 12067 Loss: 1.041050910949707\n",
      "Iteration 12068 Loss: 1.506597638130188\n",
      "Iteration 12069 Loss: 1.589008092880249\n",
      "Iteration 12069 Loss: 1.2885571718215942\n",
      "Iteration 12070 Loss: 1.5450149774551392\n",
      "Iteration 12071 Loss: 1.4299404621124268\n",
      "Iteration 12072 Loss: 1.1530756950378418\n",
      "Iteration 12073 Loss: 1.0012962818145752\n",
      "Iteration 12074 Loss: 1.314920425415039\n",
      "Iteration 12075 Loss: 1.3278557062149048\n",
      "Iteration 12076 Loss: 0.858247697353363\n",
      "Iteration 12077 Loss: 1.5581610202789307\n",
      "Iteration 12078 Loss: 1.490309715270996\n",
      "Iteration 12079 Loss: 1.2468467950820923\n",
      "Iteration 12079 Loss: 1.2925668954849243\n",
      "Iteration 12080 Loss: 1.7588911056518555\n",
      "Iteration 12081 Loss: 1.6901825666427612\n",
      "Iteration 12082 Loss: 1.3273991346359253\n",
      "Iteration 12083 Loss: 1.4495195150375366\n",
      "Iteration 12084 Loss: 1.520179271697998\n",
      "Iteration 12085 Loss: 1.3268601894378662\n",
      "Iteration 12086 Loss: 1.4911315441131592\n",
      "Iteration 12087 Loss: 1.2690802812576294\n",
      "Iteration 12088 Loss: 1.8501955270767212\n",
      "Iteration 12089 Loss: 1.219094157218933\n",
      "Iteration 12089 Loss: 1.4902534484863281\n",
      "Iteration 12090 Loss: 0.9894686937332153\n",
      "Iteration 12091 Loss: 1.1525126695632935\n",
      "Iteration 12092 Loss: 1.5279592275619507\n",
      "Iteration 12093 Loss: 1.1784528493881226\n",
      "Iteration 12094 Loss: 0.9046239256858826\n",
      "Iteration 12095 Loss: 1.2480889558792114\n",
      "Iteration 12096 Loss: 1.1270945072174072\n",
      "Iteration 12097 Loss: 1.089712142944336\n",
      "Iteration 12098 Loss: 1.1586896181106567\n",
      "Iteration 12099 Loss: 1.640113353729248\n",
      "Iteration 12099 Loss: 1.2016714811325073\n",
      "Iteration 12100 Loss: 1.7196439504623413\n",
      "Iteration 12101 Loss: 1.3333730697631836\n",
      "Iteration 12102 Loss: 1.2834582328796387\n",
      "Iteration 12103 Loss: 1.3735055923461914\n",
      "Iteration 12104 Loss: 1.2861624956130981\n",
      "Iteration 12105 Loss: 1.2681713104248047\n",
      "Iteration 12106 Loss: 0.9872896075248718\n",
      "Iteration 12107 Loss: 1.4568006992340088\n",
      "Iteration 12108 Loss: 0.7748250961303711\n",
      "Iteration 12109 Loss: 1.1868627071380615\n",
      "Iteration 12109 Loss: 1.2670092582702637\n",
      "Iteration 12110 Loss: 1.2314891815185547\n",
      "Iteration 12111 Loss: 1.58641517162323\n",
      "Iteration 12112 Loss: 1.3927149772644043\n",
      "Iteration 12113 Loss: 1.2156540155410767\n",
      "Iteration 12114 Loss: 1.1164617538452148\n",
      "Iteration 12115 Loss: 1.442444920539856\n",
      "Iteration 12116 Loss: 0.8971236944198608\n",
      "Iteration 12117 Loss: 1.259354829788208\n",
      "Iteration 12118 Loss: 1.0768113136291504\n",
      "Iteration 12119 Loss: 1.2471916675567627\n",
      "Iteration 12119 Loss: 1.2465660572052002\n",
      "Iteration 12120 Loss: 1.5209559202194214\n",
      "Iteration 12121 Loss: 1.0872273445129395\n",
      "Iteration 12122 Loss: 1.364339828491211\n",
      "Iteration 12123 Loss: 1.3881230354309082\n",
      "Iteration 12124 Loss: 1.754693627357483\n",
      "Iteration 12125 Loss: 1.1128697395324707\n",
      "Iteration 12126 Loss: 0.8975756168365479\n",
      "Iteration 12127 Loss: 1.661674976348877\n",
      "Iteration 12128 Loss: 1.0178558826446533\n",
      "Iteration 12129 Loss: 1.2001439332962036\n",
      "Iteration 12129 Loss: 1.3005459308624268\n",
      "Iteration 12130 Loss: 1.1767232418060303\n",
      "Iteration 12131 Loss: 0.932648777961731\n",
      "Iteration 12132 Loss: 1.3143904209136963\n",
      "Iteration 12133 Loss: 1.3621176481246948\n",
      "Iteration 12134 Loss: 1.8541985750198364\n",
      "Iteration 12135 Loss: 1.407528281211853\n",
      "Iteration 12136 Loss: 1.1764298677444458\n",
      "Iteration 12137 Loss: 1.5962375402450562\n",
      "Iteration 12138 Loss: 1.4852769374847412\n",
      "Iteration 12139 Loss: 1.4408701658248901\n",
      "Iteration 12139 Loss: 1.374642014503479\n",
      "Iteration 12140 Loss: 1.0847656726837158\n",
      "Iteration 12141 Loss: 1.2026187181472778\n",
      "Iteration 12142 Loss: 1.3108227252960205\n",
      "Iteration 12143 Loss: 1.354364275932312\n",
      "Iteration 12144 Loss: 1.5669387578964233\n",
      "Iteration 12145 Loss: 1.5776762962341309\n",
      "Iteration 12146 Loss: 1.4020605087280273\n",
      "Iteration 12147 Loss: 1.3607081174850464\n",
      "Iteration 12148 Loss: 1.5660818815231323\n",
      "Iteration 12149 Loss: 1.1468331813812256\n",
      "Iteration 12149 Loss: 1.3572869300842285\n",
      "Iteration 12150 Loss: 1.4622180461883545\n",
      "Iteration 12151 Loss: 1.4260799884796143\n",
      "Iteration 12152 Loss: 1.2420519590377808\n",
      "Iteration 12153 Loss: 1.0711249113082886\n",
      "Iteration 12154 Loss: 1.4745959043502808\n",
      "Iteration 12155 Loss: 0.8452599048614502\n",
      "Iteration 12156 Loss: 1.372441053390503\n",
      "Iteration 12157 Loss: 1.1468437910079956\n",
      "Iteration 12158 Loss: 1.196381688117981\n",
      "Iteration 12159 Loss: 1.0244834423065186\n",
      "Iteration 12159 Loss: 1.2261481285095215\n",
      "Iteration 12160 Loss: 0.9637992978096008\n",
      "Iteration 12161 Loss: 1.4948463439941406\n",
      "Iteration 12162 Loss: 1.556476354598999\n",
      "Iteration 12163 Loss: 1.0674258470535278\n",
      "Iteration 12164 Loss: 1.3246140480041504\n",
      "Iteration 12165 Loss: 1.5337169170379639\n",
      "Iteration 12166 Loss: 1.472840428352356\n",
      "Iteration 12167 Loss: 1.1355664730072021\n",
      "Iteration 12168 Loss: 1.154283046722412\n",
      "Iteration 12169 Loss: 0.9624029994010925\n",
      "Iteration 12169 Loss: 1.2665971517562866\n",
      "Iteration 12170 Loss: 1.4093211889266968\n",
      "Iteration 12171 Loss: 1.2594596147537231\n",
      "Iteration 12172 Loss: 1.743227481842041\n",
      "Iteration 12173 Loss: 1.1609528064727783\n",
      "Iteration 12174 Loss: 1.3547039031982422\n",
      "Iteration 12175 Loss: 1.3559871912002563\n",
      "Iteration 12176 Loss: 1.3140453100204468\n",
      "Iteration 12177 Loss: 1.58961820602417\n",
      "Iteration 12178 Loss: 1.5644680261611938\n",
      "Iteration 12179 Loss: 0.9025306701660156\n",
      "Iteration 12179 Loss: 1.3654314279556274\n",
      "Iteration 12180 Loss: 1.566030502319336\n",
      "Iteration 12181 Loss: 0.9843637943267822\n",
      "Iteration 12182 Loss: 1.1687167882919312\n",
      "Iteration 12183 Loss: 1.5889887809753418\n",
      "Iteration 12184 Loss: 1.1954915523529053\n",
      "Iteration 12185 Loss: 0.9347853064537048\n",
      "Iteration 12186 Loss: 1.5536633729934692\n",
      "Iteration 12187 Loss: 1.8649852275848389\n",
      "Iteration 12188 Loss: 1.6993212699890137\n",
      "Iteration 12189 Loss: 1.4254971742630005\n",
      "Iteration 12189 Loss: 1.3981844186782837\n",
      "Iteration 12190 Loss: 1.1773563623428345\n",
      "Iteration 12191 Loss: 1.645993709564209\n",
      "Iteration 12192 Loss: 1.0421416759490967\n",
      "Iteration 12193 Loss: 1.211241364479065\n",
      "Iteration 12194 Loss: 1.7203091382980347\n",
      "Iteration 12195 Loss: 1.4451313018798828\n",
      "Iteration 12196 Loss: 1.5304917097091675\n",
      "Iteration 12197 Loss: 1.5304749011993408\n",
      "Iteration 12198 Loss: 1.1073299646377563\n",
      "Iteration 12199 Loss: 1.007525086402893\n",
      "Iteration 12199 Loss: 1.3417994976043701\n",
      "Iteration 12200 Loss: 1.14789879322052\n",
      "Iteration 12201 Loss: 1.6925973892211914\n",
      "Iteration 12202 Loss: 1.023478388786316\n",
      "Iteration 12203 Loss: 1.3938779830932617\n",
      "Iteration 12204 Loss: 1.264491081237793\n",
      "Iteration 12205 Loss: 1.4514474868774414\n",
      "Iteration 12206 Loss: 1.5635379552841187\n",
      "Iteration 12207 Loss: 1.6836167573928833\n",
      "Iteration 12208 Loss: 1.698936939239502\n",
      "Iteration 12209 Loss: 1.1774367094039917\n",
      "Iteration 12209 Loss: 1.4097318649291992\n",
      "Iteration 12210 Loss: 1.2590728998184204\n",
      "Iteration 12211 Loss: 0.7282472848892212\n",
      "Iteration 12212 Loss: 1.2026954889297485\n",
      "Iteration 12213 Loss: 0.8480948805809021\n",
      "Iteration 12214 Loss: 1.4643585681915283\n",
      "Iteration 12215 Loss: 1.1861432790756226\n",
      "Iteration 12216 Loss: 0.921249270439148\n",
      "Iteration 12217 Loss: 1.0675588846206665\n",
      "Iteration 12218 Loss: 1.7332143783569336\n",
      "Iteration 12219 Loss: 0.9141416549682617\n",
      "Iteration 12219 Loss: 1.1324776411056519\n",
      "Iteration 12220 Loss: 1.067522644996643\n",
      "Iteration 12221 Loss: 1.2568788528442383\n",
      "Iteration 12222 Loss: 1.2117470502853394\n",
      "Iteration 12223 Loss: 1.2968404293060303\n",
      "Iteration 12224 Loss: 1.1597942113876343\n",
      "Iteration 12225 Loss: 1.7892687320709229\n",
      "Iteration 12226 Loss: 1.3768703937530518\n",
      "Iteration 12227 Loss: 1.4802935123443604\n",
      "Iteration 12228 Loss: 1.061026930809021\n",
      "Iteration 12229 Loss: 1.0171467065811157\n",
      "Iteration 12229 Loss: 1.2717387676239014\n",
      "Iteration 12230 Loss: 1.4134056568145752\n",
      "Iteration 12231 Loss: 1.2090352773666382\n",
      "Iteration 12232 Loss: 1.3809813261032104\n",
      "Iteration 12233 Loss: 1.2529141902923584\n",
      "Iteration 12234 Loss: 1.0456260442733765\n",
      "Iteration 12235 Loss: 0.8378397822380066\n",
      "Iteration 12236 Loss: 1.1327383518218994\n",
      "Iteration 12237 Loss: 1.3788020610809326\n",
      "Iteration 12238 Loss: 1.208765983581543\n",
      "Iteration 12239 Loss: 1.118111491203308\n",
      "Iteration 12239 Loss: 1.1978219747543335\n",
      "Iteration 12240 Loss: 1.2743871212005615\n",
      "Iteration 12241 Loss: 1.1900875568389893\n",
      "Iteration 12242 Loss: 1.3071173429489136\n",
      "Iteration 12243 Loss: 1.5326906442642212\n",
      "Iteration 12244 Loss: 1.3422988653182983\n",
      "Iteration 12245 Loss: 1.2736108303070068\n",
      "Iteration 12246 Loss: 1.4443730115890503\n",
      "Iteration 12247 Loss: 1.2105592489242554\n",
      "Iteration 12248 Loss: 1.4011139869689941\n",
      "Iteration 12249 Loss: 1.711034893989563\n",
      "Iteration 12249 Loss: 1.3687273263931274\n",
      "Iteration 12250 Loss: 1.2282756567001343\n",
      "Iteration 12251 Loss: 1.5425916910171509\n",
      "Iteration 12252 Loss: 1.147274136543274\n",
      "Iteration 12253 Loss: 1.224470853805542\n",
      "Iteration 12254 Loss: 1.4072362184524536\n",
      "Iteration 12255 Loss: 0.7468583583831787\n",
      "Iteration 12256 Loss: 1.6700493097305298\n",
      "Iteration 12257 Loss: 1.0780384540557861\n",
      "Iteration 12258 Loss: 1.3571803569793701\n",
      "Iteration 12259 Loss: 1.3444008827209473\n",
      "Iteration 12259 Loss: 1.2746375799179077\n",
      "Iteration 12260 Loss: 1.202734112739563\n",
      "Iteration 12261 Loss: 1.5866981744766235\n",
      "Iteration 12262 Loss: 1.1124457120895386\n",
      "Iteration 12263 Loss: 1.1503665447235107\n",
      "Iteration 12264 Loss: 1.6973552703857422\n",
      "Iteration 12265 Loss: 1.6191188097000122\n",
      "Iteration 12266 Loss: 0.9930779933929443\n",
      "Iteration 12267 Loss: 1.3067985773086548\n",
      "Iteration 12268 Loss: 1.0015403032302856\n",
      "Iteration 12269 Loss: 1.3928332328796387\n",
      "Iteration 12269 Loss: 1.306296944618225\n",
      "Iteration 12270 Loss: 0.9851252436637878\n",
      "Iteration 12271 Loss: 1.3051114082336426\n",
      "Iteration 12272 Loss: 1.6291598081588745\n",
      "Iteration 12273 Loss: 0.8111318349838257\n",
      "Iteration 12274 Loss: 1.374428391456604\n",
      "Iteration 12275 Loss: 1.2976011037826538\n",
      "Iteration 12276 Loss: 0.9029613733291626\n",
      "Iteration 12277 Loss: 1.7110885381698608\n",
      "Iteration 12278 Loss: 0.8217407464981079\n",
      "Iteration 12279 Loss: 1.11873197555542\n",
      "Iteration 12279 Loss: 1.1957080364227295\n",
      "Iteration 12280 Loss: 1.203175663948059\n",
      "Iteration 12281 Loss: 1.4580082893371582\n",
      "Iteration 12282 Loss: 1.2851603031158447\n",
      "Iteration 12283 Loss: 1.4287887811660767\n",
      "Iteration 12284 Loss: 1.0881024599075317\n",
      "Iteration 12285 Loss: 1.386784315109253\n",
      "Iteration 12286 Loss: 1.6236472129821777\n",
      "Iteration 12287 Loss: 0.9297400712966919\n",
      "Iteration 12288 Loss: 1.4421842098236084\n",
      "Iteration 12289 Loss: 1.234864354133606\n",
      "Iteration 12289 Loss: 1.3080456256866455\n",
      "Iteration 12290 Loss: 0.9253865480422974\n",
      "Iteration 12291 Loss: 1.2963106632232666\n",
      "Iteration 12292 Loss: 1.4712074995040894\n",
      "Iteration 12293 Loss: 1.173707365989685\n",
      "Iteration 12294 Loss: 1.4193300008773804\n",
      "Iteration 12295 Loss: 1.3589826822280884\n",
      "Iteration 12296 Loss: 0.8927671313285828\n",
      "Iteration 12297 Loss: 0.9952910542488098\n",
      "Iteration 12298 Loss: 1.2349246740341187\n",
      "Iteration 12299 Loss: 0.9898625016212463\n",
      "Iteration 12299 Loss: 1.1757770776748657\n",
      "Iteration 12300 Loss: 1.0957655906677246\n",
      "Iteration 12301 Loss: 1.0268104076385498\n",
      "Iteration 12302 Loss: 1.2091296911239624\n",
      "Iteration 12303 Loss: 1.2557743787765503\n",
      "Iteration 12304 Loss: 1.245020866394043\n",
      "Iteration 12305 Loss: 1.4215201139450073\n",
      "Iteration 12306 Loss: 1.4948936700820923\n",
      "Iteration 12307 Loss: 1.7183512449264526\n",
      "Iteration 12308 Loss: 1.5036745071411133\n",
      "Iteration 12309 Loss: 1.4494303464889526\n",
      "Iteration 12309 Loss: 1.3420372009277344\n",
      "Iteration 12310 Loss: 1.4151426553726196\n",
      "Iteration 12311 Loss: 0.9909036159515381\n",
      "Iteration 12312 Loss: 1.3430367708206177\n",
      "Iteration 12313 Loss: 1.0728042125701904\n",
      "Iteration 12314 Loss: 1.5009841918945312\n",
      "Iteration 12315 Loss: 1.6695399284362793\n",
      "Iteration 12316 Loss: 1.2040692567825317\n",
      "Iteration 12317 Loss: 1.2054778337478638\n",
      "Iteration 12318 Loss: 1.5218772888183594\n",
      "Iteration 12319 Loss: 1.0312421321868896\n",
      "Iteration 12319 Loss: 1.2955076694488525\n",
      "Iteration 12320 Loss: 1.1402020454406738\n",
      "Iteration 12321 Loss: 1.291159749031067\n",
      "Iteration 12322 Loss: 1.2512273788452148\n",
      "Iteration 12323 Loss: 1.2900959253311157\n",
      "Iteration 12324 Loss: 1.1325956583023071\n",
      "Iteration 12325 Loss: 1.611838459968567\n",
      "Iteration 12326 Loss: 1.0197833776474\n",
      "Iteration 12327 Loss: 0.9280914068222046\n",
      "Iteration 12328 Loss: 1.5547218322753906\n",
      "Iteration 12329 Loss: 0.9748127460479736\n",
      "Iteration 12329 Loss: 1.2194527387619019\n",
      "Iteration 12330 Loss: 1.4299900531768799\n",
      "Iteration 12331 Loss: 1.049077033996582\n",
      "Iteration 12332 Loss: 1.2431972026824951\n",
      "Iteration 12333 Loss: 1.3091036081314087\n",
      "Iteration 12334 Loss: 1.0879846811294556\n",
      "Iteration 12335 Loss: 1.3215126991271973\n",
      "Iteration 12336 Loss: 0.8966126441955566\n",
      "Iteration 12337 Loss: 1.11827552318573\n",
      "Iteration 12338 Loss: 1.0770233869552612\n",
      "Iteration 12339 Loss: 1.4294790029525757\n",
      "Iteration 12339 Loss: 1.1962254047393799\n",
      "Iteration 12340 Loss: 1.5781199932098389\n",
      "Iteration 12341 Loss: 1.6019353866577148\n",
      "Iteration 12342 Loss: 1.341004729270935\n",
      "Iteration 12343 Loss: 1.1404521465301514\n",
      "Iteration 12344 Loss: 1.3268011808395386\n",
      "Iteration 12345 Loss: 1.2014050483703613\n",
      "Iteration 12346 Loss: 1.2301945686340332\n",
      "Iteration 12347 Loss: 0.9836001992225647\n",
      "Iteration 12348 Loss: 1.2991362810134888\n",
      "Iteration 12349 Loss: 1.1484888792037964\n",
      "Iteration 12349 Loss: 1.2851139307022095\n",
      "Iteration 12350 Loss: 0.9461053609848022\n",
      "Iteration 12351 Loss: 1.0661691427230835\n",
      "Iteration 12352 Loss: 1.3426433801651\n",
      "Iteration 12353 Loss: 0.8900362849235535\n",
      "Iteration 12354 Loss: 1.3648269176483154\n",
      "Iteration 12355 Loss: 1.3207192420959473\n",
      "Iteration 12356 Loss: 1.1594138145446777\n",
      "Iteration 12357 Loss: 1.1089214086532593\n",
      "Iteration 12358 Loss: 0.9070694446563721\n",
      "Iteration 12359 Loss: 1.2650319337844849\n",
      "Iteration 12359 Loss: 1.1370937824249268\n",
      "Iteration 12360 Loss: 1.516868233680725\n",
      "Iteration 12361 Loss: 1.2503782510757446\n",
      "Iteration 12362 Loss: 0.8114254474639893\n",
      "Iteration 12363 Loss: 1.0031293630599976\n",
      "Iteration 12364 Loss: 1.8403301239013672\n",
      "Iteration 12365 Loss: 1.0438761711120605\n",
      "Iteration 12366 Loss: 1.2452001571655273\n",
      "Iteration 12367 Loss: 1.128331184387207\n",
      "Iteration 12368 Loss: 1.44098699092865\n",
      "Iteration 12369 Loss: 1.541025996208191\n",
      "Iteration 12369 Loss: 1.2821552753448486\n",
      "Iteration 12370 Loss: 1.3509392738342285\n",
      "Iteration 12371 Loss: 0.9442815184593201\n",
      "Iteration 12372 Loss: 1.458269476890564\n",
      "Iteration 12373 Loss: 1.2579214572906494\n",
      "Iteration 12374 Loss: 1.1419072151184082\n",
      "Iteration 12375 Loss: 1.3901829719543457\n",
      "Iteration 12376 Loss: 1.1479902267456055\n",
      "Iteration 12377 Loss: 1.061346173286438\n",
      "Iteration 12378 Loss: 1.5285886526107788\n",
      "Iteration 12379 Loss: 1.4475752115249634\n",
      "Iteration 12379 Loss: 1.272900104522705\n",
      "Iteration 12380 Loss: 1.2568371295928955\n",
      "Iteration 12381 Loss: 0.9526829123497009\n",
      "Iteration 12382 Loss: 1.309025526046753\n",
      "Iteration 12383 Loss: 1.3475778102874756\n",
      "Iteration 12384 Loss: 1.284555196762085\n",
      "Iteration 12385 Loss: 1.0986814498901367\n",
      "Iteration 12386 Loss: 1.0451643466949463\n",
      "Iteration 12387 Loss: 1.6260703802108765\n",
      "Iteration 12388 Loss: 1.4174220561981201\n",
      "Iteration 12389 Loss: 1.488181710243225\n",
      "Iteration 12389 Loss: 1.282619833946228\n",
      "Iteration 12390 Loss: 1.0337773561477661\n",
      "Iteration 12391 Loss: 1.1318042278289795\n",
      "Iteration 12392 Loss: 1.177932620048523\n",
      "Iteration 12393 Loss: 1.5260009765625\n",
      "Iteration 12394 Loss: 1.5646079778671265\n",
      "Iteration 12395 Loss: 1.8195956945419312\n",
      "Iteration 12396 Loss: 1.220404863357544\n",
      "Iteration 12397 Loss: 1.5679177045822144\n",
      "Iteration 12398 Loss: 1.5795276165008545\n",
      "Iteration 12399 Loss: 1.7040629386901855\n",
      "Iteration 12399 Loss: 1.432563066482544\n",
      "Iteration 12400 Loss: 1.183955430984497\n",
      "Iteration 12401 Loss: 0.9561556577682495\n",
      "Iteration 12402 Loss: 1.2978779077529907\n",
      "Iteration 12403 Loss: 1.3101274967193604\n",
      "Iteration 12404 Loss: 0.9637090563774109\n",
      "Iteration 12405 Loss: 1.2916215658187866\n",
      "Iteration 12406 Loss: 1.2234936952590942\n",
      "Iteration 12407 Loss: 1.5107030868530273\n",
      "Iteration 12408 Loss: 1.003419041633606\n",
      "Iteration 12409 Loss: 1.282444715499878\n",
      "Iteration 12409 Loss: 1.2023507356643677\n",
      "Iteration 12410 Loss: 1.0942456722259521\n",
      "Iteration 12411 Loss: 1.2530713081359863\n",
      "Iteration 12412 Loss: 1.753936767578125\n",
      "Iteration 12413 Loss: 0.9544087052345276\n",
      "Iteration 12414 Loss: 1.1903032064437866\n",
      "Iteration 12415 Loss: 1.3974456787109375\n",
      "Iteration 12416 Loss: 1.340582251548767\n",
      "Iteration 12417 Loss: 1.5070241689682007\n",
      "Iteration 12418 Loss: 1.1270992755889893\n",
      "Iteration 12419 Loss: 1.4994310140609741\n",
      "Iteration 12419 Loss: 1.3117547035217285\n",
      "Iteration 12420 Loss: 1.0068082809448242\n",
      "Iteration 12421 Loss: 1.2891417741775513\n",
      "Iteration 12422 Loss: 1.3577313423156738\n",
      "Iteration 12423 Loss: 1.2569451332092285\n",
      "Iteration 12424 Loss: 1.4131332635879517\n",
      "Iteration 12425 Loss: 1.4989947080612183\n",
      "Iteration 12426 Loss: 1.2357428073883057\n",
      "Iteration 12427 Loss: 1.2199829816818237\n",
      "Iteration 12428 Loss: 1.3268738985061646\n",
      "Iteration 12429 Loss: 0.7062630653381348\n",
      "Iteration 12429 Loss: 1.2311617136001587\n",
      "Iteration 12430 Loss: 0.8573158383369446\n",
      "Iteration 12431 Loss: 0.8472023606300354\n",
      "Iteration 12432 Loss: 1.5910733938217163\n",
      "Iteration 12433 Loss: 1.1354581117630005\n",
      "Iteration 12434 Loss: 1.3224929571151733\n",
      "Iteration 12435 Loss: 1.912792682647705\n",
      "Iteration 12436 Loss: 1.3287729024887085\n",
      "Iteration 12437 Loss: 1.3916536569595337\n",
      "Iteration 12438 Loss: 1.6003570556640625\n",
      "Iteration 12439 Loss: 1.1548597812652588\n",
      "Iteration 12439 Loss: 1.3141978979110718\n",
      "Iteration 12440 Loss: 1.3874751329421997\n",
      "Iteration 12441 Loss: 1.0954532623291016\n",
      "Iteration 12442 Loss: 1.4519003629684448\n",
      "Iteration 12443 Loss: 1.51384699344635\n",
      "Iteration 12444 Loss: 1.2930574417114258\n",
      "Iteration 12445 Loss: 1.5315322875976562\n",
      "Iteration 12446 Loss: 0.9692859053611755\n",
      "Iteration 12447 Loss: 1.4570025205612183\n",
      "Iteration 12448 Loss: 1.3371243476867676\n",
      "Iteration 12449 Loss: 1.200871229171753\n",
      "Iteration 12449 Loss: 1.3237550258636475\n",
      "Iteration 12450 Loss: 1.39430832862854\n",
      "Iteration 12451 Loss: 1.0799705982208252\n",
      "Iteration 12452 Loss: 1.1289291381835938\n",
      "Iteration 12453 Loss: 1.0923882722854614\n",
      "Iteration 12454 Loss: 1.481564998626709\n",
      "Iteration 12455 Loss: 1.1500144004821777\n",
      "Iteration 12456 Loss: 1.4801864624023438\n",
      "Iteration 12457 Loss: 1.1657718420028687\n",
      "Iteration 12458 Loss: 0.9343915581703186\n",
      "Iteration 12459 Loss: 1.3489571809768677\n",
      "Iteration 12459 Loss: 1.225648283958435\n",
      "Iteration 12460 Loss: 1.2256356477737427\n",
      "Iteration 12461 Loss: 1.4567204713821411\n",
      "Iteration 12462 Loss: 0.8608464002609253\n",
      "Iteration 12463 Loss: 0.9381057620048523\n",
      "Iteration 12464 Loss: 1.683543086051941\n",
      "Iteration 12465 Loss: 1.569990634918213\n",
      "Iteration 12466 Loss: 1.1205437183380127\n",
      "Iteration 12467 Loss: 1.1698466539382935\n",
      "Iteration 12468 Loss: 1.3684179782867432\n",
      "Iteration 12469 Loss: 1.029066801071167\n",
      "Iteration 12469 Loss: 1.2422716617584229\n",
      "Iteration 12470 Loss: 1.225786566734314\n",
      "Iteration 12471 Loss: 1.3654520511627197\n",
      "Iteration 12472 Loss: 1.3587990999221802\n",
      "Iteration 12473 Loss: 1.2786521911621094\n",
      "Iteration 12474 Loss: 1.1044745445251465\n",
      "Iteration 12475 Loss: 1.1168720722198486\n",
      "Iteration 12476 Loss: 1.646404504776001\n",
      "Iteration 12477 Loss: 1.7733252048492432\n",
      "Iteration 12478 Loss: 1.1628235578536987\n",
      "Iteration 12479 Loss: 1.419007420539856\n",
      "Iteration 12479 Loss: 1.345159649848938\n",
      "Iteration 12480 Loss: 1.2538118362426758\n",
      "Iteration 12481 Loss: 1.0701110363006592\n",
      "Iteration 12482 Loss: 1.011379599571228\n",
      "Iteration 12483 Loss: 0.8847345113754272\n",
      "Iteration 12484 Loss: 0.9969661235809326\n",
      "Iteration 12485 Loss: 1.1443852186203003\n",
      "Iteration 12486 Loss: 0.8170279860496521\n",
      "Iteration 12487 Loss: 1.1518313884735107\n",
      "Iteration 12488 Loss: 1.0182793140411377\n",
      "Iteration 12489 Loss: 1.2296332120895386\n",
      "Iteration 12489 Loss: 1.0578161478042603\n",
      "Iteration 12490 Loss: 1.1065120697021484\n",
      "Iteration 12491 Loss: 1.315565824508667\n",
      "Iteration 12492 Loss: 1.28110933303833\n",
      "Iteration 12493 Loss: 0.7560293078422546\n",
      "Iteration 12494 Loss: 1.3249887228012085\n",
      "Iteration 12495 Loss: 1.5781419277191162\n",
      "Iteration 12496 Loss: 0.8768287897109985\n",
      "Iteration 12497 Loss: 1.300295114517212\n",
      "Iteration 12498 Loss: 1.143828272819519\n",
      "Iteration 12499 Loss: 1.1000384092330933\n",
      "Iteration 12499 Loss: 1.1783337593078613\n",
      "Iteration 12500 Loss: 1.0525323152542114\n",
      "Iteration 12501 Loss: 1.4975215196609497\n",
      "Iteration 12502 Loss: 1.2687815427780151\n",
      "Iteration 12503 Loss: 0.9696592092514038\n",
      "Iteration 12504 Loss: 1.1543675661087036\n",
      "Iteration 12505 Loss: 1.4689483642578125\n",
      "Iteration 12506 Loss: 1.5447349548339844\n",
      "Iteration 12507 Loss: 1.2853487730026245\n",
      "Iteration 12508 Loss: 0.8751659393310547\n",
      "Iteration 12509 Loss: 1.4464181661605835\n",
      "Iteration 12509 Loss: 1.256347894668579\n",
      "Iteration 12510 Loss: 0.9704570770263672\n",
      "Iteration 12511 Loss: 1.4041236639022827\n",
      "Iteration 12512 Loss: 1.8864721059799194\n",
      "Iteration 12513 Loss: 1.3077609539031982\n",
      "Iteration 12514 Loss: 1.157037615776062\n",
      "Iteration 12515 Loss: 1.6194844245910645\n",
      "Iteration 12516 Loss: 1.2344218492507935\n",
      "Iteration 12517 Loss: 1.1226617097854614\n",
      "Iteration 12518 Loss: 1.1401466131210327\n",
      "Iteration 12519 Loss: 1.4208219051361084\n",
      "Iteration 12519 Loss: 1.3263388872146606\n",
      "Iteration 12520 Loss: 0.8994529247283936\n",
      "Iteration 12521 Loss: 1.1827958822250366\n",
      "Iteration 12522 Loss: 0.9641523957252502\n",
      "Iteration 12523 Loss: 1.3629751205444336\n",
      "Iteration 12524 Loss: 1.1573195457458496\n",
      "Iteration 12525 Loss: 1.223809003829956\n",
      "Iteration 12526 Loss: 1.2518056631088257\n",
      "Iteration 12527 Loss: 1.5391130447387695\n",
      "Iteration 12528 Loss: 1.4500772953033447\n",
      "Iteration 12529 Loss: 0.9220303893089294\n",
      "Iteration 12529 Loss: 1.1953531503677368\n",
      "Iteration 12530 Loss: 1.4672434329986572\n",
      "Iteration 12531 Loss: 1.0081183910369873\n",
      "Iteration 12532 Loss: 1.1636714935302734\n",
      "Iteration 12533 Loss: 1.271005630493164\n",
      "Iteration 12534 Loss: 1.5478808879852295\n",
      "Iteration 12535 Loss: 0.9272687435150146\n",
      "Iteration 12536 Loss: 1.6346962451934814\n",
      "Iteration 12537 Loss: 1.1209266185760498\n",
      "Iteration 12538 Loss: 1.1708312034606934\n",
      "Iteration 12539 Loss: 1.0983308553695679\n",
      "Iteration 12539 Loss: 1.2409974336624146\n",
      "Iteration 12540 Loss: 1.1506489515304565\n",
      "Iteration 12541 Loss: 1.4061229228973389\n",
      "Iteration 12542 Loss: 1.4276843070983887\n",
      "Iteration 12543 Loss: 1.2822879552841187\n",
      "Iteration 12544 Loss: 1.1126978397369385\n",
      "Iteration 12545 Loss: 1.4317405223846436\n",
      "Iteration 12546 Loss: 1.252915620803833\n",
      "Iteration 12547 Loss: 0.7792688012123108\n",
      "Iteration 12548 Loss: 1.0813817977905273\n",
      "Iteration 12549 Loss: 1.4372594356536865\n",
      "Iteration 12549 Loss: 1.2362008094787598\n",
      "Iteration 12550 Loss: 1.1249604225158691\n",
      "Iteration 12551 Loss: 1.2666363716125488\n",
      "Iteration 12552 Loss: 1.3661668300628662\n",
      "Iteration 12553 Loss: 1.462735652923584\n",
      "Iteration 12554 Loss: 1.318098545074463\n",
      "Iteration 12555 Loss: 1.4764224290847778\n",
      "Iteration 12556 Loss: 1.277039647102356\n",
      "Iteration 12557 Loss: 1.6733860969543457\n",
      "Iteration 12558 Loss: 1.4745495319366455\n",
      "Iteration 12559 Loss: 1.1894471645355225\n",
      "Iteration 12559 Loss: 1.36294424533844\n",
      "Iteration 12560 Loss: 1.62442946434021\n",
      "Iteration 12561 Loss: 1.4208340644836426\n",
      "Iteration 12562 Loss: 1.2890170812606812\n",
      "Iteration 12563 Loss: 0.8766932487487793\n",
      "Iteration 12564 Loss: 0.9762598276138306\n",
      "Iteration 12565 Loss: 1.551819086074829\n",
      "Iteration 12566 Loss: 1.5209113359451294\n",
      "Iteration 12567 Loss: 1.3136998414993286\n",
      "Iteration 12568 Loss: 1.4671930074691772\n",
      "Iteration 12569 Loss: 1.4124648571014404\n",
      "Iteration 12569 Loss: 1.345332145690918\n",
      "Iteration 12570 Loss: 1.583536148071289\n",
      "Iteration 12571 Loss: 1.336371898651123\n",
      "Iteration 12572 Loss: 1.8315832614898682\n",
      "Iteration 12573 Loss: 1.0376766920089722\n",
      "Iteration 12574 Loss: 1.1190812587738037\n",
      "Iteration 12575 Loss: 1.297929286956787\n",
      "Iteration 12576 Loss: 1.2221736907958984\n",
      "Iteration 12577 Loss: 0.9943559169769287\n",
      "Iteration 12578 Loss: 1.3897660970687866\n",
      "Iteration 12579 Loss: 1.0212801694869995\n",
      "Iteration 12579 Loss: 1.2833755016326904\n",
      "Iteration 12580 Loss: 1.2369669675827026\n",
      "Iteration 12581 Loss: 1.6488255262374878\n",
      "Iteration 12582 Loss: 0.8321025371551514\n",
      "Iteration 12583 Loss: 1.2904644012451172\n",
      "Iteration 12584 Loss: 1.2546460628509521\n",
      "Iteration 12585 Loss: 1.196353554725647\n",
      "Iteration 12586 Loss: 1.1788743734359741\n",
      "Iteration 12587 Loss: 1.0981063842773438\n",
      "Iteration 12588 Loss: 1.7041629552841187\n",
      "Iteration 12589 Loss: 1.0411375761032104\n",
      "Iteration 12589 Loss: 1.2481640577316284\n",
      "Iteration 12590 Loss: 1.722211480140686\n",
      "Iteration 12591 Loss: 0.9702487587928772\n",
      "Iteration 12592 Loss: 1.3631280660629272\n",
      "Iteration 12593 Loss: 1.1237138509750366\n",
      "Iteration 12594 Loss: 1.1508190631866455\n",
      "Iteration 12595 Loss: 1.343929648399353\n",
      "Iteration 12596 Loss: 1.3524119853973389\n",
      "Iteration 12597 Loss: 1.2146223783493042\n",
      "Iteration 12598 Loss: 1.1781706809997559\n",
      "Iteration 12599 Loss: 0.8726768493652344\n",
      "Iteration 12599 Loss: 1.2291933298110962\n",
      "Iteration 12600 Loss: 0.9796754121780396\n",
      "Iteration 12601 Loss: 1.2527762651443481\n",
      "Iteration 12602 Loss: 1.3791368007659912\n",
      "Iteration 12603 Loss: 1.0300220251083374\n",
      "Iteration 12604 Loss: 1.1440589427947998\n",
      "Iteration 12605 Loss: 1.4652494192123413\n",
      "Iteration 12606 Loss: 0.6223416924476624\n",
      "Iteration 12607 Loss: 1.1493759155273438\n",
      "Iteration 12608 Loss: 1.302528977394104\n",
      "Iteration 12609 Loss: 1.092568039894104\n",
      "Iteration 12609 Loss: 1.1417733430862427\n",
      "Iteration 12610 Loss: 0.8479233384132385\n",
      "Iteration 12611 Loss: 1.1559957265853882\n",
      "Iteration 12612 Loss: 1.4707651138305664\n",
      "Iteration 12613 Loss: 1.112565279006958\n",
      "Iteration 12614 Loss: 1.435275673866272\n",
      "Iteration 12615 Loss: 1.0277940034866333\n",
      "Iteration 12616 Loss: 1.32943856716156\n",
      "Iteration 12617 Loss: 1.2735463380813599\n",
      "Iteration 12618 Loss: 1.3432841300964355\n",
      "Iteration 12619 Loss: 1.1441973447799683\n",
      "Iteration 12619 Loss: 1.2140785455703735\n",
      "Iteration 12620 Loss: 1.6391661167144775\n",
      "Iteration 12621 Loss: 1.0751789808273315\n",
      "Iteration 12622 Loss: 1.1672536134719849\n",
      "Iteration 12623 Loss: 0.827363908290863\n",
      "Iteration 12624 Loss: 1.670823335647583\n",
      "Iteration 12625 Loss: 1.4818775653839111\n",
      "Iteration 12626 Loss: 1.460262417793274\n",
      "Iteration 12627 Loss: 1.2634365558624268\n",
      "Iteration 12628 Loss: 1.670827031135559\n",
      "Iteration 12629 Loss: 1.3621711730957031\n",
      "Iteration 12629 Loss: 1.3618359565734863\n",
      "Iteration 12630 Loss: 1.1802220344543457\n",
      "Iteration 12631 Loss: 0.5972496271133423\n",
      "Iteration 12632 Loss: 0.9575125575065613\n",
      "Iteration 12633 Loss: 1.2586454153060913\n",
      "Iteration 12634 Loss: 1.008483648300171\n",
      "Iteration 12635 Loss: 1.4194362163543701\n",
      "Iteration 12636 Loss: 1.213726282119751\n",
      "Iteration 12637 Loss: 1.2029130458831787\n",
      "Iteration 12638 Loss: 1.4657667875289917\n",
      "Iteration 12639 Loss: 1.3678532838821411\n",
      "Iteration 12639 Loss: 1.1671808958053589\n",
      "Iteration 12640 Loss: 0.9278764724731445\n",
      "Iteration 12641 Loss: 1.3073447942733765\n",
      "Iteration 12642 Loss: 1.2491860389709473\n",
      "Iteration 12643 Loss: 1.658731460571289\n",
      "Iteration 12644 Loss: 1.7283390760421753\n",
      "Iteration 12645 Loss: 0.9032405614852905\n",
      "Iteration 12646 Loss: 1.441828966140747\n",
      "Iteration 12647 Loss: 1.1071285009384155\n",
      "Iteration 12648 Loss: 1.3713010549545288\n",
      "Iteration 12649 Loss: 1.2091563940048218\n",
      "Iteration 12649 Loss: 1.2904131412506104\n",
      "Iteration 12650 Loss: 1.3599733114242554\n",
      "Iteration 12651 Loss: 1.2782580852508545\n",
      "Iteration 12652 Loss: 1.3470515012741089\n",
      "Iteration 12653 Loss: 1.3320751190185547\n",
      "Iteration 12654 Loss: 1.2136809825897217\n",
      "Iteration 12655 Loss: 1.245282530784607\n",
      "Iteration 12656 Loss: 1.217209815979004\n",
      "Iteration 12657 Loss: 1.8144078254699707\n",
      "Iteration 12658 Loss: 1.4278037548065186\n",
      "Iteration 12659 Loss: 1.5968064069747925\n",
      "Iteration 12659 Loss: 1.3832550048828125\n",
      "Iteration 12660 Loss: 1.0853466987609863\n",
      "Iteration 12661 Loss: 1.1075429916381836\n",
      "Iteration 12662 Loss: 1.4301609992980957\n",
      "Iteration 12663 Loss: 0.8383188247680664\n",
      "Iteration 12664 Loss: 1.072171688079834\n",
      "Iteration 12665 Loss: 1.1012996435165405\n",
      "Iteration 12666 Loss: 1.2299401760101318\n",
      "Iteration 12667 Loss: 1.2516497373580933\n",
      "Iteration 12668 Loss: 1.2637357711791992\n",
      "Iteration 12669 Loss: 1.4421385526657104\n",
      "Iteration 12669 Loss: 1.1822305917739868\n",
      "Iteration 12670 Loss: 1.3492385149002075\n",
      "Iteration 12671 Loss: 0.8222348093986511\n",
      "Iteration 12672 Loss: 1.0432251691818237\n",
      "Iteration 12673 Loss: 1.5389271974563599\n",
      "Iteration 12674 Loss: 1.2884025573730469\n",
      "Iteration 12675 Loss: 1.2983918190002441\n",
      "Iteration 12676 Loss: 1.20000159740448\n",
      "Iteration 12677 Loss: 1.2508188486099243\n",
      "Iteration 12678 Loss: 0.8080506920814514\n",
      "Iteration 12679 Loss: 0.9076796770095825\n",
      "Iteration 12679 Loss: 1.150697112083435\n",
      "Iteration 12680 Loss: 1.3198723793029785\n",
      "Iteration 12681 Loss: 1.3250689506530762\n",
      "Iteration 12682 Loss: 1.1469454765319824\n",
      "Iteration 12683 Loss: 1.1616092920303345\n",
      "Iteration 12684 Loss: 1.5543142557144165\n",
      "Iteration 12685 Loss: 1.0051841735839844\n",
      "Iteration 12686 Loss: 1.187765121459961\n",
      "Iteration 12687 Loss: 1.4940372705459595\n",
      "Iteration 12688 Loss: 1.385633111000061\n",
      "Iteration 12689 Loss: 1.1571035385131836\n",
      "Iteration 12689 Loss: 1.2737534046173096\n",
      "Iteration 12690 Loss: 1.1900036334991455\n",
      "Iteration 12691 Loss: 1.39474618434906\n",
      "Iteration 12692 Loss: 0.8398573398590088\n",
      "Iteration 12693 Loss: 1.3826102018356323\n",
      "Iteration 12694 Loss: 1.12538480758667\n",
      "Iteration 12695 Loss: 1.1708303689956665\n",
      "Iteration 12696 Loss: 1.3549500703811646\n",
      "Iteration 12697 Loss: 0.8753247261047363\n",
      "Iteration 12698 Loss: 1.512134313583374\n",
      "Iteration 12699 Loss: 1.126204013824463\n",
      "Iteration 12699 Loss: 1.19720458984375\n",
      "Iteration 12700 Loss: 1.0186291933059692\n",
      "Iteration 12701 Loss: 1.1979296207427979\n",
      "Iteration 12702 Loss: 1.235034465789795\n",
      "Iteration 12703 Loss: 0.9346688389778137\n",
      "Iteration 12704 Loss: 1.1663265228271484\n",
      "Iteration 12705 Loss: 1.255738377571106\n",
      "Iteration 12706 Loss: 0.8983264565467834\n",
      "Iteration 12707 Loss: 0.9273358583450317\n",
      "Iteration 12708 Loss: 1.336448311805725\n",
      "Iteration 12709 Loss: 1.2070231437683105\n",
      "Iteration 12709 Loss: 1.117746114730835\n",
      "Iteration 12710 Loss: 1.2955690622329712\n",
      "Iteration 12711 Loss: 1.5059764385223389\n",
      "Iteration 12712 Loss: 1.0923629999160767\n",
      "Iteration 12713 Loss: 1.3582054376602173\n",
      "Iteration 12714 Loss: 1.5652759075164795\n",
      "Iteration 12715 Loss: 1.533142328262329\n",
      "Iteration 12716 Loss: 1.07715904712677\n",
      "Iteration 12717 Loss: 1.7881782054901123\n",
      "Iteration 12718 Loss: 1.2905393838882446\n",
      "Iteration 12719 Loss: 0.9344449639320374\n",
      "Iteration 12719 Loss: 1.344085454940796\n",
      "Iteration 12720 Loss: 1.2596361637115479\n",
      "Iteration 12721 Loss: 1.264796257019043\n",
      "Iteration 12722 Loss: 1.2210650444030762\n",
      "Iteration 12723 Loss: 1.4190787076950073\n",
      "Iteration 12724 Loss: 1.1036514043807983\n",
      "Iteration 12725 Loss: 1.0663511753082275\n",
      "Iteration 12726 Loss: 0.9544590711593628\n",
      "Iteration 12727 Loss: 0.8996899127960205\n",
      "Iteration 12728 Loss: 1.1257569789886475\n",
      "Iteration 12729 Loss: 1.456616997718811\n",
      "Iteration 12729 Loss: 1.177110195159912\n",
      "Iteration 12730 Loss: 0.9132049679756165\n",
      "Iteration 12731 Loss: 1.3160792589187622\n",
      "Iteration 12732 Loss: 1.379683256149292\n",
      "Iteration 12733 Loss: 1.0581812858581543\n",
      "Iteration 12734 Loss: 1.0662034749984741\n",
      "Iteration 12735 Loss: 1.218201756477356\n",
      "Iteration 12736 Loss: 0.8071924448013306\n",
      "Iteration 12737 Loss: 1.0052176713943481\n",
      "Iteration 12738 Loss: 1.4760539531707764\n",
      "Iteration 12739 Loss: 1.2373576164245605\n",
      "Iteration 12739 Loss: 1.1477376222610474\n",
      "Iteration 12740 Loss: 1.7467042207717896\n",
      "Iteration 12741 Loss: 1.3506215810775757\n",
      "Iteration 12742 Loss: 1.6262928247451782\n",
      "Iteration 12743 Loss: 1.2566176652908325\n",
      "Iteration 12744 Loss: 0.8141178488731384\n",
      "Iteration 12745 Loss: 1.1176457405090332\n",
      "Iteration 12746 Loss: 1.502431869506836\n",
      "Iteration 12747 Loss: 1.3417693376541138\n",
      "Iteration 12748 Loss: 1.1110948324203491\n",
      "Iteration 12749 Loss: 1.1451606750488281\n",
      "Iteration 12749 Loss: 1.3012455701828003\n",
      "Iteration 12750 Loss: 1.601676106452942\n",
      "Iteration 12751 Loss: 1.1813311576843262\n",
      "Iteration 12752 Loss: 1.2942484617233276\n",
      "Iteration 12753 Loss: 1.0501484870910645\n",
      "Iteration 12754 Loss: 1.44380784034729\n",
      "Iteration 12755 Loss: 0.6304625868797302\n",
      "Iteration 12756 Loss: 0.9452877640724182\n",
      "Iteration 12757 Loss: 1.0450942516326904\n",
      "Iteration 12758 Loss: 1.4054757356643677\n",
      "Iteration 12759 Loss: 1.1286447048187256\n",
      "Iteration 12759 Loss: 1.1726176738739014\n",
      "Iteration 12760 Loss: 1.3383461236953735\n",
      "Iteration 12761 Loss: 0.7455295324325562\n",
      "Iteration 12762 Loss: 1.043273687362671\n",
      "Iteration 12763 Loss: 1.435765027999878\n",
      "Iteration 12764 Loss: 0.9229319095611572\n",
      "Iteration 12765 Loss: 1.1454510688781738\n",
      "Iteration 12766 Loss: 0.9544660449028015\n",
      "Iteration 12767 Loss: 1.7003356218338013\n",
      "Iteration 12768 Loss: 1.3008860349655151\n",
      "Iteration 12769 Loss: 0.8862650990486145\n",
      "Iteration 12769 Loss: 1.147325038909912\n",
      "Iteration 12770 Loss: 1.1515171527862549\n",
      "Iteration 12771 Loss: 1.3657350540161133\n",
      "Iteration 12772 Loss: 1.292142391204834\n",
      "Iteration 12773 Loss: 1.2755794525146484\n",
      "Iteration 12774 Loss: 0.9760230779647827\n",
      "Iteration 12775 Loss: 1.0329890251159668\n",
      "Iteration 12776 Loss: 1.1567014455795288\n",
      "Iteration 12777 Loss: 1.0181760787963867\n",
      "Iteration 12778 Loss: 1.2049397230148315\n",
      "Iteration 12779 Loss: 1.2864338159561157\n",
      "Iteration 12779 Loss: 1.1760237216949463\n",
      "Iteration 12780 Loss: 1.2321058511734009\n",
      "Iteration 12781 Loss: 0.9222817420959473\n",
      "Iteration 12782 Loss: 0.9533242583274841\n",
      "Iteration 12783 Loss: 1.3822619915008545\n",
      "Iteration 12784 Loss: 0.8643602132797241\n",
      "Iteration 12785 Loss: 1.4511431455612183\n",
      "Iteration 12786 Loss: 1.385055422782898\n",
      "Iteration 12787 Loss: 1.313812255859375\n",
      "Iteration 12788 Loss: 1.1562726497650146\n",
      "Iteration 12789 Loss: 1.6838569641113281\n",
      "Iteration 12789 Loss: 1.2344474792480469\n",
      "Iteration 12790 Loss: 1.4678171873092651\n",
      "Iteration 12791 Loss: 1.3912713527679443\n",
      "Iteration 12792 Loss: 1.686148762702942\n",
      "Iteration 12793 Loss: 1.5085538625717163\n",
      "Iteration 12794 Loss: 1.0229718685150146\n",
      "Iteration 12795 Loss: 1.2151820659637451\n",
      "Iteration 12796 Loss: 1.476702332496643\n",
      "Iteration 12797 Loss: 1.6701501607894897\n",
      "Iteration 12798 Loss: 1.4253852367401123\n",
      "Iteration 12799 Loss: 1.8395334482192993\n",
      "Iteration 12799 Loss: 1.4703716039657593\n",
      "Iteration 12800 Loss: 1.0490810871124268\n",
      "Iteration 12801 Loss: 1.372087836265564\n",
      "Iteration 12802 Loss: 1.1347953081130981\n",
      "Iteration 12803 Loss: 1.3757740259170532\n",
      "Iteration 12804 Loss: 1.3697603940963745\n",
      "Iteration 12805 Loss: 0.8320472836494446\n",
      "Iteration 12806 Loss: 1.675728678703308\n",
      "Iteration 12807 Loss: 0.8570597171783447\n",
      "Iteration 12808 Loss: 1.1243348121643066\n",
      "Iteration 12809 Loss: 1.352558970451355\n",
      "Iteration 12809 Loss: 1.214322805404663\n",
      "Iteration 12810 Loss: 1.3639439344406128\n",
      "Iteration 12811 Loss: 1.5017731189727783\n",
      "Iteration 12812 Loss: 1.3860591650009155\n",
      "Iteration 12813 Loss: 1.4041919708251953\n",
      "Iteration 12814 Loss: 0.7632909417152405\n",
      "Iteration 12815 Loss: 1.1839325428009033\n",
      "Iteration 12816 Loss: 1.2688448429107666\n",
      "Iteration 12817 Loss: 1.1097912788391113\n",
      "Iteration 12818 Loss: 1.5839205980300903\n",
      "Iteration 12819 Loss: 1.0306915044784546\n",
      "Iteration 12819 Loss: 1.2596439123153687\n",
      "Iteration 12820 Loss: 1.6103971004486084\n",
      "Iteration 12821 Loss: 1.4032663106918335\n",
      "Iteration 12822 Loss: 1.2448946237564087\n",
      "Iteration 12823 Loss: 1.0825861692428589\n",
      "Iteration 12824 Loss: 1.4507029056549072\n",
      "Iteration 12825 Loss: 1.1620179414749146\n",
      "Iteration 12826 Loss: 1.1016367673873901\n",
      "Iteration 12827 Loss: 1.2190799713134766\n",
      "Iteration 12828 Loss: 1.4250919818878174\n",
      "Iteration 12829 Loss: 1.0932998657226562\n",
      "Iteration 12829 Loss: 1.2792973518371582\n",
      "Iteration 12830 Loss: 1.2856478691101074\n",
      "Iteration 12831 Loss: 1.256640076637268\n",
      "Iteration 12832 Loss: 1.631007194519043\n",
      "Iteration 12833 Loss: 1.1087430715560913\n",
      "Iteration 12834 Loss: 0.9625210165977478\n",
      "Iteration 12835 Loss: 1.0970803499221802\n",
      "Iteration 12836 Loss: 1.0777819156646729\n",
      "Iteration 12837 Loss: 1.2970385551452637\n",
      "Iteration 12838 Loss: 1.1080251932144165\n",
      "Iteration 12839 Loss: 1.2664436101913452\n",
      "Iteration 12839 Loss: 1.2090928554534912\n",
      "Iteration 12840 Loss: 1.0425989627838135\n",
      "Iteration 12841 Loss: 1.1950931549072266\n",
      "Iteration 12842 Loss: 1.3930740356445312\n",
      "Iteration 12843 Loss: 1.3631247282028198\n",
      "Iteration 12844 Loss: 1.6484912633895874\n",
      "Iteration 12845 Loss: 1.6496251821517944\n",
      "Iteration 12846 Loss: 0.9505547881126404\n",
      "Iteration 12847 Loss: 1.1328309774398804\n",
      "Iteration 12848 Loss: 1.4701385498046875\n",
      "Iteration 12849 Loss: 1.4324336051940918\n",
      "Iteration 12849 Loss: 1.327796459197998\n",
      "Iteration 12850 Loss: 1.4979208707809448\n",
      "Iteration 12851 Loss: 1.6156588792800903\n",
      "Iteration 12852 Loss: 1.0817928314208984\n",
      "Iteration 12853 Loss: 1.5973761081695557\n",
      "Iteration 12854 Loss: 1.2440547943115234\n",
      "Iteration 12855 Loss: 1.4436742067337036\n",
      "Iteration 12856 Loss: 1.2119057178497314\n",
      "Iteration 12857 Loss: 1.529971718788147\n",
      "Iteration 12858 Loss: 1.5167428255081177\n",
      "Iteration 12859 Loss: 1.3893470764160156\n",
      "Iteration 12859 Loss: 1.4128444194793701\n",
      "Iteration 12860 Loss: 1.446337103843689\n",
      "Iteration 12861 Loss: 1.1473132371902466\n",
      "Iteration 12862 Loss: 1.2713055610656738\n",
      "Iteration 12863 Loss: 1.1244717836380005\n",
      "Iteration 12864 Loss: 0.9354107975959778\n",
      "Iteration 12865 Loss: 1.0757124423980713\n",
      "Iteration 12866 Loss: 1.2057355642318726\n",
      "Iteration 12867 Loss: 1.1108719110488892\n",
      "Iteration 12868 Loss: 1.3559982776641846\n",
      "Iteration 12869 Loss: 1.4018635749816895\n",
      "Iteration 12869 Loss: 1.207502007484436\n",
      "Iteration 12870 Loss: 1.5687036514282227\n",
      "Iteration 12871 Loss: 1.396000623703003\n",
      "Iteration 12872 Loss: 1.3555089235305786\n",
      "Iteration 12873 Loss: 1.3013478517532349\n",
      "Iteration 12874 Loss: 1.3211051225662231\n",
      "Iteration 12875 Loss: 1.3540244102478027\n",
      "Iteration 12876 Loss: 1.0767868757247925\n",
      "Iteration 12877 Loss: 0.9775415658950806\n",
      "Iteration 12878 Loss: 1.1594301462173462\n",
      "Iteration 12879 Loss: 1.0934704542160034\n",
      "Iteration 12879 Loss: 1.2603919506072998\n",
      "Iteration 12880 Loss: 1.4524881839752197\n",
      "Iteration 12881 Loss: 1.0414183139801025\n",
      "Iteration 12882 Loss: 1.3219215869903564\n",
      "Iteration 12883 Loss: 1.0138297080993652\n",
      "Iteration 12884 Loss: 1.3825898170471191\n",
      "Iteration 12885 Loss: 0.8820246458053589\n",
      "Iteration 12886 Loss: 1.3488988876342773\n",
      "Iteration 12887 Loss: 1.344783067703247\n",
      "Iteration 12888 Loss: 1.4196019172668457\n",
      "Iteration 12889 Loss: 1.3360592126846313\n",
      "Iteration 12889 Loss: 1.2543613910675049\n",
      "Iteration 12890 Loss: 0.8485928177833557\n",
      "Iteration 12891 Loss: 1.427621841430664\n",
      "Iteration 12892 Loss: 1.1416088342666626\n",
      "Iteration 12893 Loss: 1.0083003044128418\n",
      "Iteration 12894 Loss: 1.0589196681976318\n",
      "Iteration 12895 Loss: 1.0695501565933228\n",
      "Iteration 12896 Loss: 1.4199590682983398\n",
      "Iteration 12897 Loss: 1.3262208700180054\n",
      "Iteration 12898 Loss: 0.9816455841064453\n",
      "Iteration 12899 Loss: 1.7729021310806274\n",
      "Iteration 12899 Loss: 1.2055320739746094\n",
      "Iteration 12900 Loss: 1.3764010667800903\n",
      "Iteration 12901 Loss: 1.1703695058822632\n",
      "Iteration 12902 Loss: 1.3054907321929932\n",
      "Iteration 12903 Loss: 1.1945487260818481\n",
      "Iteration 12904 Loss: 1.3736687898635864\n",
      "Iteration 12905 Loss: 0.956343412399292\n",
      "Iteration 12906 Loss: 1.1165292263031006\n",
      "Iteration 12907 Loss: 1.6005570888519287\n",
      "Iteration 12908 Loss: 1.3173547983169556\n",
      "Iteration 12909 Loss: 1.05987548828125\n",
      "Iteration 12909 Loss: 1.2471139430999756\n",
      "Iteration 12910 Loss: 1.294763445854187\n",
      "Iteration 12911 Loss: 0.9397315382957458\n",
      "Iteration 12912 Loss: 1.1028385162353516\n",
      "Iteration 12913 Loss: 1.2524535655975342\n",
      "Iteration 12914 Loss: 1.4666070938110352\n",
      "Iteration 12915 Loss: 1.345360279083252\n",
      "Iteration 12916 Loss: 1.3604860305786133\n",
      "Iteration 12917 Loss: 1.3989771604537964\n",
      "Iteration 12918 Loss: 1.3180837631225586\n",
      "Iteration 12919 Loss: 1.596168041229248\n",
      "Iteration 12919 Loss: 1.307546854019165\n",
      "Iteration 12920 Loss: 1.9302600622177124\n",
      "Iteration 12921 Loss: 0.9549602270126343\n",
      "Iteration 12922 Loss: 0.890974760055542\n",
      "Iteration 12923 Loss: 1.3891544342041016\n",
      "Iteration 12924 Loss: 1.306401252746582\n",
      "Iteration 12925 Loss: 1.120627522468567\n",
      "Iteration 12926 Loss: 1.2186399698257446\n",
      "Iteration 12927 Loss: 1.1651630401611328\n",
      "Iteration 12928 Loss: 1.1634191274642944\n",
      "Iteration 12929 Loss: 1.3793578147888184\n",
      "Iteration 12929 Loss: 1.2518959045410156\n",
      "Iteration 12930 Loss: 1.0280404090881348\n",
      "Iteration 12931 Loss: 0.9692535996437073\n",
      "Iteration 12932 Loss: 0.949124276638031\n",
      "Iteration 12933 Loss: 1.605286955833435\n",
      "Iteration 12934 Loss: 1.1069378852844238\n",
      "Iteration 12935 Loss: 1.2399665117263794\n",
      "Iteration 12936 Loss: 1.3285378217697144\n",
      "Iteration 12937 Loss: 1.2293586730957031\n",
      "Iteration 12938 Loss: 1.5230621099472046\n",
      "Iteration 12939 Loss: 1.2717176675796509\n",
      "Iteration 12939 Loss: 1.2251285314559937\n",
      "Iteration 12940 Loss: 1.2241929769515991\n",
      "Iteration 12941 Loss: 1.4715827703475952\n",
      "Iteration 12942 Loss: 1.3276053667068481\n",
      "Iteration 12943 Loss: 1.145376205444336\n",
      "Iteration 12944 Loss: 1.250868558883667\n",
      "Iteration 12945 Loss: 1.524050235748291\n",
      "Iteration 12946 Loss: 1.5707213878631592\n",
      "Iteration 12947 Loss: 1.1085753440856934\n",
      "Iteration 12948 Loss: 1.3143569231033325\n",
      "Iteration 12949 Loss: 0.9722166061401367\n",
      "Iteration 12949 Loss: 1.29095458984375\n",
      "Iteration 12950 Loss: 1.1902430057525635\n",
      "Iteration 12951 Loss: 1.450361967086792\n",
      "Iteration 12952 Loss: 1.0772581100463867\n",
      "Iteration 12953 Loss: 1.3776276111602783\n",
      "Iteration 12954 Loss: 1.6941516399383545\n",
      "Iteration 12955 Loss: 1.33492112159729\n",
      "Iteration 12956 Loss: 0.9766162037849426\n",
      "Iteration 12957 Loss: 1.2614846229553223\n",
      "Iteration 12958 Loss: 1.250618577003479\n",
      "Iteration 12959 Loss: 1.3745026588439941\n",
      "Iteration 12959 Loss: 1.2987785339355469\n",
      "Iteration 12960 Loss: 1.164953351020813\n",
      "Iteration 12961 Loss: 1.5236483812332153\n",
      "Iteration 12962 Loss: 1.3037625551223755\n",
      "Iteration 12963 Loss: 1.5013355016708374\n",
      "Iteration 12964 Loss: 1.2696533203125\n",
      "Iteration 12965 Loss: 1.4239673614501953\n",
      "Iteration 12966 Loss: 1.162328839302063\n",
      "Iteration 12967 Loss: 1.5731327533721924\n",
      "Iteration 12968 Loss: 1.4997061491012573\n",
      "Iteration 12969 Loss: 1.3684779405593872\n",
      "Iteration 12969 Loss: 1.379096508026123\n",
      "Iteration 12970 Loss: 0.9817658066749573\n",
      "Iteration 12971 Loss: 1.3196533918380737\n",
      "Iteration 12972 Loss: 1.4617857933044434\n",
      "Iteration 12973 Loss: 1.0686571598052979\n",
      "Iteration 12974 Loss: 0.9429899454116821\n",
      "Iteration 12975 Loss: 1.0983327627182007\n",
      "Iteration 12976 Loss: 1.2238566875457764\n",
      "Iteration 12977 Loss: 1.0351903438568115\n",
      "Iteration 12978 Loss: 1.4528840780258179\n",
      "Iteration 12979 Loss: 1.0066792964935303\n",
      "Iteration 12979 Loss: 1.1591795682907104\n",
      "Iteration 12980 Loss: 1.5407416820526123\n",
      "Iteration 12981 Loss: 1.1026160717010498\n",
      "Iteration 12982 Loss: 1.2636281251907349\n",
      "Iteration 12983 Loss: 1.0433672666549683\n",
      "Iteration 12984 Loss: 0.9099578857421875\n",
      "Iteration 12985 Loss: 0.9480094909667969\n",
      "Iteration 12986 Loss: 1.6758298873901367\n",
      "Iteration 12987 Loss: 1.3709684610366821\n",
      "Iteration 12988 Loss: 0.9351937174797058\n",
      "Iteration 12989 Loss: 0.9143336415290833\n",
      "Iteration 12989 Loss: 1.1704647541046143\n",
      "Iteration 12990 Loss: 1.3448466062545776\n",
      "Iteration 12991 Loss: 0.9638675451278687\n",
      "Iteration 12992 Loss: 1.1689116954803467\n",
      "Iteration 12993 Loss: 1.5909260511398315\n",
      "Iteration 12994 Loss: 1.0278881788253784\n",
      "Iteration 12995 Loss: 1.4985907077789307\n",
      "Iteration 12996 Loss: 1.1909009218215942\n",
      "Iteration 12997 Loss: 1.4775887727737427\n",
      "Iteration 12998 Loss: 1.412593960762024\n",
      "Iteration 12999 Loss: 1.078284740447998\n",
      "Iteration 12999 Loss: 1.275439977645874\n",
      "Iteration 13000 Loss: 1.1383649110794067\n",
      "Iteration 13001 Loss: 1.3208171129226685\n",
      "Iteration 13002 Loss: 1.5090789794921875\n",
      "Iteration 13003 Loss: 1.2156037092208862\n",
      "Iteration 13004 Loss: 1.3163825273513794\n",
      "Iteration 13005 Loss: 0.9437286257743835\n",
      "Iteration 13006 Loss: 0.9861092567443848\n",
      "Iteration 13007 Loss: 1.3426145315170288\n",
      "Iteration 13008 Loss: 1.2053539752960205\n",
      "Iteration 13009 Loss: 1.4042835235595703\n",
      "Iteration 13009 Loss: 1.2382336854934692\n",
      "Iteration 13010 Loss: 1.3184115886688232\n",
      "Iteration 13011 Loss: 1.2302495241165161\n",
      "Iteration 13012 Loss: 1.1043713092803955\n",
      "Iteration 13013 Loss: 1.1134486198425293\n",
      "Iteration 13014 Loss: 0.9909147620201111\n",
      "Iteration 13015 Loss: 0.9816867113113403\n",
      "Iteration 13016 Loss: 1.6444450616836548\n",
      "Iteration 13017 Loss: 1.5311328172683716\n",
      "Iteration 13018 Loss: 1.4392695426940918\n",
      "Iteration 13019 Loss: 1.253448724746704\n",
      "Iteration 13019 Loss: 1.2607378959655762\n",
      "Iteration 13020 Loss: 1.2172483205795288\n",
      "Iteration 13021 Loss: 1.328320026397705\n",
      "Iteration 13022 Loss: 1.0636796951293945\n",
      "Iteration 13023 Loss: 1.661551833152771\n",
      "Iteration 13024 Loss: 1.1734871864318848\n",
      "Iteration 13025 Loss: 1.4740016460418701\n",
      "Iteration 13026 Loss: 1.6756536960601807\n",
      "Iteration 13027 Loss: 1.4158357381820679\n",
      "Iteration 13028 Loss: 0.8395692706108093\n",
      "Iteration 13029 Loss: 1.6383395195007324\n",
      "Iteration 13029 Loss: 1.3487685918807983\n",
      "Iteration 13030 Loss: 1.887827754020691\n",
      "Iteration 13031 Loss: 1.338080883026123\n",
      "Iteration 13032 Loss: 1.3123937845230103\n",
      "Iteration 13033 Loss: 1.098509430885315\n",
      "Iteration 13034 Loss: 1.4962620735168457\n",
      "Iteration 13035 Loss: 1.3130526542663574\n",
      "Iteration 13036 Loss: 1.3893134593963623\n",
      "Iteration 13037 Loss: 1.2604482173919678\n",
      "Iteration 13038 Loss: 1.1575968265533447\n",
      "Iteration 13039 Loss: 1.2608212232589722\n",
      "Iteration 13039 Loss: 1.3514307737350464\n",
      "Iteration 13040 Loss: 1.4794096946716309\n",
      "Iteration 13041 Loss: 1.0481246709823608\n",
      "Iteration 13042 Loss: 1.4007779359817505\n",
      "Iteration 13043 Loss: 0.9914834499359131\n",
      "Iteration 13044 Loss: 1.141037106513977\n",
      "Iteration 13045 Loss: 1.3618261814117432\n",
      "Iteration 13046 Loss: 1.486997127532959\n",
      "Iteration 13047 Loss: 1.0450682640075684\n",
      "Iteration 13048 Loss: 1.3741793632507324\n",
      "Iteration 13049 Loss: 1.3021881580352783\n",
      "Iteration 13049 Loss: 1.2631092071533203\n",
      "Iteration 13050 Loss: 1.2999342679977417\n",
      "Iteration 13051 Loss: 1.1756900548934937\n",
      "Iteration 13052 Loss: 1.4392346143722534\n",
      "Iteration 13053 Loss: 1.3000816106796265\n",
      "Iteration 13054 Loss: 1.1275323629379272\n",
      "Iteration 13055 Loss: 1.1008691787719727\n",
      "Iteration 13056 Loss: 1.0877597332000732\n",
      "Iteration 13057 Loss: 1.0271657705307007\n",
      "Iteration 13058 Loss: 1.4023089408874512\n",
      "Iteration 13059 Loss: 1.557214379310608\n",
      "Iteration 13059 Loss: 1.2517790794372559\n",
      "Iteration 13060 Loss: 1.5554550886154175\n",
      "Iteration 13061 Loss: 1.1341490745544434\n",
      "Iteration 13062 Loss: 1.2372453212738037\n",
      "Iteration 13063 Loss: 1.3973920345306396\n",
      "Iteration 13064 Loss: 1.3665258884429932\n",
      "Iteration 13065 Loss: 1.1675606966018677\n",
      "Iteration 13066 Loss: 0.9574508666992188\n",
      "Iteration 13067 Loss: 1.3874146938323975\n",
      "Iteration 13068 Loss: 1.2508955001831055\n",
      "Iteration 13069 Loss: 1.53987455368042\n",
      "Iteration 13069 Loss: 1.2993963956832886\n",
      "Iteration 13070 Loss: 1.0171233415603638\n",
      "Iteration 13071 Loss: 1.1907131671905518\n",
      "Iteration 13072 Loss: 1.130873203277588\n",
      "Iteration 13073 Loss: 1.0244401693344116\n",
      "Iteration 13074 Loss: 1.432692289352417\n",
      "Iteration 13075 Loss: 1.4454706907272339\n",
      "Iteration 13076 Loss: 1.5787756443023682\n",
      "Iteration 13077 Loss: 1.5453808307647705\n",
      "Iteration 13078 Loss: 1.2312263250350952\n",
      "Iteration 13079 Loss: 1.4157053232192993\n",
      "Iteration 13079 Loss: 1.301240086555481\n",
      "Iteration 13080 Loss: 1.360757827758789\n",
      "Iteration 13081 Loss: 1.3778499364852905\n",
      "Iteration 13082 Loss: 1.432815432548523\n",
      "Iteration 13083 Loss: 1.2663707733154297\n",
      "Iteration 13084 Loss: 1.2018816471099854\n",
      "Iteration 13085 Loss: 1.204553484916687\n",
      "Iteration 13086 Loss: 0.9890074729919434\n",
      "Iteration 13087 Loss: 0.9413628578186035\n",
      "Iteration 13088 Loss: 0.9912616014480591\n",
      "Iteration 13089 Loss: 0.9244244694709778\n",
      "Iteration 13089 Loss: 1.1690285205841064\n",
      "Iteration 13090 Loss: 1.4328641891479492\n",
      "Iteration 13091 Loss: 0.7759512662887573\n",
      "Iteration 13092 Loss: 1.4013471603393555\n",
      "Iteration 13093 Loss: 1.3118212223052979\n",
      "Iteration 13094 Loss: 1.3557668924331665\n",
      "Iteration 13095 Loss: 1.4353865385055542\n",
      "Iteration 13096 Loss: 1.4811482429504395\n",
      "Iteration 13097 Loss: 1.4475067853927612\n",
      "Iteration 13098 Loss: 1.2513177394866943\n",
      "Iteration 13099 Loss: 1.1574110984802246\n",
      "Iteration 13099 Loss: 1.3050520420074463\n",
      "Iteration 13100 Loss: 1.2106356620788574\n",
      "Iteration 13101 Loss: 1.0608196258544922\n",
      "Iteration 13102 Loss: 1.0507471561431885\n",
      "Iteration 13103 Loss: 0.9807195067405701\n",
      "Iteration 13104 Loss: 1.3307750225067139\n",
      "Iteration 13105 Loss: 1.1504302024841309\n",
      "Iteration 13106 Loss: 1.6103159189224243\n",
      "Iteration 13107 Loss: 1.3721426725387573\n",
      "Iteration 13108 Loss: 1.81536865234375\n",
      "Iteration 13109 Loss: 1.031773328781128\n",
      "Iteration 13109 Loss: 1.2613728046417236\n",
      "Iteration 13110 Loss: 0.8394008874893188\n",
      "Iteration 13111 Loss: 0.947211742401123\n",
      "Iteration 13112 Loss: 1.2191828489303589\n",
      "Iteration 13113 Loss: 1.4735056161880493\n",
      "Iteration 13114 Loss: 1.4186944961547852\n",
      "Iteration 13115 Loss: 1.2583880424499512\n",
      "Iteration 13116 Loss: 0.8407944440841675\n",
      "Iteration 13117 Loss: 1.1301281452178955\n",
      "Iteration 13118 Loss: 1.0553046464920044\n",
      "Iteration 13119 Loss: 1.1859115362167358\n",
      "Iteration 13119 Loss: 1.1368522644042969\n",
      "Iteration 13120 Loss: 1.4119991064071655\n",
      "Iteration 13121 Loss: 1.3707184791564941\n",
      "Iteration 13122 Loss: 1.2283684015274048\n",
      "Iteration 13123 Loss: 1.4682986736297607\n",
      "Iteration 13124 Loss: 1.4365546703338623\n",
      "Iteration 13125 Loss: 1.0107238292694092\n",
      "Iteration 13126 Loss: 1.332902193069458\n",
      "Iteration 13127 Loss: 1.0569216012954712\n",
      "Iteration 13128 Loss: 1.170113444328308\n",
      "Iteration 13129 Loss: 0.9472599625587463\n",
      "Iteration 13129 Loss: 1.2433860301971436\n",
      "Iteration 13130 Loss: 1.3601560592651367\n",
      "Iteration 13131 Loss: 1.349787712097168\n",
      "Iteration 13132 Loss: 1.4247236251831055\n",
      "Iteration 13133 Loss: 1.063240647315979\n",
      "Iteration 13134 Loss: 1.0630192756652832\n",
      "Iteration 13135 Loss: 1.082290768623352\n",
      "Iteration 13136 Loss: 0.8564644455909729\n",
      "Iteration 13137 Loss: 1.109478235244751\n",
      "Iteration 13138 Loss: 1.4964799880981445\n",
      "Iteration 13139 Loss: 1.444899320602417\n",
      "Iteration 13139 Loss: 1.2250540256500244\n",
      "Iteration 13140 Loss: 1.3112592697143555\n",
      "Iteration 13141 Loss: 1.387182354927063\n",
      "Iteration 13142 Loss: 1.2210414409637451\n",
      "Iteration 13143 Loss: 1.4550915956497192\n",
      "Iteration 13144 Loss: 0.8049938082695007\n",
      "Iteration 13145 Loss: 1.2772891521453857\n",
      "Iteration 13146 Loss: 1.2307215929031372\n",
      "Iteration 13147 Loss: 1.3690388202667236\n",
      "Iteration 13148 Loss: 1.3229280710220337\n",
      "Iteration 13149 Loss: 1.4458026885986328\n",
      "Iteration 13149 Loss: 1.2825348377227783\n",
      "Iteration 13150 Loss: 1.3154152631759644\n",
      "Iteration 13151 Loss: 0.9870187640190125\n",
      "Iteration 13152 Loss: 1.5931156873703003\n",
      "Iteration 13153 Loss: 1.158107042312622\n",
      "Iteration 13154 Loss: 1.1638500690460205\n",
      "Iteration 13155 Loss: 1.5994181632995605\n",
      "Iteration 13156 Loss: 0.9272701144218445\n",
      "Iteration 13157 Loss: 1.3174883127212524\n",
      "Iteration 13158 Loss: 1.3259927034378052\n",
      "Iteration 13159 Loss: 1.139323353767395\n",
      "Iteration 13159 Loss: 1.252699851989746\n",
      "Iteration 13160 Loss: 1.3285197019577026\n",
      "Iteration 13161 Loss: 1.2617173194885254\n",
      "Iteration 13162 Loss: 1.1139436960220337\n",
      "Iteration 13163 Loss: 1.4955064058303833\n",
      "Iteration 13164 Loss: 1.2279468774795532\n",
      "Iteration 13165 Loss: 1.2426494359970093\n",
      "Iteration 13166 Loss: 1.2682610750198364\n",
      "Iteration 13167 Loss: 1.150230884552002\n",
      "Iteration 13168 Loss: 1.2201977968215942\n",
      "Iteration 13169 Loss: 1.1695377826690674\n",
      "Iteration 13169 Loss: 1.247851014137268\n",
      "Iteration 13170 Loss: 1.8031374216079712\n",
      "Iteration 13171 Loss: 1.0552098751068115\n",
      "Iteration 13172 Loss: 1.1411651372909546\n",
      "Iteration 13173 Loss: 1.055347204208374\n",
      "Iteration 13174 Loss: 1.1528593301773071\n",
      "Iteration 13175 Loss: 1.286543607711792\n",
      "Iteration 13176 Loss: 1.4148226976394653\n",
      "Iteration 13177 Loss: 0.8691304922103882\n",
      "Iteration 13178 Loss: 1.4060461521148682\n",
      "Iteration 13179 Loss: 1.2638537883758545\n",
      "Iteration 13179 Loss: 1.2448115348815918\n",
      "Iteration 13180 Loss: 1.443577766418457\n",
      "Iteration 13181 Loss: 1.3072566986083984\n",
      "Iteration 13182 Loss: 1.2722587585449219\n",
      "Iteration 13183 Loss: 1.1532330513000488\n",
      "Iteration 13184 Loss: 1.1265318393707275\n",
      "Iteration 13185 Loss: 1.4985138177871704\n",
      "Iteration 13186 Loss: 1.6270017623901367\n",
      "Iteration 13187 Loss: 1.2732136249542236\n",
      "Iteration 13188 Loss: 1.369843602180481\n",
      "Iteration 13189 Loss: 1.2641133069992065\n",
      "Iteration 13189 Loss: 1.3335545063018799\n",
      "Iteration 13190 Loss: 1.1905608177185059\n",
      "Iteration 13191 Loss: 1.216415524482727\n",
      "Iteration 13192 Loss: 1.4153801202774048\n",
      "Iteration 13193 Loss: 1.3014452457427979\n",
      "Iteration 13194 Loss: 1.457319974899292\n",
      "Iteration 13195 Loss: 1.2891089916229248\n",
      "Iteration 13196 Loss: 1.0001792907714844\n",
      "Iteration 13197 Loss: 1.2936065196990967\n",
      "Iteration 13198 Loss: 1.5819405317306519\n",
      "Iteration 13199 Loss: 1.051753282546997\n",
      "Iteration 13199 Loss: 1.279771089553833\n",
      "Iteration 13200 Loss: 0.9859758019447327\n",
      "Iteration 13201 Loss: 1.5686166286468506\n",
      "Iteration 13202 Loss: 1.097944974899292\n",
      "Iteration 13203 Loss: 1.1190928220748901\n",
      "Iteration 13204 Loss: 1.563717246055603\n",
      "Iteration 13205 Loss: 1.425851583480835\n",
      "Iteration 13206 Loss: 1.2870758771896362\n",
      "Iteration 13207 Loss: 1.4024920463562012\n",
      "Iteration 13208 Loss: 1.3125666379928589\n",
      "Iteration 13209 Loss: 0.9479617476463318\n",
      "Iteration 13209 Loss: 1.2711296081542969\n",
      "Iteration 13210 Loss: 1.0147554874420166\n",
      "Iteration 13211 Loss: 1.4227452278137207\n",
      "Iteration 13212 Loss: 0.9445205330848694\n",
      "Iteration 13213 Loss: 1.3079780340194702\n",
      "Iteration 13214 Loss: 0.7387080788612366\n",
      "Iteration 13215 Loss: 1.136154294013977\n",
      "Iteration 13216 Loss: 0.7390539646148682\n",
      "Iteration 13217 Loss: 1.3188451528549194\n",
      "Iteration 13218 Loss: 1.315338373184204\n",
      "Iteration 13219 Loss: 1.4217908382415771\n",
      "Iteration 13219 Loss: 1.1359889507293701\n",
      "Iteration 13220 Loss: 1.5428001880645752\n",
      "Iteration 13221 Loss: 1.3185817003250122\n",
      "Iteration 13222 Loss: 1.3575351238250732\n",
      "Iteration 13223 Loss: 1.0941451787948608\n",
      "Iteration 13224 Loss: 1.1440125703811646\n",
      "Iteration 13225 Loss: 0.9578152894973755\n",
      "Iteration 13226 Loss: 1.140123724937439\n",
      "Iteration 13227 Loss: 1.5628039836883545\n",
      "Iteration 13228 Loss: 1.177828311920166\n",
      "Iteration 13229 Loss: 0.9294731616973877\n",
      "Iteration 13229 Loss: 1.2225120067596436\n",
      "Iteration 13230 Loss: 0.8268923759460449\n",
      "Iteration 13231 Loss: 1.831424593925476\n",
      "Iteration 13232 Loss: 1.1582189798355103\n",
      "Iteration 13233 Loss: 1.5065864324569702\n",
      "Iteration 13234 Loss: 1.043212652206421\n",
      "Iteration 13235 Loss: 1.6862117052078247\n",
      "Iteration 13236 Loss: 1.332177758216858\n",
      "Iteration 13237 Loss: 0.9991665482521057\n",
      "Iteration 13238 Loss: 1.05414617061615\n",
      "Iteration 13239 Loss: 1.282631754875183\n",
      "Iteration 13239 Loss: 1.2720668315887451\n",
      "Iteration 13240 Loss: 1.0830950736999512\n",
      "Iteration 13241 Loss: 0.9066213369369507\n",
      "Iteration 13242 Loss: 1.4223965406417847\n",
      "Iteration 13243 Loss: 1.232853889465332\n",
      "Iteration 13244 Loss: 1.3808066844940186\n",
      "Iteration 13245 Loss: 1.0861968994140625\n",
      "Iteration 13246 Loss: 1.3399922847747803\n",
      "Iteration 13247 Loss: 1.1335898637771606\n",
      "Iteration 13248 Loss: 1.3574358224868774\n",
      "Iteration 13249 Loss: 0.6487712264060974\n",
      "Iteration 13249 Loss: 1.159175992012024\n",
      "Iteration 13250 Loss: 1.475684642791748\n",
      "Iteration 13251 Loss: 1.3465205430984497\n",
      "Iteration 13252 Loss: 1.1140992641448975\n",
      "Iteration 13253 Loss: 1.7648963928222656\n",
      "Iteration 13254 Loss: 1.2487105131149292\n",
      "Iteration 13255 Loss: 1.2532707452774048\n",
      "Iteration 13256 Loss: 1.6666953563690186\n",
      "Iteration 13257 Loss: 1.3517754077911377\n",
      "Iteration 13258 Loss: 1.3611679077148438\n",
      "Iteration 13259 Loss: 0.9734638929367065\n",
      "Iteration 13259 Loss: 1.355628490447998\n",
      "Iteration 13260 Loss: 1.570259928703308\n",
      "Iteration 13261 Loss: 1.520221471786499\n",
      "Iteration 13262 Loss: 1.322392225265503\n",
      "Iteration 13263 Loss: 1.1754720211029053\n",
      "Iteration 13264 Loss: 1.2920674085617065\n",
      "Iteration 13265 Loss: 1.235667109489441\n",
      "Iteration 13266 Loss: 1.2091195583343506\n",
      "Iteration 13267 Loss: 1.0875017642974854\n",
      "Iteration 13268 Loss: 1.245397686958313\n",
      "Iteration 13269 Loss: 1.2699036598205566\n",
      "Iteration 13269 Loss: 1.2928003072738647\n",
      "Iteration 13270 Loss: 0.9412263631820679\n",
      "Iteration 13271 Loss: 1.2168419361114502\n",
      "Iteration 13272 Loss: 1.066510796546936\n",
      "Iteration 13273 Loss: 1.130957841873169\n",
      "Iteration 13274 Loss: 1.7133572101593018\n",
      "Iteration 13275 Loss: 1.6955788135528564\n",
      "Iteration 13276 Loss: 1.6862386465072632\n",
      "Iteration 13277 Loss: 1.3714826107025146\n",
      "Iteration 13278 Loss: 1.0424405336380005\n",
      "Iteration 13279 Loss: 1.2407703399658203\n",
      "Iteration 13279 Loss: 1.3105404376983643\n",
      "Iteration 13280 Loss: 0.8645657300949097\n",
      "Iteration 13281 Loss: 1.3103430271148682\n",
      "Iteration 13282 Loss: 1.1054056882858276\n",
      "Iteration 13283 Loss: 1.2710496187210083\n",
      "Iteration 13284 Loss: 0.9326175451278687\n",
      "Iteration 13285 Loss: 1.226029634475708\n",
      "Iteration 13286 Loss: 1.1647939682006836\n",
      "Iteration 13287 Loss: 1.2247868776321411\n",
      "Iteration 13288 Loss: 1.3792574405670166\n",
      "Iteration 13289 Loss: 1.5333600044250488\n",
      "Iteration 13289 Loss: 1.2012208700180054\n",
      "Iteration 13290 Loss: 1.220117449760437\n",
      "Iteration 13291 Loss: 1.3551794290542603\n",
      "Iteration 13292 Loss: 1.0388357639312744\n",
      "Iteration 13293 Loss: 1.109991192817688\n",
      "Iteration 13294 Loss: 1.1015506982803345\n",
      "Iteration 13295 Loss: 0.9734548926353455\n",
      "Iteration 13296 Loss: 1.634709119796753\n",
      "Iteration 13297 Loss: 1.3018256425857544\n",
      "Iteration 13298 Loss: 0.8613088726997375\n",
      "Iteration 13299 Loss: 1.0823755264282227\n",
      "Iteration 13299 Loss: 1.167934775352478\n",
      "Iteration 13300 Loss: 1.3052482604980469\n",
      "Iteration 13301 Loss: 1.3379138708114624\n",
      "Iteration 13302 Loss: 0.9919713139533997\n",
      "Iteration 13303 Loss: 0.9087362289428711\n",
      "Iteration 13304 Loss: 1.2259443998336792\n",
      "Iteration 13305 Loss: 1.1522586345672607\n",
      "Iteration 13306 Loss: 1.356521487236023\n",
      "Iteration 13307 Loss: 1.3846725225448608\n",
      "Iteration 13308 Loss: 1.2414034605026245\n",
      "Iteration 13309 Loss: 1.2548706531524658\n",
      "Iteration 13309 Loss: 1.215954065322876\n",
      "Iteration 13310 Loss: 1.396089792251587\n",
      "Iteration 13311 Loss: 1.3593100309371948\n",
      "Iteration 13312 Loss: 0.992732584476471\n",
      "Iteration 13313 Loss: 1.1306002140045166\n",
      "Iteration 13314 Loss: 1.3576654195785522\n",
      "Iteration 13315 Loss: 1.0036194324493408\n",
      "Iteration 13316 Loss: 1.503679633140564\n",
      "Iteration 13317 Loss: 1.5636773109436035\n",
      "Iteration 13318 Loss: 1.3160524368286133\n",
      "Iteration 13319 Loss: 0.8593704104423523\n",
      "Iteration 13319 Loss: 1.2482795715332031\n",
      "Iteration 13320 Loss: 1.1860899925231934\n",
      "Iteration 13321 Loss: 1.567553997039795\n",
      "Iteration 13322 Loss: 0.9323933720588684\n",
      "Iteration 13323 Loss: 1.54091215133667\n",
      "Iteration 13324 Loss: 1.2711718082427979\n",
      "Iteration 13325 Loss: 1.3340802192687988\n",
      "Iteration 13326 Loss: 1.0661064386367798\n",
      "Iteration 13327 Loss: 1.5108895301818848\n",
      "Iteration 13328 Loss: 0.9523767232894897\n",
      "Iteration 13329 Loss: 1.7550827264785767\n",
      "Iteration 13329 Loss: 1.3116657733917236\n",
      "Iteration 13330 Loss: 0.8747478127479553\n",
      "Iteration 13331 Loss: 1.4386364221572876\n",
      "Iteration 13332 Loss: 1.1492670774459839\n",
      "Iteration 13333 Loss: 1.0995745658874512\n",
      "Iteration 13334 Loss: 1.6240676641464233\n",
      "Iteration 13335 Loss: 1.3529338836669922\n",
      "Iteration 13336 Loss: 0.8987157344818115\n",
      "Iteration 13337 Loss: 1.3465447425842285\n",
      "Iteration 13338 Loss: 1.0279357433319092\n",
      "Iteration 13339 Loss: 1.1002253293991089\n",
      "Iteration 13339 Loss: 1.1912648677825928\n",
      "Iteration 13340 Loss: 1.3265987634658813\n",
      "Iteration 13341 Loss: 1.307006597518921\n",
      "Iteration 13342 Loss: 1.5629347562789917\n",
      "Iteration 13343 Loss: 1.4846071004867554\n",
      "Iteration 13344 Loss: 0.7683912515640259\n",
      "Iteration 13345 Loss: 1.344663143157959\n",
      "Iteration 13346 Loss: 1.5058013200759888\n",
      "Iteration 13347 Loss: 1.4623292684555054\n",
      "Iteration 13348 Loss: 1.401310920715332\n",
      "Iteration 13349 Loss: 1.0988603830337524\n",
      "Iteration 13349 Loss: 1.3262503147125244\n",
      "Iteration 13350 Loss: 1.9909967184066772\n",
      "Iteration 13351 Loss: 0.9917033910751343\n",
      "Iteration 13352 Loss: 1.6487107276916504\n",
      "Iteration 13353 Loss: 0.9842148423194885\n",
      "Iteration 13354 Loss: 1.3090243339538574\n",
      "Iteration 13355 Loss: 1.2696785926818848\n",
      "Iteration 13356 Loss: 1.394175410270691\n",
      "Iteration 13357 Loss: 1.013552188873291\n",
      "Iteration 13358 Loss: 1.6380459070205688\n",
      "Iteration 13359 Loss: 1.4040775299072266\n",
      "Iteration 13359 Loss: 1.3644180297851562\n",
      "Iteration 13360 Loss: 1.2923119068145752\n",
      "Iteration 13361 Loss: 1.1196237802505493\n",
      "Iteration 13362 Loss: 1.168059229850769\n",
      "Iteration 13363 Loss: 0.9310253262519836\n",
      "Iteration 13364 Loss: 1.0318502187728882\n",
      "Iteration 13365 Loss: 1.6752527952194214\n",
      "Iteration 13366 Loss: 1.0185755491256714\n",
      "Iteration 13367 Loss: 1.0700751543045044\n",
      "Iteration 13368 Loss: 1.4812531471252441\n",
      "Iteration 13369 Loss: 1.1648797988891602\n",
      "Iteration 13369 Loss: 1.1952908039093018\n",
      "Iteration 13370 Loss: 0.9890527725219727\n",
      "Iteration 13371 Loss: 1.5578747987747192\n",
      "Iteration 13372 Loss: 1.034171223640442\n",
      "Iteration 13373 Loss: 1.1375553607940674\n",
      "Iteration 13374 Loss: 1.7110787630081177\n",
      "Iteration 13375 Loss: 1.0299403667449951\n",
      "Iteration 13376 Loss: 1.0025535821914673\n",
      "Iteration 13377 Loss: 1.2541035413742065\n",
      "Iteration 13378 Loss: 1.0735188722610474\n",
      "Iteration 13379 Loss: 1.2771354913711548\n",
      "Iteration 13379 Loss: 1.2066985368728638\n",
      "Iteration 13380 Loss: 1.2042758464813232\n",
      "Iteration 13381 Loss: 0.9322669506072998\n",
      "Iteration 13382 Loss: 0.823011577129364\n",
      "Iteration 13383 Loss: 1.3362483978271484\n",
      "Iteration 13384 Loss: 1.1678874492645264\n",
      "Iteration 13385 Loss: 1.244098424911499\n",
      "Iteration 13386 Loss: 1.1085991859436035\n",
      "Iteration 13387 Loss: 1.1443573236465454\n",
      "Iteration 13388 Loss: 1.2759250402450562\n",
      "Iteration 13389 Loss: 1.0916879177093506\n",
      "Iteration 13389 Loss: 1.1328357458114624\n",
      "Iteration 13390 Loss: 1.339827060699463\n",
      "Iteration 13391 Loss: 1.4009515047073364\n",
      "Iteration 13392 Loss: 1.0910311937332153\n",
      "Iteration 13393 Loss: 0.7761949300765991\n",
      "Iteration 13394 Loss: 1.6313663721084595\n",
      "Iteration 13395 Loss: 1.0355178117752075\n",
      "Iteration 13396 Loss: 1.1560512781143188\n",
      "Iteration 13397 Loss: 1.0610312223434448\n",
      "Iteration 13398 Loss: 1.7064096927642822\n",
      "Iteration 13399 Loss: 1.3425159454345703\n",
      "Iteration 13399 Loss: 1.2540897130966187\n",
      "Iteration 13400 Loss: 1.3875778913497925\n",
      "Iteration 13401 Loss: 1.3856616020202637\n",
      "Iteration 13402 Loss: 1.4433531761169434\n",
      "Iteration 13403 Loss: 1.2611163854599\n",
      "Iteration 13404 Loss: 1.1871223449707031\n",
      "Iteration 13405 Loss: 1.6996212005615234\n",
      "Iteration 13406 Loss: 1.0966628789901733\n",
      "Iteration 13407 Loss: 1.25503671169281\n",
      "Iteration 13408 Loss: 1.4309169054031372\n",
      "Iteration 13409 Loss: 1.1336369514465332\n",
      "Iteration 13409 Loss: 1.3280705213546753\n",
      "Iteration 13410 Loss: 1.2664856910705566\n",
      "Iteration 13411 Loss: 1.4926973581314087\n",
      "Iteration 13412 Loss: 1.0741990804672241\n",
      "Iteration 13413 Loss: 1.7786359786987305\n",
      "Iteration 13414 Loss: 1.2187217473983765\n",
      "Iteration 13415 Loss: 1.2492926120758057\n",
      "Iteration 13416 Loss: 0.9930664300918579\n",
      "Iteration 13417 Loss: 0.963405191898346\n",
      "Iteration 13418 Loss: 1.1304571628570557\n",
      "Iteration 13419 Loss: 1.2346853017807007\n",
      "Iteration 13419 Loss: 1.2401646375656128\n",
      "Iteration 13420 Loss: 0.8007479906082153\n",
      "Iteration 13421 Loss: 1.1399662494659424\n",
      "Iteration 13422 Loss: 1.4410085678100586\n",
      "Iteration 13423 Loss: 1.7083431482315063\n",
      "Iteration 13424 Loss: 1.29300057888031\n",
      "Iteration 13425 Loss: 1.0415323972702026\n",
      "Iteration 13426 Loss: 0.9939460158348083\n",
      "Iteration 13427 Loss: 1.0085989236831665\n",
      "Iteration 13428 Loss: 1.1631313562393188\n",
      "Iteration 13429 Loss: 1.042720913887024\n",
      "Iteration 13429 Loss: 1.1632996797561646\n",
      "Iteration 13430 Loss: 1.3984397649765015\n",
      "Iteration 13431 Loss: 1.424833059310913\n",
      "Iteration 13432 Loss: 1.2350270748138428\n",
      "Iteration 13433 Loss: 1.4415693283081055\n",
      "Iteration 13434 Loss: 1.380966305732727\n",
      "Iteration 13435 Loss: 1.125704050064087\n",
      "Iteration 13436 Loss: 0.9861452579498291\n",
      "Iteration 13437 Loss: 1.1593176126480103\n",
      "Iteration 13438 Loss: 1.053674578666687\n",
      "Iteration 13439 Loss: 1.7088160514831543\n",
      "Iteration 13439 Loss: 1.2914493083953857\n",
      "Iteration 13440 Loss: 0.9897136688232422\n",
      "Iteration 13441 Loss: 1.131471037864685\n",
      "Iteration 13442 Loss: 0.8807752132415771\n",
      "Iteration 13443 Loss: 1.3869478702545166\n",
      "Iteration 13444 Loss: 1.6569474935531616\n",
      "Iteration 13445 Loss: 1.0349334478378296\n",
      "Iteration 13446 Loss: 1.0966402292251587\n",
      "Iteration 13447 Loss: 1.3693054914474487\n",
      "Iteration 13448 Loss: 1.167036771774292\n",
      "Iteration 13449 Loss: 1.2174084186553955\n",
      "Iteration 13449 Loss: 1.1931178569793701\n",
      "Iteration 13450 Loss: 1.2558107376098633\n",
      "Iteration 13451 Loss: 0.7710291147232056\n",
      "Iteration 13452 Loss: 1.4926488399505615\n",
      "Iteration 13453 Loss: 1.6631108522415161\n",
      "Iteration 13454 Loss: 1.1295158863067627\n",
      "Iteration 13455 Loss: 1.4409596920013428\n",
      "Iteration 13456 Loss: 1.0195280313491821\n",
      "Iteration 13457 Loss: 1.1488208770751953\n",
      "Iteration 13458 Loss: 1.2553712129592896\n",
      "Iteration 13459 Loss: 1.5341241359710693\n",
      "Iteration 13459 Loss: 1.2710919380187988\n",
      "Iteration 13460 Loss: 1.1942542791366577\n",
      "Iteration 13461 Loss: 1.2875210046768188\n",
      "Iteration 13462 Loss: 1.24079167842865\n",
      "Iteration 13463 Loss: 1.3549352884292603\n",
      "Iteration 13464 Loss: 1.3060940504074097\n",
      "Iteration 13465 Loss: 1.1821643114089966\n",
      "Iteration 13466 Loss: 0.8414233326911926\n",
      "Iteration 13467 Loss: 1.7552896738052368\n",
      "Iteration 13468 Loss: 1.4226722717285156\n",
      "Iteration 13469 Loss: 1.3155299425125122\n",
      "Iteration 13469 Loss: 1.2900675535202026\n",
      "Iteration 13470 Loss: 1.2787812948226929\n",
      "Iteration 13471 Loss: 1.3367249965667725\n",
      "Iteration 13472 Loss: 1.3578646183013916\n",
      "Iteration 13473 Loss: 1.2495919466018677\n",
      "Iteration 13474 Loss: 1.6610859632492065\n",
      "Iteration 13475 Loss: 1.2923732995986938\n",
      "Iteration 13476 Loss: 1.4453206062316895\n",
      "Iteration 13477 Loss: 1.4708398580551147\n",
      "Iteration 13478 Loss: 1.3824213743209839\n",
      "Iteration 13479 Loss: 1.3123981952667236\n",
      "Iteration 13479 Loss: 1.3787401914596558\n",
      "Iteration 13480 Loss: 1.0864343643188477\n",
      "Iteration 13481 Loss: 1.641567349433899\n",
      "Iteration 13482 Loss: 1.3551183938980103\n",
      "Iteration 13483 Loss: 1.3135144710540771\n",
      "Iteration 13484 Loss: 1.652434229850769\n",
      "Iteration 13485 Loss: 0.9060152769088745\n",
      "Iteration 13486 Loss: 1.2015979290008545\n",
      "Iteration 13487 Loss: 0.8223338723182678\n",
      "Iteration 13488 Loss: 1.3211772441864014\n",
      "Iteration 13489 Loss: 1.0103681087493896\n",
      "Iteration 13489 Loss: 1.2310562133789062\n",
      "Iteration 13490 Loss: 1.0785655975341797\n",
      "Iteration 13491 Loss: 1.1973432302474976\n",
      "Iteration 13492 Loss: 1.3321821689605713\n",
      "Iteration 13493 Loss: 1.493181824684143\n",
      "Iteration 13494 Loss: 1.9788037538528442\n",
      "Iteration 13495 Loss: 1.498191475868225\n",
      "Iteration 13496 Loss: 1.770194411277771\n",
      "Iteration 13497 Loss: 1.4473272562026978\n",
      "Iteration 13498 Loss: 1.5124564170837402\n",
      "Iteration 13499 Loss: 1.1616761684417725\n",
      "Iteration 13499 Loss: 1.44699227809906\n",
      "Iteration 13500 Loss: 1.1968296766281128\n",
      "Iteration 13501 Loss: 1.1869163513183594\n",
      "Iteration 13502 Loss: 1.4604607820510864\n",
      "Iteration 13503 Loss: 1.4707046747207642\n",
      "Iteration 13504 Loss: 1.3868986368179321\n",
      "Iteration 13505 Loss: 1.741575002670288\n",
      "Iteration 13506 Loss: 1.183964729309082\n",
      "Iteration 13507 Loss: 1.2287020683288574\n",
      "Iteration 13508 Loss: 1.1602610349655151\n",
      "Iteration 13509 Loss: 1.5678541660308838\n",
      "Iteration 13509 Loss: 1.3584167957305908\n",
      "Iteration 13510 Loss: 1.3641782999038696\n",
      "Iteration 13511 Loss: 1.540806770324707\n",
      "Iteration 13512 Loss: 1.4732272624969482\n",
      "Iteration 13513 Loss: 1.2902392148971558\n",
      "Iteration 13514 Loss: 1.0374199151992798\n",
      "Iteration 13515 Loss: 1.0599043369293213\n",
      "Iteration 13516 Loss: 1.337917685508728\n",
      "Iteration 13517 Loss: 1.452185034751892\n",
      "Iteration 13518 Loss: 0.9842932224273682\n",
      "Iteration 13519 Loss: 1.6616896390914917\n",
      "Iteration 13519 Loss: 1.3201860189437866\n",
      "Iteration 13520 Loss: 1.3342291116714478\n",
      "Iteration 13521 Loss: 0.9603086709976196\n",
      "Iteration 13522 Loss: 1.1394691467285156\n",
      "Iteration 13523 Loss: 1.1076983213424683\n",
      "Iteration 13524 Loss: 1.1640121936798096\n",
      "Iteration 13525 Loss: 1.1116547584533691\n",
      "Iteration 13526 Loss: 1.3948354721069336\n",
      "Iteration 13527 Loss: 1.4801762104034424\n",
      "Iteration 13528 Loss: 1.261719822883606\n",
      "Iteration 13529 Loss: 1.108555793762207\n",
      "Iteration 13529 Loss: 1.2062658071517944\n",
      "Iteration 13530 Loss: 1.0675122737884521\n",
      "Iteration 13531 Loss: 1.315354585647583\n",
      "Iteration 13532 Loss: 1.7708245515823364\n",
      "Iteration 13533 Loss: 1.3467615842819214\n",
      "Iteration 13534 Loss: 0.9053360819816589\n",
      "Iteration 13535 Loss: 1.0797481536865234\n",
      "Iteration 13536 Loss: 1.218985676765442\n",
      "Iteration 13537 Loss: 1.2144205570220947\n",
      "Iteration 13538 Loss: 1.302507996559143\n",
      "Iteration 13539 Loss: 1.202834963798523\n",
      "Iteration 13539 Loss: 1.2424285411834717\n",
      "Iteration 13540 Loss: 0.9181901812553406\n",
      "Iteration 13541 Loss: 1.372436761856079\n",
      "Iteration 13542 Loss: 0.7422279715538025\n",
      "Iteration 13543 Loss: 1.0861313343048096\n",
      "Iteration 13544 Loss: 1.1946055889129639\n",
      "Iteration 13545 Loss: 0.9409499168395996\n",
      "Iteration 13546 Loss: 1.5221302509307861\n",
      "Iteration 13547 Loss: 1.0201393365859985\n",
      "Iteration 13548 Loss: 1.0679829120635986\n",
      "Iteration 13549 Loss: 1.4491376876831055\n",
      "Iteration 13549 Loss: 1.131393313407898\n",
      "Iteration 13550 Loss: 1.7482964992523193\n",
      "Iteration 13551 Loss: 1.1629542112350464\n",
      "Iteration 13552 Loss: 1.1605448722839355\n",
      "Iteration 13553 Loss: 1.5962129831314087\n",
      "Iteration 13554 Loss: 0.8752771615982056\n",
      "Iteration 13555 Loss: 1.447299838066101\n",
      "Iteration 13556 Loss: 1.2309359312057495\n",
      "Iteration 13557 Loss: 1.048532485961914\n",
      "Iteration 13558 Loss: 1.531592607498169\n",
      "Iteration 13559 Loss: 1.3364003896713257\n",
      "Iteration 13559 Loss: 1.3138047456741333\n",
      "Iteration 13560 Loss: 1.2986209392547607\n",
      "Iteration 13561 Loss: 1.3578548431396484\n",
      "Iteration 13562 Loss: 1.2953037023544312\n",
      "Iteration 13563 Loss: 1.2832390069961548\n",
      "Iteration 13564 Loss: 1.1712149381637573\n",
      "Iteration 13565 Loss: 1.3195290565490723\n",
      "Iteration 13566 Loss: 1.1503866910934448\n",
      "Iteration 13567 Loss: 0.9335867166519165\n",
      "Iteration 13568 Loss: 1.7031078338623047\n",
      "Iteration 13569 Loss: 1.5716965198516846\n",
      "Iteration 13569 Loss: 1.3084540367126465\n",
      "Iteration 13570 Loss: 0.9549776315689087\n",
      "Iteration 13571 Loss: 1.060089349746704\n",
      "Iteration 13572 Loss: 1.4728914499282837\n",
      "Iteration 13573 Loss: 0.8655557036399841\n",
      "Iteration 13574 Loss: 1.2657395601272583\n",
      "Iteration 13575 Loss: 1.1362457275390625\n",
      "Iteration 13576 Loss: 0.8150929808616638\n",
      "Iteration 13577 Loss: 1.6355847120285034\n",
      "Iteration 13578 Loss: 1.2436165809631348\n",
      "Iteration 13579 Loss: 1.3056787252426147\n",
      "Iteration 13579 Loss: 1.1755473613739014\n",
      "Iteration 13580 Loss: 1.2857900857925415\n",
      "Iteration 13581 Loss: 1.4726372957229614\n",
      "Iteration 13582 Loss: 1.5058872699737549\n",
      "Iteration 13583 Loss: 1.0341118574142456\n",
      "Iteration 13584 Loss: 1.280915379524231\n",
      "Iteration 13585 Loss: 1.4818360805511475\n",
      "Iteration 13586 Loss: 1.0206725597381592\n",
      "Iteration 13587 Loss: 1.1509097814559937\n",
      "Iteration 13588 Loss: 1.2130169868469238\n",
      "Iteration 13589 Loss: 1.0900304317474365\n",
      "Iteration 13589 Loss: 1.2535808086395264\n",
      "Iteration 13590 Loss: 1.5920377969741821\n",
      "Iteration 13591 Loss: 1.1995011568069458\n",
      "Iteration 13592 Loss: 1.1119762659072876\n",
      "Iteration 13593 Loss: 1.2298485040664673\n",
      "Iteration 13594 Loss: 1.322075366973877\n",
      "Iteration 13595 Loss: 1.290697455406189\n",
      "Iteration 13596 Loss: 1.3055238723754883\n",
      "Iteration 13597 Loss: 1.1525778770446777\n",
      "Iteration 13598 Loss: 0.8843904733657837\n",
      "Iteration 13599 Loss: 1.0065271854400635\n",
      "Iteration 13599 Loss: 1.2095155715942383\n",
      "Iteration 13600 Loss: 1.1514530181884766\n",
      "Iteration 13601 Loss: 1.0973286628723145\n",
      "Iteration 13602 Loss: 1.2317794561386108\n",
      "Iteration 13603 Loss: 1.0125060081481934\n",
      "Iteration 13604 Loss: 0.910815417766571\n",
      "Iteration 13605 Loss: 1.394761323928833\n",
      "Iteration 13606 Loss: 0.7997440099716187\n",
      "Iteration 13607 Loss: 1.2784873247146606\n",
      "Iteration 13608 Loss: 1.7341231107711792\n",
      "Iteration 13609 Loss: 0.9161010384559631\n",
      "Iteration 13609 Loss: 1.1527098417282104\n",
      "Iteration 13610 Loss: 0.9948040843009949\n",
      "Iteration 13611 Loss: 1.1418299674987793\n",
      "Iteration 13612 Loss: 1.631483793258667\n",
      "Iteration 13613 Loss: 1.6385102272033691\n",
      "Iteration 13614 Loss: 1.191129446029663\n",
      "Iteration 13615 Loss: 1.3136563301086426\n",
      "Iteration 13616 Loss: 0.9705679416656494\n",
      "Iteration 13617 Loss: 1.4195977449417114\n",
      "Iteration 13618 Loss: 1.0608291625976562\n",
      "Iteration 13619 Loss: 1.48436439037323\n",
      "Iteration 13619 Loss: 1.284677267074585\n",
      "Iteration 13620 Loss: 1.3058912754058838\n",
      "Iteration 13621 Loss: 1.4921436309814453\n",
      "Iteration 13622 Loss: 1.302526593208313\n",
      "Iteration 13623 Loss: 0.9818078279495239\n",
      "Iteration 13624 Loss: 1.0312906503677368\n",
      "Iteration 13625 Loss: 1.0536680221557617\n",
      "Iteration 13626 Loss: 1.4921690225601196\n",
      "Iteration 13627 Loss: 1.1773537397384644\n",
      "Iteration 13628 Loss: 1.2573463916778564\n",
      "Iteration 13629 Loss: 1.542930245399475\n",
      "Iteration 13629 Loss: 1.263712763786316\n",
      "Iteration 13630 Loss: 1.1315412521362305\n",
      "Iteration 13631 Loss: 1.53902006149292\n",
      "Iteration 13632 Loss: 1.2242915630340576\n",
      "Iteration 13633 Loss: 1.2354998588562012\n",
      "Iteration 13634 Loss: 0.9097708463668823\n",
      "Iteration 13635 Loss: 1.410467505455017\n",
      "Iteration 13636 Loss: 1.165976643562317\n",
      "Iteration 13637 Loss: 1.4124919176101685\n",
      "Iteration 13638 Loss: 1.1839240789413452\n",
      "Iteration 13639 Loss: 1.200386643409729\n",
      "Iteration 13639 Loss: 1.2413369417190552\n",
      "Iteration 13640 Loss: 1.079421043395996\n",
      "Iteration 13641 Loss: 0.8285565972328186\n",
      "Iteration 13642 Loss: 1.1133849620819092\n",
      "Iteration 13643 Loss: 0.9116108417510986\n",
      "Iteration 13644 Loss: 1.2785091400146484\n",
      "Iteration 13645 Loss: 1.2523967027664185\n",
      "Iteration 13646 Loss: 1.5667195320129395\n",
      "Iteration 13647 Loss: 1.3861137628555298\n",
      "Iteration 13648 Loss: 1.001634955406189\n",
      "Iteration 13649 Loss: 1.2033933401107788\n",
      "Iteration 13649 Loss: 1.1621739864349365\n",
      "Iteration 13650 Loss: 1.1963465213775635\n",
      "Iteration 13651 Loss: 1.4463160037994385\n",
      "Iteration 13652 Loss: 1.3029944896697998\n",
      "Iteration 13653 Loss: 1.5280650854110718\n",
      "Iteration 13654 Loss: 1.340190052986145\n",
      "Iteration 13655 Loss: 1.3856734037399292\n",
      "Iteration 13656 Loss: 1.0985051393508911\n",
      "Iteration 13657 Loss: 0.919528067111969\n",
      "Iteration 13658 Loss: 0.9284577369689941\n",
      "Iteration 13659 Loss: 1.505727767944336\n",
      "Iteration 13659 Loss: 1.2651804685592651\n",
      "Iteration 13660 Loss: 1.0905646085739136\n",
      "Iteration 13661 Loss: 1.4420528411865234\n",
      "Iteration 13662 Loss: 1.1583210229873657\n",
      "Iteration 13663 Loss: 1.1483891010284424\n",
      "Iteration 13664 Loss: 1.3326423168182373\n",
      "Iteration 13665 Loss: 1.2058051824569702\n",
      "Iteration 13666 Loss: 1.0806059837341309\n",
      "Iteration 13667 Loss: 1.0805038213729858\n",
      "Iteration 13668 Loss: 1.365830898284912\n",
      "Iteration 13669 Loss: 1.3316608667373657\n",
      "Iteration 13669 Loss: 1.223637580871582\n",
      "Iteration 13670 Loss: 1.138351321220398\n",
      "Iteration 13671 Loss: 0.7756051421165466\n",
      "Iteration 13672 Loss: 1.3341383934020996\n",
      "Iteration 13673 Loss: 1.4319416284561157\n",
      "Iteration 13674 Loss: 1.592769980430603\n",
      "Iteration 13675 Loss: 1.0442626476287842\n",
      "Iteration 13676 Loss: 1.1903012990951538\n",
      "Iteration 13677 Loss: 1.3475624322891235\n",
      "Iteration 13678 Loss: 1.192029595375061\n",
      "Iteration 13679 Loss: 1.5406420230865479\n",
      "Iteration 13679 Loss: 1.2587604522705078\n",
      "Iteration 13680 Loss: 1.540639877319336\n",
      "Iteration 13681 Loss: 1.2590744495391846\n",
      "Iteration 13682 Loss: 0.84296053647995\n",
      "Iteration 13683 Loss: 1.3375024795532227\n",
      "Iteration 13684 Loss: 1.1119271516799927\n",
      "Iteration 13685 Loss: 1.2868062257766724\n",
      "Iteration 13686 Loss: 1.5664899349212646\n",
      "Iteration 13687 Loss: 1.495021104812622\n",
      "Iteration 13688 Loss: 1.1325198411941528\n",
      "Iteration 13689 Loss: 0.9626531600952148\n",
      "Iteration 13689 Loss: 1.2535593509674072\n",
      "Iteration 13690 Loss: 1.1409721374511719\n",
      "Iteration 13691 Loss: 0.8958588242530823\n",
      "Iteration 13692 Loss: 1.194266438484192\n",
      "Iteration 13693 Loss: 0.8389920592308044\n",
      "Iteration 13694 Loss: 1.3479713201522827\n",
      "Iteration 13695 Loss: 1.063914179801941\n",
      "Iteration 13696 Loss: 1.3538323640823364\n",
      "Iteration 13697 Loss: 1.4380202293395996\n",
      "Iteration 13698 Loss: 0.9150127172470093\n",
      "Iteration 13699 Loss: 1.0901192426681519\n",
      "Iteration 13699 Loss: 1.1278959512710571\n",
      "Iteration 13700 Loss: 0.8335156440734863\n",
      "Iteration 13701 Loss: 1.4610261917114258\n",
      "Iteration 13702 Loss: 1.198439121246338\n",
      "Iteration 13703 Loss: 1.3242785930633545\n",
      "Iteration 13704 Loss: 1.1424109935760498\n",
      "Iteration 13705 Loss: 0.9869323372840881\n",
      "Iteration 13706 Loss: 1.2829924821853638\n",
      "Iteration 13707 Loss: 1.2939256429672241\n",
      "Iteration 13708 Loss: 1.2002977132797241\n",
      "Iteration 13709 Loss: 0.933672308921814\n",
      "Iteration 13709 Loss: 1.1657490730285645\n",
      "Iteration 13710 Loss: 1.6711336374282837\n",
      "Iteration 13711 Loss: 1.2119548320770264\n",
      "Iteration 13712 Loss: 1.0983492136001587\n",
      "Iteration 13713 Loss: 1.4179004430770874\n",
      "Iteration 13714 Loss: 1.3536841869354248\n",
      "Iteration 13715 Loss: 1.4215283393859863\n",
      "Iteration 13716 Loss: 1.1729803085327148\n",
      "Iteration 13717 Loss: 1.263909935951233\n",
      "Iteration 13718 Loss: 1.2605974674224854\n",
      "Iteration 13719 Loss: 1.0886592864990234\n",
      "Iteration 13719 Loss: 1.296069860458374\n",
      "Iteration 13720 Loss: 1.2630890607833862\n",
      "Iteration 13721 Loss: 1.2088860273361206\n",
      "Iteration 13722 Loss: 1.1203824281692505\n",
      "Iteration 13723 Loss: 0.9281015992164612\n",
      "Iteration 13724 Loss: 1.6001170873641968\n",
      "Iteration 13725 Loss: 1.448410987854004\n",
      "Iteration 13726 Loss: 0.9759489893913269\n",
      "Iteration 13727 Loss: 1.1451140642166138\n",
      "Iteration 13728 Loss: 1.5117783546447754\n",
      "Iteration 13729 Loss: 1.3417737483978271\n",
      "Iteration 13729 Loss: 1.2543601989746094\n",
      "Iteration 13730 Loss: 1.4940398931503296\n",
      "Iteration 13731 Loss: 0.765550434589386\n",
      "Iteration 13732 Loss: 1.4209721088409424\n",
      "Iteration 13733 Loss: 1.1929630041122437\n",
      "Iteration 13734 Loss: 1.2789547443389893\n",
      "Iteration 13735 Loss: 1.1907743215560913\n",
      "Iteration 13736 Loss: 1.3834786415100098\n",
      "Iteration 13737 Loss: 0.9106860160827637\n",
      "Iteration 13738 Loss: 1.003175973892212\n",
      "Iteration 13739 Loss: 0.988916277885437\n",
      "Iteration 13739 Loss: 1.1629512310028076\n",
      "Iteration 13740 Loss: 1.3783035278320312\n",
      "Iteration 13741 Loss: 0.9724767208099365\n",
      "Iteration 13742 Loss: 1.4655609130859375\n",
      "Iteration 13743 Loss: 1.2378143072128296\n",
      "Iteration 13744 Loss: 1.2634106874465942\n",
      "Iteration 13745 Loss: 1.0487371683120728\n",
      "Iteration 13746 Loss: 1.0709288120269775\n",
      "Iteration 13747 Loss: 0.7313545942306519\n",
      "Iteration 13748 Loss: 1.1478493213653564\n",
      "Iteration 13749 Loss: 1.4080638885498047\n",
      "Iteration 13749 Loss: 1.172450065612793\n",
      "Iteration 13750 Loss: 1.1574841737747192\n",
      "Iteration 13751 Loss: 1.0829380750656128\n",
      "Iteration 13752 Loss: 1.3427766561508179\n",
      "Iteration 13753 Loss: 1.581758975982666\n",
      "Iteration 13754 Loss: 0.8905207514762878\n",
      "Iteration 13755 Loss: 1.306136965751648\n",
      "Iteration 13756 Loss: 1.4979369640350342\n",
      "Iteration 13757 Loss: 1.153878927230835\n",
      "Iteration 13758 Loss: 1.3505687713623047\n",
      "Iteration 13759 Loss: 0.8775972127914429\n",
      "Iteration 13759 Loss: 1.224159836769104\n",
      "Iteration 13760 Loss: 1.0591695308685303\n",
      "Iteration 13761 Loss: 1.0183775424957275\n",
      "Iteration 13762 Loss: 1.638298511505127\n",
      "Iteration 13763 Loss: 0.9947181344032288\n",
      "Iteration 13764 Loss: 1.531442642211914\n",
      "Iteration 13765 Loss: 1.2355258464813232\n",
      "Iteration 13766 Loss: 1.3968584537506104\n",
      "Iteration 13767 Loss: 1.3819445371627808\n",
      "Iteration 13768 Loss: 1.1496953964233398\n",
      "Iteration 13769 Loss: 1.0283329486846924\n",
      "Iteration 13769 Loss: 1.243436336517334\n",
      "Iteration 13770 Loss: 1.2065085172653198\n",
      "Iteration 13771 Loss: 1.2214335203170776\n",
      "Iteration 13772 Loss: 1.379759669303894\n",
      "Iteration 13773 Loss: 1.327541470527649\n",
      "Iteration 13774 Loss: 1.0762590169906616\n",
      "Iteration 13775 Loss: 1.0816943645477295\n",
      "Iteration 13776 Loss: 1.145147442817688\n",
      "Iteration 13777 Loss: 1.5941917896270752\n",
      "Iteration 13778 Loss: 0.8929232954978943\n",
      "Iteration 13779 Loss: 0.8343608975410461\n",
      "Iteration 13779 Loss: 1.1759819984436035\n",
      "Iteration 13780 Loss: 1.000043511390686\n",
      "Iteration 13781 Loss: 1.1536749601364136\n",
      "Iteration 13782 Loss: 1.2403004169464111\n",
      "Iteration 13783 Loss: 1.564897060394287\n",
      "Iteration 13784 Loss: 1.4835350513458252\n",
      "Iteration 13785 Loss: 1.1782128810882568\n",
      "Iteration 13786 Loss: 0.9821221232414246\n",
      "Iteration 13787 Loss: 1.2923588752746582\n",
      "Iteration 13788 Loss: 1.1993701457977295\n",
      "Iteration 13789 Loss: 1.6676597595214844\n",
      "Iteration 13789 Loss: 1.2762176990509033\n",
      "Iteration 13790 Loss: 0.8735070824623108\n",
      "Iteration 13791 Loss: 1.0894485712051392\n",
      "Iteration 13792 Loss: 0.6915607452392578\n",
      "Iteration 13793 Loss: 1.5372015237808228\n",
      "Iteration 13794 Loss: 1.316426157951355\n",
      "Iteration 13795 Loss: 0.9679102897644043\n",
      "Iteration 13796 Loss: 1.1731215715408325\n",
      "Iteration 13797 Loss: 0.9740325212478638\n",
      "Iteration 13798 Loss: 1.324522614479065\n",
      "Iteration 13799 Loss: 1.8119478225708008\n",
      "Iteration 13799 Loss: 1.175967812538147\n",
      "Iteration 13800 Loss: 1.1471445560455322\n",
      "Iteration 13801 Loss: 1.0457552671432495\n",
      "Iteration 13802 Loss: 0.840064287185669\n",
      "Iteration 13803 Loss: 1.2124707698822021\n",
      "Iteration 13804 Loss: 1.2474523782730103\n",
      "Iteration 13805 Loss: 1.8238439559936523\n",
      "Iteration 13806 Loss: 1.4541884660720825\n",
      "Iteration 13807 Loss: 1.1545331478118896\n",
      "Iteration 13808 Loss: 1.001240611076355\n",
      "Iteration 13809 Loss: 0.8999183177947998\n",
      "Iteration 13809 Loss: 1.1826611757278442\n",
      "Iteration 13810 Loss: 1.5183875560760498\n",
      "Iteration 13811 Loss: 1.1224727630615234\n",
      "Iteration 13812 Loss: 1.240821123123169\n",
      "Iteration 13813 Loss: 1.3862011432647705\n",
      "Iteration 13814 Loss: 1.5467942953109741\n",
      "Iteration 13815 Loss: 1.1004130840301514\n",
      "Iteration 13816 Loss: 1.1657403707504272\n",
      "Iteration 13817 Loss: 1.3537732362747192\n",
      "Iteration 13818 Loss: 1.3283699750900269\n",
      "Iteration 13819 Loss: 1.1180006265640259\n",
      "Iteration 13819 Loss: 1.2880972623825073\n",
      "Iteration 13820 Loss: 1.3778016567230225\n",
      "Iteration 13821 Loss: 0.978860080242157\n",
      "Iteration 13822 Loss: 1.2059967517852783\n",
      "Iteration 13823 Loss: 1.1627047061920166\n",
      "Iteration 13824 Loss: 1.0957163572311401\n",
      "Iteration 13825 Loss: 1.5565989017486572\n",
      "Iteration 13826 Loss: 1.1661185026168823\n",
      "Iteration 13827 Loss: 1.3008393049240112\n",
      "Iteration 13828 Loss: 1.658966302871704\n",
      "Iteration 13829 Loss: 1.5634831190109253\n",
      "Iteration 13829 Loss: 1.306708574295044\n",
      "Iteration 13830 Loss: 1.2022289037704468\n",
      "Iteration 13831 Loss: 1.1032025814056396\n",
      "Iteration 13832 Loss: 1.203415870666504\n",
      "Iteration 13833 Loss: 0.9079599976539612\n",
      "Iteration 13834 Loss: 1.495847463607788\n",
      "Iteration 13835 Loss: 0.9546250700950623\n",
      "Iteration 13836 Loss: 1.6384892463684082\n",
      "Iteration 13837 Loss: 1.0307520627975464\n",
      "Iteration 13838 Loss: 0.9226775169372559\n",
      "Iteration 13839 Loss: 1.2562519311904907\n",
      "Iteration 13839 Loss: 1.1715450286865234\n",
      "Iteration 13840 Loss: 1.8682721853256226\n",
      "Iteration 13841 Loss: 2.082563638687134\n",
      "Iteration 13842 Loss: 1.2518880367279053\n",
      "Iteration 13843 Loss: 1.4720759391784668\n",
      "Iteration 13844 Loss: 1.4760043621063232\n",
      "Iteration 13845 Loss: 1.3784700632095337\n",
      "Iteration 13846 Loss: 1.6058859825134277\n",
      "Iteration 13847 Loss: 1.1770175695419312\n",
      "Iteration 13848 Loss: 1.6463099718093872\n",
      "Iteration 13849 Loss: 1.3987786769866943\n",
      "Iteration 13849 Loss: 1.53572678565979\n",
      "Iteration 13850 Loss: 0.7542088627815247\n",
      "Iteration 13851 Loss: 1.3545180559158325\n",
      "Iteration 13852 Loss: 1.0503329038619995\n",
      "Iteration 13853 Loss: 1.4053874015808105\n",
      "Iteration 13854 Loss: 1.3976887464523315\n",
      "Iteration 13855 Loss: 1.0499231815338135\n",
      "Iteration 13856 Loss: 1.2516427040100098\n",
      "Iteration 13857 Loss: 0.9371412396430969\n",
      "Iteration 13858 Loss: 0.8032974600791931\n",
      "Iteration 13859 Loss: 1.203751564025879\n",
      "Iteration 13859 Loss: 1.1207892894744873\n",
      "Iteration 13860 Loss: 1.4605993032455444\n",
      "Iteration 13861 Loss: 1.6138862371444702\n",
      "Iteration 13862 Loss: 1.650820016860962\n",
      "Iteration 13863 Loss: 1.116042137145996\n",
      "Iteration 13864 Loss: 1.258156180381775\n",
      "Iteration 13865 Loss: 1.3808976411819458\n",
      "Iteration 13866 Loss: 1.1533095836639404\n",
      "Iteration 13867 Loss: 1.2832303047180176\n",
      "Iteration 13868 Loss: 1.2062219381332397\n",
      "Iteration 13869 Loss: 1.3205357789993286\n",
      "Iteration 13869 Loss: 1.344369888305664\n",
      "Iteration 13870 Loss: 1.320654034614563\n",
      "Iteration 13871 Loss: 1.403281331062317\n",
      "Iteration 13872 Loss: 0.8705555200576782\n",
      "Iteration 13873 Loss: 1.4613126516342163\n",
      "Iteration 13874 Loss: 1.4999277591705322\n",
      "Iteration 13875 Loss: 1.4976813793182373\n",
      "Iteration 13876 Loss: 1.3837354183197021\n",
      "Iteration 13877 Loss: 1.4901623725891113\n",
      "Iteration 13878 Loss: 1.267805576324463\n",
      "Iteration 13879 Loss: 1.3716652393341064\n",
      "Iteration 13879 Loss: 1.3566782474517822\n",
      "Iteration 13880 Loss: 0.9685828685760498\n",
      "Iteration 13881 Loss: 1.3404693603515625\n",
      "Iteration 13882 Loss: 1.2437211275100708\n",
      "Iteration 13883 Loss: 1.5137498378753662\n",
      "Iteration 13884 Loss: 1.4085476398468018\n",
      "Iteration 13885 Loss: 1.3235150575637817\n",
      "Iteration 13886 Loss: 0.9619781374931335\n",
      "Iteration 13887 Loss: 0.8272950649261475\n",
      "Iteration 13888 Loss: 1.051296591758728\n",
      "Iteration 13889 Loss: 0.8584352731704712\n",
      "Iteration 13889 Loss: 1.14975905418396\n",
      "Iteration 13890 Loss: 1.1014270782470703\n",
      "Iteration 13891 Loss: 1.4759777784347534\n",
      "Iteration 13892 Loss: 1.1704198122024536\n",
      "Iteration 13893 Loss: 1.5785211324691772\n",
      "Iteration 13894 Loss: 1.5818582773208618\n",
      "Iteration 13895 Loss: 0.9051388502120972\n",
      "Iteration 13896 Loss: 1.2356566190719604\n",
      "Iteration 13897 Loss: 0.9841048121452332\n",
      "Iteration 13898 Loss: 1.1225959062576294\n",
      "Iteration 13899 Loss: 1.411576747894287\n",
      "Iteration 13899 Loss: 1.2567278146743774\n",
      "Iteration 13900 Loss: 1.349565029144287\n",
      "Iteration 13901 Loss: 0.7892925143241882\n",
      "Iteration 13902 Loss: 1.3312182426452637\n",
      "Iteration 13903 Loss: 0.9958305358886719\n",
      "Iteration 13904 Loss: 1.326601266860962\n",
      "Iteration 13905 Loss: 0.8979140520095825\n",
      "Iteration 13906 Loss: 1.327750563621521\n",
      "Iteration 13907 Loss: 0.950003445148468\n",
      "Iteration 13908 Loss: 1.296933650970459\n",
      "Iteration 13909 Loss: 0.8888888359069824\n",
      "Iteration 13909 Loss: 1.115399718284607\n",
      "Iteration 13910 Loss: 1.2231181859970093\n",
      "Iteration 13911 Loss: 1.2722529172897339\n",
      "Iteration 13912 Loss: 1.3049888610839844\n",
      "Iteration 13913 Loss: 1.7299317121505737\n",
      "Iteration 13914 Loss: 1.5256580114364624\n",
      "Iteration 13915 Loss: 1.5888417959213257\n",
      "Iteration 13916 Loss: 1.1337380409240723\n",
      "Iteration 13917 Loss: 1.2040562629699707\n",
      "Iteration 13918 Loss: 0.8585138320922852\n",
      "Iteration 13919 Loss: 1.4320815801620483\n",
      "Iteration 13919 Loss: 1.3273179531097412\n",
      "Iteration 13920 Loss: 1.076612114906311\n",
      "Iteration 13921 Loss: 1.4159208536148071\n",
      "Iteration 13922 Loss: 0.888617217540741\n",
      "Iteration 13923 Loss: 1.6055092811584473\n",
      "Iteration 13924 Loss: 1.3591734170913696\n",
      "Iteration 13925 Loss: 1.2887027263641357\n",
      "Iteration 13926 Loss: 1.0808062553405762\n",
      "Iteration 13927 Loss: 1.2790005207061768\n",
      "Iteration 13928 Loss: 1.322885513305664\n",
      "Iteration 13929 Loss: 1.2518815994262695\n",
      "Iteration 13929 Loss: 1.256911039352417\n",
      "Iteration 13930 Loss: 1.4917874336242676\n",
      "Iteration 13931 Loss: 1.030556559562683\n",
      "Iteration 13932 Loss: 1.4219316244125366\n",
      "Iteration 13933 Loss: 1.234216332435608\n",
      "Iteration 13934 Loss: 0.9026329517364502\n",
      "Iteration 13935 Loss: 0.9347391724586487\n",
      "Iteration 13936 Loss: 1.1396701335906982\n",
      "Iteration 13937 Loss: 1.7677595615386963\n",
      "Iteration 13938 Loss: 1.6252905130386353\n",
      "Iteration 13939 Loss: 0.8815165162086487\n",
      "Iteration 13939 Loss: 1.2430100440979004\n",
      "Iteration 13940 Loss: 1.1843773126602173\n",
      "Iteration 13941 Loss: 1.3064887523651123\n",
      "Iteration 13942 Loss: 1.1718127727508545\n",
      "Iteration 13943 Loss: 1.1064754724502563\n",
      "Iteration 13944 Loss: 1.4476919174194336\n",
      "Iteration 13945 Loss: 1.153122901916504\n",
      "Iteration 13946 Loss: 1.1109298467636108\n",
      "Iteration 13947 Loss: 1.4648727178573608\n",
      "Iteration 13948 Loss: 1.0466686487197876\n",
      "Iteration 13949 Loss: 1.1475136280059814\n",
      "Iteration 13949 Loss: 1.2139952182769775\n",
      "Iteration 13950 Loss: 1.3439711332321167\n",
      "Iteration 13951 Loss: 1.0538389682769775\n",
      "Iteration 13952 Loss: 1.1781489849090576\n",
      "Iteration 13953 Loss: 1.310910940170288\n",
      "Iteration 13954 Loss: 1.085404872894287\n",
      "Iteration 13955 Loss: 1.9099117517471313\n",
      "Iteration 13956 Loss: 1.3700833320617676\n",
      "Iteration 13957 Loss: 1.3043259382247925\n",
      "Iteration 13958 Loss: 0.9477728009223938\n",
      "Iteration 13959 Loss: 1.1510988473892212\n",
      "Iteration 13959 Loss: 1.2655469179153442\n",
      "Iteration 13960 Loss: 1.0790258646011353\n",
      "Iteration 13961 Loss: 1.0507539510726929\n",
      "Iteration 13962 Loss: 1.3363083600997925\n",
      "Iteration 13963 Loss: 1.2405720949172974\n",
      "Iteration 13964 Loss: 1.4022976160049438\n",
      "Iteration 13965 Loss: 1.0642449855804443\n",
      "Iteration 13966 Loss: 0.8273128271102905\n",
      "Iteration 13967 Loss: 0.9195014834403992\n",
      "Iteration 13968 Loss: 1.1545825004577637\n",
      "Iteration 13969 Loss: 1.4980380535125732\n",
      "Iteration 13969 Loss: 1.1572637557983398\n",
      "Iteration 13970 Loss: 1.3124898672103882\n",
      "Iteration 13971 Loss: 1.4250218868255615\n",
      "Iteration 13972 Loss: 1.2772526741027832\n",
      "Iteration 13973 Loss: 1.2767887115478516\n",
      "Iteration 13974 Loss: 0.9190292358398438\n",
      "Iteration 13975 Loss: 1.3054581880569458\n",
      "Iteration 13976 Loss: 1.5374999046325684\n",
      "Iteration 13977 Loss: 1.144638180732727\n",
      "Iteration 13978 Loss: 0.9985687732696533\n",
      "Iteration 13979 Loss: 0.8472992181777954\n",
      "Iteration 13979 Loss: 1.204404592514038\n",
      "Iteration 13980 Loss: 0.9913703799247742\n",
      "Iteration 13981 Loss: 1.192858099937439\n",
      "Iteration 13982 Loss: 1.2835265398025513\n",
      "Iteration 13983 Loss: 1.6269457340240479\n",
      "Iteration 13984 Loss: 1.067815899848938\n",
      "Iteration 13985 Loss: 1.2682738304138184\n",
      "Iteration 13986 Loss: 1.3054678440093994\n",
      "Iteration 13987 Loss: 1.4495866298675537\n",
      "Iteration 13988 Loss: 1.3013737201690674\n",
      "Iteration 13989 Loss: 1.4500715732574463\n",
      "Iteration 13989 Loss: 1.2937290668487549\n",
      "Iteration 13990 Loss: 1.2429436445236206\n",
      "Iteration 13991 Loss: 1.4972834587097168\n",
      "Iteration 13992 Loss: 1.2742571830749512\n",
      "Iteration 13993 Loss: 1.4535853862762451\n",
      "Iteration 13994 Loss: 1.303209662437439\n",
      "Iteration 13995 Loss: 1.2159740924835205\n",
      "Iteration 13996 Loss: 1.7045999765396118\n",
      "Iteration 13997 Loss: 1.1276600360870361\n",
      "Iteration 13998 Loss: 1.5053412914276123\n",
      "Iteration 13999 Loss: 1.391007661819458\n",
      "Iteration 13999 Loss: 1.3715862035751343\n",
      "Iteration 14000 Loss: 0.9846833944320679\n",
      "Iteration 14001 Loss: 1.1748363971710205\n",
      "Iteration 14002 Loss: 1.488152027130127\n",
      "Iteration 14003 Loss: 0.7823238372802734\n",
      "Iteration 14004 Loss: 1.2024075984954834\n",
      "Iteration 14005 Loss: 0.8765785098075867\n",
      "Iteration 14006 Loss: 1.2098944187164307\n",
      "Iteration 14007 Loss: 1.4339364767074585\n",
      "Iteration 14008 Loss: 1.1247649192810059\n",
      "Iteration 14009 Loss: 1.3325512409210205\n",
      "Iteration 14009 Loss: 1.1610127687454224\n",
      "Iteration 14010 Loss: 1.0228347778320312\n",
      "Iteration 14011 Loss: 1.0952098369598389\n",
      "Iteration 14012 Loss: 1.1062045097351074\n",
      "Iteration 14013 Loss: 1.1826841831207275\n",
      "Iteration 14014 Loss: 1.52523934841156\n",
      "Iteration 14015 Loss: 1.6915714740753174\n",
      "Iteration 14016 Loss: 0.9802515506744385\n",
      "Iteration 14017 Loss: 1.2113553285598755\n",
      "Iteration 14018 Loss: 0.9645500779151917\n",
      "Iteration 14019 Loss: 0.8739547729492188\n",
      "Iteration 14019 Loss: 1.1653854846954346\n",
      "Iteration 14020 Loss: 1.2029720544815063\n",
      "Iteration 14021 Loss: 1.4267122745513916\n",
      "Iteration 14022 Loss: 1.3034409284591675\n",
      "Iteration 14023 Loss: 1.3546781539916992\n",
      "Iteration 14024 Loss: 1.496728539466858\n",
      "Iteration 14025 Loss: 1.1330833435058594\n",
      "Iteration 14026 Loss: 1.0946401357650757\n",
      "Iteration 14027 Loss: 0.7673837542533875\n",
      "Iteration 14028 Loss: 0.5997244715690613\n",
      "Iteration 14029 Loss: 1.060072660446167\n",
      "Iteration 14029 Loss: 1.1439435482025146\n",
      "Iteration 14030 Loss: 1.2854485511779785\n",
      "Iteration 14031 Loss: 1.2816810607910156\n",
      "Iteration 14032 Loss: 1.2651933431625366\n",
      "Iteration 14033 Loss: 1.1776789426803589\n",
      "Iteration 14034 Loss: 1.6197680234909058\n",
      "Iteration 14035 Loss: 1.7482355833053589\n",
      "Iteration 14036 Loss: 1.182552695274353\n",
      "Iteration 14037 Loss: 1.1896188259124756\n",
      "Iteration 14038 Loss: 0.9643075466156006\n",
      "Iteration 14039 Loss: 1.3427948951721191\n",
      "Iteration 14039 Loss: 1.3057279586791992\n",
      "Iteration 14040 Loss: 0.9889880418777466\n",
      "Iteration 14041 Loss: 1.6031986474990845\n",
      "Iteration 14042 Loss: 1.377107858657837\n",
      "Iteration 14043 Loss: 1.0641515254974365\n",
      "Iteration 14044 Loss: 1.6573691368103027\n",
      "Iteration 14045 Loss: 1.0485061407089233\n",
      "Iteration 14046 Loss: 1.1362024545669556\n",
      "Iteration 14047 Loss: 1.5007412433624268\n",
      "Iteration 14048 Loss: 1.3914542198181152\n",
      "Iteration 14049 Loss: 0.866549015045166\n",
      "Iteration 14049 Loss: 1.2634267807006836\n",
      "Iteration 14050 Loss: 0.8920271992683411\n",
      "Iteration 14051 Loss: 0.9435446858406067\n",
      "Iteration 14052 Loss: 1.1629786491394043\n",
      "Iteration 14053 Loss: 1.3364835977554321\n",
      "Iteration 14054 Loss: 1.2920844554901123\n",
      "Iteration 14055 Loss: 1.0614097118377686\n",
      "Iteration 14056 Loss: 1.110155463218689\n",
      "Iteration 14057 Loss: 1.3552627563476562\n",
      "Iteration 14058 Loss: 1.2625459432601929\n",
      "Iteration 14059 Loss: 1.5066118240356445\n",
      "Iteration 14059 Loss: 1.1923104524612427\n",
      "Iteration 14060 Loss: 1.395833969116211\n",
      "Iteration 14061 Loss: 1.5275850296020508\n",
      "Iteration 14062 Loss: 1.026227593421936\n",
      "Iteration 14063 Loss: 1.2720363140106201\n",
      "Iteration 14064 Loss: 0.9707105755805969\n",
      "Iteration 14065 Loss: 0.8712119460105896\n",
      "Iteration 14066 Loss: 1.1265028715133667\n",
      "Iteration 14067 Loss: 1.6131395101547241\n",
      "Iteration 14068 Loss: 1.3840117454528809\n",
      "Iteration 14069 Loss: 1.3841792345046997\n",
      "Iteration 14069 Loss: 1.2571438550949097\n",
      "Iteration 14070 Loss: 1.3014575242996216\n",
      "Iteration 14071 Loss: 1.2179179191589355\n",
      "Iteration 14072 Loss: 1.1742379665374756\n",
      "Iteration 14073 Loss: 1.3912596702575684\n",
      "Iteration 14074 Loss: 1.4579781293869019\n",
      "Iteration 14075 Loss: 1.5551674365997314\n",
      "Iteration 14076 Loss: 1.278357982635498\n",
      "Iteration 14077 Loss: 1.2903072834014893\n",
      "Iteration 14078 Loss: 1.1272315979003906\n",
      "Iteration 14079 Loss: 1.3177491426467896\n",
      "Iteration 14079 Loss: 1.3111662864685059\n",
      "Iteration 14080 Loss: 0.8950725793838501\n",
      "Iteration 14081 Loss: 1.40143883228302\n",
      "Iteration 14082 Loss: 1.5221318006515503\n",
      "Iteration 14083 Loss: 1.0905550718307495\n",
      "Iteration 14084 Loss: 1.3208820819854736\n",
      "Iteration 14085 Loss: 1.1526395082473755\n",
      "Iteration 14086 Loss: 1.4144065380096436\n",
      "Iteration 14087 Loss: 0.9503769278526306\n",
      "Iteration 14088 Loss: 1.1889103651046753\n",
      "Iteration 14089 Loss: 1.1744412183761597\n",
      "Iteration 14089 Loss: 1.211085557937622\n",
      "Iteration 14090 Loss: 0.960023045539856\n",
      "Iteration 14091 Loss: 1.328065276145935\n",
      "Iteration 14092 Loss: 1.0439374446868896\n",
      "Iteration 14093 Loss: 1.150832176208496\n",
      "Iteration 14094 Loss: 1.2127729654312134\n",
      "Iteration 14095 Loss: 1.1897941827774048\n",
      "Iteration 14096 Loss: 0.9441796541213989\n",
      "Iteration 14097 Loss: 1.2072303295135498\n",
      "Iteration 14098 Loss: 0.9477174878120422\n",
      "Iteration 14099 Loss: 1.344814419746399\n",
      "Iteration 14099 Loss: 1.132936716079712\n",
      "Iteration 14100 Loss: 1.057624101638794\n",
      "Iteration 14101 Loss: 1.730867862701416\n",
      "Iteration 14102 Loss: 1.4262086153030396\n",
      "Iteration 14103 Loss: 0.9452587962150574\n",
      "Iteration 14104 Loss: 1.0100408792495728\n",
      "Iteration 14105 Loss: 1.2774378061294556\n",
      "Iteration 14106 Loss: 0.9566328525543213\n",
      "Iteration 14107 Loss: 1.53075110912323\n",
      "Iteration 14108 Loss: 1.2928996086120605\n",
      "Iteration 14109 Loss: 1.6665740013122559\n",
      "Iteration 14109 Loss: 1.2894295454025269\n",
      "Iteration 14110 Loss: 1.3900806903839111\n",
      "Iteration 14111 Loss: 1.493325114250183\n",
      "Iteration 14112 Loss: 1.4314496517181396\n",
      "Iteration 14113 Loss: 1.0020570755004883\n",
      "Iteration 14114 Loss: 1.409085750579834\n",
      "Iteration 14115 Loss: 1.3236747980117798\n",
      "Iteration 14116 Loss: 1.524304747581482\n",
      "Iteration 14117 Loss: 1.3773858547210693\n",
      "Iteration 14118 Loss: 1.1820710897445679\n",
      "Iteration 14119 Loss: 1.4613866806030273\n",
      "Iteration 14119 Loss: 1.3594822883605957\n",
      "Iteration 14120 Loss: 1.2070834636688232\n",
      "Iteration 14121 Loss: 1.2153574228286743\n",
      "Iteration 14122 Loss: 1.1605195999145508\n",
      "Iteration 14123 Loss: 1.552443027496338\n",
      "Iteration 14124 Loss: 1.5306717157363892\n",
      "Iteration 14125 Loss: 1.1866081953048706\n",
      "Iteration 14126 Loss: 0.9338569641113281\n",
      "Iteration 14127 Loss: 1.362463355064392\n",
      "Iteration 14128 Loss: 1.328034520149231\n",
      "Iteration 14129 Loss: 1.2649550437927246\n",
      "Iteration 14129 Loss: 1.2741992473602295\n",
      "Iteration 14130 Loss: 0.7084420919418335\n",
      "Iteration 14131 Loss: 1.312528133392334\n",
      "Iteration 14132 Loss: 1.464687466621399\n",
      "Iteration 14133 Loss: 1.1717352867126465\n",
      "Iteration 14134 Loss: 1.3169621229171753\n",
      "Iteration 14135 Loss: 1.0453763008117676\n",
      "Iteration 14136 Loss: 1.2140302658081055\n",
      "Iteration 14137 Loss: 1.2155430316925049\n",
      "Iteration 14138 Loss: 1.4658993482589722\n",
      "Iteration 14139 Loss: 1.1967874765396118\n",
      "Iteration 14139 Loss: 1.211199164390564\n",
      "Iteration 14140 Loss: 1.273139238357544\n",
      "Iteration 14141 Loss: 0.8054437041282654\n",
      "Iteration 14142 Loss: 1.1667169332504272\n",
      "Iteration 14143 Loss: 1.3675928115844727\n",
      "Iteration 14144 Loss: 1.1326634883880615\n",
      "Iteration 14145 Loss: 0.9385030269622803\n",
      "Iteration 14146 Loss: 1.2651680707931519\n",
      "Iteration 14147 Loss: 1.1356620788574219\n",
      "Iteration 14148 Loss: 1.084709882736206\n",
      "Iteration 14149 Loss: 0.9816321134567261\n",
      "Iteration 14149 Loss: 1.1151231527328491\n",
      "Iteration 14150 Loss: 1.4035570621490479\n",
      "Iteration 14151 Loss: 0.9434210062026978\n",
      "Iteration 14152 Loss: 1.6992636919021606\n",
      "Iteration 14153 Loss: 0.9460028409957886\n",
      "Iteration 14154 Loss: 0.6927765607833862\n",
      "Iteration 14155 Loss: 1.0471080541610718\n",
      "Iteration 14156 Loss: 1.0538392066955566\n",
      "Iteration 14157 Loss: 0.9587497711181641\n",
      "Iteration 14158 Loss: 1.835248351097107\n",
      "Iteration 14159 Loss: 1.4531970024108887\n",
      "Iteration 14159 Loss: 1.203316330909729\n",
      "Iteration 14160 Loss: 1.3049157857894897\n",
      "Iteration 14161 Loss: 1.277575135231018\n",
      "Iteration 14162 Loss: 1.394676685333252\n",
      "Iteration 14163 Loss: 0.9008593559265137\n",
      "Iteration 14164 Loss: 1.3048397302627563\n",
      "Iteration 14165 Loss: 1.2703797817230225\n",
      "Iteration 14166 Loss: 1.3143895864486694\n",
      "Iteration 14167 Loss: 1.3166242837905884\n",
      "Iteration 14168 Loss: 1.1959619522094727\n",
      "Iteration 14169 Loss: 1.3903934955596924\n",
      "Iteration 14169 Loss: 1.2670615911483765\n",
      "Iteration 14170 Loss: 1.3035916090011597\n",
      "Iteration 14171 Loss: 1.34402596950531\n",
      "Iteration 14172 Loss: 1.1515047550201416\n",
      "Iteration 14173 Loss: 1.2777023315429688\n",
      "Iteration 14174 Loss: 1.3086508512496948\n",
      "Iteration 14175 Loss: 1.015892744064331\n",
      "Iteration 14176 Loss: 1.431671142578125\n",
      "Iteration 14177 Loss: 0.9563010931015015\n",
      "Iteration 14178 Loss: 1.311574101448059\n",
      "Iteration 14179 Loss: 1.061906099319458\n",
      "Iteration 14179 Loss: 1.2162821292877197\n",
      "Iteration 14180 Loss: 1.2120441198349\n",
      "Iteration 14181 Loss: 1.3179931640625\n",
      "Iteration 14182 Loss: 1.3018120527267456\n",
      "Iteration 14183 Loss: 1.2044752836227417\n",
      "Iteration 14184 Loss: 1.2821189165115356\n",
      "Iteration 14185 Loss: 1.3748838901519775\n",
      "Iteration 14186 Loss: 1.348765254020691\n",
      "Iteration 14187 Loss: 1.3548861742019653\n",
      "Iteration 14188 Loss: 1.0617015361785889\n",
      "Iteration 14189 Loss: 1.1367592811584473\n",
      "Iteration 14189 Loss: 1.259544014930725\n",
      "Iteration 14190 Loss: 1.4718868732452393\n",
      "Iteration 14191 Loss: 0.9734979271888733\n",
      "Iteration 14192 Loss: 1.109843134880066\n",
      "Iteration 14193 Loss: 1.611423134803772\n",
      "Iteration 14194 Loss: 1.2064385414123535\n",
      "Iteration 14195 Loss: 1.6522971391677856\n",
      "Iteration 14196 Loss: 1.7072978019714355\n",
      "Iteration 14197 Loss: 1.0004411935806274\n",
      "Iteration 14198 Loss: 0.9173571467399597\n",
      "Iteration 14199 Loss: 1.3268293142318726\n",
      "Iteration 14199 Loss: 1.2977312803268433\n",
      "Iteration 14200 Loss: 1.4361878633499146\n",
      "Iteration 14201 Loss: 1.0416982173919678\n",
      "Iteration 14202 Loss: 1.1345078945159912\n",
      "Iteration 14203 Loss: 1.004892110824585\n",
      "Iteration 14204 Loss: 0.9798930883407593\n",
      "Iteration 14205 Loss: 1.235920786857605\n",
      "Iteration 14206 Loss: 1.4457041025161743\n",
      "Iteration 14207 Loss: 1.273456335067749\n",
      "Iteration 14208 Loss: 1.1520047187805176\n",
      "Iteration 14209 Loss: 1.29878830909729\n",
      "Iteration 14209 Loss: 1.200305461883545\n",
      "Iteration 14210 Loss: 0.8452288508415222\n",
      "Iteration 14211 Loss: 1.2638537883758545\n",
      "Iteration 14212 Loss: 1.7175830602645874\n",
      "Iteration 14213 Loss: 1.270795464515686\n",
      "Iteration 14214 Loss: 1.1860992908477783\n",
      "Iteration 14215 Loss: 1.04288649559021\n",
      "Iteration 14216 Loss: 1.2665377855300903\n",
      "Iteration 14217 Loss: 1.169870376586914\n",
      "Iteration 14218 Loss: 1.1340453624725342\n",
      "Iteration 14219 Loss: 0.785875678062439\n",
      "Iteration 14219 Loss: 1.168277621269226\n",
      "Iteration 14220 Loss: 1.244463324546814\n",
      "Iteration 14221 Loss: 1.4722362756729126\n",
      "Iteration 14222 Loss: 1.1747628450393677\n",
      "Iteration 14223 Loss: 0.8481945395469666\n",
      "Iteration 14224 Loss: 1.3625152111053467\n",
      "Iteration 14225 Loss: 1.1771459579467773\n",
      "Iteration 14226 Loss: 0.9441057443618774\n",
      "Iteration 14227 Loss: 1.5100241899490356\n",
      "Iteration 14228 Loss: 1.263702392578125\n",
      "Iteration 14229 Loss: 0.8309682607650757\n",
      "Iteration 14229 Loss: 1.182811975479126\n",
      "Iteration 14230 Loss: 1.1644220352172852\n",
      "Iteration 14231 Loss: 1.857089638710022\n",
      "Iteration 14232 Loss: 1.4172370433807373\n",
      "Iteration 14233 Loss: 1.0128023624420166\n",
      "Iteration 14234 Loss: 1.2476073503494263\n",
      "Iteration 14235 Loss: 1.1025015115737915\n",
      "Iteration 14236 Loss: 1.2595458030700684\n",
      "Iteration 14237 Loss: 1.2571780681610107\n",
      "Iteration 14238 Loss: 1.1393413543701172\n",
      "Iteration 14239 Loss: 1.0071171522140503\n",
      "Iteration 14239 Loss: 1.2464842796325684\n",
      "Iteration 14240 Loss: 0.9950236678123474\n",
      "Iteration 14241 Loss: 1.502548098564148\n",
      "Iteration 14242 Loss: 1.1834100484848022\n",
      "Iteration 14243 Loss: 1.1283912658691406\n",
      "Iteration 14244 Loss: 1.3269572257995605\n",
      "Iteration 14245 Loss: 0.925703763961792\n",
      "Iteration 14246 Loss: 1.0599045753479004\n",
      "Iteration 14247 Loss: 1.2759941816329956\n",
      "Iteration 14248 Loss: 0.7787806391716003\n",
      "Iteration 14249 Loss: 1.3681750297546387\n",
      "Iteration 14249 Loss: 1.1544888019561768\n",
      "Iteration 14250 Loss: 1.3778680562973022\n",
      "Iteration 14251 Loss: 0.916792094707489\n",
      "Iteration 14252 Loss: 1.0811316967010498\n",
      "Iteration 14253 Loss: 1.3980283737182617\n",
      "Iteration 14254 Loss: 1.0023974180221558\n",
      "Iteration 14255 Loss: 1.1101382970809937\n",
      "Iteration 14256 Loss: 0.800598680973053\n",
      "Iteration 14257 Loss: 1.643373727798462\n",
      "Iteration 14258 Loss: 1.3112194538116455\n",
      "Iteration 14259 Loss: 1.3769581317901611\n",
      "Iteration 14259 Loss: 1.2018506526947021\n",
      "Iteration 14260 Loss: 1.5224567651748657\n",
      "Iteration 14261 Loss: 1.2357892990112305\n",
      "Iteration 14262 Loss: 1.2943382263183594\n",
      "Iteration 14263 Loss: 1.2497001886367798\n",
      "Iteration 14264 Loss: 1.320035696029663\n",
      "Iteration 14265 Loss: 1.1158584356307983\n",
      "Iteration 14266 Loss: 1.1993776559829712\n",
      "Iteration 14267 Loss: 1.2815464735031128\n",
      "Iteration 14268 Loss: 1.4643304347991943\n",
      "Iteration 14269 Loss: 1.0418226718902588\n",
      "Iteration 14269 Loss: 1.2725255489349365\n",
      "Iteration 14270 Loss: 1.299418330192566\n",
      "Iteration 14271 Loss: 1.3818998336791992\n",
      "Iteration 14272 Loss: 1.1545311212539673\n",
      "Iteration 14273 Loss: 1.1220557689666748\n",
      "Iteration 14274 Loss: 1.3643221855163574\n",
      "Iteration 14275 Loss: 1.1267735958099365\n",
      "Iteration 14276 Loss: 1.0824941396713257\n",
      "Iteration 14277 Loss: 1.1139116287231445\n",
      "Iteration 14278 Loss: 1.108303427696228\n",
      "Iteration 14279 Loss: 1.2167344093322754\n",
      "Iteration 14279 Loss: 1.1970443725585938\n",
      "Iteration 14280 Loss: 1.0211948156356812\n",
      "Iteration 14281 Loss: 1.3886147737503052\n",
      "Iteration 14282 Loss: 1.4543291330337524\n",
      "Iteration 14283 Loss: 1.3780639171600342\n",
      "Iteration 14284 Loss: 1.0784831047058105\n",
      "Iteration 14285 Loss: 1.3372832536697388\n",
      "Iteration 14286 Loss: 1.1201380491256714\n",
      "Iteration 14287 Loss: 1.235683560371399\n",
      "Iteration 14288 Loss: 1.6868268251419067\n",
      "Iteration 14289 Loss: 1.1984144449234009\n",
      "Iteration 14289 Loss: 1.289903163909912\n",
      "Iteration 14290 Loss: 1.3613293170928955\n",
      "Iteration 14291 Loss: 0.9820659160614014\n",
      "Iteration 14292 Loss: 1.0629867315292358\n",
      "Iteration 14293 Loss: 1.0672345161437988\n",
      "Iteration 14294 Loss: 1.1309146881103516\n",
      "Iteration 14295 Loss: 0.7218111753463745\n",
      "Iteration 14296 Loss: 1.266101360321045\n",
      "Iteration 14297 Loss: 1.1166770458221436\n",
      "Iteration 14298 Loss: 1.0694552659988403\n",
      "Iteration 14299 Loss: 1.1259974241256714\n",
      "Iteration 14299 Loss: 1.0904574394226074\n",
      "Iteration 14300 Loss: 1.267593264579773\n",
      "Iteration 14301 Loss: 1.3629863262176514\n",
      "Iteration 14302 Loss: 1.3319151401519775\n",
      "Iteration 14303 Loss: 0.7439740896224976\n",
      "Iteration 14304 Loss: 1.486431360244751\n",
      "Iteration 14305 Loss: 1.2565970420837402\n",
      "Iteration 14306 Loss: 1.1666693687438965\n",
      "Iteration 14307 Loss: 1.1937857866287231\n",
      "Iteration 14308 Loss: 1.067360281944275\n",
      "Iteration 14309 Loss: 0.87691330909729\n",
      "Iteration 14309 Loss: 1.1754225492477417\n",
      "Iteration 14310 Loss: 1.0935436487197876\n",
      "Iteration 14311 Loss: 0.8723441362380981\n",
      "Iteration 14312 Loss: 0.8519814610481262\n",
      "Iteration 14313 Loss: 1.1878507137298584\n",
      "Iteration 14314 Loss: 1.3550704717636108\n",
      "Iteration 14315 Loss: 1.1644095182418823\n",
      "Iteration 14316 Loss: 1.2060797214508057\n",
      "Iteration 14317 Loss: 1.1852625608444214\n",
      "Iteration 14318 Loss: 1.1555097103118896\n",
      "Iteration 14319 Loss: 0.9647983312606812\n",
      "Iteration 14319 Loss: 1.1036850214004517\n",
      "Iteration 14320 Loss: 1.3928582668304443\n",
      "Iteration 14321 Loss: 1.7285571098327637\n",
      "Iteration 14322 Loss: 1.6004279851913452\n",
      "Iteration 14323 Loss: 1.574666142463684\n",
      "Iteration 14324 Loss: 1.0906728506088257\n",
      "Iteration 14325 Loss: 1.421525001525879\n",
      "Iteration 14326 Loss: 1.2386934757232666\n",
      "Iteration 14327 Loss: 1.3013556003570557\n",
      "Iteration 14328 Loss: 1.15883207321167\n",
      "Iteration 14329 Loss: 1.4158110618591309\n",
      "Iteration 14329 Loss: 1.3923399448394775\n",
      "Iteration 14330 Loss: 1.1343905925750732\n",
      "Iteration 14331 Loss: 1.6227962970733643\n",
      "Iteration 14332 Loss: 1.1082854270935059\n",
      "Iteration 14333 Loss: 1.3949815034866333\n",
      "Iteration 14334 Loss: 1.4629838466644287\n",
      "Iteration 14335 Loss: 1.309053659439087\n",
      "Iteration 14336 Loss: 1.159134864807129\n",
      "Iteration 14337 Loss: 0.8856245279312134\n",
      "Iteration 14338 Loss: 1.7394537925720215\n",
      "Iteration 14339 Loss: 1.149314284324646\n",
      "Iteration 14339 Loss: 1.2966020107269287\n",
      "Iteration 14340 Loss: 0.5409752130508423\n",
      "Iteration 14341 Loss: 1.1412307024002075\n",
      "Iteration 14342 Loss: 1.467334508895874\n",
      "Iteration 14343 Loss: 1.1653938293457031\n",
      "Iteration 14344 Loss: 1.0676078796386719\n",
      "Iteration 14345 Loss: 1.4314032793045044\n",
      "Iteration 14346 Loss: 1.0993539094924927\n",
      "Iteration 14347 Loss: 1.1749361753463745\n",
      "Iteration 14348 Loss: 1.2093089818954468\n",
      "Iteration 14349 Loss: 1.134835124015808\n",
      "Iteration 14349 Loss: 1.1432379484176636\n",
      "Iteration 14350 Loss: 1.443544626235962\n",
      "Iteration 14351 Loss: 1.453762412071228\n",
      "Iteration 14352 Loss: 1.4401966333389282\n",
      "Iteration 14353 Loss: 1.4063348770141602\n",
      "Iteration 14354 Loss: 1.0873643159866333\n",
      "Iteration 14355 Loss: 1.9540410041809082\n",
      "Iteration 14356 Loss: 1.4834965467453003\n",
      "Iteration 14357 Loss: 1.097367286682129\n",
      "Iteration 14358 Loss: 1.3110013008117676\n",
      "Iteration 14359 Loss: 0.7870298624038696\n",
      "Iteration 14359 Loss: 1.3464138507843018\n",
      "Iteration 14360 Loss: 0.7426562309265137\n",
      "Iteration 14361 Loss: 0.8933483958244324\n",
      "Iteration 14362 Loss: 0.8504759073257446\n",
      "Iteration 14363 Loss: 1.2099640369415283\n",
      "Iteration 14364 Loss: 1.515099048614502\n",
      "Iteration 14365 Loss: 0.722673237323761\n",
      "Iteration 14366 Loss: 1.3341981172561646\n",
      "Iteration 14367 Loss: 1.316279411315918\n",
      "Iteration 14368 Loss: 1.469487190246582\n",
      "Iteration 14369 Loss: 1.1919207572937012\n",
      "Iteration 14369 Loss: 1.124610185623169\n",
      "Iteration 14370 Loss: 1.3289153575897217\n",
      "Iteration 14371 Loss: 0.8269224166870117\n",
      "Iteration 14372 Loss: 1.1528875827789307\n",
      "Iteration 14373 Loss: 1.3318078517913818\n",
      "Iteration 14374 Loss: 1.3677974939346313\n",
      "Iteration 14375 Loss: 1.1198290586471558\n",
      "Iteration 14376 Loss: 1.1572437286376953\n",
      "Iteration 14377 Loss: 0.8987300395965576\n",
      "Iteration 14378 Loss: 1.0878071784973145\n",
      "Iteration 14379 Loss: 1.026018738746643\n",
      "Iteration 14379 Loss: 1.129796028137207\n",
      "Iteration 14380 Loss: 0.7237201929092407\n",
      "Iteration 14381 Loss: 1.4267566204071045\n",
      "Iteration 14382 Loss: 1.8068249225616455\n",
      "Iteration 14383 Loss: 1.3055272102355957\n",
      "Iteration 14384 Loss: 1.7602863311767578\n",
      "Iteration 14385 Loss: 1.4124197959899902\n",
      "Iteration 14386 Loss: 1.2574670314788818\n",
      "Iteration 14387 Loss: 1.0432827472686768\n",
      "Iteration 14388 Loss: 1.3063455820083618\n",
      "Iteration 14389 Loss: 1.607948660850525\n",
      "Iteration 14389 Loss: 1.3650578260421753\n",
      "Iteration 14390 Loss: 1.553369402885437\n",
      "Iteration 14391 Loss: 1.2221261262893677\n",
      "Iteration 14392 Loss: 1.3365558385849\n",
      "Iteration 14393 Loss: 1.1154781579971313\n",
      "Iteration 14394 Loss: 1.3134509325027466\n",
      "Iteration 14395 Loss: 1.1661288738250732\n",
      "Iteration 14396 Loss: 1.1318224668502808\n",
      "Iteration 14397 Loss: 1.2830677032470703\n",
      "Iteration 14398 Loss: 1.482386589050293\n",
      "Iteration 14399 Loss: 1.5872540473937988\n",
      "Iteration 14399 Loss: 1.3191640377044678\n",
      "Iteration 14400 Loss: 1.2620030641555786\n",
      "Iteration 14401 Loss: 1.3360856771469116\n",
      "Iteration 14402 Loss: 1.4514743089675903\n",
      "Iteration 14403 Loss: 1.2373400926589966\n",
      "Iteration 14404 Loss: 1.3397700786590576\n",
      "Iteration 14405 Loss: 0.8797907829284668\n",
      "Iteration 14406 Loss: 1.7599267959594727\n",
      "Iteration 14407 Loss: 1.4466348886489868\n",
      "Iteration 14408 Loss: 1.3239805698394775\n",
      "Iteration 14409 Loss: 1.186560034751892\n",
      "Iteration 14409 Loss: 1.3223565816879272\n",
      "Iteration 14410 Loss: 1.0734273195266724\n",
      "Iteration 14411 Loss: 1.413812518119812\n",
      "Iteration 14412 Loss: 1.4383955001831055\n",
      "Iteration 14413 Loss: 1.1461896896362305\n",
      "Iteration 14414 Loss: 0.9099289774894714\n",
      "Iteration 14415 Loss: 1.066440463066101\n",
      "Iteration 14416 Loss: 1.3837722539901733\n",
      "Iteration 14417 Loss: 1.3483128547668457\n",
      "Iteration 14418 Loss: 1.537427306175232\n",
      "Iteration 14419 Loss: 0.979120135307312\n",
      "Iteration 14419 Loss: 1.2296826839447021\n",
      "Iteration 14420 Loss: 1.7379066944122314\n",
      "Iteration 14421 Loss: 1.2021141052246094\n",
      "Iteration 14422 Loss: 0.9332879781723022\n",
      "Iteration 14423 Loss: 1.0102583169937134\n",
      "Iteration 14424 Loss: 1.4574203491210938\n",
      "Iteration 14425 Loss: 0.9727964401245117\n",
      "Iteration 14426 Loss: 1.2550699710845947\n",
      "Iteration 14427 Loss: 1.0677744150161743\n",
      "Iteration 14428 Loss: 1.179279088973999\n",
      "Iteration 14429 Loss: 1.0739227533340454\n",
      "Iteration 14429 Loss: 1.1889830827713013\n",
      "Iteration 14430 Loss: 1.1016912460327148\n",
      "Iteration 14431 Loss: 0.9166272282600403\n",
      "Iteration 14432 Loss: 1.0166606903076172\n",
      "Iteration 14433 Loss: 1.1835330724716187\n",
      "Iteration 14434 Loss: 1.3173729181289673\n",
      "Iteration 14435 Loss: 1.4154292345046997\n",
      "Iteration 14436 Loss: 1.2922207117080688\n",
      "Iteration 14437 Loss: 1.7140494585037231\n",
      "Iteration 14438 Loss: 1.0420743227005005\n",
      "Iteration 14439 Loss: 1.5514148473739624\n",
      "Iteration 14439 Loss: 1.2551074028015137\n",
      "Iteration 14440 Loss: 1.2330514192581177\n",
      "Iteration 14441 Loss: 1.058066964149475\n",
      "Iteration 14442 Loss: 1.5124858617782593\n",
      "Iteration 14443 Loss: 0.8048990964889526\n",
      "Iteration 14444 Loss: 1.0388273000717163\n",
      "Iteration 14445 Loss: 1.2502241134643555\n",
      "Iteration 14446 Loss: 1.2591127157211304\n",
      "Iteration 14447 Loss: 1.123220443725586\n",
      "Iteration 14448 Loss: 0.8570698499679565\n",
      "Iteration 14449 Loss: 0.9367483854293823\n",
      "Iteration 14449 Loss: 1.1073706150054932\n",
      "Iteration 14450 Loss: 1.5446885824203491\n",
      "Iteration 14451 Loss: 1.0066404342651367\n",
      "Iteration 14452 Loss: 1.2274854183197021\n",
      "Iteration 14453 Loss: 1.2588847875595093\n",
      "Iteration 14454 Loss: 1.4143307209014893\n",
      "Iteration 14455 Loss: 0.9706575870513916\n",
      "Iteration 14456 Loss: 1.2859433889389038\n",
      "Iteration 14457 Loss: 1.181078553199768\n",
      "Iteration 14458 Loss: 1.4527406692504883\n",
      "Iteration 14459 Loss: 1.5228826999664307\n",
      "Iteration 14459 Loss: 1.2865331172943115\n",
      "Iteration 14460 Loss: 1.260055422782898\n",
      "Iteration 14461 Loss: 1.0044533014297485\n",
      "Iteration 14462 Loss: 1.3187183141708374\n",
      "Iteration 14463 Loss: 1.3458203077316284\n",
      "Iteration 14464 Loss: 0.723378598690033\n",
      "Iteration 14465 Loss: 0.9688755869865417\n",
      "Iteration 14466 Loss: 1.099058985710144\n",
      "Iteration 14467 Loss: 1.1768414974212646\n",
      "Iteration 14468 Loss: 1.2937343120574951\n",
      "Iteration 14469 Loss: 1.3814513683319092\n",
      "Iteration 14469 Loss: 1.1572388410568237\n",
      "Iteration 14470 Loss: 1.328963041305542\n",
      "Iteration 14471 Loss: 0.9999563097953796\n",
      "Iteration 14472 Loss: 1.4550644159317017\n",
      "Iteration 14473 Loss: 1.4587712287902832\n",
      "Iteration 14474 Loss: 1.1592234373092651\n",
      "Iteration 14475 Loss: 1.2845888137817383\n",
      "Iteration 14476 Loss: 1.210862159729004\n",
      "Iteration 14477 Loss: 1.5583317279815674\n",
      "Iteration 14478 Loss: 1.46785569190979\n",
      "Iteration 14479 Loss: 1.0637311935424805\n",
      "Iteration 14479 Loss: 1.2987347841262817\n",
      "Iteration 14480 Loss: 1.2088634967803955\n",
      "Iteration 14481 Loss: 1.3301774263381958\n",
      "Iteration 14482 Loss: 1.3433713912963867\n",
      "Iteration 14483 Loss: 1.2391072511672974\n",
      "Iteration 14484 Loss: 1.0714788436889648\n",
      "Iteration 14485 Loss: 1.539319396018982\n",
      "Iteration 14486 Loss: 1.038707971572876\n",
      "Iteration 14487 Loss: 1.4215750694274902\n",
      "Iteration 14488 Loss: 1.4531235694885254\n",
      "Iteration 14489 Loss: 1.0121934413909912\n",
      "Iteration 14489 Loss: 1.265791654586792\n",
      "Iteration 14490 Loss: 1.2653515338897705\n",
      "Iteration 14491 Loss: 0.9947601556777954\n",
      "Iteration 14492 Loss: 1.6081793308258057\n",
      "Iteration 14493 Loss: 1.3428298234939575\n",
      "Iteration 14494 Loss: 0.9725068211555481\n",
      "Iteration 14495 Loss: 1.0649453401565552\n",
      "Iteration 14496 Loss: 1.1141709089279175\n",
      "Iteration 14497 Loss: 1.0608372688293457\n",
      "Iteration 14498 Loss: 1.1507281064987183\n",
      "Iteration 14499 Loss: 1.470905065536499\n",
      "Iteration 14499 Loss: 1.2045214176177979\n",
      "Iteration 14500 Loss: 1.2294952869415283\n",
      "Iteration 14501 Loss: 1.4029771089553833\n",
      "Iteration 14502 Loss: 1.2306264638900757\n",
      "Iteration 14503 Loss: 1.1525952816009521\n",
      "Iteration 14504 Loss: 1.7165184020996094\n",
      "Iteration 14505 Loss: 1.2936936616897583\n",
      "Iteration 14506 Loss: 1.0147945880889893\n",
      "Iteration 14507 Loss: 1.5180740356445312\n",
      "Iteration 14508 Loss: 1.6092292070388794\n",
      "Iteration 14509 Loss: 1.141757607460022\n",
      "Iteration 14509 Loss: 1.330976128578186\n",
      "Iteration 14510 Loss: 1.1485836505889893\n",
      "Iteration 14511 Loss: 0.9023013710975647\n",
      "Iteration 14512 Loss: 1.1875579357147217\n",
      "Iteration 14513 Loss: 1.2388311624526978\n",
      "Iteration 14514 Loss: 0.8355206251144409\n",
      "Iteration 14515 Loss: 0.9800245761871338\n",
      "Iteration 14516 Loss: 1.4399749040603638\n",
      "Iteration 14517 Loss: 1.3431339263916016\n",
      "Iteration 14518 Loss: 1.323999047279358\n",
      "Iteration 14519 Loss: 0.7807084321975708\n",
      "Iteration 14519 Loss: 1.1180635690689087\n",
      "Iteration 14520 Loss: 0.9568160772323608\n",
      "Iteration 14521 Loss: 0.9914761185646057\n",
      "Iteration 14522 Loss: 1.1975942850112915\n",
      "Iteration 14523 Loss: 1.6298381090164185\n",
      "Iteration 14524 Loss: 1.159281849861145\n",
      "Iteration 14525 Loss: 1.31374990940094\n",
      "Iteration 14526 Loss: 0.8830975890159607\n",
      "Iteration 14527 Loss: 1.1644057035446167\n",
      "Iteration 14528 Loss: 1.259829044342041\n",
      "Iteration 14529 Loss: 1.1512211561203003\n",
      "Iteration 14529 Loss: 1.1707310676574707\n",
      "Iteration 14530 Loss: 1.4427504539489746\n",
      "Iteration 14531 Loss: 1.2410800457000732\n",
      "Iteration 14532 Loss: 1.0044922828674316\n",
      "Iteration 14533 Loss: 1.1202962398529053\n",
      "Iteration 14534 Loss: 1.4388575553894043\n",
      "Iteration 14535 Loss: 1.519222378730774\n",
      "Iteration 14536 Loss: 1.457907795906067\n",
      "Iteration 14537 Loss: 1.0694183111190796\n",
      "Iteration 14538 Loss: 1.2794809341430664\n",
      "Iteration 14539 Loss: 1.1583471298217773\n",
      "Iteration 14539 Loss: 1.2731852531433105\n",
      "Iteration 14540 Loss: 1.242276668548584\n",
      "Iteration 14541 Loss: 0.8136752247810364\n",
      "Iteration 14542 Loss: 1.1641501188278198\n",
      "Iteration 14543 Loss: 1.126773476600647\n",
      "Iteration 14544 Loss: 1.154145359992981\n",
      "Iteration 14545 Loss: 1.6001887321472168\n",
      "Iteration 14546 Loss: 1.4354559183120728\n",
      "Iteration 14547 Loss: 1.5168838500976562\n",
      "Iteration 14548 Loss: 1.2111927270889282\n",
      "Iteration 14549 Loss: 1.1525379419326782\n",
      "Iteration 14549 Loss: 1.2417280673980713\n",
      "Iteration 14550 Loss: 1.177220344543457\n",
      "Iteration 14551 Loss: 0.9864899516105652\n",
      "Iteration 14552 Loss: 1.3116962909698486\n",
      "Iteration 14553 Loss: 1.436527967453003\n",
      "Iteration 14554 Loss: 1.4370356798171997\n",
      "Iteration 14555 Loss: 1.2691097259521484\n",
      "Iteration 14556 Loss: 1.287316918373108\n",
      "Iteration 14557 Loss: 1.0558199882507324\n",
      "Iteration 14558 Loss: 0.9498360753059387\n",
      "Iteration 14559 Loss: 1.5143036842346191\n",
      "Iteration 14559 Loss: 1.2425358295440674\n",
      "Iteration 14560 Loss: 1.0913481712341309\n",
      "Iteration 14561 Loss: 0.8604077100753784\n",
      "Iteration 14562 Loss: 1.4998494386672974\n",
      "Iteration 14563 Loss: 1.2788183689117432\n",
      "Iteration 14564 Loss: 1.3855036497116089\n",
      "Iteration 14565 Loss: 1.3998292684555054\n",
      "Iteration 14566 Loss: 1.3623484373092651\n",
      "Iteration 14567 Loss: 1.1434844732284546\n",
      "Iteration 14568 Loss: 1.2937891483306885\n",
      "Iteration 14569 Loss: 1.7820161581039429\n",
      "Iteration 14569 Loss: 1.309739351272583\n",
      "Iteration 14570 Loss: 1.043771743774414\n",
      "Iteration 14571 Loss: 1.0844852924346924\n",
      "Iteration 14572 Loss: 1.5556321144104004\n",
      "Iteration 14573 Loss: 1.0311100482940674\n",
      "Iteration 14574 Loss: 1.151038408279419\n",
      "Iteration 14575 Loss: 1.6554025411605835\n",
      "Iteration 14576 Loss: 1.5563631057739258\n",
      "Iteration 14577 Loss: 1.3484740257263184\n",
      "Iteration 14578 Loss: 1.1259342432022095\n",
      "Iteration 14579 Loss: 1.0556931495666504\n",
      "Iteration 14579 Loss: 1.260790467262268\n",
      "Iteration 14580 Loss: 0.8575332760810852\n",
      "Iteration 14581 Loss: 1.58328378200531\n",
      "Iteration 14582 Loss: 1.0348399877548218\n",
      "Iteration 14583 Loss: 1.3529157638549805\n",
      "Iteration 14584 Loss: 1.5067665576934814\n",
      "Iteration 14585 Loss: 1.5071333646774292\n",
      "Iteration 14586 Loss: 1.1196118593215942\n",
      "Iteration 14587 Loss: 1.05075204372406\n",
      "Iteration 14588 Loss: 1.1184085607528687\n",
      "Iteration 14589 Loss: 1.2182286977767944\n",
      "Iteration 14589 Loss: 1.2349474430084229\n",
      "Iteration 14590 Loss: 1.234066367149353\n",
      "Iteration 14591 Loss: 1.2852994203567505\n",
      "Iteration 14592 Loss: 1.2189602851867676\n",
      "Iteration 14593 Loss: 0.7537149786949158\n",
      "Iteration 14594 Loss: 1.0267093181610107\n",
      "Iteration 14595 Loss: 1.033604621887207\n",
      "Iteration 14596 Loss: 1.238166093826294\n",
      "Iteration 14597 Loss: 1.012538194656372\n",
      "Iteration 14598 Loss: 0.96823650598526\n",
      "Iteration 14599 Loss: 1.466080665588379\n",
      "Iteration 14599 Loss: 1.1237375736236572\n",
      "Iteration 14600 Loss: 1.2037572860717773\n",
      "Iteration 14601 Loss: 1.3949947357177734\n",
      "Iteration 14602 Loss: 1.1379342079162598\n",
      "Iteration 14603 Loss: 1.4196113348007202\n",
      "Iteration 14604 Loss: 1.228986144065857\n",
      "Iteration 14605 Loss: 1.315329909324646\n",
      "Iteration 14606 Loss: 1.3929169178009033\n",
      "Iteration 14607 Loss: 0.8181227445602417\n",
      "Iteration 14608 Loss: 0.893742024898529\n",
      "Iteration 14609 Loss: 1.3253819942474365\n",
      "Iteration 14609 Loss: 1.2130776643753052\n",
      "Iteration 14610 Loss: 0.8986348509788513\n",
      "Iteration 14611 Loss: 1.4855875968933105\n",
      "Iteration 14612 Loss: 1.3066805601119995\n",
      "Iteration 14613 Loss: 1.1654210090637207\n",
      "Iteration 14614 Loss: 1.3529824018478394\n",
      "Iteration 14615 Loss: 1.4760149717330933\n",
      "Iteration 14616 Loss: 1.196119785308838\n",
      "Iteration 14617 Loss: 1.0612056255340576\n",
      "Iteration 14618 Loss: 1.2123266458511353\n",
      "Iteration 14619 Loss: 1.1434143781661987\n",
      "Iteration 14619 Loss: 1.2298388481140137\n",
      "Iteration 14620 Loss: 1.0949182510375977\n",
      "Iteration 14621 Loss: 1.401414394378662\n",
      "Iteration 14622 Loss: 1.2043678760528564\n",
      "Iteration 14623 Loss: 0.7612293362617493\n",
      "Iteration 14624 Loss: 0.9903505444526672\n",
      "Iteration 14625 Loss: 1.032327651977539\n",
      "Iteration 14626 Loss: 1.5748515129089355\n",
      "Iteration 14627 Loss: 0.8990172743797302\n",
      "Iteration 14628 Loss: 1.3797911405563354\n",
      "Iteration 14629 Loss: 1.0047616958618164\n",
      "Iteration 14629 Loss: 1.134303092956543\n",
      "Iteration 14630 Loss: 1.0749480724334717\n",
      "Iteration 14631 Loss: 1.3129538297653198\n",
      "Iteration 14632 Loss: 1.245626449584961\n",
      "Iteration 14633 Loss: 0.7708684802055359\n",
      "Iteration 14634 Loss: 1.2770098447799683\n",
      "Iteration 14635 Loss: 0.7135049700737\n",
      "Iteration 14636 Loss: 1.1940290927886963\n",
      "Iteration 14637 Loss: 1.0307073593139648\n",
      "Iteration 14638 Loss: 1.2607932090759277\n",
      "Iteration 14639 Loss: 1.5069490671157837\n",
      "Iteration 14639 Loss: 1.1387388706207275\n",
      "Iteration 14640 Loss: 0.8499411344528198\n",
      "Iteration 14641 Loss: 1.3654141426086426\n",
      "Iteration 14642 Loss: 1.1146026849746704\n",
      "Iteration 14643 Loss: 0.9686354398727417\n",
      "Iteration 14644 Loss: 1.471158504486084\n",
      "Iteration 14645 Loss: 1.4010545015335083\n",
      "Iteration 14646 Loss: 1.2995400428771973\n",
      "Iteration 14647 Loss: 1.3770064115524292\n",
      "Iteration 14648 Loss: 1.278172492980957\n",
      "Iteration 14649 Loss: 1.0501728057861328\n",
      "Iteration 14649 Loss: 1.2175697088241577\n",
      "Iteration 14650 Loss: 1.3339998722076416\n",
      "Iteration 14651 Loss: 0.9179196357727051\n",
      "Iteration 14652 Loss: 1.2252477407455444\n",
      "Iteration 14653 Loss: 0.9348576068878174\n",
      "Iteration 14654 Loss: 0.6486001014709473\n",
      "Iteration 14655 Loss: 1.778989315032959\n",
      "Iteration 14656 Loss: 1.199648141860962\n",
      "Iteration 14657 Loss: 1.3825291395187378\n",
      "Iteration 14658 Loss: 1.1988757848739624\n",
      "Iteration 14659 Loss: 1.4434393644332886\n",
      "Iteration 14659 Loss: 1.2064106464385986\n",
      "Iteration 14660 Loss: 1.5257712602615356\n",
      "Iteration 14661 Loss: 0.9764029383659363\n",
      "Iteration 14662 Loss: 1.2497588396072388\n",
      "Iteration 14663 Loss: 1.6728761196136475\n",
      "Iteration 14664 Loss: 0.9772361516952515\n",
      "Iteration 14665 Loss: 0.97652268409729\n",
      "Iteration 14666 Loss: 1.1970186233520508\n",
      "Iteration 14667 Loss: 1.3257544040679932\n",
      "Iteration 14668 Loss: 1.3902531862258911\n",
      "Iteration 14669 Loss: 0.819412112236023\n",
      "Iteration 14669 Loss: 1.211100459098816\n",
      "Iteration 14670 Loss: 1.0759035348892212\n",
      "Iteration 14671 Loss: 1.1439356803894043\n",
      "Iteration 14672 Loss: 1.1032040119171143\n",
      "Iteration 14673 Loss: 1.1025190353393555\n",
      "Iteration 14674 Loss: 1.3457425832748413\n",
      "Iteration 14675 Loss: 0.8375912308692932\n",
      "Iteration 14676 Loss: 1.0938184261322021\n",
      "Iteration 14677 Loss: 1.1987656354904175\n",
      "Iteration 14678 Loss: 1.4699164628982544\n",
      "Iteration 14679 Loss: 1.4069321155548096\n",
      "Iteration 14679 Loss: 1.177832841873169\n",
      "Iteration 14680 Loss: 1.585750937461853\n",
      "Iteration 14681 Loss: 0.9946855902671814\n",
      "Iteration 14682 Loss: 1.5988250970840454\n",
      "Iteration 14683 Loss: 1.0821051597595215\n",
      "Iteration 14684 Loss: 1.0208467245101929\n",
      "Iteration 14685 Loss: 1.042236328125\n",
      "Iteration 14686 Loss: 0.9304925203323364\n",
      "Iteration 14687 Loss: 1.2018054723739624\n",
      "Iteration 14688 Loss: 1.4824612140655518\n",
      "Iteration 14689 Loss: 1.1069623231887817\n",
      "Iteration 14689 Loss: 1.2046171426773071\n",
      "Iteration 14690 Loss: 0.9340190887451172\n",
      "Iteration 14691 Loss: 1.4658832550048828\n",
      "Iteration 14692 Loss: 1.0435646772384644\n",
      "Iteration 14693 Loss: 1.4701039791107178\n",
      "Iteration 14694 Loss: 1.6961110830307007\n",
      "Iteration 14695 Loss: 1.0092432498931885\n",
      "Iteration 14696 Loss: 1.1555334329605103\n",
      "Iteration 14697 Loss: 1.428934931755066\n",
      "Iteration 14698 Loss: 1.312925100326538\n",
      "Iteration 14699 Loss: 1.120216965675354\n",
      "Iteration 14699 Loss: 1.2636535167694092\n",
      "Iteration 14700 Loss: 1.3938597440719604\n",
      "Iteration 14701 Loss: 0.9042419791221619\n",
      "Iteration 14702 Loss: 0.9276396632194519\n",
      "Iteration 14703 Loss: 1.3509927988052368\n",
      "Iteration 14704 Loss: 1.0482054948806763\n",
      "Iteration 14705 Loss: 0.8751868009567261\n",
      "Iteration 14706 Loss: 1.005070447921753\n",
      "Iteration 14707 Loss: 1.158486247062683\n",
      "Iteration 14708 Loss: 1.127882957458496\n",
      "Iteration 14709 Loss: 0.9150701761245728\n",
      "Iteration 14709 Loss: 1.0706636905670166\n",
      "Iteration 14710 Loss: 1.0594490766525269\n",
      "Iteration 14711 Loss: 1.5404152870178223\n",
      "Iteration 14712 Loss: 1.3587673902511597\n",
      "Iteration 14713 Loss: 1.164612889289856\n",
      "Iteration 14714 Loss: 1.1860400438308716\n",
      "Iteration 14715 Loss: 1.1730575561523438\n",
      "Iteration 14716 Loss: 1.0603628158569336\n",
      "Iteration 14717 Loss: 1.108474612236023\n",
      "Iteration 14718 Loss: 1.449995756149292\n",
      "Iteration 14719 Loss: 1.149062991142273\n",
      "Iteration 14719 Loss: 1.225023865699768\n",
      "Iteration 14720 Loss: 1.4870812892913818\n",
      "Iteration 14721 Loss: 1.190398931503296\n",
      "Iteration 14722 Loss: 1.1736011505126953\n",
      "Iteration 14723 Loss: 1.0488314628601074\n",
      "Iteration 14724 Loss: 1.2364082336425781\n",
      "Iteration 14725 Loss: 1.339835524559021\n",
      "Iteration 14726 Loss: 0.9833288192749023\n",
      "Iteration 14727 Loss: 1.017903208732605\n",
      "Iteration 14728 Loss: 1.406006932258606\n",
      "Iteration 14729 Loss: 1.004022240638733\n",
      "Iteration 14729 Loss: 1.1887418031692505\n",
      "Iteration 14730 Loss: 1.1944561004638672\n",
      "Iteration 14731 Loss: 1.632286787033081\n",
      "Iteration 14732 Loss: 1.0709398984909058\n",
      "Iteration 14733 Loss: 1.2829281091690063\n",
      "Iteration 14734 Loss: 1.3051632642745972\n",
      "Iteration 14735 Loss: 1.202866554260254\n",
      "Iteration 14736 Loss: 1.3262090682983398\n",
      "Iteration 14737 Loss: 1.4369670152664185\n",
      "Iteration 14738 Loss: 1.3776044845581055\n",
      "Iteration 14739 Loss: 1.0665804147720337\n",
      "Iteration 14739 Loss: 1.289600133895874\n",
      "Iteration 14740 Loss: 1.1714588403701782\n",
      "Iteration 14741 Loss: 1.3354445695877075\n",
      "Iteration 14742 Loss: 0.998069703578949\n",
      "Iteration 14743 Loss: 1.0781561136245728\n",
      "Iteration 14744 Loss: 1.245324730873108\n",
      "Iteration 14745 Loss: 0.980164110660553\n",
      "Iteration 14746 Loss: 1.3757421970367432\n",
      "Iteration 14747 Loss: 1.5971870422363281\n",
      "Iteration 14748 Loss: 0.8876347541809082\n",
      "Iteration 14749 Loss: 1.1267389059066772\n",
      "Iteration 14749 Loss: 1.1795920133590698\n",
      "Iteration 14750 Loss: 1.2354583740234375\n",
      "Iteration 14751 Loss: 1.2362010478973389\n",
      "Iteration 14752 Loss: 1.3623322248458862\n",
      "Iteration 14753 Loss: 1.1636202335357666\n",
      "Iteration 14754 Loss: 1.70257568359375\n",
      "Iteration 14755 Loss: 0.9250209927558899\n",
      "Iteration 14756 Loss: 0.7749849557876587\n",
      "Iteration 14757 Loss: 0.9820895195007324\n",
      "Iteration 14758 Loss: 1.3956291675567627\n",
      "Iteration 14759 Loss: 1.4003771543502808\n",
      "Iteration 14759 Loss: 1.2178289890289307\n",
      "Iteration 14760 Loss: 1.0432634353637695\n",
      "Iteration 14761 Loss: 1.637612223625183\n",
      "Iteration 14762 Loss: 0.8267958760261536\n",
      "Iteration 14763 Loss: 1.1988937854766846\n",
      "Iteration 14764 Loss: 1.3273729085922241\n",
      "Iteration 14765 Loss: 1.2613811492919922\n",
      "Iteration 14766 Loss: 0.6958139538764954\n",
      "Iteration 14767 Loss: 1.0817115306854248\n",
      "Iteration 14768 Loss: 1.4407767057418823\n",
      "Iteration 14769 Loss: 1.02714204788208\n",
      "Iteration 14769 Loss: 1.154076337814331\n",
      "Iteration 14770 Loss: 1.1112104654312134\n",
      "Iteration 14771 Loss: 1.352428674697876\n",
      "Iteration 14772 Loss: 1.1840152740478516\n",
      "Iteration 14773 Loss: 1.777251124382019\n",
      "Iteration 14774 Loss: 1.0013333559036255\n",
      "Iteration 14775 Loss: 1.2012423276901245\n",
      "Iteration 14776 Loss: 0.9944956302642822\n",
      "Iteration 14777 Loss: 1.0471447706222534\n",
      "Iteration 14778 Loss: 1.4515725374221802\n",
      "Iteration 14779 Loss: 0.9456177949905396\n",
      "Iteration 14779 Loss: 1.2066311836242676\n",
      "Iteration 14780 Loss: 0.9561532139778137\n",
      "Iteration 14781 Loss: 1.3496485948562622\n",
      "Iteration 14782 Loss: 0.7725139856338501\n",
      "Iteration 14783 Loss: 1.1677370071411133\n",
      "Iteration 14784 Loss: 1.3750637769699097\n",
      "Iteration 14785 Loss: 1.3227059841156006\n",
      "Iteration 14786 Loss: 1.6171200275421143\n",
      "Iteration 14787 Loss: 1.0206856727600098\n",
      "Iteration 14788 Loss: 0.967600405216217\n",
      "Iteration 14789 Loss: 1.2774285078048706\n",
      "Iteration 14789 Loss: 1.1826655864715576\n",
      "Iteration 14790 Loss: 1.3696941137313843\n",
      "Iteration 14791 Loss: 1.0259380340576172\n",
      "Iteration 14792 Loss: 1.2316235303878784\n",
      "Iteration 14793 Loss: 1.6106865406036377\n",
      "Iteration 14794 Loss: 1.0182788372039795\n",
      "Iteration 14795 Loss: 1.0778506994247437\n",
      "Iteration 14796 Loss: 0.8130709528923035\n",
      "Iteration 14797 Loss: 1.308190941810608\n",
      "Iteration 14798 Loss: 1.2780767679214478\n",
      "Iteration 14799 Loss: 1.3720452785491943\n",
      "Iteration 14799 Loss: 1.2105456590652466\n",
      "Iteration 14800 Loss: 1.0216386318206787\n",
      "Iteration 14801 Loss: 0.9185175895690918\n",
      "Iteration 14802 Loss: 1.5262773036956787\n",
      "Iteration 14803 Loss: 1.452828288078308\n",
      "Iteration 14804 Loss: 1.1374390125274658\n",
      "Iteration 14805 Loss: 1.2232413291931152\n",
      "Iteration 14806 Loss: 1.0945302248001099\n",
      "Iteration 14807 Loss: 1.1373255252838135\n",
      "Iteration 14808 Loss: 1.5926430225372314\n",
      "Iteration 14809 Loss: 1.2839850187301636\n",
      "Iteration 14809 Loss: 1.2388426065444946\n",
      "Iteration 14810 Loss: 1.6315302848815918\n",
      "Iteration 14811 Loss: 1.3090639114379883\n",
      "Iteration 14812 Loss: 1.4747393131256104\n",
      "Iteration 14813 Loss: 1.066250205039978\n",
      "Iteration 14814 Loss: 1.3676444292068481\n",
      "Iteration 14815 Loss: 1.3005527257919312\n",
      "Iteration 14816 Loss: 1.1404937505722046\n",
      "Iteration 14817 Loss: 0.9313294887542725\n",
      "Iteration 14818 Loss: 1.5123226642608643\n",
      "Iteration 14819 Loss: 1.3369961977005005\n",
      "Iteration 14819 Loss: 1.3070921897888184\n",
      "Iteration 14820 Loss: 1.1372126340866089\n",
      "Iteration 14821 Loss: 1.3725837469100952\n",
      "Iteration 14822 Loss: 1.3200514316558838\n",
      "Iteration 14823 Loss: 0.9583584070205688\n",
      "Iteration 14824 Loss: 1.2331644296646118\n",
      "Iteration 14825 Loss: 1.0433926582336426\n",
      "Iteration 14826 Loss: 1.1866401433944702\n",
      "Iteration 14827 Loss: 1.2893640995025635\n",
      "Iteration 14828 Loss: 1.0886738300323486\n",
      "Iteration 14829 Loss: 0.9103467464447021\n",
      "Iteration 14829 Loss: 1.153978705406189\n",
      "Iteration 14830 Loss: 1.0429447889328003\n",
      "Iteration 14831 Loss: 0.9463816285133362\n",
      "Iteration 14832 Loss: 1.414050579071045\n",
      "Iteration 14833 Loss: 0.685411810874939\n",
      "Iteration 14834 Loss: 0.8412110805511475\n",
      "Iteration 14835 Loss: 1.626314640045166\n",
      "Iteration 14836 Loss: 1.0758323669433594\n",
      "Iteration 14837 Loss: 1.177939772605896\n",
      "Iteration 14838 Loss: 1.102428674697876\n",
      "Iteration 14839 Loss: 0.8507615327835083\n",
      "Iteration 14839 Loss: 1.0763275623321533\n",
      "Iteration 14840 Loss: 1.6858913898468018\n",
      "Iteration 14841 Loss: 0.8330922722816467\n",
      "Iteration 14842 Loss: 1.1036823987960815\n",
      "Iteration 14843 Loss: 1.5701961517333984\n",
      "Iteration 14844 Loss: 1.3167561292648315\n",
      "Iteration 14845 Loss: 0.8013097047805786\n",
      "Iteration 14846 Loss: 0.8883146047592163\n",
      "Iteration 14847 Loss: 0.8346223831176758\n",
      "Iteration 14848 Loss: 1.4622788429260254\n",
      "Iteration 14849 Loss: 0.9666417241096497\n",
      "Iteration 14849 Loss: 1.1462785005569458\n",
      "Iteration 14850 Loss: 1.5545861721038818\n",
      "Iteration 14851 Loss: 1.6851227283477783\n",
      "Iteration 14852 Loss: 1.0699506998062134\n",
      "Iteration 14853 Loss: 0.8650957942008972\n",
      "Iteration 14854 Loss: 0.9823229312896729\n",
      "Iteration 14855 Loss: 1.6209677457809448\n",
      "Iteration 14856 Loss: 1.4258466958999634\n",
      "Iteration 14857 Loss: 1.1862223148345947\n",
      "Iteration 14858 Loss: 1.0030572414398193\n",
      "Iteration 14859 Loss: 1.3453418016433716\n",
      "Iteration 14859 Loss: 1.2738513946533203\n",
      "Iteration 14860 Loss: 1.1214523315429688\n",
      "Iteration 14861 Loss: 1.288548469543457\n",
      "Iteration 14862 Loss: 0.8455957770347595\n",
      "Iteration 14863 Loss: 1.4860405921936035\n",
      "Iteration 14864 Loss: 1.3124545812606812\n",
      "Iteration 14865 Loss: 1.1600816249847412\n",
      "Iteration 14866 Loss: 1.723291277885437\n",
      "Iteration 14867 Loss: 1.5692673921585083\n",
      "Iteration 14868 Loss: 0.9659591913223267\n",
      "Iteration 14869 Loss: 1.2034708261489868\n",
      "Iteration 14869 Loss: 1.2676161527633667\n",
      "Iteration 14870 Loss: 1.0779236555099487\n",
      "Iteration 14871 Loss: 1.031713843345642\n",
      "Iteration 14872 Loss: 1.4140746593475342\n",
      "Iteration 14873 Loss: 1.5893629789352417\n",
      "Iteration 14874 Loss: 0.8481948375701904\n",
      "Iteration 14875 Loss: 1.319280982017517\n",
      "Iteration 14876 Loss: 1.342989444732666\n",
      "Iteration 14877 Loss: 1.5578720569610596\n",
      "Iteration 14878 Loss: 1.1907455921173096\n",
      "Iteration 14879 Loss: 1.3385281562805176\n",
      "Iteration 14879 Loss: 1.2710686922073364\n",
      "Iteration 14880 Loss: 1.144095778465271\n",
      "Iteration 14881 Loss: 1.3737139701843262\n",
      "Iteration 14882 Loss: 1.0597898960113525\n",
      "Iteration 14883 Loss: 1.2757771015167236\n",
      "Iteration 14884 Loss: 1.3047574758529663\n",
      "Iteration 14885 Loss: 0.9293240308761597\n",
      "Iteration 14886 Loss: 1.083730936050415\n",
      "Iteration 14887 Loss: 0.8005291819572449\n",
      "Iteration 14888 Loss: 1.273099422454834\n",
      "Iteration 14889 Loss: 1.111328125\n",
      "Iteration 14889 Loss: 1.1356146335601807\n",
      "Iteration 14890 Loss: 1.1037335395812988\n",
      "Iteration 14891 Loss: 1.0499827861785889\n",
      "Iteration 14892 Loss: 1.1666525602340698\n",
      "Iteration 14893 Loss: 1.107485294342041\n",
      "Iteration 14894 Loss: 1.0811936855316162\n",
      "Iteration 14895 Loss: 1.719681978225708\n",
      "Iteration 14896 Loss: 1.2985178232192993\n",
      "Iteration 14897 Loss: 1.1678532361984253\n",
      "Iteration 14898 Loss: 0.9070807695388794\n",
      "Iteration 14899 Loss: 1.0733203887939453\n",
      "Iteration 14899 Loss: 1.1675502061843872\n",
      "Iteration 14900 Loss: 1.2075644731521606\n",
      "Iteration 14901 Loss: 1.0645030736923218\n",
      "Iteration 14902 Loss: 1.5600014925003052\n",
      "Iteration 14903 Loss: 1.5738755464553833\n",
      "Iteration 14904 Loss: 0.8648774027824402\n",
      "Iteration 14905 Loss: 1.1227911710739136\n",
      "Iteration 14906 Loss: 1.1481842994689941\n",
      "Iteration 14907 Loss: 0.8193261027336121\n",
      "Iteration 14908 Loss: 0.9154734015464783\n",
      "Iteration 14909 Loss: 1.1142504215240479\n",
      "Iteration 14909 Loss: 1.139084815979004\n",
      "Iteration 14910 Loss: 1.7049061059951782\n",
      "Iteration 14911 Loss: 1.4726547002792358\n",
      "Iteration 14912 Loss: 1.1612536907196045\n",
      "Iteration 14913 Loss: 1.3387279510498047\n",
      "Iteration 14914 Loss: 0.8147934675216675\n",
      "Iteration 14915 Loss: 1.1391105651855469\n",
      "Iteration 14916 Loss: 1.0347800254821777\n",
      "Iteration 14917 Loss: 1.5567352771759033\n",
      "Iteration 14918 Loss: 1.376129388809204\n",
      "Iteration 14919 Loss: 1.1044381856918335\n",
      "Iteration 14919 Loss: 1.270352840423584\n",
      "Iteration 14920 Loss: 1.2420759201049805\n",
      "Iteration 14921 Loss: 1.2301914691925049\n",
      "Iteration 14922 Loss: 0.9025399088859558\n",
      "Iteration 14923 Loss: 0.9847520589828491\n",
      "Iteration 14924 Loss: 1.2571427822113037\n",
      "Iteration 14925 Loss: 1.355157494544983\n",
      "Iteration 14926 Loss: 0.8711291551589966\n",
      "Iteration 14927 Loss: 1.274123191833496\n",
      "Iteration 14928 Loss: 0.933261513710022\n",
      "Iteration 14929 Loss: 0.8808554410934448\n",
      "Iteration 14929 Loss: 1.093122959136963\n",
      "Iteration 14930 Loss: 1.379659652709961\n",
      "Iteration 14931 Loss: 1.3964803218841553\n",
      "Iteration 14932 Loss: 1.4692447185516357\n",
      "Iteration 14933 Loss: 1.5550014972686768\n",
      "Iteration 14934 Loss: 1.0566658973693848\n",
      "Iteration 14935 Loss: 1.3498921394348145\n",
      "Iteration 14936 Loss: 1.2091374397277832\n",
      "Iteration 14937 Loss: 1.0325841903686523\n",
      "Iteration 14938 Loss: 0.9663891792297363\n",
      "Iteration 14939 Loss: 0.8801828622817993\n",
      "Iteration 14939 Loss: 1.229523777961731\n",
      "Iteration 14940 Loss: 1.2013283967971802\n",
      "Iteration 14941 Loss: 1.43771493434906\n",
      "Iteration 14942 Loss: 1.3991718292236328\n",
      "Iteration 14943 Loss: 1.5358777046203613\n",
      "Iteration 14944 Loss: 1.4351993799209595\n",
      "Iteration 14945 Loss: 1.2950687408447266\n",
      "Iteration 14946 Loss: 1.5914666652679443\n",
      "Iteration 14947 Loss: 0.9795767664909363\n",
      "Iteration 14948 Loss: 0.959242045879364\n",
      "Iteration 14949 Loss: 1.398851752281189\n",
      "Iteration 14949 Loss: 1.3233498334884644\n",
      "Iteration 14950 Loss: 1.0457490682601929\n",
      "Iteration 14951 Loss: 1.3604182004928589\n",
      "Iteration 14952 Loss: 1.5901319980621338\n",
      "Iteration 14953 Loss: 0.9604403376579285\n",
      "Iteration 14954 Loss: 1.1706815958023071\n",
      "Iteration 14955 Loss: 1.697830319404602\n",
      "Iteration 14956 Loss: 0.9563359022140503\n",
      "Iteration 14957 Loss: 1.1146553754806519\n",
      "Iteration 14958 Loss: 1.583495855331421\n",
      "Iteration 14959 Loss: 0.7701681852340698\n",
      "Iteration 14959 Loss: 1.2249906063079834\n",
      "Iteration 14960 Loss: 1.09439218044281\n",
      "Iteration 14961 Loss: 1.2715789079666138\n",
      "Iteration 14962 Loss: 1.1524325609207153\n",
      "Iteration 14963 Loss: 1.3823554515838623\n",
      "Iteration 14964 Loss: 1.1791821718215942\n",
      "Iteration 14965 Loss: 1.1058475971221924\n",
      "Iteration 14966 Loss: 0.9884426593780518\n",
      "Iteration 14967 Loss: 1.6124136447906494\n",
      "Iteration 14968 Loss: 1.512705683708191\n",
      "Iteration 14969 Loss: 0.9997686743736267\n",
      "Iteration 14969 Loss: 1.2299118041992188\n",
      "Iteration 14970 Loss: 1.0537946224212646\n",
      "Iteration 14971 Loss: 1.1833245754241943\n",
      "Iteration 14972 Loss: 0.9886370897293091\n",
      "Iteration 14973 Loss: 1.4563381671905518\n",
      "Iteration 14974 Loss: 1.0875670909881592\n",
      "Iteration 14975 Loss: 1.3788233995437622\n",
      "Iteration 14976 Loss: 1.238019585609436\n",
      "Iteration 14977 Loss: 1.384777307510376\n",
      "Iteration 14978 Loss: 1.005960464477539\n",
      "Iteration 14979 Loss: 1.1857705116271973\n",
      "Iteration 14979 Loss: 1.1963012218475342\n",
      "Iteration 14980 Loss: 1.2417457103729248\n",
      "Iteration 14981 Loss: 0.9558202028274536\n",
      "Iteration 14982 Loss: 1.1413140296936035\n",
      "Iteration 14983 Loss: 1.164615273475647\n",
      "Iteration 14984 Loss: 1.1516493558883667\n",
      "Iteration 14985 Loss: 1.4971083402633667\n",
      "Iteration 14986 Loss: 0.9146455526351929\n",
      "Iteration 14987 Loss: 1.0409951210021973\n",
      "Iteration 14988 Loss: 1.0957443714141846\n",
      "Iteration 14989 Loss: 1.1544734239578247\n",
      "Iteration 14989 Loss: 1.1358110904693604\n",
      "Iteration 14990 Loss: 1.0823240280151367\n",
      "Iteration 14991 Loss: 1.1585966348648071\n",
      "Iteration 14992 Loss: 1.1246485710144043\n",
      "Iteration 14993 Loss: 1.3125133514404297\n",
      "Iteration 14994 Loss: 1.5805023908615112\n",
      "Iteration 14995 Loss: 0.9374556541442871\n",
      "Iteration 14996 Loss: 1.1792508363723755\n",
      "Iteration 14997 Loss: 1.0851225852966309\n",
      "Iteration 14998 Loss: 1.0896687507629395\n",
      "Iteration 14999 Loss: 1.0870870351791382\n",
      "Iteration 14999 Loss: 1.1637170314788818\n",
      "Iteration 15000 Loss: 1.4533740282058716\n",
      "Iteration 15001 Loss: 1.2252073287963867\n",
      "Iteration 15002 Loss: 1.4282981157302856\n",
      "Iteration 15003 Loss: 1.7433252334594727\n",
      "Iteration 15004 Loss: 1.230283260345459\n",
      "Iteration 15005 Loss: 1.128420352935791\n",
      "Iteration 15006 Loss: 1.153860092163086\n",
      "Iteration 15007 Loss: 1.1821894645690918\n",
      "Iteration 15008 Loss: 1.372362494468689\n",
      "Iteration 15009 Loss: 1.128524661064148\n",
      "Iteration 15009 Loss: 1.3045845031738281\n",
      "Iteration 15010 Loss: 1.2339158058166504\n",
      "Iteration 15011 Loss: 1.2188924551010132\n",
      "Iteration 15012 Loss: 0.9916369318962097\n",
      "Iteration 15013 Loss: 1.4516706466674805\n",
      "Iteration 15014 Loss: 0.7799661159515381\n",
      "Iteration 15015 Loss: 1.2863637208938599\n",
      "Iteration 15016 Loss: 1.2956218719482422\n",
      "Iteration 15017 Loss: 1.3740664720535278\n",
      "Iteration 15018 Loss: 1.3088706731796265\n",
      "Iteration 15019 Loss: 1.0913678407669067\n",
      "Iteration 15019 Loss: 1.2032372951507568\n",
      "Iteration 15020 Loss: 1.1372219324111938\n",
      "Iteration 15021 Loss: 1.3086574077606201\n",
      "Iteration 15022 Loss: 1.0562320947647095\n",
      "Iteration 15023 Loss: 1.3774291276931763\n",
      "Iteration 15024 Loss: 1.5787649154663086\n",
      "Iteration 15025 Loss: 1.1536463499069214\n",
      "Iteration 15026 Loss: 1.342518925666809\n",
      "Iteration 15027 Loss: 1.229690432548523\n",
      "Iteration 15028 Loss: 1.4548903703689575\n",
      "Iteration 15029 Loss: 1.1204451322555542\n",
      "Iteration 15029 Loss: 1.2759495973587036\n",
      "Iteration 15030 Loss: 1.6977717876434326\n",
      "Iteration 15031 Loss: 1.0502407550811768\n",
      "Iteration 15032 Loss: 1.2030277252197266\n",
      "Iteration 15033 Loss: 1.6586596965789795\n",
      "Iteration 15034 Loss: 0.8617107272148132\n",
      "Iteration 15035 Loss: 1.5983070135116577\n",
      "Iteration 15036 Loss: 1.407413363456726\n",
      "Iteration 15037 Loss: 0.9246360063552856\n",
      "Iteration 15038 Loss: 1.5098774433135986\n",
      "Iteration 15039 Loss: 1.3026306629180908\n",
      "Iteration 15039 Loss: 1.3214274644851685\n",
      "Iteration 15040 Loss: 1.2805347442626953\n",
      "Iteration 15041 Loss: 1.1777397394180298\n",
      "Iteration 15042 Loss: 1.3184814453125\n",
      "Iteration 15043 Loss: 0.8109737038612366\n",
      "Iteration 15044 Loss: 1.320737600326538\n",
      "Iteration 15045 Loss: 0.844275951385498\n",
      "Iteration 15046 Loss: 1.1351267099380493\n",
      "Iteration 15047 Loss: 1.177223801612854\n",
      "Iteration 15048 Loss: 1.3714390993118286\n",
      "Iteration 15049 Loss: 1.22081458568573\n",
      "Iteration 15049 Loss: 1.165734887123108\n",
      "Iteration 15050 Loss: 1.1984047889709473\n",
      "Iteration 15051 Loss: 1.0463422536849976\n",
      "Iteration 15052 Loss: 0.9973978400230408\n",
      "Iteration 15053 Loss: 1.1498440504074097\n",
      "Iteration 15054 Loss: 0.8106284141540527\n",
      "Iteration 15055 Loss: 1.6462050676345825\n",
      "Iteration 15056 Loss: 0.9956969022750854\n",
      "Iteration 15057 Loss: 1.044251561164856\n",
      "Iteration 15058 Loss: 1.5999493598937988\n",
      "Iteration 15059 Loss: 0.907004714012146\n",
      "Iteration 15059 Loss: 1.1395725011825562\n",
      "Iteration 15060 Loss: 1.084713101387024\n",
      "Iteration 15061 Loss: 1.4083067178726196\n",
      "Iteration 15062 Loss: 1.4050883054733276\n",
      "Iteration 15063 Loss: 1.1263940334320068\n",
      "Iteration 15064 Loss: 1.3909356594085693\n",
      "Iteration 15065 Loss: 0.7748905420303345\n",
      "Iteration 15066 Loss: 1.3609923124313354\n",
      "Iteration 15067 Loss: 1.1314867734909058\n",
      "Iteration 15068 Loss: 1.6205525398254395\n",
      "Iteration 15069 Loss: 1.0351835489273071\n",
      "Iteration 15069 Loss: 1.2338544130325317\n",
      "Iteration 15070 Loss: 1.3162083625793457\n",
      "Iteration 15071 Loss: 1.2223758697509766\n",
      "Iteration 15072 Loss: 1.3480838537216187\n",
      "Iteration 15073 Loss: 1.2712637186050415\n",
      "Iteration 15074 Loss: 0.9520352482795715\n",
      "Iteration 15075 Loss: 1.2843669652938843\n",
      "Iteration 15076 Loss: 1.1754882335662842\n",
      "Iteration 15077 Loss: 1.4394534826278687\n",
      "Iteration 15078 Loss: 1.2780768871307373\n",
      "Iteration 15079 Loss: 0.8338260650634766\n",
      "Iteration 15079 Loss: 1.2121177911758423\n",
      "Iteration 15080 Loss: 1.6696008443832397\n",
      "Iteration 15081 Loss: 1.6725146770477295\n",
      "Iteration 15082 Loss: 1.045182228088379\n",
      "Iteration 15083 Loss: 1.1859314441680908\n",
      "Iteration 15084 Loss: 0.9797430634498596\n",
      "Iteration 15085 Loss: 1.270574688911438\n",
      "Iteration 15086 Loss: 1.1528725624084473\n",
      "Iteration 15087 Loss: 1.001607060432434\n",
      "Iteration 15088 Loss: 1.0723917484283447\n",
      "Iteration 15089 Loss: 1.0696678161621094\n",
      "Iteration 15089 Loss: 1.2120085954666138\n",
      "Iteration 15090 Loss: 1.2240175008773804\n",
      "Iteration 15091 Loss: 1.1343367099761963\n",
      "Iteration 15092 Loss: 1.2808380126953125\n",
      "Iteration 15093 Loss: 1.1390891075134277\n",
      "Iteration 15094 Loss: 1.1371830701828003\n",
      "Iteration 15095 Loss: 0.8830777406692505\n",
      "Iteration 15096 Loss: 1.417947769165039\n",
      "Iteration 15097 Loss: 1.262489914894104\n",
      "Iteration 15098 Loss: 1.4426368474960327\n",
      "Iteration 15099 Loss: 1.1644638776779175\n",
      "Iteration 15099 Loss: 1.2086080312728882\n",
      "Iteration 15100 Loss: 0.8393632173538208\n",
      "Iteration 15101 Loss: 1.2697895765304565\n",
      "Iteration 15102 Loss: 1.0594226121902466\n",
      "Iteration 15103 Loss: 0.925058901309967\n",
      "Iteration 15104 Loss: 0.8692917823791504\n",
      "Iteration 15105 Loss: 1.061614990234375\n",
      "Iteration 15106 Loss: 1.7649972438812256\n",
      "Iteration 15107 Loss: 1.3980627059936523\n",
      "Iteration 15108 Loss: 0.9535930752754211\n",
      "Iteration 15109 Loss: 0.9995238780975342\n",
      "Iteration 15109 Loss: 1.1140717267990112\n",
      "Iteration 15110 Loss: 1.0765951871871948\n",
      "Iteration 15111 Loss: 1.2400624752044678\n",
      "Iteration 15112 Loss: 1.2254637479782104\n",
      "Iteration 15113 Loss: 1.2607237100601196\n",
      "Iteration 15114 Loss: 1.6871267557144165\n",
      "Iteration 15115 Loss: 0.9604220986366272\n",
      "Iteration 15116 Loss: 0.9267939329147339\n",
      "Iteration 15117 Loss: 1.4965696334838867\n",
      "Iteration 15118 Loss: 1.1797399520874023\n",
      "Iteration 15119 Loss: 1.0658005475997925\n",
      "Iteration 15119 Loss: 1.2119299173355103\n",
      "Iteration 15120 Loss: 1.5730854272842407\n",
      "Iteration 15121 Loss: 0.9461718797683716\n",
      "Iteration 15122 Loss: 1.4224976301193237\n",
      "Iteration 15123 Loss: 1.293924331665039\n",
      "Iteration 15124 Loss: 0.9039103984832764\n",
      "Iteration 15125 Loss: 1.1102856397628784\n",
      "Iteration 15126 Loss: 1.2280972003936768\n",
      "Iteration 15127 Loss: 0.86872398853302\n",
      "Iteration 15128 Loss: 1.0809067487716675\n",
      "Iteration 15129 Loss: 1.4286057949066162\n",
      "Iteration 15129 Loss: 1.185620903968811\n",
      "Iteration 15130 Loss: 1.556553840637207\n",
      "Iteration 15131 Loss: 0.6764540672302246\n",
      "Iteration 15132 Loss: 0.9386259317398071\n",
      "Iteration 15133 Loss: 0.8660992383956909\n",
      "Iteration 15134 Loss: 1.4337090253829956\n",
      "Iteration 15135 Loss: 1.1951030492782593\n",
      "Iteration 15136 Loss: 1.2020440101623535\n",
      "Iteration 15137 Loss: 1.047478199005127\n",
      "Iteration 15138 Loss: 0.8599284291267395\n",
      "Iteration 15139 Loss: 0.939213752746582\n",
      "Iteration 15139 Loss: 1.0715210437774658\n",
      "Iteration 15140 Loss: 1.0548748970031738\n",
      "Iteration 15141 Loss: 1.347792387008667\n",
      "Iteration 15142 Loss: 1.0263876914978027\n",
      "Iteration 15143 Loss: 1.197097897529602\n",
      "Iteration 15144 Loss: 0.9338408708572388\n",
      "Iteration 15145 Loss: 1.3169147968292236\n",
      "Iteration 15146 Loss: 0.8764547109603882\n",
      "Iteration 15147 Loss: 0.9837367534637451\n",
      "Iteration 15148 Loss: 0.838772177696228\n",
      "Iteration 15149 Loss: 1.0685513019561768\n",
      "Iteration 15149 Loss: 1.0644423961639404\n",
      "Iteration 15150 Loss: 1.339324712753296\n",
      "Iteration 15151 Loss: 1.1255265474319458\n",
      "Iteration 15152 Loss: 1.2502833604812622\n",
      "Iteration 15153 Loss: 1.1450252532958984\n",
      "Iteration 15154 Loss: 1.245092511177063\n",
      "Iteration 15155 Loss: 1.4834246635437012\n",
      "Iteration 15156 Loss: 1.4696123600006104\n",
      "Iteration 15157 Loss: 1.4869227409362793\n",
      "Iteration 15158 Loss: 1.6037198305130005\n",
      "Iteration 15159 Loss: 0.9464260935783386\n",
      "Iteration 15159 Loss: 1.3095357418060303\n",
      "Iteration 15160 Loss: 0.9028291702270508\n",
      "Iteration 15161 Loss: 1.4527947902679443\n",
      "Iteration 15162 Loss: 1.1547971963882446\n",
      "Iteration 15163 Loss: 0.9769955277442932\n",
      "Iteration 15164 Loss: 1.2715060710906982\n",
      "Iteration 15165 Loss: 0.9501049518585205\n",
      "Iteration 15166 Loss: 1.2025352716445923\n",
      "Iteration 15167 Loss: 1.2778855562210083\n",
      "Iteration 15168 Loss: 1.4010968208312988\n",
      "Iteration 15169 Loss: 1.3368122577667236\n",
      "Iteration 15169 Loss: 1.1927357912063599\n",
      "Iteration 15170 Loss: 1.3461421728134155\n",
      "Iteration 15171 Loss: 1.194338083267212\n",
      "Iteration 15172 Loss: 0.9223047494888306\n",
      "Iteration 15173 Loss: 1.1483032703399658\n",
      "Iteration 15174 Loss: 1.7337400913238525\n",
      "Iteration 15175 Loss: 1.0096806287765503\n",
      "Iteration 15176 Loss: 1.3840221166610718\n",
      "Iteration 15177 Loss: 1.089964509010315\n",
      "Iteration 15178 Loss: 1.190488576889038\n",
      "Iteration 15179 Loss: 0.8149695992469788\n",
      "Iteration 15179 Loss: 1.1833953857421875\n",
      "Iteration 15180 Loss: 1.4152101278305054\n",
      "Iteration 15181 Loss: 1.0030131340026855\n",
      "Iteration 15182 Loss: 1.5285650491714478\n",
      "Iteration 15183 Loss: 0.7632324695587158\n",
      "Iteration 15184 Loss: 1.0325217247009277\n",
      "Iteration 15185 Loss: 1.270511507987976\n",
      "Iteration 15186 Loss: 1.5325329303741455\n",
      "Iteration 15187 Loss: 1.0706803798675537\n",
      "Iteration 15188 Loss: 1.108627438545227\n",
      "Iteration 15189 Loss: 1.1854925155639648\n",
      "Iteration 15189 Loss: 1.1910388469696045\n",
      "Iteration 15190 Loss: 1.0493369102478027\n",
      "Iteration 15191 Loss: 1.0728044509887695\n",
      "Iteration 15192 Loss: 1.3050252199172974\n",
      "Iteration 15193 Loss: 0.5945740938186646\n",
      "Iteration 15194 Loss: 1.2406158447265625\n",
      "Iteration 15195 Loss: 1.2259278297424316\n",
      "Iteration 15196 Loss: 1.19989013671875\n",
      "Iteration 15197 Loss: 1.0435539484024048\n",
      "Iteration 15198 Loss: 1.1065027713775635\n",
      "Iteration 15199 Loss: 0.995826244354248\n",
      "Iteration 15199 Loss: 1.0834057331085205\n",
      "Iteration 15200 Loss: 1.4681339263916016\n",
      "Iteration 15201 Loss: 1.2954715490341187\n",
      "Iteration 15202 Loss: 1.296136498451233\n",
      "Iteration 15203 Loss: 1.1532889604568481\n",
      "Iteration 15204 Loss: 1.2232195138931274\n",
      "Iteration 15205 Loss: 1.1814044713974\n",
      "Iteration 15206 Loss: 1.3004655838012695\n",
      "Iteration 15207 Loss: 1.7934949398040771\n",
      "Iteration 15208 Loss: 1.4208918809890747\n",
      "Iteration 15209 Loss: 1.1615666151046753\n",
      "Iteration 15209 Loss: 1.3294074535369873\n",
      "Iteration 15210 Loss: 1.5679131746292114\n",
      "Iteration 15211 Loss: 1.4922547340393066\n",
      "Iteration 15212 Loss: 1.377346158027649\n",
      "Iteration 15213 Loss: 1.4792146682739258\n",
      "Iteration 15214 Loss: 1.3197623491287231\n",
      "Iteration 15215 Loss: 1.2960898876190186\n",
      "Iteration 15216 Loss: 1.2954260110855103\n",
      "Iteration 15217 Loss: 1.431586742401123\n",
      "Iteration 15218 Loss: 0.9907702803611755\n",
      "Iteration 15219 Loss: 1.2859867811203003\n",
      "Iteration 15219 Loss: 1.3536350727081299\n",
      "Iteration 15220 Loss: 1.4487587213516235\n",
      "Iteration 15221 Loss: 1.3555065393447876\n",
      "Iteration 15222 Loss: 1.4357765913009644\n",
      "Iteration 15223 Loss: 1.4279685020446777\n",
      "Iteration 15224 Loss: 1.0423916578292847\n",
      "Iteration 15225 Loss: 1.124039649963379\n",
      "Iteration 15226 Loss: 0.9184166193008423\n",
      "Iteration 15227 Loss: 1.391243577003479\n",
      "Iteration 15228 Loss: 1.1695051193237305\n",
      "Iteration 15229 Loss: 1.2683809995651245\n",
      "Iteration 15229 Loss: 1.258198857307434\n",
      "Iteration 15230 Loss: 1.2364211082458496\n",
      "Iteration 15231 Loss: 1.6119712591171265\n",
      "Iteration 15232 Loss: 1.2590168714523315\n",
      "Iteration 15233 Loss: 1.2098864316940308\n",
      "Iteration 15234 Loss: 1.0977582931518555\n",
      "Iteration 15235 Loss: 1.0015900135040283\n",
      "Iteration 15236 Loss: 1.2362358570098877\n",
      "Iteration 15237 Loss: 0.7936071753501892\n",
      "Iteration 15238 Loss: 0.7950816750526428\n",
      "Iteration 15239 Loss: 1.1977719068527222\n",
      "Iteration 15239 Loss: 1.1439340114593506\n",
      "Iteration 15240 Loss: 1.3334482908248901\n",
      "Iteration 15241 Loss: 1.1250360012054443\n",
      "Iteration 15242 Loss: 1.3399813175201416\n",
      "Iteration 15243 Loss: 1.4137450456619263\n",
      "Iteration 15244 Loss: 1.2778444290161133\n",
      "Iteration 15245 Loss: 0.9905514717102051\n",
      "Iteration 15246 Loss: 0.8863875269889832\n",
      "Iteration 15247 Loss: 1.3312084674835205\n",
      "Iteration 15248 Loss: 1.3020367622375488\n",
      "Iteration 15249 Loss: 1.53026282787323\n",
      "Iteration 15249 Loss: 1.2530502080917358\n",
      "Iteration 15250 Loss: 1.1806690692901611\n",
      "Iteration 15251 Loss: 1.1143192052841187\n",
      "Iteration 15252 Loss: 0.7236024141311646\n",
      "Iteration 15253 Loss: 0.6092242002487183\n",
      "Iteration 15254 Loss: 1.3391081094741821\n",
      "Iteration 15255 Loss: 1.0496931076049805\n",
      "Iteration 15256 Loss: 1.156183123588562\n",
      "Iteration 15257 Loss: 0.9011346101760864\n",
      "Iteration 15258 Loss: 1.0670238733291626\n",
      "Iteration 15259 Loss: 1.182663917541504\n",
      "Iteration 15259 Loss: 1.0323622226715088\n",
      "Iteration 15260 Loss: 1.404153823852539\n",
      "Iteration 15261 Loss: 1.0398930311203003\n",
      "Iteration 15262 Loss: 0.9427531361579895\n",
      "Iteration 15263 Loss: 1.4301012754440308\n",
      "Iteration 15264 Loss: 1.057305932044983\n",
      "Iteration 15265 Loss: 1.406516671180725\n",
      "Iteration 15266 Loss: 1.234419584274292\n",
      "Iteration 15267 Loss: 1.32455313205719\n",
      "Iteration 15268 Loss: 1.4796887636184692\n",
      "Iteration 15269 Loss: 1.069826364517212\n",
      "Iteration 15269 Loss: 1.2389214038848877\n",
      "Iteration 15270 Loss: 1.4070273637771606\n",
      "Iteration 15271 Loss: 1.2320936918258667\n",
      "Iteration 15272 Loss: 1.2580024003982544\n",
      "Iteration 15273 Loss: 1.6447231769561768\n",
      "Iteration 15274 Loss: 0.9153764247894287\n",
      "Iteration 15275 Loss: 1.3460161685943604\n",
      "Iteration 15276 Loss: 1.1593704223632812\n",
      "Iteration 15277 Loss: 1.1644841432571411\n",
      "Iteration 15278 Loss: 1.1790391206741333\n",
      "Iteration 15279 Loss: 1.5608875751495361\n",
      "Iteration 15279 Loss: 1.286702036857605\n",
      "Iteration 15280 Loss: 1.043622374534607\n",
      "Iteration 15281 Loss: 1.3643988370895386\n",
      "Iteration 15282 Loss: 1.0307527780532837\n",
      "Iteration 15283 Loss: 0.9861478805541992\n",
      "Iteration 15284 Loss: 1.2448500394821167\n",
      "Iteration 15285 Loss: 1.2717764377593994\n",
      "Iteration 15286 Loss: 1.1511608362197876\n",
      "Iteration 15287 Loss: 1.5688185691833496\n",
      "Iteration 15288 Loss: 1.1451950073242188\n",
      "Iteration 15289 Loss: 0.7426052689552307\n",
      "Iteration 15289 Loss: 1.1549327373504639\n",
      "Iteration 15290 Loss: 1.0478519201278687\n",
      "Iteration 15291 Loss: 1.005113959312439\n",
      "Iteration 15292 Loss: 0.9197958707809448\n",
      "Iteration 15293 Loss: 1.185625672340393\n",
      "Iteration 15294 Loss: 1.5808238983154297\n",
      "Iteration 15295 Loss: 1.3458383083343506\n",
      "Iteration 15296 Loss: 1.1591596603393555\n",
      "Iteration 15297 Loss: 1.268466830253601\n",
      "Iteration 15298 Loss: 0.9409551620483398\n",
      "Iteration 15299 Loss: 1.0145974159240723\n",
      "Iteration 15299 Loss: 1.1468229293823242\n",
      "Iteration 15300 Loss: 1.2362405061721802\n",
      "Iteration 15301 Loss: 1.0890259742736816\n",
      "Iteration 15302 Loss: 1.3057985305786133\n",
      "Iteration 15303 Loss: 1.3964426517486572\n",
      "Iteration 15304 Loss: 1.3809925317764282\n",
      "Iteration 15305 Loss: 0.8600017428398132\n",
      "Iteration 15306 Loss: 1.1652405261993408\n",
      "Iteration 15307 Loss: 1.102726936340332\n",
      "Iteration 15308 Loss: 1.4073641300201416\n",
      "Iteration 15309 Loss: 0.8801061511039734\n",
      "Iteration 15309 Loss: 1.1823939085006714\n",
      "Iteration 15310 Loss: 1.3167953491210938\n",
      "Iteration 15311 Loss: 1.4240097999572754\n",
      "Iteration 15312 Loss: 1.160688042640686\n",
      "Iteration 15313 Loss: 1.2557899951934814\n",
      "Iteration 15314 Loss: 1.0730267763137817\n",
      "Iteration 15315 Loss: 0.6638544201850891\n",
      "Iteration 15316 Loss: 1.3165210485458374\n",
      "Iteration 15317 Loss: 1.3715407848358154\n",
      "Iteration 15318 Loss: 0.6590785384178162\n",
      "Iteration 15319 Loss: 1.1937772035598755\n",
      "Iteration 15319 Loss: 1.1435081958770752\n",
      "Iteration 15320 Loss: 1.5165693759918213\n",
      "Iteration 15321 Loss: 1.0565356016159058\n",
      "Iteration 15322 Loss: 1.0246086120605469\n",
      "Iteration 15323 Loss: 1.5736076831817627\n",
      "Iteration 15324 Loss: 1.5477616786956787\n",
      "Iteration 15325 Loss: 1.391312837600708\n",
      "Iteration 15326 Loss: 0.8715727925300598\n",
      "Iteration 15327 Loss: 0.9184319972991943\n",
      "Iteration 15328 Loss: 0.8194372057914734\n",
      "Iteration 15329 Loss: 1.4228119850158691\n",
      "Iteration 15329 Loss: 1.214264988899231\n",
      "Iteration 15330 Loss: 1.1458743810653687\n",
      "Iteration 15331 Loss: 1.2979536056518555\n",
      "Iteration 15332 Loss: 1.3471118211746216\n",
      "Iteration 15333 Loss: 1.5196566581726074\n",
      "Iteration 15334 Loss: 1.2536306381225586\n",
      "Iteration 15335 Loss: 1.1464287042617798\n",
      "Iteration 15336 Loss: 1.1532983779907227\n",
      "Iteration 15337 Loss: 1.3829593658447266\n",
      "Iteration 15338 Loss: 1.3939273357391357\n",
      "Iteration 15339 Loss: 1.4314912557601929\n",
      "Iteration 15339 Loss: 1.307233214378357\n",
      "Iteration 15340 Loss: 1.345072865486145\n",
      "Iteration 15341 Loss: 1.001447081565857\n",
      "Iteration 15342 Loss: 1.1875295639038086\n",
      "Iteration 15343 Loss: 1.239905595779419\n",
      "Iteration 15344 Loss: 1.047141194343567\n",
      "Iteration 15345 Loss: 1.0000399351119995\n",
      "Iteration 15346 Loss: 1.0932193994522095\n",
      "Iteration 15347 Loss: 0.9098681211471558\n",
      "Iteration 15348 Loss: 0.8274598717689514\n",
      "Iteration 15349 Loss: 1.312179446220398\n",
      "Iteration 15349 Loss: 1.0963863134384155\n",
      "Iteration 15350 Loss: 1.1268694400787354\n",
      "Iteration 15351 Loss: 1.2232216596603394\n",
      "Iteration 15352 Loss: 1.3036381006240845\n",
      "Iteration 15353 Loss: 0.9568936228752136\n",
      "Iteration 15354 Loss: 1.195725679397583\n",
      "Iteration 15355 Loss: 1.3615318536758423\n",
      "Iteration 15356 Loss: 1.1371177434921265\n",
      "Iteration 15357 Loss: 1.2021541595458984\n",
      "Iteration 15358 Loss: 1.3000422716140747\n",
      "Iteration 15359 Loss: 1.1313189268112183\n",
      "Iteration 15359 Loss: 1.1938512325286865\n",
      "Iteration 15360 Loss: 1.0142614841461182\n",
      "Iteration 15361 Loss: 0.9256758689880371\n",
      "Iteration 15362 Loss: 1.774796962738037\n",
      "Iteration 15363 Loss: 1.0528582334518433\n",
      "Iteration 15364 Loss: 1.1305756568908691\n",
      "Iteration 15365 Loss: 1.4728703498840332\n",
      "Iteration 15366 Loss: 1.0454574823379517\n",
      "Iteration 15367 Loss: 1.2503317594528198\n",
      "Iteration 15368 Loss: 1.2602120637893677\n",
      "Iteration 15369 Loss: 1.8211605548858643\n",
      "Iteration 15369 Loss: 1.27482008934021\n",
      "Iteration 15370 Loss: 1.4947220087051392\n",
      "Iteration 15371 Loss: 1.2831978797912598\n",
      "Iteration 15372 Loss: 1.3660210371017456\n",
      "Iteration 15373 Loss: 1.2533608675003052\n",
      "Iteration 15374 Loss: 1.1985620260238647\n",
      "Iteration 15375 Loss: 1.3508882522583008\n",
      "Iteration 15376 Loss: 1.3785765171051025\n",
      "Iteration 15377 Loss: 1.1699849367141724\n",
      "Iteration 15378 Loss: 1.3215689659118652\n",
      "Iteration 15379 Loss: 1.4773744344711304\n",
      "Iteration 15379 Loss: 1.329425573348999\n",
      "Iteration 15380 Loss: 0.9905822277069092\n",
      "Iteration 15381 Loss: 1.4130936861038208\n",
      "Iteration 15382 Loss: 1.5919264554977417\n",
      "Iteration 15383 Loss: 1.497802495956421\n",
      "Iteration 15384 Loss: 1.122727870941162\n",
      "Iteration 15385 Loss: 1.6877295970916748\n",
      "Iteration 15386 Loss: 1.4107682704925537\n",
      "Iteration 15387 Loss: 1.3632338047027588\n",
      "Iteration 15388 Loss: 1.3748478889465332\n",
      "Iteration 15389 Loss: 1.214680790901184\n",
      "Iteration 15389 Loss: 1.3667393922805786\n",
      "Iteration 15390 Loss: 1.3851318359375\n",
      "Iteration 15391 Loss: 1.4137632846832275\n",
      "Iteration 15392 Loss: 1.6517583131790161\n",
      "Iteration 15393 Loss: 1.1452631950378418\n",
      "Iteration 15394 Loss: 1.162369728088379\n",
      "Iteration 15395 Loss: 1.35823392868042\n",
      "Iteration 15396 Loss: 1.0626001358032227\n",
      "Iteration 15397 Loss: 1.2780218124389648\n",
      "Iteration 15398 Loss: 1.329102873802185\n",
      "Iteration 15399 Loss: 1.2005330324172974\n",
      "Iteration 15399 Loss: 1.2986778020858765\n",
      "Iteration 15400 Loss: 1.7334082126617432\n",
      "Iteration 15401 Loss: 1.1753379106521606\n",
      "Iteration 15402 Loss: 1.1357407569885254\n",
      "Iteration 15403 Loss: 1.1780973672866821\n",
      "Iteration 15404 Loss: 0.9641314148902893\n",
      "Iteration 15405 Loss: 1.4846856594085693\n",
      "Iteration 15406 Loss: 1.1610790491104126\n",
      "Iteration 15407 Loss: 1.4313788414001465\n",
      "Iteration 15408 Loss: 1.4426947832107544\n",
      "Iteration 15409 Loss: 1.1618404388427734\n",
      "Iteration 15409 Loss: 1.286839485168457\n",
      "Iteration 15410 Loss: 1.0996899604797363\n",
      "Iteration 15411 Loss: 1.3647353649139404\n",
      "Iteration 15412 Loss: 1.1771048307418823\n",
      "Iteration 15413 Loss: 1.2831898927688599\n",
      "Iteration 15414 Loss: 1.2681224346160889\n",
      "Iteration 15415 Loss: 0.8835450410842896\n",
      "Iteration 15416 Loss: 1.3708109855651855\n",
      "Iteration 15417 Loss: 1.2597732543945312\n",
      "Iteration 15418 Loss: 1.3618464469909668\n",
      "Iteration 15419 Loss: 1.2898463010787964\n",
      "Iteration 15419 Loss: 1.2358665466308594\n",
      "Iteration 15420 Loss: 1.1559021472930908\n",
      "Iteration 15421 Loss: 1.2119370698928833\n",
      "Iteration 15422 Loss: 1.195327639579773\n",
      "Iteration 15423 Loss: 1.6295558214187622\n",
      "Iteration 15424 Loss: 1.3271344900131226\n",
      "Iteration 15425 Loss: 1.1078600883483887\n",
      "Iteration 15426 Loss: 1.3064215183258057\n",
      "Iteration 15427 Loss: 1.4120978116989136\n",
      "Iteration 15428 Loss: 1.3882969617843628\n",
      "Iteration 15429 Loss: 0.9887184500694275\n",
      "Iteration 15429 Loss: 1.2723251581192017\n",
      "Iteration 15430 Loss: 0.9615151882171631\n",
      "Iteration 15431 Loss: 1.3870275020599365\n",
      "Iteration 15432 Loss: 1.0985702276229858\n",
      "Iteration 15433 Loss: 1.1013002395629883\n",
      "Iteration 15434 Loss: 1.347694754600525\n",
      "Iteration 15435 Loss: 0.9713494181632996\n",
      "Iteration 15436 Loss: 1.0637519359588623\n",
      "Iteration 15437 Loss: 1.3803194761276245\n",
      "Iteration 15438 Loss: 1.3520636558532715\n",
      "Iteration 15439 Loss: 1.2252322435379028\n",
      "Iteration 15439 Loss: 1.18888258934021\n",
      "Iteration 15440 Loss: 1.413991928100586\n",
      "Iteration 15441 Loss: 0.8700351119041443\n",
      "Iteration 15442 Loss: 1.1394398212432861\n",
      "Iteration 15443 Loss: 1.3450990915298462\n",
      "Iteration 15444 Loss: 1.4074444770812988\n",
      "Iteration 15445 Loss: 1.1653308868408203\n",
      "Iteration 15446 Loss: 0.8383380174636841\n",
      "Iteration 15447 Loss: 0.9068166017532349\n",
      "Iteration 15448 Loss: 1.0029217004776\n",
      "Iteration 15449 Loss: 1.1757826805114746\n",
      "Iteration 15449 Loss: 1.1265199184417725\n",
      "Iteration 15450 Loss: 1.0376673936843872\n",
      "Iteration 15451 Loss: 0.8075596690177917\n",
      "Iteration 15452 Loss: 1.2306313514709473\n",
      "Iteration 15453 Loss: 0.639358401298523\n",
      "Iteration 15454 Loss: 0.6603652834892273\n",
      "Iteration 15455 Loss: 1.2654041051864624\n",
      "Iteration 15456 Loss: 1.1378451585769653\n",
      "Iteration 15457 Loss: 1.380103588104248\n",
      "Iteration 15458 Loss: 1.2408674955368042\n",
      "Iteration 15459 Loss: 1.107276439666748\n",
      "Iteration 15459 Loss: 1.0507078170776367\n",
      "Iteration 15460 Loss: 1.0363215208053589\n",
      "Iteration 15461 Loss: 1.3391140699386597\n",
      "Iteration 15462 Loss: 0.9957674741744995\n",
      "Iteration 15463 Loss: 0.8890251517295837\n",
      "Iteration 15464 Loss: 1.0456899404525757\n",
      "Iteration 15465 Loss: 1.4174308776855469\n",
      "Iteration 15466 Loss: 1.1438027620315552\n",
      "Iteration 15467 Loss: 1.0314316749572754\n",
      "Iteration 15468 Loss: 1.2610442638397217\n",
      "Iteration 15469 Loss: 1.1464202404022217\n",
      "Iteration 15469 Loss: 1.1306047439575195\n",
      "Iteration 15470 Loss: 1.1766107082366943\n",
      "Iteration 15471 Loss: 1.1184427738189697\n",
      "Iteration 15472 Loss: 0.7844973206520081\n",
      "Iteration 15473 Loss: 1.3141728639602661\n",
      "Iteration 15474 Loss: 0.997549295425415\n",
      "Iteration 15475 Loss: 1.2138776779174805\n",
      "Iteration 15476 Loss: 1.2402026653289795\n",
      "Iteration 15477 Loss: 1.2860729694366455\n",
      "Iteration 15478 Loss: 1.4502984285354614\n",
      "Iteration 15479 Loss: 1.2990436553955078\n",
      "Iteration 15479 Loss: 1.1880767345428467\n",
      "Iteration 15480 Loss: 1.453719973564148\n",
      "Iteration 15481 Loss: 0.9405702352523804\n",
      "Iteration 15482 Loss: 1.13357675075531\n",
      "Iteration 15483 Loss: 1.293534278869629\n",
      "Iteration 15484 Loss: 1.0933191776275635\n",
      "Iteration 15485 Loss: 1.1101024150848389\n",
      "Iteration 15486 Loss: 1.2815169095993042\n",
      "Iteration 15487 Loss: 1.274031639099121\n",
      "Iteration 15488 Loss: 1.1593042612075806\n",
      "Iteration 15489 Loss: 0.9380614757537842\n",
      "Iteration 15489 Loss: 1.1677738428115845\n",
      "Iteration 15490 Loss: 1.3811676502227783\n",
      "Iteration 15491 Loss: 1.4351356029510498\n",
      "Iteration 15492 Loss: 1.1942558288574219\n",
      "Iteration 15493 Loss: 1.0457122325897217\n",
      "Iteration 15494 Loss: 1.405617117881775\n",
      "Iteration 15495 Loss: 1.1027470827102661\n",
      "Iteration 15496 Loss: 1.0982621908187866\n",
      "Iteration 15497 Loss: 1.3604012727737427\n",
      "Iteration 15498 Loss: 1.5188168287277222\n",
      "Iteration 15499 Loss: 1.1354047060012817\n",
      "Iteration 15499 Loss: 1.267751932144165\n",
      "Iteration 15500 Loss: 1.2759839296340942\n",
      "Iteration 15501 Loss: 1.0577943325042725\n",
      "Iteration 15502 Loss: 0.8834185600280762\n",
      "Iteration 15503 Loss: 1.2022868394851685\n",
      "Iteration 15504 Loss: 1.1625676155090332\n",
      "Iteration 15505 Loss: 0.7721666097640991\n",
      "Iteration 15506 Loss: 1.6634122133255005\n",
      "Iteration 15507 Loss: 1.2404264211654663\n",
      "Iteration 15508 Loss: 1.4066096544265747\n",
      "Iteration 15509 Loss: 0.9617043733596802\n",
      "Iteration 15509 Loss: 1.1626369953155518\n",
      "Iteration 15510 Loss: 1.0452396869659424\n",
      "Iteration 15511 Loss: 1.3766577243804932\n",
      "Iteration 15512 Loss: 1.2788087129592896\n",
      "Iteration 15513 Loss: 1.0142854452133179\n",
      "Iteration 15514 Loss: 1.1769689321517944\n",
      "Iteration 15515 Loss: 1.2575947046279907\n",
      "Iteration 15516 Loss: 1.3957258462905884\n",
      "Iteration 15517 Loss: 1.2231013774871826\n",
      "Iteration 15518 Loss: 1.491873860359192\n",
      "Iteration 15519 Loss: 0.6632637977600098\n",
      "Iteration 15519 Loss: 1.192352056503296\n",
      "Iteration 15520 Loss: 1.4615504741668701\n",
      "Iteration 15521 Loss: 1.1956392526626587\n",
      "Iteration 15522 Loss: 1.6433640718460083\n",
      "Iteration 15523 Loss: 1.0499483346939087\n",
      "Iteration 15524 Loss: 1.4391051530838013\n",
      "Iteration 15525 Loss: 1.3813648223876953\n",
      "Iteration 15526 Loss: 1.42952561378479\n",
      "Iteration 15527 Loss: 1.3073207139968872\n",
      "Iteration 15528 Loss: 1.0442280769348145\n",
      "Iteration 15529 Loss: 1.3381288051605225\n",
      "Iteration 15529 Loss: 1.3290174007415771\n",
      "Iteration 15530 Loss: 1.144243597984314\n",
      "Iteration 15531 Loss: 1.2518682479858398\n",
      "Iteration 15532 Loss: 1.3163411617279053\n",
      "Iteration 15533 Loss: 1.2464969158172607\n",
      "Iteration 15534 Loss: 1.3361895084381104\n",
      "Iteration 15535 Loss: 1.6654510498046875\n",
      "Iteration 15536 Loss: 1.0975793600082397\n",
      "Iteration 15537 Loss: 1.0110797882080078\n",
      "Iteration 15538 Loss: 1.52022385597229\n",
      "Iteration 15539 Loss: 0.947804868221283\n",
      "Iteration 15539 Loss: 1.2537277936935425\n",
      "Iteration 15540 Loss: 1.1715174913406372\n",
      "Iteration 15541 Loss: 1.5249643325805664\n",
      "Iteration 15542 Loss: 1.281762957572937\n",
      "Iteration 15543 Loss: 1.0531270503997803\n",
      "Iteration 15544 Loss: 1.1039592027664185\n",
      "Iteration 15545 Loss: 0.8311573266983032\n",
      "Iteration 15546 Loss: 1.013654112815857\n",
      "Iteration 15547 Loss: 1.3465402126312256\n",
      "Iteration 15548 Loss: 1.1520280838012695\n",
      "Iteration 15549 Loss: 0.9710804224014282\n",
      "Iteration 15549 Loss: 1.1449791193008423\n",
      "Iteration 15550 Loss: 1.0313446521759033\n",
      "Iteration 15551 Loss: 1.1667802333831787\n",
      "Iteration 15552 Loss: 1.2118979692459106\n",
      "Iteration 15553 Loss: 0.9591320157051086\n",
      "Iteration 15554 Loss: 0.968626856803894\n",
      "Iteration 15555 Loss: 1.4765565395355225\n",
      "Iteration 15556 Loss: 1.4503402709960938\n",
      "Iteration 15557 Loss: 1.1825460195541382\n",
      "Iteration 15558 Loss: 1.0900746583938599\n",
      "Iteration 15559 Loss: 1.4020628929138184\n",
      "Iteration 15559 Loss: 1.1939361095428467\n",
      "Iteration 15560 Loss: 0.7037042379379272\n",
      "Iteration 15561 Loss: 1.5690821409225464\n",
      "Iteration 15562 Loss: 0.9107820987701416\n",
      "Iteration 15563 Loss: 0.9509754180908203\n",
      "Iteration 15564 Loss: 1.155342698097229\n",
      "Iteration 15565 Loss: 0.8826645612716675\n",
      "Iteration 15566 Loss: 1.0403879880905151\n",
      "Iteration 15567 Loss: 1.3753045797348022\n",
      "Iteration 15568 Loss: 1.0170363187789917\n",
      "Iteration 15569 Loss: 1.0573856830596924\n",
      "Iteration 15569 Loss: 1.0662665367126465\n",
      "Iteration 15570 Loss: 1.5107015371322632\n",
      "Iteration 15571 Loss: 1.3461847305297852\n",
      "Iteration 15572 Loss: 0.9804559946060181\n",
      "Iteration 15573 Loss: 1.0861400365829468\n",
      "Iteration 15574 Loss: 1.0422953367233276\n",
      "Iteration 15575 Loss: 1.4343143701553345\n",
      "Iteration 15576 Loss: 1.195180892944336\n",
      "Iteration 15577 Loss: 1.3624709844589233\n",
      "Iteration 15578 Loss: 0.9851742386817932\n",
      "Iteration 15579 Loss: 1.2684448957443237\n",
      "Iteration 15579 Loss: 1.2211363315582275\n",
      "Iteration 15580 Loss: 0.9938603043556213\n",
      "Iteration 15581 Loss: 1.0193642377853394\n",
      "Iteration 15582 Loss: 1.0804007053375244\n",
      "Iteration 15583 Loss: 1.317479133605957\n",
      "Iteration 15584 Loss: 1.2210403680801392\n",
      "Iteration 15585 Loss: 1.0874861478805542\n",
      "Iteration 15586 Loss: 1.4138448238372803\n",
      "Iteration 15587 Loss: 1.1865997314453125\n",
      "Iteration 15588 Loss: 1.2552884817123413\n",
      "Iteration 15589 Loss: 0.8817360401153564\n",
      "Iteration 15589 Loss: 1.1457099914550781\n",
      "Iteration 15590 Loss: 1.3844060897827148\n",
      "Iteration 15591 Loss: 1.112212896347046\n",
      "Iteration 15592 Loss: 1.1884816884994507\n",
      "Iteration 15593 Loss: 1.019089698791504\n",
      "Iteration 15594 Loss: 1.5012681484222412\n",
      "Iteration 15595 Loss: 0.7002748250961304\n",
      "Iteration 15596 Loss: 1.189235806465149\n",
      "Iteration 15597 Loss: 1.5211305618286133\n",
      "Iteration 15598 Loss: 1.3129117488861084\n",
      "Iteration 15599 Loss: 1.4909570217132568\n",
      "Iteration 15599 Loss: 1.2419967651367188\n",
      "Iteration 15600 Loss: 0.7989623546600342\n",
      "Iteration 15601 Loss: 1.1829240322113037\n",
      "Iteration 15602 Loss: 1.3021589517593384\n",
      "Iteration 15603 Loss: 1.3715943098068237\n",
      "Iteration 15604 Loss: 1.0202186107635498\n",
      "Iteration 15605 Loss: 1.4004806280136108\n",
      "Iteration 15606 Loss: 0.9183763265609741\n",
      "Iteration 15607 Loss: 1.5027852058410645\n",
      "Iteration 15608 Loss: 1.183933973312378\n",
      "Iteration 15609 Loss: 0.7952836155891418\n",
      "Iteration 15609 Loss: 1.1476716995239258\n",
      "Iteration 15610 Loss: 1.4831160306930542\n",
      "Iteration 15611 Loss: 1.3414503335952759\n",
      "Iteration 15612 Loss: 0.8825600147247314\n",
      "Iteration 15613 Loss: 1.3021390438079834\n",
      "Iteration 15614 Loss: 1.3438241481781006\n",
      "Iteration 15615 Loss: 1.3799651861190796\n",
      "Iteration 15616 Loss: 1.5332014560699463\n",
      "Iteration 15617 Loss: 1.1288143396377563\n",
      "Iteration 15618 Loss: 1.176269292831421\n",
      "Iteration 15619 Loss: 1.1618993282318115\n",
      "Iteration 15619 Loss: 1.273323893547058\n",
      "Iteration 15620 Loss: 1.264915108680725\n",
      "Iteration 15621 Loss: 0.9715906381607056\n",
      "Iteration 15622 Loss: 0.8875687122344971\n",
      "Iteration 15623 Loss: 1.268449306488037\n",
      "Iteration 15624 Loss: 1.4422436952590942\n",
      "Iteration 15625 Loss: 1.0913151502609253\n",
      "Iteration 15626 Loss: 1.5166056156158447\n",
      "Iteration 15627 Loss: 0.898138701915741\n",
      "Iteration 15628 Loss: 1.2798964977264404\n",
      "Iteration 15629 Loss: 1.0290837287902832\n",
      "Iteration 15629 Loss: 1.1649807691574097\n",
      "Iteration 15630 Loss: 1.2598410844802856\n",
      "Iteration 15631 Loss: 1.5529167652130127\n",
      "Iteration 15632 Loss: 1.252750039100647\n",
      "Iteration 15633 Loss: 1.0660074949264526\n",
      "Iteration 15634 Loss: 1.0740219354629517\n",
      "Iteration 15635 Loss: 1.2726401090621948\n",
      "Iteration 15636 Loss: 1.2684787511825562\n",
      "Iteration 15637 Loss: 0.8368854522705078\n",
      "Iteration 15638 Loss: 1.0794544219970703\n",
      "Iteration 15639 Loss: 1.1844033002853394\n",
      "Iteration 15639 Loss: 1.1847398281097412\n",
      "Iteration 15640 Loss: 0.9411717653274536\n",
      "Iteration 15641 Loss: 1.0490154027938843\n",
      "Iteration 15642 Loss: 1.1806892156600952\n",
      "Iteration 15643 Loss: 1.4521355628967285\n",
      "Iteration 15644 Loss: 1.168203353881836\n",
      "Iteration 15645 Loss: 1.269745111465454\n",
      "Iteration 15646 Loss: 1.233044147491455\n",
      "Iteration 15647 Loss: 1.1981011629104614\n",
      "Iteration 15648 Loss: 0.9079604148864746\n",
      "Iteration 15649 Loss: 1.5057779550552368\n",
      "Iteration 15649 Loss: 1.190584421157837\n",
      "Iteration 15650 Loss: 1.4531325101852417\n",
      "Iteration 15651 Loss: 1.3800164461135864\n",
      "Iteration 15652 Loss: 1.3160138130187988\n",
      "Iteration 15653 Loss: 0.8480184674263\n",
      "Iteration 15654 Loss: 1.1135307550430298\n",
      "Iteration 15655 Loss: 1.0528712272644043\n",
      "Iteration 15656 Loss: 1.429753065109253\n",
      "Iteration 15657 Loss: 1.3200243711471558\n",
      "Iteration 15658 Loss: 0.8620358109474182\n",
      "Iteration 15659 Loss: 1.141711711883545\n",
      "Iteration 15659 Loss: 1.1917109489440918\n",
      "Iteration 15660 Loss: 1.459295392036438\n",
      "Iteration 15661 Loss: 1.5106645822525024\n",
      "Iteration 15662 Loss: 1.452692985534668\n",
      "Iteration 15663 Loss: 0.9168762564659119\n",
      "Iteration 15664 Loss: 0.962826132774353\n",
      "Iteration 15665 Loss: 1.4634727239608765\n",
      "Iteration 15666 Loss: 1.420569896697998\n",
      "Iteration 15667 Loss: 1.1046154499053955\n",
      "Iteration 15668 Loss: 1.051140308380127\n",
      "Iteration 15669 Loss: 1.4136935472488403\n",
      "Iteration 15669 Loss: 1.2755846977233887\n",
      "Iteration 15670 Loss: 1.0184299945831299\n",
      "Iteration 15671 Loss: 1.4206345081329346\n",
      "Iteration 15672 Loss: 0.938204288482666\n",
      "Iteration 15673 Loss: 1.2816284894943237\n",
      "Iteration 15674 Loss: 1.2826082706451416\n",
      "Iteration 15675 Loss: 1.2942843437194824\n",
      "Iteration 15676 Loss: 1.0309617519378662\n",
      "Iteration 15677 Loss: 1.1479007005691528\n",
      "Iteration 15678 Loss: 0.8608536720275879\n",
      "Iteration 15679 Loss: 1.1400644779205322\n",
      "Iteration 15679 Loss: 1.141556978225708\n",
      "Iteration 15680 Loss: 1.4152412414550781\n",
      "Iteration 15681 Loss: 1.3118001222610474\n",
      "Iteration 15682 Loss: 1.1440831422805786\n",
      "Iteration 15683 Loss: 1.2937971353530884\n",
      "Iteration 15684 Loss: 1.2197222709655762\n",
      "Iteration 15685 Loss: 1.0612057447433472\n",
      "Iteration 15686 Loss: 0.6622589826583862\n",
      "Iteration 15687 Loss: 0.9824075698852539\n",
      "Iteration 15688 Loss: 1.4012657403945923\n",
      "Iteration 15689 Loss: 1.1197960376739502\n",
      "Iteration 15689 Loss: 1.1611578464508057\n",
      "Iteration 15690 Loss: 0.922122597694397\n",
      "Iteration 15691 Loss: 1.1269360780715942\n",
      "Iteration 15692 Loss: 1.1074594259262085\n",
      "Iteration 15693 Loss: 1.103044867515564\n",
      "Iteration 15694 Loss: 1.1353731155395508\n",
      "Iteration 15695 Loss: 1.1361267566680908\n",
      "Iteration 15696 Loss: 0.9687238335609436\n",
      "Iteration 15697 Loss: 0.9604779481887817\n",
      "Iteration 15698 Loss: 1.1095266342163086\n",
      "Iteration 15699 Loss: 1.4182195663452148\n",
      "Iteration 15699 Loss: 1.0988011360168457\n",
      "Iteration 15700 Loss: 1.1282217502593994\n",
      "Iteration 15701 Loss: 0.9514071941375732\n",
      "Iteration 15702 Loss: 0.8523141145706177\n",
      "Iteration 15703 Loss: 1.1285096406936646\n",
      "Iteration 15704 Loss: 1.0619126558303833\n",
      "Iteration 15705 Loss: 1.3223607540130615\n",
      "Iteration 15706 Loss: 1.1748814582824707\n",
      "Iteration 15707 Loss: 1.1864867210388184\n",
      "Iteration 15708 Loss: 1.1865280866622925\n",
      "Iteration 15709 Loss: 0.8348191976547241\n",
      "Iteration 15709 Loss: 1.0827442407608032\n",
      "Iteration 15710 Loss: 0.8321080803871155\n",
      "Iteration 15711 Loss: 1.4584760665893555\n",
      "Iteration 15712 Loss: 1.3659664392471313\n",
      "Iteration 15713 Loss: 1.0533074140548706\n",
      "Iteration 15714 Loss: 1.0230172872543335\n",
      "Iteration 15715 Loss: 0.7624890208244324\n",
      "Iteration 15716 Loss: 1.480157732963562\n",
      "Iteration 15717 Loss: 1.4337621927261353\n",
      "Iteration 15718 Loss: 0.999573826789856\n",
      "Iteration 15719 Loss: 1.0703742504119873\n",
      "Iteration 15719 Loss: 1.147923231124878\n",
      "Iteration 15720 Loss: 1.099725365638733\n",
      "Iteration 15721 Loss: 1.0215415954589844\n",
      "Iteration 15722 Loss: 1.4493257999420166\n",
      "Iteration 15723 Loss: 1.1408692598342896\n",
      "Iteration 15724 Loss: 1.2259249687194824\n",
      "Iteration 15725 Loss: 1.0226478576660156\n",
      "Iteration 15726 Loss: 0.9621742963790894\n",
      "Iteration 15727 Loss: 1.035435676574707\n",
      "Iteration 15728 Loss: 1.3830903768539429\n",
      "Iteration 15729 Loss: 1.5054304599761963\n",
      "Iteration 15729 Loss: 1.1846164464950562\n",
      "Iteration 15730 Loss: 1.2430256605148315\n",
      "Iteration 15731 Loss: 0.9090433716773987\n",
      "Iteration 15732 Loss: 1.4791446924209595\n",
      "Iteration 15733 Loss: 0.9611518383026123\n",
      "Iteration 15734 Loss: 1.2884869575500488\n",
      "Iteration 15735 Loss: 1.7291215658187866\n",
      "Iteration 15736 Loss: 1.1871933937072754\n",
      "Iteration 15737 Loss: 0.8373338580131531\n",
      "Iteration 15738 Loss: 0.9614652395248413\n",
      "Iteration 15739 Loss: 1.430746078491211\n",
      "Iteration 15739 Loss: 1.2026712894439697\n",
      "Iteration 15740 Loss: 1.0973851680755615\n",
      "Iteration 15741 Loss: 1.2609195709228516\n",
      "Iteration 15742 Loss: 1.0005214214324951\n",
      "Iteration 15743 Loss: 1.4363256692886353\n",
      "Iteration 15744 Loss: 1.121994137763977\n",
      "Iteration 15745 Loss: 1.3094415664672852\n",
      "Iteration 15746 Loss: 1.1592470407485962\n",
      "Iteration 15747 Loss: 1.4516452550888062\n",
      "Iteration 15748 Loss: 1.0697764158248901\n",
      "Iteration 15749 Loss: 1.3387309312820435\n",
      "Iteration 15749 Loss: 1.22459876537323\n",
      "Iteration 15750 Loss: 1.2469136714935303\n",
      "Iteration 15751 Loss: 1.9494386911392212\n",
      "Iteration 15752 Loss: 1.0945695638656616\n",
      "Iteration 15753 Loss: 1.036887526512146\n",
      "Iteration 15754 Loss: 1.0886024236679077\n",
      "Iteration 15755 Loss: 1.4321551322937012\n",
      "Iteration 15756 Loss: 1.448072910308838\n",
      "Iteration 15757 Loss: 1.3700003623962402\n",
      "Iteration 15758 Loss: 1.458774447441101\n",
      "Iteration 15759 Loss: 0.9050538539886475\n",
      "Iteration 15759 Loss: 1.3030468225479126\n",
      "Iteration 15760 Loss: 1.4502735137939453\n",
      "Iteration 15761 Loss: 1.3342669010162354\n",
      "Iteration 15762 Loss: 0.9294750690460205\n",
      "Iteration 15763 Loss: 1.235185980796814\n",
      "Iteration 15764 Loss: 0.9462855458259583\n",
      "Iteration 15765 Loss: 1.6115262508392334\n",
      "Iteration 15766 Loss: 1.2119780778884888\n",
      "Iteration 15767 Loss: 1.145226001739502\n",
      "Iteration 15768 Loss: 1.348952293395996\n",
      "Iteration 15769 Loss: 0.9552327990531921\n",
      "Iteration 15769 Loss: 1.216840386390686\n",
      "Iteration 15770 Loss: 1.2466514110565186\n",
      "Iteration 15771 Loss: 0.9558786749839783\n",
      "Iteration 15772 Loss: 1.1945972442626953\n",
      "Iteration 15773 Loss: 1.1635196208953857\n",
      "Iteration 15774 Loss: 1.1389436721801758\n",
      "Iteration 15775 Loss: 1.0262625217437744\n",
      "Iteration 15776 Loss: 1.2547152042388916\n",
      "Iteration 15777 Loss: 0.8125090599060059\n",
      "Iteration 15778 Loss: 1.6839683055877686\n",
      "Iteration 15779 Loss: 0.8150786757469177\n",
      "Iteration 15779 Loss: 1.1292123794555664\n",
      "Iteration 15780 Loss: 1.0835806131362915\n",
      "Iteration 15781 Loss: 1.338436245918274\n",
      "Iteration 15782 Loss: 1.0676896572113037\n",
      "Iteration 15783 Loss: 1.3481628894805908\n",
      "Iteration 15784 Loss: 1.0887715816497803\n",
      "Iteration 15785 Loss: 1.1561397314071655\n",
      "Iteration 15786 Loss: 1.4001610279083252\n",
      "Iteration 15787 Loss: 0.8705324530601501\n",
      "Iteration 15788 Loss: 1.2674671411514282\n",
      "Iteration 15789 Loss: 0.8553909659385681\n",
      "Iteration 15789 Loss: 1.1476330757141113\n",
      "Iteration 15790 Loss: 1.3514208793640137\n",
      "Iteration 15791 Loss: 1.2061562538146973\n",
      "Iteration 15792 Loss: 1.3688712120056152\n",
      "Iteration 15793 Loss: 0.9159124493598938\n",
      "Iteration 15794 Loss: 0.7081793546676636\n",
      "Iteration 15795 Loss: 0.9357256889343262\n",
      "Iteration 15796 Loss: 1.014865756034851\n",
      "Iteration 15797 Loss: 1.882847547531128\n",
      "Iteration 15798 Loss: 1.6263948678970337\n",
      "Iteration 15799 Loss: 1.0547089576721191\n",
      "Iteration 15799 Loss: 1.2065083980560303\n",
      "Iteration 15800 Loss: 1.1000630855560303\n",
      "Iteration 15801 Loss: 0.7562347650527954\n",
      "Iteration 15802 Loss: 1.148803949356079\n",
      "Iteration 15803 Loss: 1.4440020322799683\n",
      "Iteration 15804 Loss: 1.0557096004486084\n",
      "Iteration 15805 Loss: 0.6327301263809204\n",
      "Iteration 15806 Loss: 0.9706051349639893\n",
      "Iteration 15807 Loss: 1.001344919204712\n",
      "Iteration 15808 Loss: 1.1787669658660889\n",
      "Iteration 15809 Loss: 1.2521967887878418\n",
      "Iteration 15809 Loss: 1.0540457963943481\n",
      "Iteration 15810 Loss: 1.1585280895233154\n",
      "Iteration 15811 Loss: 1.4125831127166748\n",
      "Iteration 15812 Loss: 1.5101927518844604\n",
      "Iteration 15813 Loss: 1.2665879726409912\n",
      "Iteration 15814 Loss: 1.00452721118927\n",
      "Iteration 15815 Loss: 1.0108091831207275\n",
      "Iteration 15816 Loss: 1.142177939414978\n",
      "Iteration 15817 Loss: 1.1937618255615234\n",
      "Iteration 15818 Loss: 1.0623096227645874\n",
      "Iteration 15819 Loss: 1.2282048463821411\n",
      "Iteration 15819 Loss: 1.1989681720733643\n",
      "Iteration 15820 Loss: 0.8694582581520081\n",
      "Iteration 15821 Loss: 1.315081000328064\n",
      "Iteration 15822 Loss: 1.3212409019470215\n",
      "Iteration 15823 Loss: 1.3354853391647339\n",
      "Iteration 15824 Loss: 1.044676661491394\n",
      "Iteration 15825 Loss: 0.9946606159210205\n",
      "Iteration 15826 Loss: 1.0622553825378418\n",
      "Iteration 15827 Loss: 1.6689605712890625\n",
      "Iteration 15828 Loss: 1.1457302570343018\n",
      "Iteration 15829 Loss: 1.3573219776153564\n",
      "Iteration 15829 Loss: 1.211487054824829\n",
      "Iteration 15830 Loss: 1.0881372690200806\n",
      "Iteration 15831 Loss: 0.7665090560913086\n",
      "Iteration 15832 Loss: 0.9733645915985107\n",
      "Iteration 15833 Loss: 1.2456554174423218\n",
      "Iteration 15834 Loss: 0.7194132804870605\n",
      "Iteration 15835 Loss: 1.4367377758026123\n",
      "Iteration 15836 Loss: 1.4291516542434692\n",
      "Iteration 15837 Loss: 0.9869188070297241\n",
      "Iteration 15838 Loss: 1.1783190965652466\n",
      "Iteration 15839 Loss: 1.388563632965088\n",
      "Iteration 15839 Loss: 1.121277093887329\n",
      "Iteration 15840 Loss: 1.070912480354309\n",
      "Iteration 15841 Loss: 1.4112118482589722\n",
      "Iteration 15842 Loss: 0.8222000002861023\n",
      "Iteration 15843 Loss: 1.1351008415222168\n",
      "Iteration 15844 Loss: 1.2242759466171265\n",
      "Iteration 15845 Loss: 0.9698109030723572\n",
      "Iteration 15846 Loss: 0.9850416779518127\n",
      "Iteration 15847 Loss: 1.0697517395019531\n",
      "Iteration 15848 Loss: 1.2161266803741455\n",
      "Iteration 15849 Loss: 1.053708791732788\n",
      "Iteration 15849 Loss: 1.0958139896392822\n",
      "Iteration 15850 Loss: 1.1353533267974854\n",
      "Iteration 15851 Loss: 1.3589208126068115\n",
      "Iteration 15852 Loss: 1.4837262630462646\n",
      "Iteration 15853 Loss: 0.9729498028755188\n",
      "Iteration 15854 Loss: 1.1501402854919434\n",
      "Iteration 15855 Loss: 1.180220603942871\n",
      "Iteration 15856 Loss: 1.3395085334777832\n",
      "Iteration 15857 Loss: 1.0932385921478271\n",
      "Iteration 15858 Loss: 1.7824054956436157\n",
      "Iteration 15859 Loss: 1.4120947122573853\n",
      "Iteration 15859 Loss: 1.2908560037612915\n",
      "Iteration 15860 Loss: 1.1787166595458984\n",
      "Iteration 15861 Loss: 1.0655474662780762\n",
      "Iteration 15862 Loss: 1.2650617361068726\n",
      "Iteration 15863 Loss: 1.422141671180725\n",
      "Iteration 15864 Loss: 1.2411606311798096\n",
      "Iteration 15865 Loss: 1.177527904510498\n",
      "Iteration 15866 Loss: 1.056499719619751\n",
      "Iteration 15867 Loss: 1.0207927227020264\n",
      "Iteration 15868 Loss: 0.8605928421020508\n",
      "Iteration 15869 Loss: 1.0056596994400024\n",
      "Iteration 15869 Loss: 1.1293699741363525\n",
      "Iteration 15870 Loss: 1.3478295803070068\n",
      "Iteration 15871 Loss: 1.006191372871399\n",
      "Iteration 15872 Loss: 1.2531023025512695\n",
      "Iteration 15873 Loss: 1.4230279922485352\n",
      "Iteration 15874 Loss: 1.6200916767120361\n",
      "Iteration 15875 Loss: 1.0767256021499634\n",
      "Iteration 15876 Loss: 1.2242156267166138\n",
      "Iteration 15877 Loss: 1.2855536937713623\n",
      "Iteration 15878 Loss: 1.434849500656128\n",
      "Iteration 15879 Loss: 1.0819519758224487\n",
      "Iteration 15879 Loss: 1.275354027748108\n",
      "Iteration 15880 Loss: 1.173455834388733\n",
      "Iteration 15881 Loss: 0.9658983945846558\n",
      "Iteration 15882 Loss: 1.3895773887634277\n",
      "Iteration 15883 Loss: 0.7592796683311462\n",
      "Iteration 15884 Loss: 1.218847632408142\n",
      "Iteration 15885 Loss: 1.4170877933502197\n",
      "Iteration 15886 Loss: 1.660058617591858\n",
      "Iteration 15887 Loss: 0.7997081875801086\n",
      "Iteration 15888 Loss: 0.9236220121383667\n",
      "Iteration 15889 Loss: 0.9881656765937805\n",
      "Iteration 15889 Loss: 1.1295702457427979\n",
      "Iteration 15890 Loss: 0.9869337677955627\n",
      "Iteration 15891 Loss: 1.0574681758880615\n",
      "Iteration 15892 Loss: 1.3994333744049072\n",
      "Iteration 15893 Loss: 1.2691770792007446\n",
      "Iteration 15894 Loss: 1.2803175449371338\n",
      "Iteration 15895 Loss: 1.1600112915039062\n",
      "Iteration 15896 Loss: 1.0463136434555054\n",
      "Iteration 15897 Loss: 1.467718243598938\n",
      "Iteration 15898 Loss: 1.0945342779159546\n",
      "Iteration 15899 Loss: 1.2192468643188477\n",
      "Iteration 15899 Loss: 1.198115348815918\n",
      "Iteration 15900 Loss: 1.2826330661773682\n",
      "Iteration 15901 Loss: 1.480624794960022\n",
      "Iteration 15902 Loss: 1.018122911453247\n",
      "Iteration 15903 Loss: 1.2346388101577759\n",
      "Iteration 15904 Loss: 1.0733118057250977\n",
      "Iteration 15905 Loss: 1.2672892808914185\n",
      "Iteration 15906 Loss: 1.635495662689209\n",
      "Iteration 15907 Loss: 1.842707633972168\n",
      "Iteration 15908 Loss: 1.249329686164856\n",
      "Iteration 15909 Loss: 0.9323709607124329\n",
      "Iteration 15909 Loss: 1.301652431488037\n",
      "Iteration 15910 Loss: 1.260724663734436\n",
      "Iteration 15911 Loss: 0.6925395131111145\n",
      "Iteration 15912 Loss: 1.416916847229004\n",
      "Iteration 15913 Loss: 1.0173940658569336\n",
      "Iteration 15914 Loss: 0.9940956830978394\n",
      "Iteration 15915 Loss: 1.1849943399429321\n",
      "Iteration 15916 Loss: 1.72036612033844\n",
      "Iteration 15917 Loss: 0.8736597895622253\n",
      "Iteration 15918 Loss: 0.9859323501586914\n",
      "Iteration 15919 Loss: 1.0141278505325317\n",
      "Iteration 15919 Loss: 1.1160752773284912\n",
      "Iteration 15920 Loss: 1.2068928480148315\n",
      "Iteration 15921 Loss: 1.2871159315109253\n",
      "Iteration 15922 Loss: 1.1112194061279297\n",
      "Iteration 15923 Loss: 1.4403901100158691\n",
      "Iteration 15924 Loss: 0.979587972164154\n",
      "Iteration 15925 Loss: 1.1809028387069702\n",
      "Iteration 15926 Loss: 0.8628432750701904\n",
      "Iteration 15927 Loss: 1.1563953161239624\n",
      "Iteration 15928 Loss: 1.3328524827957153\n",
      "Iteration 15929 Loss: 1.102221965789795\n",
      "Iteration 15929 Loss: 1.1660420894622803\n",
      "Iteration 15930 Loss: 1.5595817565917969\n",
      "Iteration 15931 Loss: 1.7374838590621948\n",
      "Iteration 15932 Loss: 1.4530459642410278\n",
      "Iteration 15933 Loss: 1.288049340248108\n",
      "Iteration 15934 Loss: 1.4542125463485718\n",
      "Iteration 15935 Loss: 1.1401628255844116\n",
      "Iteration 15936 Loss: 1.3953208923339844\n",
      "Iteration 15937 Loss: 1.1556390523910522\n",
      "Iteration 15938 Loss: 1.0613385438919067\n",
      "Iteration 15939 Loss: 1.0851635932922363\n",
      "Iteration 15939 Loss: 1.3329997062683105\n",
      "Iteration 15940 Loss: 1.7336924076080322\n",
      "Iteration 15941 Loss: 0.9659191966056824\n",
      "Iteration 15942 Loss: 1.0881621837615967\n",
      "Iteration 15943 Loss: 1.138170599937439\n",
      "Iteration 15944 Loss: 1.3607947826385498\n",
      "Iteration 15945 Loss: 1.138733148574829\n",
      "Iteration 15946 Loss: 1.3426872491836548\n",
      "Iteration 15947 Loss: 0.9899494051933289\n",
      "Iteration 15948 Loss: 1.0611753463745117\n",
      "Iteration 15949 Loss: 1.1066850423812866\n",
      "Iteration 15949 Loss: 1.1925969123840332\n",
      "Iteration 15950 Loss: 1.2049041986465454\n",
      "Iteration 15951 Loss: 0.892726719379425\n",
      "Iteration 15952 Loss: 1.1307919025421143\n",
      "Iteration 15953 Loss: 1.7305763959884644\n",
      "Iteration 15954 Loss: 1.2669274806976318\n",
      "Iteration 15955 Loss: 1.2187364101409912\n",
      "Iteration 15956 Loss: 1.3698288202285767\n",
      "Iteration 15957 Loss: 1.41972017288208\n",
      "Iteration 15958 Loss: 0.7951266765594482\n",
      "Iteration 15959 Loss: 1.6538461446762085\n",
      "Iteration 15959 Loss: 1.2683185338974\n",
      "Iteration 15960 Loss: 1.346716046333313\n",
      "Iteration 15961 Loss: 0.8281283974647522\n",
      "Iteration 15962 Loss: 1.3751676082611084\n",
      "Iteration 15963 Loss: 1.5230441093444824\n",
      "Iteration 15964 Loss: 1.217252492904663\n",
      "Iteration 15965 Loss: 0.9277353882789612\n",
      "Iteration 15966 Loss: 1.2100772857666016\n",
      "Iteration 15967 Loss: 1.514265537261963\n",
      "Iteration 15968 Loss: 1.1604617834091187\n",
      "Iteration 15969 Loss: 1.247107982635498\n",
      "Iteration 15969 Loss: 1.2349956035614014\n",
      "Iteration 15970 Loss: 1.3489634990692139\n",
      "Iteration 15971 Loss: 1.455222725868225\n",
      "Iteration 15972 Loss: 1.2465471029281616\n",
      "Iteration 15973 Loss: 1.2653506994247437\n",
      "Iteration 15974 Loss: 1.1120725870132446\n",
      "Iteration 15975 Loss: 1.1603375673294067\n",
      "Iteration 15976 Loss: 0.8922442197799683\n",
      "Iteration 15977 Loss: 0.7976043820381165\n",
      "Iteration 15978 Loss: 1.0214835405349731\n",
      "Iteration 15979 Loss: 1.1350996494293213\n",
      "Iteration 15979 Loss: 1.1434926986694336\n",
      "Iteration 15980 Loss: 1.3929640054702759\n",
      "Iteration 15981 Loss: 1.1740292310714722\n",
      "Iteration 15982 Loss: 1.3630545139312744\n",
      "Iteration 15983 Loss: 1.322925090789795\n",
      "Iteration 15984 Loss: 1.4243077039718628\n",
      "Iteration 15985 Loss: 1.1868704557418823\n",
      "Iteration 15986 Loss: 1.1219072341918945\n",
      "Iteration 15987 Loss: 1.4077168703079224\n",
      "Iteration 15988 Loss: 1.168913722038269\n",
      "Iteration 15989 Loss: 1.3938320875167847\n",
      "Iteration 15989 Loss: 1.295652151107788\n",
      "Iteration 15990 Loss: 1.0995652675628662\n",
      "Iteration 15991 Loss: 1.1579152345657349\n",
      "Iteration 15992 Loss: 1.1435487270355225\n",
      "Iteration 15993 Loss: 1.3737834692001343\n",
      "Iteration 15994 Loss: 1.5779845714569092\n",
      "Iteration 15995 Loss: 1.1538503170013428\n",
      "Iteration 15996 Loss: 1.1727547645568848\n",
      "Iteration 15997 Loss: 0.8995707035064697\n",
      "Iteration 15998 Loss: 1.0239771604537964\n",
      "Iteration 15999 Loss: 1.0804345607757568\n",
      "Iteration 15999 Loss: 1.168338418006897\n",
      "Iteration 16000 Loss: 1.1814600229263306\n",
      "Iteration 16001 Loss: 1.0556871891021729\n",
      "Iteration 16002 Loss: 1.2808133363723755\n",
      "Iteration 16003 Loss: 1.1529401540756226\n",
      "Iteration 16004 Loss: 1.0438776016235352\n",
      "Iteration 16005 Loss: 0.9782701730728149\n",
      "Iteration 16006 Loss: 1.606636643409729\n",
      "Iteration 16007 Loss: 1.5418779850006104\n",
      "Iteration 16008 Loss: 1.1617127656936646\n",
      "Iteration 16009 Loss: 1.081079125404358\n",
      "Iteration 16009 Loss: 1.2084355354309082\n",
      "Iteration 16010 Loss: 1.391351580619812\n",
      "Iteration 16011 Loss: 0.9075726866722107\n",
      "Iteration 16012 Loss: 1.582115650177002\n",
      "Iteration 16013 Loss: 1.4109536409378052\n",
      "Iteration 16014 Loss: 1.296318769454956\n",
      "Iteration 16015 Loss: 1.0087323188781738\n",
      "Iteration 16016 Loss: 1.3364554643630981\n",
      "Iteration 16017 Loss: 1.142144799232483\n",
      "Iteration 16018 Loss: 1.2280993461608887\n",
      "Iteration 16019 Loss: 1.4645808935165405\n",
      "Iteration 16019 Loss: 1.2768324613571167\n",
      "Iteration 16020 Loss: 1.036702036857605\n",
      "Iteration 16021 Loss: 1.1998964548110962\n",
      "Iteration 16022 Loss: 1.595217227935791\n",
      "Iteration 16023 Loss: 0.889377772808075\n",
      "Iteration 16024 Loss: 1.5453202724456787\n",
      "Iteration 16025 Loss: 1.0634762048721313\n",
      "Iteration 16026 Loss: 1.0671672821044922\n",
      "Iteration 16027 Loss: 1.1107523441314697\n",
      "Iteration 16028 Loss: 1.0508224964141846\n",
      "Iteration 16029 Loss: 1.0810294151306152\n",
      "Iteration 16029 Loss: 1.1639760732650757\n",
      "Iteration 16030 Loss: 0.6486849188804626\n",
      "Iteration 16031 Loss: 1.3164982795715332\n",
      "Iteration 16032 Loss: 1.0735008716583252\n",
      "Iteration 16033 Loss: 0.8814341425895691\n",
      "Iteration 16034 Loss: 1.2909713983535767\n",
      "Iteration 16035 Loss: 1.3092690706253052\n",
      "Iteration 16036 Loss: 1.139668583869934\n",
      "Iteration 16037 Loss: 1.4582879543304443\n",
      "Iteration 16038 Loss: 0.7800967693328857\n",
      "Iteration 16039 Loss: 0.96555495262146\n",
      "Iteration 16039 Loss: 1.0863966941833496\n",
      "Iteration 16040 Loss: 0.9697244763374329\n",
      "Iteration 16041 Loss: 1.311828374862671\n",
      "Iteration 16042 Loss: 0.6969690918922424\n",
      "Iteration 16043 Loss: 0.8642953038215637\n",
      "Iteration 16044 Loss: 1.2351289987564087\n",
      "Iteration 16045 Loss: 1.2102628946304321\n",
      "Iteration 16046 Loss: 1.2575318813323975\n",
      "Iteration 16047 Loss: 1.1524473428726196\n",
      "Iteration 16048 Loss: 1.1413155794143677\n",
      "Iteration 16049 Loss: 1.2683994770050049\n",
      "Iteration 16049 Loss: 1.110790491104126\n",
      "Iteration 16050 Loss: 1.4843071699142456\n",
      "Iteration 16051 Loss: 0.9968947768211365\n",
      "Iteration 16052 Loss: 1.4851112365722656\n",
      "Iteration 16053 Loss: 1.501802921295166\n",
      "Iteration 16054 Loss: 0.9530770778656006\n",
      "Iteration 16055 Loss: 1.0780495405197144\n",
      "Iteration 16056 Loss: 1.6703016757965088\n",
      "Iteration 16057 Loss: 0.9805765151977539\n",
      "Iteration 16058 Loss: 0.8995800018310547\n",
      "Iteration 16059 Loss: 0.8296960592269897\n",
      "Iteration 16059 Loss: 1.1879396438598633\n",
      "Iteration 16060 Loss: 1.5755432844161987\n",
      "Iteration 16061 Loss: 1.3780944347381592\n",
      "Iteration 16062 Loss: 0.8211418390274048\n",
      "Iteration 16063 Loss: 1.2480347156524658\n",
      "Iteration 16064 Loss: 1.4065309762954712\n",
      "Iteration 16065 Loss: 1.4840278625488281\n",
      "Iteration 16066 Loss: 1.24109947681427\n",
      "Iteration 16067 Loss: 1.2147361040115356\n",
      "Iteration 16068 Loss: 1.3065166473388672\n",
      "Iteration 16069 Loss: 1.312690019607544\n",
      "Iteration 16069 Loss: 1.2988415956497192\n",
      "Iteration 16070 Loss: 1.4046465158462524\n",
      "Iteration 16071 Loss: 1.2449188232421875\n",
      "Iteration 16072 Loss: 1.028817892074585\n",
      "Iteration 16073 Loss: 1.5111546516418457\n",
      "Iteration 16074 Loss: 1.092941164970398\n",
      "Iteration 16075 Loss: 1.0589475631713867\n",
      "Iteration 16076 Loss: 1.0564013719558716\n",
      "Iteration 16077 Loss: 1.2172784805297852\n",
      "Iteration 16078 Loss: 1.2928996086120605\n",
      "Iteration 16079 Loss: 0.9756139516830444\n",
      "Iteration 16079 Loss: 1.1883618831634521\n",
      "Iteration 16080 Loss: 1.1270530223846436\n",
      "Iteration 16081 Loss: 1.090163230895996\n",
      "Iteration 16082 Loss: 1.2029776573181152\n",
      "Iteration 16083 Loss: 1.4333810806274414\n",
      "Iteration 16084 Loss: 1.3778738975524902\n",
      "Iteration 16085 Loss: 1.248610019683838\n",
      "Iteration 16086 Loss: 0.8498679399490356\n",
      "Iteration 16087 Loss: 1.1270991563796997\n",
      "Iteration 16088 Loss: 1.282063364982605\n",
      "Iteration 16089 Loss: 1.3239842653274536\n",
      "Iteration 16089 Loss: 1.2063074111938477\n",
      "Iteration 16090 Loss: 1.252787709236145\n",
      "Iteration 16091 Loss: 1.3881568908691406\n",
      "Iteration 16092 Loss: 1.2925881147384644\n",
      "Iteration 16093 Loss: 1.0785130262374878\n",
      "Iteration 16094 Loss: 1.0473228693008423\n",
      "Iteration 16095 Loss: 1.2750672101974487\n",
      "Iteration 16096 Loss: 0.8395286798477173\n",
      "Iteration 16097 Loss: 1.141619086265564\n",
      "Iteration 16098 Loss: 1.1985379457473755\n",
      "Iteration 16099 Loss: 0.8502427935600281\n",
      "Iteration 16099 Loss: 1.1364364624023438\n",
      "Iteration 16100 Loss: 1.231488823890686\n",
      "Iteration 16101 Loss: 1.0790653228759766\n",
      "Iteration 16102 Loss: 1.231860637664795\n",
      "Iteration 16103 Loss: 1.0644481182098389\n",
      "Iteration 16104 Loss: 1.101183295249939\n",
      "Iteration 16105 Loss: 1.3097689151763916\n",
      "Iteration 16106 Loss: 1.1344996690750122\n",
      "Iteration 16107 Loss: 1.3432844877243042\n",
      "Iteration 16108 Loss: 0.9876742959022522\n",
      "Iteration 16109 Loss: 1.2327334880828857\n",
      "Iteration 16109 Loss: 1.1716006994247437\n",
      "Iteration 16110 Loss: 1.1066972017288208\n",
      "Iteration 16111 Loss: 1.2676736116409302\n",
      "Iteration 16112 Loss: 1.038319706916809\n",
      "Iteration 16113 Loss: 1.3673807382583618\n",
      "Iteration 16114 Loss: 0.9227543473243713\n",
      "Iteration 16115 Loss: 1.2907472848892212\n",
      "Iteration 16116 Loss: 1.0325782299041748\n",
      "Iteration 16117 Loss: 1.5078349113464355\n",
      "Iteration 16118 Loss: 1.2320137023925781\n",
      "Iteration 16119 Loss: 1.1534817218780518\n",
      "Iteration 16119 Loss: 1.1919481754302979\n",
      "Iteration 16120 Loss: 0.9847432374954224\n",
      "Iteration 16121 Loss: 1.32257080078125\n",
      "Iteration 16122 Loss: 1.0169856548309326\n",
      "Iteration 16123 Loss: 1.0637339353561401\n",
      "Iteration 16124 Loss: 1.1208548545837402\n",
      "Iteration 16125 Loss: 1.384111762046814\n",
      "Iteration 16126 Loss: 1.4511194229125977\n",
      "Iteration 16127 Loss: 1.177101969718933\n",
      "Iteration 16128 Loss: 1.4185400009155273\n",
      "Iteration 16129 Loss: 1.148079514503479\n",
      "Iteration 16129 Loss: 1.2087841033935547\n",
      "Iteration 16130 Loss: 1.0826036930084229\n",
      "Iteration 16131 Loss: 1.309410572052002\n",
      "Iteration 16132 Loss: 1.1495842933654785\n",
      "Iteration 16133 Loss: 1.3886353969573975\n",
      "Iteration 16134 Loss: 1.365526795387268\n",
      "Iteration 16135 Loss: 1.470329761505127\n",
      "Iteration 16136 Loss: 1.0527747869491577\n",
      "Iteration 16137 Loss: 1.1030058860778809\n",
      "Iteration 16138 Loss: 1.109038233757019\n",
      "Iteration 16139 Loss: 1.2617522478103638\n",
      "Iteration 16139 Loss: 1.2292661666870117\n",
      "Iteration 16140 Loss: 1.2614715099334717\n",
      "Iteration 16141 Loss: 1.0131502151489258\n",
      "Iteration 16142 Loss: 1.35415780544281\n",
      "Iteration 16143 Loss: 1.0622217655181885\n",
      "Iteration 16144 Loss: 0.8358510136604309\n",
      "Iteration 16145 Loss: 0.8472112417221069\n",
      "Iteration 16146 Loss: 1.2778505086898804\n",
      "Iteration 16147 Loss: 0.6094942092895508\n",
      "Iteration 16148 Loss: 1.1499338150024414\n",
      "Iteration 16149 Loss: 0.7098933458328247\n",
      "Iteration 16149 Loss: 1.0121235847473145\n",
      "Iteration 16150 Loss: 1.1025526523590088\n",
      "Iteration 16151 Loss: 1.3006526231765747\n",
      "Iteration 16152 Loss: 1.2450352907180786\n",
      "Iteration 16153 Loss: 1.3616857528686523\n",
      "Iteration 16154 Loss: 1.2527583837509155\n",
      "Iteration 16155 Loss: 1.3854261636734009\n",
      "Iteration 16156 Loss: 1.4834588766098022\n",
      "Iteration 16157 Loss: 1.0617375373840332\n",
      "Iteration 16158 Loss: 1.3908735513687134\n",
      "Iteration 16159 Loss: 1.4259268045425415\n",
      "Iteration 16159 Loss: 1.3010107278823853\n",
      "Iteration 16160 Loss: 1.183915376663208\n",
      "Iteration 16161 Loss: 1.2610188722610474\n",
      "Iteration 16162 Loss: 1.0596646070480347\n",
      "Iteration 16163 Loss: 1.1841517686843872\n",
      "Iteration 16164 Loss: 1.5315090417861938\n",
      "Iteration 16165 Loss: 1.0113414525985718\n",
      "Iteration 16166 Loss: 1.1961116790771484\n",
      "Iteration 16167 Loss: 0.8464921116828918\n",
      "Iteration 16168 Loss: 1.2149332761764526\n",
      "Iteration 16169 Loss: 1.5184032917022705\n",
      "Iteration 16169 Loss: 1.2007540464401245\n",
      "Iteration 16170 Loss: 0.9953687787055969\n",
      "Iteration 16171 Loss: 1.2877161502838135\n",
      "Iteration 16172 Loss: 0.9926196336746216\n",
      "Iteration 16173 Loss: 1.199542760848999\n",
      "Iteration 16174 Loss: 0.7139357328414917\n",
      "Iteration 16175 Loss: 1.1303627490997314\n",
      "Iteration 16176 Loss: 0.9961780905723572\n",
      "Iteration 16177 Loss: 1.0854581594467163\n",
      "Iteration 16178 Loss: 1.6085004806518555\n",
      "Iteration 16179 Loss: 1.026352882385254\n",
      "Iteration 16179 Loss: 1.103603482246399\n",
      "Iteration 16180 Loss: 1.2858316898345947\n",
      "Iteration 16181 Loss: 0.9310764074325562\n",
      "Iteration 16182 Loss: 1.1873233318328857\n",
      "Iteration 16183 Loss: 1.3326822519302368\n",
      "Iteration 16184 Loss: 0.7672854065895081\n",
      "Iteration 16185 Loss: 1.1046998500823975\n",
      "Iteration 16186 Loss: 1.1971793174743652\n",
      "Iteration 16187 Loss: 1.3426235914230347\n",
      "Iteration 16188 Loss: 0.8130253553390503\n",
      "Iteration 16189 Loss: 0.998792290687561\n",
      "Iteration 16189 Loss: 1.0960519313812256\n",
      "Iteration 16190 Loss: 1.0852917432785034\n",
      "Iteration 16191 Loss: 1.5197970867156982\n",
      "Iteration 16192 Loss: 0.9795646071434021\n",
      "Iteration 16193 Loss: 1.6358082294464111\n",
      "Iteration 16194 Loss: 1.1536046266555786\n",
      "Iteration 16195 Loss: 0.8586580753326416\n",
      "Iteration 16196 Loss: 1.1606353521347046\n",
      "Iteration 16197 Loss: 1.4368839263916016\n",
      "Iteration 16198 Loss: 1.3774904012680054\n",
      "Iteration 16199 Loss: 0.8550478219985962\n",
      "Iteration 16199 Loss: 1.2062780857086182\n",
      "Iteration 16200 Loss: 1.1921708583831787\n",
      "Iteration 16201 Loss: 1.3879317045211792\n",
      "Iteration 16202 Loss: 0.9931247234344482\n",
      "Iteration 16203 Loss: 1.2693167924880981\n",
      "Iteration 16204 Loss: 1.1822872161865234\n",
      "Iteration 16205 Loss: 1.126833438873291\n",
      "Iteration 16206 Loss: 0.7994821667671204\n",
      "Iteration 16207 Loss: 1.1401094198226929\n",
      "Iteration 16208 Loss: 0.9120465517044067\n",
      "Iteration 16209 Loss: 1.1173433065414429\n",
      "Iteration 16209 Loss: 1.1120645999908447\n",
      "Iteration 16210 Loss: 1.0474613904953003\n",
      "Iteration 16211 Loss: 1.2866398096084595\n",
      "Iteration 16212 Loss: 1.6763916015625\n",
      "Iteration 16213 Loss: 1.0865939855575562\n",
      "Iteration 16214 Loss: 0.9562749862670898\n",
      "Iteration 16215 Loss: 1.021830439567566\n",
      "Iteration 16216 Loss: 0.6882765889167786\n",
      "Iteration 16217 Loss: 0.9998773336410522\n",
      "Iteration 16218 Loss: 1.3349944353103638\n",
      "Iteration 16219 Loss: 1.4603991508483887\n",
      "Iteration 16219 Loss: 1.1558738946914673\n",
      "Iteration 16220 Loss: 0.9359474778175354\n",
      "Iteration 16221 Loss: 1.0211145877838135\n",
      "Iteration 16222 Loss: 0.8014905452728271\n",
      "Iteration 16223 Loss: 1.268248200416565\n",
      "Iteration 16224 Loss: 1.5519089698791504\n",
      "Iteration 16225 Loss: 1.3790829181671143\n",
      "Iteration 16226 Loss: 1.3817687034606934\n",
      "Iteration 16227 Loss: 1.090245008468628\n",
      "Iteration 16228 Loss: 1.7265937328338623\n",
      "Iteration 16229 Loss: 1.0360008478164673\n",
      "Iteration 16229 Loss: 1.2192400693893433\n",
      "Iteration 16230 Loss: 0.9543512463569641\n",
      "Iteration 16231 Loss: 1.2060606479644775\n",
      "Iteration 16232 Loss: 1.3921655416488647\n",
      "Iteration 16233 Loss: 0.9893481135368347\n",
      "Iteration 16234 Loss: 1.302391529083252\n",
      "Iteration 16235 Loss: 1.3812267780303955\n",
      "Iteration 16236 Loss: 1.0679922103881836\n",
      "Iteration 16237 Loss: 1.2862268686294556\n",
      "Iteration 16238 Loss: 1.2808656692504883\n",
      "Iteration 16239 Loss: 1.283163070678711\n",
      "Iteration 16239 Loss: 1.2143791913986206\n",
      "Iteration 16240 Loss: 1.2666327953338623\n",
      "Iteration 16241 Loss: 1.3235137462615967\n",
      "Iteration 16242 Loss: 1.290245771408081\n",
      "Iteration 16243 Loss: 1.1022638082504272\n",
      "Iteration 16244 Loss: 1.5635045766830444\n",
      "Iteration 16245 Loss: 1.3932197093963623\n",
      "Iteration 16246 Loss: 1.2051976919174194\n",
      "Iteration 16247 Loss: 1.1553946733474731\n",
      "Iteration 16248 Loss: 1.0644676685333252\n",
      "Iteration 16249 Loss: 1.2479372024536133\n",
      "Iteration 16249 Loss: 1.2612378597259521\n",
      "Iteration 16250 Loss: 1.5860909223556519\n",
      "Iteration 16251 Loss: 1.1885640621185303\n",
      "Iteration 16252 Loss: 1.2601736783981323\n",
      "Iteration 16253 Loss: 1.2807016372680664\n",
      "Iteration 16254 Loss: 0.850335955619812\n",
      "Iteration 16255 Loss: 0.6753401160240173\n",
      "Iteration 16256 Loss: 0.833979070186615\n",
      "Iteration 16257 Loss: 0.7935304045677185\n",
      "Iteration 16258 Loss: 1.3397504091262817\n",
      "Iteration 16259 Loss: 0.9033441543579102\n",
      "Iteration 16259 Loss: 1.071181058883667\n",
      "Iteration 16260 Loss: 1.5070741176605225\n",
      "Iteration 16261 Loss: 1.2944978475570679\n",
      "Iteration 16262 Loss: 1.19119131565094\n",
      "Iteration 16263 Loss: 0.6077741384506226\n",
      "Iteration 16264 Loss: 1.062509298324585\n",
      "Iteration 16265 Loss: 0.7780207395553589\n",
      "Iteration 16266 Loss: 0.6661180257797241\n",
      "Iteration 16267 Loss: 0.9824813604354858\n",
      "Iteration 16268 Loss: 1.5163416862487793\n",
      "Iteration 16269 Loss: 1.4437061548233032\n",
      "Iteration 16269 Loss: 1.1049714088439941\n",
      "Iteration 16270 Loss: 0.9617511034011841\n",
      "Iteration 16271 Loss: 1.133487343788147\n",
      "Iteration 16272 Loss: 1.4036566019058228\n",
      "Iteration 16273 Loss: 1.0510903596878052\n",
      "Iteration 16274 Loss: 1.2318228483200073\n",
      "Iteration 16275 Loss: 1.1759352684020996\n",
      "Iteration 16276 Loss: 1.3277463912963867\n",
      "Iteration 16277 Loss: 1.549803376197815\n",
      "Iteration 16278 Loss: 1.2908833026885986\n",
      "Iteration 16279 Loss: 0.9436693787574768\n",
      "Iteration 16279 Loss: 1.206984519958496\n",
      "Iteration 16280 Loss: 0.7626096606254578\n",
      "Iteration 16281 Loss: 1.0257407426834106\n",
      "Iteration 16282 Loss: 1.068910002708435\n",
      "Iteration 16283 Loss: 1.4430456161499023\n",
      "Iteration 16284 Loss: 1.1226986646652222\n",
      "Iteration 16285 Loss: 0.8128911256790161\n",
      "Iteration 16286 Loss: 1.1646250486373901\n",
      "Iteration 16287 Loss: 1.4073734283447266\n",
      "Iteration 16288 Loss: 1.4680943489074707\n",
      "Iteration 16289 Loss: 1.363625168800354\n",
      "Iteration 16289 Loss: 1.163961410522461\n",
      "Iteration 16290 Loss: 1.210286259651184\n",
      "Iteration 16291 Loss: 1.2203924655914307\n",
      "Iteration 16292 Loss: 1.652321457862854\n",
      "Iteration 16293 Loss: 0.9760273694992065\n",
      "Iteration 16294 Loss: 0.8259536027908325\n",
      "Iteration 16295 Loss: 1.4968374967575073\n",
      "Iteration 16296 Loss: 1.3264470100402832\n",
      "Iteration 16297 Loss: 1.5852677822113037\n",
      "Iteration 16298 Loss: 1.1537539958953857\n",
      "Iteration 16299 Loss: 1.1908214092254639\n",
      "Iteration 16299 Loss: 1.2638108730316162\n",
      "Iteration 16300 Loss: 0.8671544790267944\n",
      "Iteration 16301 Loss: 0.972407877445221\n",
      "Iteration 16302 Loss: 1.3364219665527344\n",
      "Iteration 16303 Loss: 1.1690109968185425\n",
      "Iteration 16304 Loss: 1.5690288543701172\n",
      "Iteration 16305 Loss: 1.1389374732971191\n",
      "Iteration 16306 Loss: 1.2509241104125977\n",
      "Iteration 16307 Loss: 1.3295691013336182\n",
      "Iteration 16308 Loss: 0.8697187304496765\n",
      "Iteration 16309 Loss: 1.3526325225830078\n",
      "Iteration 16309 Loss: 1.1855804920196533\n",
      "Iteration 16310 Loss: 1.3194574117660522\n",
      "Iteration 16311 Loss: 0.9584874510765076\n",
      "Iteration 16312 Loss: 1.4571987390518188\n",
      "Iteration 16313 Loss: 1.0813323259353638\n",
      "Iteration 16314 Loss: 0.7939688563346863\n",
      "Iteration 16315 Loss: 1.1625959873199463\n",
      "Iteration 16316 Loss: 1.2143237590789795\n",
      "Iteration 16317 Loss: 1.2312626838684082\n",
      "Iteration 16318 Loss: 0.921459436416626\n",
      "Iteration 16319 Loss: 1.364634394645691\n",
      "Iteration 16319 Loss: 1.1504720449447632\n",
      "Iteration 16320 Loss: 1.7380787134170532\n",
      "Iteration 16321 Loss: 1.364116907119751\n",
      "Iteration 16322 Loss: 0.862571656703949\n",
      "Iteration 16323 Loss: 0.8167774677276611\n",
      "Iteration 16324 Loss: 1.2485935688018799\n",
      "Iteration 16325 Loss: 1.5452269315719604\n",
      "Iteration 16326 Loss: 0.90244460105896\n",
      "Iteration 16327 Loss: 1.1520462036132812\n",
      "Iteration 16328 Loss: 1.3176445960998535\n",
      "Iteration 16329 Loss: 1.610063076019287\n",
      "Iteration 16329 Loss: 1.2557563781738281\n",
      "Iteration 16330 Loss: 1.5259231328964233\n",
      "Iteration 16331 Loss: 1.157509207725525\n",
      "Iteration 16332 Loss: 1.4094581604003906\n",
      "Iteration 16333 Loss: 1.0828354358673096\n",
      "Iteration 16334 Loss: 1.050227403640747\n",
      "Iteration 16335 Loss: 1.1125047206878662\n",
      "Iteration 16336 Loss: 1.6784602403640747\n",
      "Iteration 16337 Loss: 0.9721254706382751\n",
      "Iteration 16338 Loss: 1.25567626953125\n",
      "Iteration 16339 Loss: 1.415017008781433\n",
      "Iteration 16339 Loss: 1.265973687171936\n",
      "Iteration 16340 Loss: 1.0146747827529907\n",
      "Iteration 16341 Loss: 1.1440651416778564\n",
      "Iteration 16342 Loss: 1.018191933631897\n",
      "Iteration 16343 Loss: 1.2206765413284302\n",
      "Iteration 16344 Loss: 1.0912495851516724\n",
      "Iteration 16345 Loss: 1.0234041213989258\n",
      "Iteration 16346 Loss: 1.2461800575256348\n",
      "Iteration 16347 Loss: 1.5009685754776\n",
      "Iteration 16348 Loss: 1.0707937479019165\n",
      "Iteration 16349 Loss: 1.54371178150177\n",
      "Iteration 16349 Loss: 1.1873916387557983\n",
      "Iteration 16350 Loss: 1.6306222677230835\n",
      "Iteration 16351 Loss: 1.0591533184051514\n",
      "Iteration 16352 Loss: 1.0526288747787476\n",
      "Iteration 16353 Loss: 1.21726393699646\n",
      "Iteration 16354 Loss: 1.167544960975647\n",
      "Iteration 16355 Loss: 1.229254126548767\n",
      "Iteration 16356 Loss: 1.4038020372390747\n",
      "Iteration 16357 Loss: 1.2972105741500854\n",
      "Iteration 16358 Loss: 1.189876914024353\n",
      "Iteration 16359 Loss: 1.3404641151428223\n",
      "Iteration 16359 Loss: 1.258782148361206\n",
      "Iteration 16360 Loss: 1.1737537384033203\n",
      "Iteration 16361 Loss: 1.1027275323867798\n",
      "Iteration 16362 Loss: 1.1970306634902954\n",
      "Iteration 16363 Loss: 0.9922562837600708\n",
      "Iteration 16364 Loss: 1.0431008338928223\n",
      "Iteration 16365 Loss: 1.3059751987457275\n",
      "Iteration 16366 Loss: 1.2033426761627197\n",
      "Iteration 16367 Loss: 1.0447918176651\n",
      "Iteration 16368 Loss: 1.2825347185134888\n",
      "Iteration 16369 Loss: 1.174543857574463\n",
      "Iteration 16369 Loss: 1.152005672454834\n",
      "Iteration 16370 Loss: 1.1814470291137695\n",
      "Iteration 16371 Loss: 1.0227769613265991\n",
      "Iteration 16372 Loss: 0.9957926273345947\n",
      "Iteration 16373 Loss: 0.9666745662689209\n",
      "Iteration 16374 Loss: 1.1442012786865234\n",
      "Iteration 16375 Loss: 0.9610670208930969\n",
      "Iteration 16376 Loss: 0.8731745481491089\n",
      "Iteration 16377 Loss: 0.803281307220459\n",
      "Iteration 16378 Loss: 1.2932649850845337\n",
      "Iteration 16379 Loss: 1.4412051439285278\n",
      "Iteration 16379 Loss: 1.0682885646820068\n",
      "Iteration 16380 Loss: 1.206289529800415\n",
      "Iteration 16381 Loss: 1.3829506635665894\n",
      "Iteration 16382 Loss: 1.4735819101333618\n",
      "Iteration 16383 Loss: 1.3752381801605225\n",
      "Iteration 16384 Loss: 1.0179482698440552\n",
      "Iteration 16385 Loss: 1.4538627862930298\n",
      "Iteration 16386 Loss: 0.9170867800712585\n",
      "Iteration 16387 Loss: 1.0269845724105835\n",
      "Iteration 16388 Loss: 1.121659755706787\n",
      "Iteration 16389 Loss: 1.0821919441223145\n",
      "Iteration 16389 Loss: 1.2057793140411377\n",
      "Iteration 16390 Loss: 1.6105574369430542\n",
      "Iteration 16391 Loss: 1.2793718576431274\n",
      "Iteration 16392 Loss: 1.5834978818893433\n",
      "Iteration 16393 Loss: 1.2177892923355103\n",
      "Iteration 16394 Loss: 1.352996587753296\n",
      "Iteration 16395 Loss: 0.9513697028160095\n",
      "Iteration 16396 Loss: 1.0925997495651245\n",
      "Iteration 16397 Loss: 1.1500929594039917\n",
      "Iteration 16398 Loss: 0.8772948980331421\n",
      "Iteration 16399 Loss: 1.1466684341430664\n",
      "Iteration 16399 Loss: 1.2262238264083862\n",
      "Iteration 16400 Loss: 1.096259355545044\n",
      "Iteration 16401 Loss: 1.2012674808502197\n",
      "Iteration 16402 Loss: 1.2867820262908936\n",
      "Iteration 16403 Loss: 1.3116906881332397\n",
      "Iteration 16404 Loss: 1.4145426750183105\n",
      "Iteration 16405 Loss: 1.2344651222229004\n",
      "Iteration 16406 Loss: 0.9968199729919434\n",
      "Iteration 16407 Loss: 1.4415860176086426\n",
      "Iteration 16408 Loss: 0.9176583886146545\n",
      "Iteration 16409 Loss: 1.2606565952301025\n",
      "Iteration 16409 Loss: 1.2161728143692017\n",
      "Iteration 16410 Loss: 1.3464009761810303\n",
      "Iteration 16411 Loss: 1.0978041887283325\n",
      "Iteration 16412 Loss: 1.063923954963684\n",
      "Iteration 16413 Loss: 1.0147806406021118\n",
      "Iteration 16414 Loss: 1.264927625656128\n",
      "Iteration 16415 Loss: 1.2005140781402588\n",
      "Iteration 16416 Loss: 1.2498139142990112\n",
      "Iteration 16417 Loss: 1.4636726379394531\n",
      "Iteration 16418 Loss: 0.9235831499099731\n",
      "Iteration 16419 Loss: 1.3644802570343018\n",
      "Iteration 16419 Loss: 1.1989901065826416\n",
      "Iteration 16420 Loss: 1.2631679773330688\n",
      "Iteration 16421 Loss: 1.1258125305175781\n",
      "Iteration 16422 Loss: 1.2994292974472046\n",
      "Iteration 16423 Loss: 1.0460584163665771\n",
      "Iteration 16424 Loss: 0.8976333141326904\n",
      "Iteration 16425 Loss: 1.0971367359161377\n",
      "Iteration 16426 Loss: 1.2653799057006836\n",
      "Iteration 16427 Loss: 1.3442412614822388\n",
      "Iteration 16428 Loss: 1.1614384651184082\n",
      "Iteration 16429 Loss: 1.06221342086792\n",
      "Iteration 16429 Loss: 1.1562511920928955\n",
      "Iteration 16430 Loss: 1.1204825639724731\n",
      "Iteration 16431 Loss: 1.182549238204956\n",
      "Iteration 16432 Loss: 1.386185884475708\n",
      "Iteration 16433 Loss: 1.4150612354278564\n",
      "Iteration 16434 Loss: 1.2926729917526245\n",
      "Iteration 16435 Loss: 1.029240369796753\n",
      "Iteration 16436 Loss: 1.4652010202407837\n",
      "Iteration 16437 Loss: 1.5058704614639282\n",
      "Iteration 16438 Loss: 1.5831148624420166\n",
      "Iteration 16439 Loss: 0.9802406430244446\n",
      "Iteration 16439 Loss: 1.2960619926452637\n",
      "Iteration 16440 Loss: 1.0216248035430908\n",
      "Iteration 16441 Loss: 1.4022234678268433\n",
      "Iteration 16442 Loss: 1.082267165184021\n",
      "Iteration 16443 Loss: 1.3929609060287476\n",
      "Iteration 16444 Loss: 1.546578288078308\n",
      "Iteration 16445 Loss: 1.6121399402618408\n",
      "Iteration 16446 Loss: 1.5648083686828613\n",
      "Iteration 16447 Loss: 1.1983851194381714\n",
      "Iteration 16448 Loss: 1.2665073871612549\n",
      "Iteration 16449 Loss: 0.8001033067703247\n",
      "Iteration 16449 Loss: 1.2887598276138306\n",
      "Iteration 16450 Loss: 1.2059211730957031\n",
      "Iteration 16451 Loss: 1.1444252729415894\n",
      "Iteration 16452 Loss: 0.8989142775535583\n",
      "Iteration 16453 Loss: 0.7076091170310974\n",
      "Iteration 16454 Loss: 1.34171462059021\n",
      "Iteration 16455 Loss: 0.9251134395599365\n",
      "Iteration 16456 Loss: 1.1609611511230469\n",
      "Iteration 16457 Loss: 0.8825796842575073\n",
      "Iteration 16458 Loss: 0.8204089403152466\n",
      "Iteration 16459 Loss: 1.1569979190826416\n",
      "Iteration 16459 Loss: 1.0244646072387695\n",
      "Iteration 16460 Loss: 1.1818512678146362\n",
      "Iteration 16461 Loss: 1.201338529586792\n",
      "Iteration 16462 Loss: 0.8736776113510132\n",
      "Iteration 16463 Loss: 1.1625977754592896\n",
      "Iteration 16464 Loss: 1.46768319606781\n",
      "Iteration 16465 Loss: 0.9081625938415527\n",
      "Iteration 16466 Loss: 1.2983412742614746\n",
      "Iteration 16467 Loss: 1.2681753635406494\n",
      "Iteration 16468 Loss: 0.9772536754608154\n",
      "Iteration 16469 Loss: 0.9200316667556763\n",
      "Iteration 16469 Loss: 1.1259113550186157\n",
      "Iteration 16470 Loss: 1.3827989101409912\n",
      "Iteration 16471 Loss: 1.4307993650436401\n",
      "Iteration 16472 Loss: 1.1031619310379028\n",
      "Iteration 16473 Loss: 1.0970138311386108\n",
      "Iteration 16474 Loss: 1.50967538356781\n",
      "Iteration 16475 Loss: 0.9797030091285706\n",
      "Iteration 16476 Loss: 0.8790264129638672\n",
      "Iteration 16477 Loss: 1.6633132696151733\n",
      "Iteration 16478 Loss: 1.286928653717041\n",
      "Iteration 16479 Loss: 1.0357556343078613\n",
      "Iteration 16479 Loss: 1.2368175983428955\n",
      "Iteration 16480 Loss: 0.8836612105369568\n",
      "Iteration 16481 Loss: 1.6326926946640015\n",
      "Iteration 16482 Loss: 1.0744743347167969\n",
      "Iteration 16483 Loss: 1.2683321237564087\n",
      "Iteration 16484 Loss: 1.2237098217010498\n",
      "Iteration 16485 Loss: 1.4770259857177734\n",
      "Iteration 16486 Loss: 1.1593198776245117\n",
      "Iteration 16487 Loss: 0.831577479839325\n",
      "Iteration 16488 Loss: 1.59516441822052\n",
      "Iteration 16489 Loss: 1.0774778127670288\n",
      "Iteration 16489 Loss: 1.2223435640335083\n",
      "Iteration 16490 Loss: 1.067197322845459\n",
      "Iteration 16491 Loss: 1.095448613166809\n",
      "Iteration 16492 Loss: 0.9064692258834839\n",
      "Iteration 16493 Loss: 1.3056360483169556\n",
      "Iteration 16494 Loss: 1.2785106897354126\n",
      "Iteration 16495 Loss: 1.3563549518585205\n",
      "Iteration 16496 Loss: 1.3662089109420776\n",
      "Iteration 16497 Loss: 1.5669852495193481\n",
      "Iteration 16498 Loss: 0.9533270597457886\n",
      "Iteration 16499 Loss: 1.1199272871017456\n",
      "Iteration 16499 Loss: 1.2016065120697021\n",
      "Iteration 16500 Loss: 1.297153115272522\n",
      "Iteration 16501 Loss: 1.0831396579742432\n",
      "Iteration 16502 Loss: 1.2228310108184814\n",
      "Iteration 16503 Loss: 1.4488873481750488\n",
      "Iteration 16504 Loss: 1.2951873540878296\n",
      "Iteration 16505 Loss: 1.2858301401138306\n",
      "Iteration 16506 Loss: 1.3828749656677246\n",
      "Iteration 16507 Loss: 1.2116889953613281\n",
      "Iteration 16508 Loss: 1.1704905033111572\n",
      "Iteration 16509 Loss: 0.9428748488426208\n",
      "Iteration 16509 Loss: 1.234095811843872\n",
      "Iteration 16510 Loss: 1.0825682878494263\n",
      "Iteration 16511 Loss: 1.230392575263977\n",
      "Iteration 16512 Loss: 1.318699836730957\n",
      "Iteration 16513 Loss: 1.0870718955993652\n",
      "Iteration 16514 Loss: 1.3799442052841187\n",
      "Iteration 16515 Loss: 1.0721924304962158\n",
      "Iteration 16516 Loss: 1.192155361175537\n",
      "Iteration 16517 Loss: 1.5260499715805054\n",
      "Iteration 16518 Loss: 1.139328122138977\n",
      "Iteration 16519 Loss: 1.1252589225769043\n",
      "Iteration 16519 Loss: 1.2153661251068115\n",
      "Iteration 16520 Loss: 0.9071407318115234\n",
      "Iteration 16521 Loss: 1.3079878091812134\n",
      "Iteration 16522 Loss: 1.5239964723587036\n",
      "Iteration 16523 Loss: 1.1739329099655151\n",
      "Iteration 16524 Loss: 1.3671163320541382\n",
      "Iteration 16525 Loss: 1.3676398992538452\n",
      "Iteration 16526 Loss: 1.2879468202590942\n",
      "Iteration 16527 Loss: 1.479512333869934\n",
      "Iteration 16528 Loss: 1.4816135168075562\n",
      "Iteration 16529 Loss: 1.073085069656372\n",
      "Iteration 16529 Loss: 1.2969970703125\n",
      "Iteration 16530 Loss: 1.7683122158050537\n",
      "Iteration 16531 Loss: 0.9748097062110901\n",
      "Iteration 16532 Loss: 1.0884615182876587\n",
      "Iteration 16533 Loss: 1.1741310358047485\n",
      "Iteration 16534 Loss: 1.3177658319473267\n",
      "Iteration 16535 Loss: 1.1999768018722534\n",
      "Iteration 16536 Loss: 1.0344306230545044\n",
      "Iteration 16537 Loss: 1.1574511528015137\n",
      "Iteration 16538 Loss: 1.074764609336853\n",
      "Iteration 16539 Loss: 1.3484230041503906\n",
      "Iteration 16539 Loss: 1.2138526439666748\n",
      "Iteration 16540 Loss: 0.9397024512290955\n",
      "Iteration 16541 Loss: 1.1192686557769775\n",
      "Iteration 16542 Loss: 1.4415806531906128\n",
      "Iteration 16543 Loss: 1.5736011266708374\n",
      "Iteration 16544 Loss: 1.0658440589904785\n",
      "Iteration 16545 Loss: 0.9815363883972168\n",
      "Iteration 16546 Loss: 1.2990686893463135\n",
      "Iteration 16547 Loss: 1.0933669805526733\n",
      "Iteration 16548 Loss: 1.1287392377853394\n",
      "Iteration 16549 Loss: 1.6065771579742432\n",
      "Iteration 16549 Loss: 1.2249284982681274\n",
      "Iteration 16550 Loss: 1.2401320934295654\n",
      "Iteration 16551 Loss: 1.1529836654663086\n",
      "Iteration 16552 Loss: 1.1675167083740234\n",
      "Iteration 16553 Loss: 1.0298151969909668\n",
      "Iteration 16554 Loss: 1.1464707851409912\n",
      "Iteration 16555 Loss: 0.8151677250862122\n",
      "Iteration 16556 Loss: 1.37674081325531\n",
      "Iteration 16557 Loss: 1.17535400390625\n",
      "Iteration 16558 Loss: 0.9871017336845398\n",
      "Iteration 16559 Loss: 1.1106022596359253\n",
      "Iteration 16559 Loss: 1.1201884746551514\n",
      "Iteration 16560 Loss: 1.225518822669983\n",
      "Iteration 16561 Loss: 1.0018348693847656\n",
      "Iteration 16562 Loss: 1.3982484340667725\n",
      "Iteration 16563 Loss: 1.4094548225402832\n",
      "Iteration 16564 Loss: 0.9985128045082092\n",
      "Iteration 16565 Loss: 1.0828486680984497\n",
      "Iteration 16566 Loss: 1.140197515487671\n",
      "Iteration 16567 Loss: 1.1689211130142212\n",
      "Iteration 16568 Loss: 0.9010950922966003\n",
      "Iteration 16569 Loss: 0.9743464589118958\n",
      "Iteration 16569 Loss: 1.1300978660583496\n",
      "Iteration 16570 Loss: 1.3229931592941284\n",
      "Iteration 16571 Loss: 1.1334565877914429\n",
      "Iteration 16572 Loss: 1.2365787029266357\n",
      "Iteration 16573 Loss: 0.9914929866790771\n",
      "Iteration 16574 Loss: 0.860380232334137\n",
      "Iteration 16575 Loss: 1.2540987730026245\n",
      "Iteration 16576 Loss: 1.0919936895370483\n",
      "Iteration 16577 Loss: 1.112516164779663\n",
      "Iteration 16578 Loss: 0.9137533903121948\n",
      "Iteration 16579 Loss: 1.6721875667572021\n",
      "Iteration 16579 Loss: 1.1589452028274536\n",
      "Iteration 16580 Loss: 1.2670344114303589\n",
      "Iteration 16581 Loss: 1.3712973594665527\n",
      "Iteration 16582 Loss: 0.7733453512191772\n",
      "Iteration 16583 Loss: 0.9415603280067444\n",
      "Iteration 16584 Loss: 1.5294657945632935\n",
      "Iteration 16585 Loss: 1.4129626750946045\n",
      "Iteration 16586 Loss: 1.2902034521102905\n",
      "Iteration 16587 Loss: 1.1353689432144165\n",
      "Iteration 16588 Loss: 1.4925932884216309\n",
      "Iteration 16589 Loss: 1.010589599609375\n",
      "Iteration 16589 Loss: 1.2224421501159668\n",
      "Iteration 16590 Loss: 1.7210079431533813\n",
      "Iteration 16591 Loss: 1.0993890762329102\n",
      "Iteration 16592 Loss: 0.760051965713501\n",
      "Iteration 16593 Loss: 1.0576202869415283\n",
      "Iteration 16594 Loss: 1.1051292419433594\n",
      "Iteration 16595 Loss: 1.4670461416244507\n",
      "Iteration 16596 Loss: 1.0037094354629517\n",
      "Iteration 16597 Loss: 1.55494225025177\n",
      "Iteration 16598 Loss: 1.625739336013794\n",
      "Iteration 16599 Loss: 1.0169955492019653\n",
      "Iteration 16599 Loss: 1.2411630153656006\n",
      "Iteration 16600 Loss: 1.129248023033142\n",
      "Iteration 16601 Loss: 1.9673981666564941\n",
      "Iteration 16602 Loss: 1.3313422203063965\n",
      "Iteration 16603 Loss: 1.3802672624588013\n",
      "Iteration 16604 Loss: 1.0961443185806274\n",
      "Iteration 16605 Loss: 1.280811071395874\n",
      "Iteration 16606 Loss: 1.164343237876892\n",
      "Iteration 16607 Loss: 0.9217715263366699\n",
      "Iteration 16608 Loss: 1.0178056955337524\n",
      "Iteration 16609 Loss: 1.2922154664993286\n",
      "Iteration 16609 Loss: 1.2581346035003662\n",
      "Iteration 16610 Loss: 1.0735818147659302\n",
      "Iteration 16611 Loss: 0.8715438842773438\n",
      "Iteration 16612 Loss: 1.1232072114944458\n",
      "Iteration 16613 Loss: 1.3549913167953491\n",
      "Iteration 16614 Loss: 1.5515094995498657\n",
      "Iteration 16615 Loss: 1.2665408849716187\n",
      "Iteration 16616 Loss: 1.238574743270874\n",
      "Iteration 16617 Loss: 1.2954788208007812\n",
      "Iteration 16618 Loss: 1.0300389528274536\n",
      "Iteration 16619 Loss: 1.3306983709335327\n",
      "Iteration 16619 Loss: 1.2136166095733643\n",
      "Iteration 16620 Loss: 1.1344999074935913\n",
      "Iteration 16621 Loss: 1.3040682077407837\n",
      "Iteration 16622 Loss: 1.1596441268920898\n",
      "Iteration 16623 Loss: 0.9807889461517334\n",
      "Iteration 16624 Loss: 1.1868534088134766\n",
      "Iteration 16625 Loss: 1.095438003540039\n",
      "Iteration 16626 Loss: 1.0730617046356201\n",
      "Iteration 16627 Loss: 1.172320008277893\n",
      "Iteration 16628 Loss: 0.8991618752479553\n",
      "Iteration 16629 Loss: 1.4889848232269287\n",
      "Iteration 16629 Loss: 1.1494821310043335\n",
      "Iteration 16630 Loss: 1.2713162899017334\n",
      "Iteration 16631 Loss: 1.3244415521621704\n",
      "Iteration 16632 Loss: 1.20186185836792\n",
      "Iteration 16633 Loss: 1.236375331878662\n",
      "Iteration 16634 Loss: 1.232743263244629\n",
      "Iteration 16635 Loss: 1.2849644422531128\n",
      "Iteration 16636 Loss: 1.1402709484100342\n",
      "Iteration 16637 Loss: 1.2930041551589966\n",
      "Iteration 16638 Loss: 1.4050579071044922\n",
      "Iteration 16639 Loss: 0.9148711562156677\n",
      "Iteration 16639 Loss: 1.230490803718567\n",
      "Iteration 16640 Loss: 1.262296438217163\n",
      "Iteration 16641 Loss: 0.6506098508834839\n",
      "Iteration 16642 Loss: 1.1570050716400146\n",
      "Iteration 16643 Loss: 1.2614983320236206\n",
      "Iteration 16644 Loss: 1.2662352323532104\n",
      "Iteration 16645 Loss: 1.638290286064148\n",
      "Iteration 16646 Loss: 0.9732211232185364\n",
      "Iteration 16647 Loss: 0.805702805519104\n",
      "Iteration 16648 Loss: 1.1505537033081055\n",
      "Iteration 16649 Loss: 1.1314231157302856\n",
      "Iteration 16649 Loss: 1.1296836137771606\n",
      "Iteration 16650 Loss: 1.3262933492660522\n",
      "Iteration 16651 Loss: 1.5480449199676514\n",
      "Iteration 16652 Loss: 1.1417818069458008\n",
      "Iteration 16653 Loss: 1.052097201347351\n",
      "Iteration 16654 Loss: 1.2535468339920044\n",
      "Iteration 16655 Loss: 0.7725616693496704\n",
      "Iteration 16656 Loss: 1.287564992904663\n",
      "Iteration 16657 Loss: 1.4065866470336914\n",
      "Iteration 16658 Loss: 1.3393574953079224\n",
      "Iteration 16659 Loss: 1.687089443206787\n",
      "Iteration 16659 Loss: 1.2814924716949463\n",
      "Iteration 16660 Loss: 1.083862066268921\n",
      "Iteration 16661 Loss: 0.9859306812286377\n",
      "Iteration 16662 Loss: 1.0649328231811523\n",
      "Iteration 16663 Loss: 1.1815028190612793\n",
      "Iteration 16664 Loss: 1.4005738496780396\n",
      "Iteration 16665 Loss: 1.1622198820114136\n",
      "Iteration 16666 Loss: 1.209345817565918\n",
      "Iteration 16667 Loss: 1.0509129762649536\n",
      "Iteration 16668 Loss: 1.349362850189209\n",
      "Iteration 16669 Loss: 1.4705158472061157\n",
      "Iteration 16669 Loss: 1.195915937423706\n",
      "Iteration 16670 Loss: 1.2186230421066284\n",
      "Iteration 16671 Loss: 1.3173801898956299\n",
      "Iteration 16672 Loss: 1.2770603895187378\n",
      "Iteration 16673 Loss: 0.9327907562255859\n",
      "Iteration 16674 Loss: 1.30048668384552\n",
      "Iteration 16675 Loss: 0.7609025835990906\n",
      "Iteration 16676 Loss: 0.8780416250228882\n",
      "Iteration 16677 Loss: 1.3783591985702515\n",
      "Iteration 16678 Loss: 1.0480118989944458\n",
      "Iteration 16679 Loss: 1.1998298168182373\n",
      "Iteration 16679 Loss: 1.1311485767364502\n",
      "Iteration 16680 Loss: 1.0473549365997314\n",
      "Iteration 16681 Loss: 1.3899986743927002\n",
      "Iteration 16682 Loss: 1.0780799388885498\n",
      "Iteration 16683 Loss: 1.1835006475448608\n",
      "Iteration 16684 Loss: 1.3992202281951904\n",
      "Iteration 16685 Loss: 1.2426897287368774\n",
      "Iteration 16686 Loss: 1.0695834159851074\n",
      "Iteration 16687 Loss: 1.0349829196929932\n",
      "Iteration 16688 Loss: 1.1196703910827637\n",
      "Iteration 16689 Loss: 1.2611280679702759\n",
      "Iteration 16689 Loss: 1.1826207637786865\n",
      "Iteration 16690 Loss: 1.2740525007247925\n",
      "Iteration 16691 Loss: 1.768225073814392\n",
      "Iteration 16692 Loss: 1.323366403579712\n",
      "Iteration 16693 Loss: 0.7198225259780884\n",
      "Iteration 16694 Loss: 1.602646827697754\n",
      "Iteration 16695 Loss: 1.1967573165893555\n",
      "Iteration 16696 Loss: 1.3683433532714844\n",
      "Iteration 16697 Loss: 1.0236345529556274\n",
      "Iteration 16698 Loss: 1.3636282682418823\n",
      "Iteration 16699 Loss: 1.6573421955108643\n",
      "Iteration 16699 Loss: 1.3297820091247559\n",
      "Iteration 16700 Loss: 1.0375375747680664\n",
      "Iteration 16701 Loss: 1.0231945514678955\n",
      "Iteration 16702 Loss: 1.2351020574569702\n",
      "Iteration 16703 Loss: 1.2164205312728882\n",
      "Iteration 16704 Loss: 0.7247272729873657\n",
      "Iteration 16705 Loss: 1.2259875535964966\n",
      "Iteration 16706 Loss: 1.4581266641616821\n",
      "Iteration 16707 Loss: 1.2176661491394043\n",
      "Iteration 16708 Loss: 1.2931180000305176\n",
      "Iteration 16709 Loss: 1.2254718542099\n",
      "Iteration 16709 Loss: 1.1657352447509766\n",
      "Iteration 16710 Loss: 1.1968708038330078\n",
      "Iteration 16711 Loss: 1.7123777866363525\n",
      "Iteration 16712 Loss: 1.3097960948944092\n",
      "Iteration 16713 Loss: 1.3245413303375244\n",
      "Iteration 16714 Loss: 1.0714033842086792\n",
      "Iteration 16715 Loss: 1.3716487884521484\n",
      "Iteration 16716 Loss: 0.762671947479248\n",
      "Iteration 16717 Loss: 1.1060874462127686\n",
      "Iteration 16718 Loss: 1.4992799758911133\n",
      "Iteration 16719 Loss: 0.8513040542602539\n",
      "Iteration 16719 Loss: 1.2205981016159058\n",
      "Iteration 16720 Loss: 1.0131022930145264\n",
      "Iteration 16721 Loss: 1.4086322784423828\n",
      "Iteration 16722 Loss: 1.3582903146743774\n",
      "Iteration 16723 Loss: 1.4530233144760132\n",
      "Iteration 16724 Loss: 1.4038430452346802\n",
      "Iteration 16725 Loss: 1.0078096389770508\n",
      "Iteration 16726 Loss: 0.9301008582115173\n",
      "Iteration 16727 Loss: 1.5101128816604614\n",
      "Iteration 16728 Loss: 2.0612950325012207\n",
      "Iteration 16729 Loss: 0.9516493678092957\n",
      "Iteration 16729 Loss: 1.3097858428955078\n",
      "Iteration 16730 Loss: 1.1664146184921265\n",
      "Iteration 16731 Loss: 0.9370996356010437\n",
      "Iteration 16732 Loss: 1.2323360443115234\n",
      "Iteration 16733 Loss: 1.4187939167022705\n",
      "Iteration 16734 Loss: 1.374852180480957\n",
      "Iteration 16735 Loss: 0.8415308594703674\n",
      "Iteration 16736 Loss: 1.147574543952942\n",
      "Iteration 16737 Loss: 1.4407833814620972\n",
      "Iteration 16738 Loss: 1.2505983114242554\n",
      "Iteration 16739 Loss: 0.9695661067962646\n",
      "Iteration 16739 Loss: 1.177954912185669\n",
      "Iteration 16740 Loss: 0.9901422262191772\n",
      "Iteration 16741 Loss: 1.1913414001464844\n",
      "Iteration 16742 Loss: 0.9817773699760437\n",
      "Iteration 16743 Loss: 1.1746374368667603\n",
      "Iteration 16744 Loss: 1.1609749794006348\n",
      "Iteration 16745 Loss: 1.482538104057312\n",
      "Iteration 16746 Loss: 1.2777658700942993\n",
      "Iteration 16747 Loss: 1.2404371500015259\n",
      "Iteration 16748 Loss: 1.05540132522583\n",
      "Iteration 16749 Loss: 1.6866929531097412\n",
      "Iteration 16749 Loss: 1.2241709232330322\n",
      "Iteration 16750 Loss: 1.228149652481079\n",
      "Iteration 16751 Loss: 0.9103341698646545\n",
      "Iteration 16752 Loss: 1.255058765411377\n",
      "Iteration 16753 Loss: 1.2751539945602417\n",
      "Iteration 16754 Loss: 1.023486852645874\n",
      "Iteration 16755 Loss: 1.4287091493606567\n",
      "Iteration 16756 Loss: 1.3305333852767944\n",
      "Iteration 16757 Loss: 1.2896614074707031\n",
      "Iteration 16758 Loss: 1.7129460573196411\n",
      "Iteration 16759 Loss: 0.899135172367096\n",
      "Iteration 16759 Loss: 1.2353168725967407\n",
      "Iteration 16760 Loss: 1.2854068279266357\n",
      "Iteration 16761 Loss: 1.1252772808074951\n",
      "Iteration 16762 Loss: 1.552910327911377\n",
      "Iteration 16763 Loss: 1.4939002990722656\n",
      "Iteration 16764 Loss: 0.7222294211387634\n",
      "Iteration 16765 Loss: 1.3372653722763062\n",
      "Iteration 16766 Loss: 1.6376209259033203\n",
      "Iteration 16767 Loss: 1.2165796756744385\n",
      "Iteration 16768 Loss: 1.2120482921600342\n",
      "Iteration 16769 Loss: 0.9484384059906006\n",
      "Iteration 16769 Loss: 1.2531676292419434\n",
      "Iteration 16770 Loss: 0.8842954039573669\n",
      "Iteration 16771 Loss: 1.4188454151153564\n",
      "Iteration 16772 Loss: 0.8076072335243225\n",
      "Iteration 16773 Loss: 0.9909110069274902\n",
      "Iteration 16774 Loss: 1.149929404258728\n",
      "Iteration 16775 Loss: 0.8421385884284973\n",
      "Iteration 16776 Loss: 1.8557541370391846\n",
      "Iteration 16777 Loss: 1.3203259706497192\n",
      "Iteration 16778 Loss: 1.2086149454116821\n",
      "Iteration 16779 Loss: 0.9853740930557251\n",
      "Iteration 16779 Loss: 1.1463795900344849\n",
      "Iteration 16780 Loss: 1.239253044128418\n",
      "Iteration 16781 Loss: 1.1093803644180298\n",
      "Iteration 16782 Loss: 1.1000161170959473\n",
      "Iteration 16783 Loss: 0.7303000688552856\n",
      "Iteration 16784 Loss: 1.113898515701294\n",
      "Iteration 16785 Loss: 1.2794420719146729\n",
      "Iteration 16786 Loss: 0.9786351919174194\n",
      "Iteration 16787 Loss: 0.9854419231414795\n",
      "Iteration 16788 Loss: 1.2261650562286377\n",
      "Iteration 16789 Loss: 0.9229273200035095\n",
      "Iteration 16789 Loss: 1.068545937538147\n",
      "Iteration 16790 Loss: 1.3442144393920898\n",
      "Iteration 16791 Loss: 1.177590012550354\n",
      "Iteration 16792 Loss: 1.0944141149520874\n",
      "Iteration 16793 Loss: 1.232645869255066\n",
      "Iteration 16794 Loss: 1.4663400650024414\n",
      "Iteration 16795 Loss: 1.1260428428649902\n",
      "Iteration 16796 Loss: 0.9709445834159851\n",
      "Iteration 16797 Loss: 1.01886785030365\n",
      "Iteration 16798 Loss: 1.3159122467041016\n",
      "Iteration 16799 Loss: 1.009859561920166\n",
      "Iteration 16799 Loss: 1.1756831407546997\n",
      "Iteration 16800 Loss: 1.6898914575576782\n",
      "Iteration 16801 Loss: 1.0048158168792725\n",
      "Iteration 16802 Loss: 1.2808715105056763\n",
      "Iteration 16803 Loss: 1.456095576286316\n",
      "Iteration 16804 Loss: 1.105964183807373\n",
      "Iteration 16805 Loss: 1.1551473140716553\n",
      "Iteration 16806 Loss: 1.422852873802185\n",
      "Iteration 16807 Loss: 1.3811143636703491\n",
      "Iteration 16808 Loss: 1.06552255153656\n",
      "Iteration 16809 Loss: 0.8625304698944092\n",
      "Iteration 16809 Loss: 1.2424805164337158\n",
      "Iteration 16810 Loss: 1.464614987373352\n",
      "Iteration 16811 Loss: 1.429734230041504\n",
      "Iteration 16812 Loss: 1.6625828742980957\n",
      "Iteration 16813 Loss: 1.4658054113388062\n",
      "Iteration 16814 Loss: 1.2645175457000732\n",
      "Iteration 16815 Loss: 1.4379229545593262\n",
      "Iteration 16816 Loss: 1.272788405418396\n",
      "Iteration 16817 Loss: 0.726912260055542\n",
      "Iteration 16818 Loss: 1.3184994459152222\n",
      "Iteration 16819 Loss: 0.8974751234054565\n",
      "Iteration 16819 Loss: 1.2940852642059326\n",
      "Iteration 16820 Loss: 0.9711608290672302\n",
      "Iteration 16821 Loss: 1.4253708124160767\n",
      "Iteration 16822 Loss: 1.32278311252594\n",
      "Iteration 16823 Loss: 0.7497877478599548\n",
      "Iteration 16824 Loss: 1.0750715732574463\n",
      "Iteration 16825 Loss: 1.003105878829956\n",
      "Iteration 16826 Loss: 0.9942108988761902\n",
      "Iteration 16827 Loss: 1.3477866649627686\n",
      "Iteration 16828 Loss: 1.4885321855545044\n",
      "Iteration 16829 Loss: 1.1391645669937134\n",
      "Iteration 16829 Loss: 1.1516975164413452\n",
      "Iteration 16830 Loss: 1.5429773330688477\n",
      "Iteration 16831 Loss: 1.009285569190979\n",
      "Iteration 16832 Loss: 1.4542739391326904\n",
      "Iteration 16833 Loss: 0.9009187817573547\n",
      "Iteration 16834 Loss: 0.7034329175949097\n",
      "Iteration 16835 Loss: 1.0218852758407593\n",
      "Iteration 16836 Loss: 1.1066868305206299\n",
      "Iteration 16837 Loss: 0.8788923025131226\n",
      "Iteration 16838 Loss: 1.3882801532745361\n",
      "Iteration 16839 Loss: 1.0704251527786255\n",
      "Iteration 16839 Loss: 1.10770583152771\n",
      "Iteration 16840 Loss: 1.0931299924850464\n",
      "Iteration 16841 Loss: 1.5733675956726074\n",
      "Iteration 16842 Loss: 1.1874468326568604\n",
      "Iteration 16843 Loss: 1.4080500602722168\n",
      "Iteration 16844 Loss: 0.787919819355011\n",
      "Iteration 16845 Loss: 1.1371256113052368\n",
      "Iteration 16846 Loss: 1.0862469673156738\n",
      "Iteration 16847 Loss: 0.9852275252342224\n",
      "Iteration 16848 Loss: 1.1393115520477295\n",
      "Iteration 16849 Loss: 1.2626005411148071\n",
      "Iteration 16849 Loss: 1.166042685508728\n",
      "Iteration 16850 Loss: 1.2441898584365845\n",
      "Iteration 16851 Loss: 0.9412474036216736\n",
      "Iteration 16852 Loss: 1.090010643005371\n",
      "Iteration 16853 Loss: 1.4906020164489746\n",
      "Iteration 16854 Loss: 1.0923240184783936\n",
      "Iteration 16855 Loss: 1.317204236984253\n",
      "Iteration 16856 Loss: 1.4195523262023926\n",
      "Iteration 16857 Loss: 1.774810552597046\n",
      "Iteration 16858 Loss: 1.470898151397705\n",
      "Iteration 16859 Loss: 1.6756727695465088\n",
      "Iteration 16859 Loss: 1.3516511917114258\n",
      "Iteration 16860 Loss: 1.4751249551773071\n",
      "Iteration 16861 Loss: 1.3407033681869507\n",
      "Iteration 16862 Loss: 1.0204286575317383\n",
      "Iteration 16863 Loss: 1.142287254333496\n",
      "Iteration 16864 Loss: 1.0613396167755127\n",
      "Iteration 16865 Loss: 0.9106599688529968\n",
      "Iteration 16866 Loss: 1.0082616806030273\n",
      "Iteration 16867 Loss: 1.4380922317504883\n",
      "Iteration 16868 Loss: 1.167738676071167\n",
      "Iteration 16869 Loss: 1.1090326309204102\n",
      "Iteration 16869 Loss: 1.167366862297058\n",
      "Iteration 16870 Loss: 0.9523793458938599\n",
      "Iteration 16871 Loss: 0.8636898398399353\n",
      "Iteration 16872 Loss: 1.0326770544052124\n",
      "Iteration 16873 Loss: 1.1371468305587769\n",
      "Iteration 16874 Loss: 0.9269498586654663\n",
      "Iteration 16875 Loss: 0.7957687377929688\n",
      "Iteration 16876 Loss: 1.533022403717041\n",
      "Iteration 16877 Loss: 1.1534910202026367\n",
      "Iteration 16878 Loss: 1.1578426361083984\n",
      "Iteration 16879 Loss: 0.848602831363678\n",
      "Iteration 16879 Loss: 1.0401570796966553\n",
      "Iteration 16880 Loss: 1.0170842409133911\n",
      "Iteration 16881 Loss: 1.4489871263504028\n",
      "Iteration 16882 Loss: 1.3809363842010498\n",
      "Iteration 16883 Loss: 1.2445242404937744\n",
      "Iteration 16884 Loss: 1.261267066001892\n",
      "Iteration 16885 Loss: 1.262014627456665\n",
      "Iteration 16886 Loss: 1.5284783840179443\n",
      "Iteration 16887 Loss: 1.4451695680618286\n",
      "Iteration 16888 Loss: 1.1512343883514404\n",
      "Iteration 16889 Loss: 1.1805495023727417\n",
      "Iteration 16889 Loss: 1.2920243740081787\n",
      "Iteration 16890 Loss: 1.0990439653396606\n",
      "Iteration 16891 Loss: 1.1895897388458252\n",
      "Iteration 16892 Loss: 1.3409321308135986\n",
      "Iteration 16893 Loss: 0.9556488394737244\n",
      "Iteration 16894 Loss: 0.9676848649978638\n",
      "Iteration 16895 Loss: 1.165935754776001\n",
      "Iteration 16896 Loss: 1.270960807800293\n",
      "Iteration 16897 Loss: 1.5038856267929077\n",
      "Iteration 16898 Loss: 1.3426189422607422\n",
      "Iteration 16899 Loss: 1.2442245483398438\n",
      "Iteration 16899 Loss: 1.208052396774292\n",
      "Iteration 16900 Loss: 1.5790183544158936\n",
      "Iteration 16901 Loss: 1.3307427167892456\n",
      "Iteration 16902 Loss: 0.8778761029243469\n",
      "Iteration 16903 Loss: 0.9576969146728516\n",
      "Iteration 16904 Loss: 0.9690436124801636\n",
      "Iteration 16905 Loss: 0.9488334059715271\n",
      "Iteration 16906 Loss: 1.3400051593780518\n",
      "Iteration 16907 Loss: 1.1543444395065308\n",
      "Iteration 16908 Loss: 1.1608390808105469\n",
      "Iteration 16909 Loss: 1.1053835153579712\n",
      "Iteration 16909 Loss: 1.142378330230713\n",
      "Iteration 16910 Loss: 1.2049425840377808\n",
      "Iteration 16911 Loss: 1.0905264616012573\n",
      "Iteration 16912 Loss: 1.0824790000915527\n",
      "Iteration 16913 Loss: 1.1139159202575684\n",
      "Iteration 16914 Loss: 1.2538368701934814\n",
      "Iteration 16915 Loss: 1.4619758129119873\n",
      "Iteration 16916 Loss: 1.098517656326294\n",
      "Iteration 16917 Loss: 1.4092243909835815\n",
      "Iteration 16918 Loss: 1.1827445030212402\n",
      "Iteration 16919 Loss: 1.1412732601165771\n",
      "Iteration 16919 Loss: 1.2039436101913452\n",
      "Iteration 16920 Loss: 0.9209356307983398\n",
      "Iteration 16921 Loss: 1.380110740661621\n",
      "Iteration 16922 Loss: 1.2290743589401245\n",
      "Iteration 16923 Loss: 1.1307294368743896\n",
      "Iteration 16924 Loss: 0.7704789042472839\n",
      "Iteration 16925 Loss: 0.7251951098442078\n",
      "Iteration 16926 Loss: 1.2195744514465332\n",
      "Iteration 16927 Loss: 1.0428539514541626\n",
      "Iteration 16928 Loss: 0.8898524641990662\n",
      "Iteration 16929 Loss: 0.994178831577301\n",
      "Iteration 16929 Loss: 1.0302984714508057\n",
      "Iteration 16930 Loss: 0.8087707161903381\n",
      "Iteration 16931 Loss: 1.8450298309326172\n",
      "Iteration 16932 Loss: 1.3689379692077637\n",
      "Iteration 16933 Loss: 0.7795642018318176\n",
      "Iteration 16934 Loss: 1.173810362815857\n",
      "Iteration 16935 Loss: 1.4008928537368774\n",
      "Iteration 16936 Loss: 1.3427460193634033\n",
      "Iteration 16937 Loss: 1.1553256511688232\n",
      "Iteration 16938 Loss: 1.3446905612945557\n",
      "Iteration 16939 Loss: 1.606507420539856\n",
      "Iteration 16939 Loss: 1.2826275825500488\n",
      "Iteration 16940 Loss: 0.973677396774292\n",
      "Iteration 16941 Loss: 1.2255138158798218\n",
      "Iteration 16942 Loss: 1.3593413829803467\n",
      "Iteration 16943 Loss: 1.4800479412078857\n",
      "Iteration 16944 Loss: 1.198989987373352\n",
      "Iteration 16945 Loss: 1.2415555715560913\n",
      "Iteration 16946 Loss: 0.7214198708534241\n",
      "Iteration 16947 Loss: 1.655996561050415\n",
      "Iteration 16948 Loss: 1.5406337976455688\n",
      "Iteration 16949 Loss: 1.163978934288025\n",
      "Iteration 16949 Loss: 1.2561155557632446\n",
      "Iteration 16950 Loss: 1.3405731916427612\n",
      "Iteration 16951 Loss: 1.068938136100769\n",
      "Iteration 16952 Loss: 0.8990802764892578\n",
      "Iteration 16953 Loss: 1.2941877841949463\n",
      "Iteration 16954 Loss: 1.1020478010177612\n",
      "Iteration 16955 Loss: 0.9215796589851379\n",
      "Iteration 16956 Loss: 0.7582160234451294\n",
      "Iteration 16957 Loss: 0.9761475920677185\n",
      "Iteration 16958 Loss: 1.535351037979126\n",
      "Iteration 16959 Loss: 1.1591671705245972\n",
      "Iteration 16959 Loss: 1.1055288314819336\n",
      "Iteration 16960 Loss: 0.989611029624939\n",
      "Iteration 16961 Loss: 1.2609175443649292\n",
      "Iteration 16962 Loss: 1.1938226222991943\n",
      "Iteration 16963 Loss: 0.9674914479255676\n",
      "Iteration 16964 Loss: 1.2829262018203735\n",
      "Iteration 16965 Loss: 1.0607779026031494\n",
      "Iteration 16966 Loss: 1.3077054023742676\n",
      "Iteration 16967 Loss: 1.1400365829467773\n",
      "Iteration 16968 Loss: 1.044237732887268\n",
      "Iteration 16969 Loss: 1.190504550933838\n",
      "Iteration 16969 Loss: 1.1438031196594238\n",
      "Iteration 16970 Loss: 1.003746747970581\n",
      "Iteration 16971 Loss: 1.5207122564315796\n",
      "Iteration 16972 Loss: 1.3738340139389038\n",
      "Iteration 16973 Loss: 1.790785312652588\n",
      "Iteration 16974 Loss: 1.047101378440857\n",
      "Iteration 16975 Loss: 1.412426471710205\n",
      "Iteration 16976 Loss: 1.6529533863067627\n",
      "Iteration 16977 Loss: 1.2939119338989258\n",
      "Iteration 16978 Loss: 1.045752763748169\n",
      "Iteration 16979 Loss: 0.7136648893356323\n",
      "Iteration 16979 Loss: 1.2854888439178467\n",
      "Iteration 16980 Loss: 0.8420137166976929\n",
      "Iteration 16981 Loss: 1.1581348180770874\n",
      "Iteration 16982 Loss: 1.2350811958312988\n",
      "Iteration 16983 Loss: 1.357870101928711\n",
      "Iteration 16984 Loss: 1.1436773538589478\n",
      "Iteration 16985 Loss: 1.2559524774551392\n",
      "Iteration 16986 Loss: 0.87154620885849\n",
      "Iteration 16987 Loss: 1.0302339792251587\n",
      "Iteration 16988 Loss: 1.2415406703948975\n",
      "Iteration 16989 Loss: 0.95709228515625\n",
      "Iteration 16989 Loss: 1.109314203262329\n",
      "Iteration 16990 Loss: 1.6136332750320435\n",
      "Iteration 16991 Loss: 1.5363638401031494\n",
      "Iteration 16992 Loss: 1.1664495468139648\n",
      "Iteration 16993 Loss: 1.5502259731292725\n",
      "Iteration 16994 Loss: 1.52692449092865\n",
      "Iteration 16995 Loss: 0.9026986956596375\n",
      "Iteration 16996 Loss: 1.4231853485107422\n",
      "Iteration 16997 Loss: 1.2844007015228271\n",
      "Iteration 16998 Loss: 0.948320746421814\n",
      "Iteration 16999 Loss: 0.8598384857177734\n",
      "Iteration 16999 Loss: 1.281204104423523\n",
      "Iteration 17000 Loss: 1.01399564743042\n",
      "Iteration 17001 Loss: 1.1551637649536133\n",
      "Iteration 17002 Loss: 1.3845980167388916\n",
      "Iteration 17003 Loss: 1.716078758239746\n",
      "Iteration 17004 Loss: 1.042722225189209\n",
      "Iteration 17005 Loss: 1.0589195489883423\n",
      "Iteration 17006 Loss: 0.9314673542976379\n",
      "Iteration 17007 Loss: 1.3545054197311401\n",
      "Iteration 17008 Loss: 1.3006373643875122\n",
      "Iteration 17009 Loss: 0.9671107530593872\n",
      "Iteration 17009 Loss: 1.1925199031829834\n",
      "Iteration 17010 Loss: 1.5957748889923096\n",
      "Iteration 17011 Loss: 1.2259442806243896\n",
      "Iteration 17012 Loss: 0.8741136193275452\n",
      "Iteration 17013 Loss: 1.1352777481079102\n",
      "Iteration 17014 Loss: 1.2814738750457764\n",
      "Iteration 17015 Loss: 1.4241304397583008\n",
      "Iteration 17016 Loss: 1.1059958934783936\n",
      "Iteration 17017 Loss: 1.0203008651733398\n",
      "Iteration 17018 Loss: 1.271713137626648\n",
      "Iteration 17019 Loss: 1.2881041765213013\n",
      "Iteration 17019 Loss: 1.222282886505127\n",
      "Iteration 17020 Loss: 1.0902972221374512\n",
      "Iteration 17021 Loss: 1.209506630897522\n",
      "Iteration 17022 Loss: 0.8789375424385071\n",
      "Iteration 17023 Loss: 1.5334477424621582\n",
      "Iteration 17024 Loss: 1.4731667041778564\n",
      "Iteration 17025 Loss: 0.8964038491249084\n",
      "Iteration 17026 Loss: 1.3023213148117065\n",
      "Iteration 17027 Loss: 1.5457754135131836\n",
      "Iteration 17028 Loss: 1.4013532400131226\n",
      "Iteration 17029 Loss: 1.744460105895996\n",
      "Iteration 17029 Loss: 1.3075670003890991\n",
      "Iteration 17030 Loss: 1.5072180032730103\n",
      "Iteration 17031 Loss: 0.9989088773727417\n",
      "Iteration 17032 Loss: 1.1947413682937622\n",
      "Iteration 17033 Loss: 1.295371651649475\n",
      "Iteration 17034 Loss: 1.4697210788726807\n",
      "Iteration 17035 Loss: 1.3242387771606445\n",
      "Iteration 17036 Loss: 1.3564348220825195\n",
      "Iteration 17037 Loss: 1.2462540864944458\n",
      "Iteration 17038 Loss: 1.2152045965194702\n",
      "Iteration 17039 Loss: 1.0867539644241333\n",
      "Iteration 17039 Loss: 1.2694847583770752\n",
      "Iteration 17040 Loss: 1.2341474294662476\n",
      "Iteration 17041 Loss: 1.0072855949401855\n",
      "Iteration 17042 Loss: 1.2244142293930054\n",
      "Iteration 17043 Loss: 1.436133861541748\n",
      "Iteration 17044 Loss: 1.1916285753250122\n",
      "Iteration 17045 Loss: 1.6090019941329956\n",
      "Iteration 17046 Loss: 1.1599043607711792\n",
      "Iteration 17047 Loss: 0.7037889361381531\n",
      "Iteration 17048 Loss: 1.3888555765151978\n",
      "Iteration 17049 Loss: 0.9914007782936096\n",
      "Iteration 17049 Loss: 1.1946561336517334\n",
      "Iteration 17050 Loss: 0.8662070035934448\n",
      "Iteration 17051 Loss: 0.9333033561706543\n",
      "Iteration 17052 Loss: 1.0483192205429077\n",
      "Iteration 17053 Loss: 1.2481082677841187\n",
      "Iteration 17054 Loss: 1.1533845663070679\n",
      "Iteration 17055 Loss: 1.1497018337249756\n",
      "Iteration 17056 Loss: 1.3033900260925293\n",
      "Iteration 17057 Loss: 0.8728395104408264\n",
      "Iteration 17058 Loss: 1.0964783430099487\n",
      "Iteration 17059 Loss: 1.290767788887024\n",
      "Iteration 17059 Loss: 1.096250057220459\n",
      "Iteration 17060 Loss: 1.2510876655578613\n",
      "Iteration 17061 Loss: 1.269549012184143\n",
      "Iteration 17062 Loss: 1.361229419708252\n",
      "Iteration 17063 Loss: 1.3119109869003296\n",
      "Iteration 17064 Loss: 1.0731042623519897\n",
      "Iteration 17065 Loss: 1.5124272108078003\n",
      "Iteration 17066 Loss: 1.2124335765838623\n",
      "Iteration 17067 Loss: 0.9686089754104614\n",
      "Iteration 17068 Loss: 0.7646042704582214\n",
      "Iteration 17069 Loss: 1.175613522529602\n",
      "Iteration 17069 Loss: 1.1900569200515747\n",
      "Iteration 17070 Loss: 1.3337630033493042\n",
      "Iteration 17071 Loss: 0.83583003282547\n",
      "Iteration 17072 Loss: 1.211986780166626\n",
      "Iteration 17073 Loss: 1.4040155410766602\n",
      "Iteration 17074 Loss: 1.2736889123916626\n",
      "Iteration 17075 Loss: 1.0948225259780884\n",
      "Iteration 17076 Loss: 1.2603389024734497\n",
      "Iteration 17077 Loss: 1.0337938070297241\n",
      "Iteration 17078 Loss: 1.1313257217407227\n",
      "Iteration 17079 Loss: 0.904738187789917\n",
      "Iteration 17079 Loss: 1.148430347442627\n",
      "Iteration 17080 Loss: 0.9623514413833618\n",
      "Iteration 17081 Loss: 1.0663809776306152\n",
      "Iteration 17082 Loss: 1.412912130355835\n",
      "Iteration 17083 Loss: 0.9837282299995422\n",
      "Iteration 17084 Loss: 0.9556264281272888\n",
      "Iteration 17085 Loss: 1.3862617015838623\n",
      "Iteration 17086 Loss: 1.8866409063339233\n",
      "Iteration 17087 Loss: 1.1404966115951538\n",
      "Iteration 17088 Loss: 1.1708616018295288\n",
      "Iteration 17089 Loss: 0.9294164180755615\n",
      "Iteration 17089 Loss: 1.1894676685333252\n",
      "Iteration 17090 Loss: 1.0742688179016113\n",
      "Iteration 17091 Loss: 1.2298469543457031\n",
      "Iteration 17092 Loss: 1.0937999486923218\n",
      "Iteration 17093 Loss: 1.273925542831421\n",
      "Iteration 17094 Loss: 0.8340598344802856\n",
      "Iteration 17095 Loss: 1.0839853286743164\n",
      "Iteration 17096 Loss: 1.5367153882980347\n",
      "Iteration 17097 Loss: 1.201086163520813\n",
      "Iteration 17098 Loss: 1.6739227771759033\n",
      "Iteration 17099 Loss: 1.3597933053970337\n",
      "Iteration 17099 Loss: 1.236140489578247\n",
      "Iteration 17100 Loss: 1.052416205406189\n",
      "Iteration 17101 Loss: 0.8973216414451599\n",
      "Iteration 17102 Loss: 1.1914774179458618\n",
      "Iteration 17103 Loss: 0.8659220933914185\n",
      "Iteration 17104 Loss: 1.0883331298828125\n",
      "Iteration 17105 Loss: 1.0700300931930542\n",
      "Iteration 17106 Loss: 1.092052698135376\n",
      "Iteration 17107 Loss: 1.0911203622817993\n",
      "Iteration 17108 Loss: 0.7940574884414673\n",
      "Iteration 17109 Loss: 0.9800227284431458\n",
      "Iteration 17109 Loss: 1.0122754573822021\n",
      "Iteration 17110 Loss: 0.7387315630912781\n",
      "Iteration 17111 Loss: 1.1984375715255737\n",
      "Iteration 17112 Loss: 1.1860090494155884\n",
      "Iteration 17113 Loss: 1.1238785982131958\n",
      "Iteration 17114 Loss: 1.3564183712005615\n",
      "Iteration 17115 Loss: 1.6226977109909058\n",
      "Iteration 17116 Loss: 1.1134324073791504\n",
      "Iteration 17117 Loss: 1.3231043815612793\n",
      "Iteration 17118 Loss: 0.9299731850624084\n",
      "Iteration 17119 Loss: 0.8516882061958313\n",
      "Iteration 17119 Loss: 1.1444371938705444\n",
      "Iteration 17120 Loss: 0.991168200969696\n",
      "Iteration 17121 Loss: 0.9795886278152466\n",
      "Iteration 17122 Loss: 0.9273415207862854\n",
      "Iteration 17123 Loss: 1.0036165714263916\n",
      "Iteration 17124 Loss: 1.167175531387329\n",
      "Iteration 17125 Loss: 1.4537873268127441\n",
      "Iteration 17126 Loss: 1.0608329772949219\n",
      "Iteration 17127 Loss: 1.2260178327560425\n",
      "Iteration 17128 Loss: 1.1455236673355103\n",
      "Iteration 17129 Loss: 0.7805991768836975\n",
      "Iteration 17129 Loss: 1.0735652446746826\n",
      "Iteration 17130 Loss: 1.105972170829773\n",
      "Iteration 17131 Loss: 0.6535558104515076\n",
      "Iteration 17132 Loss: 1.367017388343811\n",
      "Iteration 17133 Loss: 0.9286822080612183\n",
      "Iteration 17134 Loss: 0.9716048836708069\n",
      "Iteration 17135 Loss: 1.1752053499221802\n",
      "Iteration 17136 Loss: 1.1543428897857666\n",
      "Iteration 17137 Loss: 1.0823485851287842\n",
      "Iteration 17138 Loss: 1.1201918125152588\n",
      "Iteration 17139 Loss: 1.134048581123352\n",
      "Iteration 17139 Loss: 1.0692970752716064\n",
      "Iteration 17140 Loss: 1.1488299369812012\n",
      "Iteration 17141 Loss: 1.249404788017273\n",
      "Iteration 17142 Loss: 1.5164052248001099\n",
      "Iteration 17143 Loss: 1.2631518840789795\n",
      "Iteration 17144 Loss: 0.7736654877662659\n",
      "Iteration 17145 Loss: 0.9565717577934265\n",
      "Iteration 17146 Loss: 0.9741449356079102\n",
      "Iteration 17147 Loss: 1.0442092418670654\n",
      "Iteration 17148 Loss: 1.1391288042068481\n",
      "Iteration 17149 Loss: 1.2029715776443481\n",
      "Iteration 17149 Loss: 1.1268483400344849\n",
      "Iteration 17150 Loss: 1.1340290307998657\n",
      "Iteration 17151 Loss: 1.0191500186920166\n",
      "Iteration 17152 Loss: 1.0338619947433472\n",
      "Iteration 17153 Loss: 1.2659108638763428\n",
      "Iteration 17154 Loss: 1.0696667432785034\n",
      "Iteration 17155 Loss: 1.4158273935317993\n",
      "Iteration 17156 Loss: 1.6751383543014526\n",
      "Iteration 17157 Loss: 1.0742546319961548\n",
      "Iteration 17158 Loss: 1.1622966527938843\n",
      "Iteration 17159 Loss: 1.1619477272033691\n",
      "Iteration 17159 Loss: 1.201208472251892\n",
      "Iteration 17160 Loss: 1.4935792684555054\n",
      "Iteration 17161 Loss: 1.2842762470245361\n",
      "Iteration 17162 Loss: 1.1613746881484985\n",
      "Iteration 17163 Loss: 1.1644636392593384\n",
      "Iteration 17164 Loss: 1.33089017868042\n",
      "Iteration 17165 Loss: 1.5500164031982422\n",
      "Iteration 17166 Loss: 1.0031040906906128\n",
      "Iteration 17167 Loss: 1.3361955881118774\n",
      "Iteration 17168 Loss: 1.1913878917694092\n",
      "Iteration 17169 Loss: 1.1425402164459229\n",
      "Iteration 17169 Loss: 1.2657829523086548\n",
      "Iteration 17170 Loss: 1.2510193586349487\n",
      "Iteration 17171 Loss: 0.8353601098060608\n",
      "Iteration 17172 Loss: 1.0787267684936523\n",
      "Iteration 17173 Loss: 1.4783120155334473\n",
      "Iteration 17174 Loss: 1.2645677328109741\n",
      "Iteration 17175 Loss: 0.9691277146339417\n",
      "Iteration 17176 Loss: 1.17657470703125\n",
      "Iteration 17177 Loss: 1.2440533638000488\n",
      "Iteration 17178 Loss: 1.240580439567566\n",
      "Iteration 17179 Loss: 0.8419442176818848\n",
      "Iteration 17179 Loss: 1.1380265951156616\n",
      "Iteration 17180 Loss: 0.6767405271530151\n",
      "Iteration 17181 Loss: 0.9947476387023926\n",
      "Iteration 17182 Loss: 0.9318559765815735\n",
      "Iteration 17183 Loss: 1.0349961519241333\n",
      "Iteration 17184 Loss: 1.639471173286438\n",
      "Iteration 17185 Loss: 1.1579278707504272\n",
      "Iteration 17186 Loss: 0.9889827370643616\n",
      "Iteration 17187 Loss: 1.328362226486206\n",
      "Iteration 17188 Loss: 1.3061630725860596\n",
      "Iteration 17189 Loss: 1.2153900861740112\n",
      "Iteration 17189 Loss: 1.1274638175964355\n",
      "Iteration 17190 Loss: 1.602829098701477\n",
      "Iteration 17191 Loss: 0.9405226111412048\n",
      "Iteration 17192 Loss: 0.76930832862854\n",
      "Iteration 17193 Loss: 1.567203164100647\n",
      "Iteration 17194 Loss: 1.3321837186813354\n",
      "Iteration 17195 Loss: 1.3034707307815552\n",
      "Iteration 17196 Loss: 1.071354627609253\n",
      "Iteration 17197 Loss: 1.297930121421814\n",
      "Iteration 17198 Loss: 1.185853123664856\n",
      "Iteration 17199 Loss: 0.8814293146133423\n",
      "Iteration 17199 Loss: 1.1952084302902222\n",
      "Iteration 17200 Loss: 1.2753865718841553\n",
      "Iteration 17201 Loss: 0.8290868997573853\n",
      "Iteration 17202 Loss: 0.8513414859771729\n",
      "Iteration 17203 Loss: 0.8855932950973511\n",
      "Iteration 17204 Loss: 1.2625874280929565\n",
      "Iteration 17205 Loss: 1.613271713256836\n",
      "Iteration 17206 Loss: 1.0383647680282593\n",
      "Iteration 17207 Loss: 1.064382791519165\n",
      "Iteration 17208 Loss: 0.999196469783783\n",
      "Iteration 17209 Loss: 1.0599223375320435\n",
      "Iteration 17209 Loss: 1.0879132747650146\n",
      "Iteration 17210 Loss: 1.371313214302063\n",
      "Iteration 17211 Loss: 1.146226406097412\n",
      "Iteration 17212 Loss: 0.9770619869232178\n",
      "Iteration 17213 Loss: 0.9478206038475037\n",
      "Iteration 17214 Loss: 1.2370328903198242\n",
      "Iteration 17215 Loss: 0.7320351600646973\n",
      "Iteration 17216 Loss: 1.066662073135376\n",
      "Iteration 17217 Loss: 1.1107679605484009\n",
      "Iteration 17218 Loss: 1.3718239068984985\n",
      "Iteration 17219 Loss: 0.9280804395675659\n",
      "Iteration 17219 Loss: 1.088882565498352\n",
      "Iteration 17220 Loss: 1.353930115699768\n",
      "Iteration 17221 Loss: 1.3394023180007935\n",
      "Iteration 17222 Loss: 0.9416849613189697\n",
      "Iteration 17223 Loss: 0.9188210368156433\n",
      "Iteration 17224 Loss: 1.4595979452133179\n",
      "Iteration 17225 Loss: 1.1147783994674683\n",
      "Iteration 17226 Loss: 1.3489774465560913\n",
      "Iteration 17227 Loss: 1.3455196619033813\n",
      "Iteration 17228 Loss: 1.1843591928482056\n",
      "Iteration 17229 Loss: 1.210707426071167\n",
      "Iteration 17229 Loss: 1.2217777967453003\n",
      "Iteration 17230 Loss: 1.074971318244934\n",
      "Iteration 17231 Loss: 1.1184165477752686\n",
      "Iteration 17232 Loss: 1.2269349098205566\n",
      "Iteration 17233 Loss: 1.073686957359314\n",
      "Iteration 17234 Loss: 1.4179450273513794\n",
      "Iteration 17235 Loss: 1.2668142318725586\n",
      "Iteration 17236 Loss: 1.0158648490905762\n",
      "Iteration 17237 Loss: 1.1612383127212524\n",
      "Iteration 17238 Loss: 1.248412847518921\n",
      "Iteration 17239 Loss: 1.532262921333313\n",
      "Iteration 17239 Loss: 1.2136547565460205\n",
      "Iteration 17240 Loss: 1.7546025514602661\n",
      "Iteration 17241 Loss: 1.1781163215637207\n",
      "Iteration 17242 Loss: 1.290413498878479\n",
      "Iteration 17243 Loss: 0.7389904856681824\n",
      "Iteration 17244 Loss: 0.9739187359809875\n",
      "Iteration 17245 Loss: 1.0557997226715088\n",
      "Iteration 17246 Loss: 1.151241421699524\n",
      "Iteration 17247 Loss: 1.1980654001235962\n",
      "Iteration 17248 Loss: 1.0858941078186035\n",
      "Iteration 17249 Loss: 1.1609901189804077\n",
      "Iteration 17249 Loss: 1.1588032245635986\n",
      "Iteration 17250 Loss: 1.228057622909546\n",
      "Iteration 17251 Loss: 0.5517496466636658\n",
      "Iteration 17252 Loss: 1.725906252861023\n",
      "Iteration 17253 Loss: 1.052413821220398\n",
      "Iteration 17254 Loss: 0.7554811835289001\n",
      "Iteration 17255 Loss: 0.9820348024368286\n",
      "Iteration 17256 Loss: 1.2325295209884644\n",
      "Iteration 17257 Loss: 1.1638572216033936\n",
      "Iteration 17258 Loss: 1.1053293943405151\n",
      "Iteration 17259 Loss: 0.864468514919281\n",
      "Iteration 17259 Loss: 1.0661828517913818\n",
      "Iteration 17260 Loss: 0.8021807670593262\n",
      "Iteration 17261 Loss: 1.204197645187378\n",
      "Iteration 17262 Loss: 1.3503167629241943\n",
      "Iteration 17263 Loss: 1.6670595407485962\n",
      "Iteration 17264 Loss: 1.1718742847442627\n",
      "Iteration 17265 Loss: 1.4840492010116577\n",
      "Iteration 17266 Loss: 1.1263928413391113\n",
      "Iteration 17267 Loss: 0.9137464165687561\n",
      "Iteration 17268 Loss: 1.2562737464904785\n",
      "Iteration 17269 Loss: 1.2656913995742798\n",
      "Iteration 17269 Loss: 1.2241781949996948\n",
      "Iteration 17270 Loss: 1.432774543762207\n",
      "Iteration 17271 Loss: 1.1931394338607788\n",
      "Iteration 17272 Loss: 1.158263087272644\n",
      "Iteration 17273 Loss: 1.2471493482589722\n",
      "Iteration 17274 Loss: 1.1028845310211182\n",
      "Iteration 17275 Loss: 1.1356159448623657\n",
      "Iteration 17276 Loss: 1.2522720098495483\n",
      "Iteration 17277 Loss: 1.4371273517608643\n",
      "Iteration 17278 Loss: 1.1357976198196411\n",
      "Iteration 17279 Loss: 0.9118381142616272\n",
      "Iteration 17279 Loss: 1.2006862163543701\n",
      "Iteration 17280 Loss: 1.1723707914352417\n",
      "Iteration 17281 Loss: 0.8231179118156433\n",
      "Iteration 17282 Loss: 1.2528668642044067\n",
      "Iteration 17283 Loss: 1.2094699144363403\n",
      "Iteration 17284 Loss: 1.08297860622406\n",
      "Iteration 17285 Loss: 1.6323540210723877\n",
      "Iteration 17286 Loss: 1.2937045097351074\n",
      "Iteration 17287 Loss: 1.5481351613998413\n",
      "Iteration 17288 Loss: 1.2280247211456299\n",
      "Iteration 17289 Loss: 1.0013669729232788\n",
      "Iteration 17289 Loss: 1.2244389057159424\n",
      "Iteration 17290 Loss: 1.1796025037765503\n",
      "Iteration 17291 Loss: 0.7928056120872498\n",
      "Iteration 17292 Loss: 1.1231573820114136\n",
      "Iteration 17293 Loss: 1.1086164712905884\n",
      "Iteration 17294 Loss: 1.308159589767456\n",
      "Iteration 17295 Loss: 1.4486932754516602\n",
      "Iteration 17296 Loss: 0.9490417838096619\n",
      "Iteration 17297 Loss: 1.3023583889007568\n",
      "Iteration 17298 Loss: 1.1904608011245728\n",
      "Iteration 17299 Loss: 1.2297327518463135\n",
      "Iteration 17299 Loss: 1.1632628440856934\n",
      "Iteration 17300 Loss: 1.2189658880233765\n",
      "Iteration 17301 Loss: 1.005937099456787\n",
      "Iteration 17302 Loss: 0.9079737067222595\n",
      "Iteration 17303 Loss: 1.3087846040725708\n",
      "Iteration 17304 Loss: 0.9678589105606079\n",
      "Iteration 17305 Loss: 1.5526057481765747\n",
      "Iteration 17306 Loss: 1.3118808269500732\n",
      "Iteration 17307 Loss: 1.8967361450195312\n",
      "Iteration 17308 Loss: 1.1957451105117798\n",
      "Iteration 17309 Loss: 1.2702080011367798\n",
      "Iteration 17309 Loss: 1.2636696100234985\n",
      "Iteration 17310 Loss: 1.6724607944488525\n",
      "Iteration 17311 Loss: 1.0643906593322754\n",
      "Iteration 17312 Loss: 0.8142361044883728\n",
      "Iteration 17313 Loss: 1.0062141418457031\n",
      "Iteration 17314 Loss: 1.1948078870773315\n",
      "Iteration 17315 Loss: 1.1759597063064575\n",
      "Iteration 17316 Loss: 0.555469274520874\n",
      "Iteration 17317 Loss: 1.459900975227356\n",
      "Iteration 17318 Loss: 1.3171865940093994\n",
      "Iteration 17319 Loss: 1.024119257926941\n",
      "Iteration 17319 Loss: 1.128474473953247\n",
      "Iteration 17320 Loss: 1.372862696647644\n",
      "Iteration 17321 Loss: 0.9235941171646118\n",
      "Iteration 17322 Loss: 1.2721444368362427\n",
      "Iteration 17323 Loss: 1.6276637315750122\n",
      "Iteration 17324 Loss: 1.2216792106628418\n",
      "Iteration 17325 Loss: 1.264766812324524\n",
      "Iteration 17326 Loss: 1.2526918649673462\n",
      "Iteration 17327 Loss: 1.1284799575805664\n",
      "Iteration 17328 Loss: 1.2924121618270874\n",
      "Iteration 17329 Loss: 0.8940317034721375\n",
      "Iteration 17329 Loss: 1.2250325679779053\n",
      "Iteration 17330 Loss: 1.0733308792114258\n",
      "Iteration 17331 Loss: 1.5745712518692017\n",
      "Iteration 17332 Loss: 1.199601411819458\n",
      "Iteration 17333 Loss: 1.8680880069732666\n",
      "Iteration 17334 Loss: 1.115437626838684\n",
      "Iteration 17335 Loss: 1.3808575868606567\n",
      "Iteration 17336 Loss: 1.0017180442810059\n",
      "Iteration 17337 Loss: 1.0517044067382812\n",
      "Iteration 17338 Loss: 1.3447728157043457\n",
      "Iteration 17339 Loss: 0.7663061618804932\n",
      "Iteration 17339 Loss: 1.2376388311386108\n",
      "Iteration 17340 Loss: 1.1356428861618042\n",
      "Iteration 17341 Loss: 1.2837728261947632\n",
      "Iteration 17342 Loss: 1.4608181715011597\n",
      "Iteration 17343 Loss: 1.0613009929656982\n",
      "Iteration 17344 Loss: 1.0494245290756226\n",
      "Iteration 17345 Loss: 1.141379714012146\n",
      "Iteration 17346 Loss: 1.157781720161438\n",
      "Iteration 17347 Loss: 1.4266778230667114\n",
      "Iteration 17348 Loss: 1.5245789289474487\n",
      "Iteration 17349 Loss: 1.1588795185089111\n",
      "Iteration 17349 Loss: 1.2400256395339966\n",
      "Iteration 17350 Loss: 0.9002703428268433\n",
      "Iteration 17351 Loss: 1.0227925777435303\n",
      "Iteration 17352 Loss: 1.0400099754333496\n",
      "Iteration 17353 Loss: 1.3810921907424927\n",
      "Iteration 17354 Loss: 0.9471695423126221\n",
      "Iteration 17355 Loss: 1.7312960624694824\n",
      "Iteration 17356 Loss: 1.243916392326355\n",
      "Iteration 17357 Loss: 0.9245927929878235\n",
      "Iteration 17358 Loss: 1.415968418121338\n",
      "Iteration 17359 Loss: 1.0811517238616943\n",
      "Iteration 17359 Loss: 1.1688259840011597\n",
      "Iteration 17360 Loss: 0.9807255864143372\n",
      "Iteration 17361 Loss: 1.3311766386032104\n",
      "Iteration 17362 Loss: 0.8278388977050781\n",
      "Iteration 17363 Loss: 0.9329715371131897\n",
      "Iteration 17364 Loss: 0.8873369097709656\n",
      "Iteration 17365 Loss: 1.2806261777877808\n",
      "Iteration 17366 Loss: 1.3314924240112305\n",
      "Iteration 17367 Loss: 1.090041995048523\n",
      "Iteration 17368 Loss: 1.0992639064788818\n",
      "Iteration 17369 Loss: 0.8474786877632141\n",
      "Iteration 17369 Loss: 1.0608952045440674\n",
      "Iteration 17370 Loss: 0.7420486807823181\n",
      "Iteration 17371 Loss: 1.5370301008224487\n",
      "Iteration 17372 Loss: 1.31650972366333\n",
      "Iteration 17373 Loss: 1.0064232349395752\n",
      "Iteration 17374 Loss: 1.1078917980194092\n",
      "Iteration 17375 Loss: 0.9901477694511414\n",
      "Iteration 17376 Loss: 0.9183825850486755\n",
      "Iteration 17377 Loss: 1.469008445739746\n",
      "Iteration 17378 Loss: 1.2674957513809204\n",
      "Iteration 17379 Loss: 0.974095344543457\n",
      "Iteration 17379 Loss: 1.1329033374786377\n",
      "Iteration 17380 Loss: 1.0619319677352905\n",
      "Iteration 17381 Loss: 1.318759799003601\n",
      "Iteration 17382 Loss: 1.1416112184524536\n",
      "Iteration 17383 Loss: 1.0685937404632568\n",
      "Iteration 17384 Loss: 0.9496838450431824\n",
      "Iteration 17385 Loss: 1.2864692211151123\n",
      "Iteration 17386 Loss: 1.044813632965088\n",
      "Iteration 17387 Loss: 1.3942792415618896\n",
      "Iteration 17388 Loss: 1.1688910722732544\n",
      "Iteration 17389 Loss: 1.101428747177124\n",
      "Iteration 17389 Loss: 1.1536462306976318\n",
      "Iteration 17390 Loss: 1.296629548072815\n",
      "Iteration 17391 Loss: 1.0762828588485718\n",
      "Iteration 17392 Loss: 1.0702431201934814\n",
      "Iteration 17393 Loss: 1.1059998273849487\n",
      "Iteration 17394 Loss: 1.2805519104003906\n",
      "Iteration 17395 Loss: 1.5800879001617432\n",
      "Iteration 17396 Loss: 0.9830047488212585\n",
      "Iteration 17397 Loss: 1.10597825050354\n",
      "Iteration 17398 Loss: 1.3345592021942139\n",
      "Iteration 17399 Loss: 1.4296907186508179\n",
      "Iteration 17399 Loss: 1.2263027429580688\n",
      "Iteration 17400 Loss: 1.0831712484359741\n",
      "Iteration 17401 Loss: 1.2404197454452515\n",
      "Iteration 17402 Loss: 0.9453269243240356\n",
      "Iteration 17403 Loss: 1.1333191394805908\n",
      "Iteration 17404 Loss: 1.6347465515136719\n",
      "Iteration 17405 Loss: 1.113837718963623\n",
      "Iteration 17406 Loss: 1.4006528854370117\n",
      "Iteration 17407 Loss: 1.0920413732528687\n",
      "Iteration 17408 Loss: 1.07234525680542\n",
      "Iteration 17409 Loss: 0.9945257902145386\n",
      "Iteration 17409 Loss: 1.1710387468338013\n",
      "Iteration 17410 Loss: 1.472508192062378\n",
      "Iteration 17411 Loss: 1.1376506090164185\n",
      "Iteration 17412 Loss: 1.174185037612915\n",
      "Iteration 17413 Loss: 1.3539100885391235\n",
      "Iteration 17414 Loss: 0.9222167730331421\n",
      "Iteration 17415 Loss: 0.8306352496147156\n",
      "Iteration 17416 Loss: 0.9544435143470764\n",
      "Iteration 17417 Loss: 1.1167182922363281\n",
      "Iteration 17418 Loss: 1.2143023014068604\n",
      "Iteration 17419 Loss: 1.8116487264633179\n",
      "Iteration 17419 Loss: 1.198821783065796\n",
      "Iteration 17420 Loss: 1.3070425987243652\n",
      "Iteration 17421 Loss: 1.37722647190094\n",
      "Iteration 17422 Loss: 1.2505452632904053\n",
      "Iteration 17423 Loss: 1.3608185052871704\n",
      "Iteration 17424 Loss: 1.3958210945129395\n",
      "Iteration 17425 Loss: 1.154727816581726\n",
      "Iteration 17426 Loss: 0.9168552756309509\n",
      "Iteration 17427 Loss: 0.76019686460495\n",
      "Iteration 17428 Loss: 0.6914489269256592\n",
      "Iteration 17429 Loss: 0.8157363533973694\n",
      "Iteration 17429 Loss: 1.1030418872833252\n",
      "Iteration 17430 Loss: 1.048190951347351\n",
      "Iteration 17431 Loss: 1.1521753072738647\n",
      "Iteration 17432 Loss: 1.0811920166015625\n",
      "Iteration 17433 Loss: 1.0515501499176025\n",
      "Iteration 17434 Loss: 1.305346131324768\n",
      "Iteration 17435 Loss: 1.2309832572937012\n",
      "Iteration 17436 Loss: 1.2205792665481567\n",
      "Iteration 17437 Loss: 1.1290712356567383\n",
      "Iteration 17438 Loss: 1.1886237859725952\n",
      "Iteration 17439 Loss: 1.256866216659546\n",
      "Iteration 17439 Loss: 1.1664578914642334\n",
      "Iteration 17440 Loss: 1.2573624849319458\n",
      "Iteration 17441 Loss: 1.3607518672943115\n",
      "Iteration 17442 Loss: 0.861870288848877\n",
      "Iteration 17443 Loss: 0.7737693786621094\n",
      "Iteration 17444 Loss: 1.522574543952942\n",
      "Iteration 17445 Loss: 1.0632354021072388\n",
      "Iteration 17446 Loss: 1.617053508758545\n",
      "Iteration 17447 Loss: 1.041377305984497\n",
      "Iteration 17448 Loss: 1.236687183380127\n",
      "Iteration 17449 Loss: 1.1174089908599854\n",
      "Iteration 17449 Loss: 1.1852091550827026\n",
      "Iteration 17450 Loss: 1.5546762943267822\n",
      "Iteration 17451 Loss: 1.0061181783676147\n",
      "Iteration 17452 Loss: 1.3749316930770874\n",
      "Iteration 17453 Loss: 0.9802280068397522\n",
      "Iteration 17454 Loss: 1.237074613571167\n",
      "Iteration 17455 Loss: 1.1712987422943115\n",
      "Iteration 17456 Loss: 1.1092835664749146\n",
      "Iteration 17457 Loss: 1.0158663988113403\n",
      "Iteration 17458 Loss: 1.0865198373794556\n",
      "Iteration 17459 Loss: 0.8807972073554993\n",
      "Iteration 17459 Loss: 1.1416795253753662\n",
      "Iteration 17460 Loss: 1.2151859998703003\n",
      "Iteration 17461 Loss: 1.2107179164886475\n",
      "Iteration 17462 Loss: 1.4329428672790527\n",
      "Iteration 17463 Loss: 1.2031244039535522\n",
      "Iteration 17464 Loss: 0.8879433870315552\n",
      "Iteration 17465 Loss: 1.1515733003616333\n",
      "Iteration 17466 Loss: 1.2710256576538086\n",
      "Iteration 17467 Loss: 0.9430224895477295\n",
      "Iteration 17468 Loss: 0.9928547143936157\n",
      "Iteration 17469 Loss: 1.1986466646194458\n",
      "Iteration 17469 Loss: 1.1507036685943604\n",
      "Iteration 17470 Loss: 1.3899526596069336\n",
      "Iteration 17471 Loss: 1.4287476539611816\n",
      "Iteration 17472 Loss: 1.3225326538085938\n",
      "Iteration 17473 Loss: 0.8943431377410889\n",
      "Iteration 17474 Loss: 0.9667999148368835\n",
      "Iteration 17475 Loss: 1.1116642951965332\n",
      "Iteration 17476 Loss: 1.1274739503860474\n",
      "Iteration 17477 Loss: 0.962245762348175\n",
      "Iteration 17478 Loss: 1.5730916261672974\n",
      "Iteration 17479 Loss: 1.1379222869873047\n",
      "Iteration 17479 Loss: 1.1914774179458618\n",
      "Iteration 17480 Loss: 1.4987949132919312\n",
      "Iteration 17481 Loss: 1.2632068395614624\n",
      "Iteration 17482 Loss: 1.1581692695617676\n",
      "Iteration 17483 Loss: 1.3795596361160278\n",
      "Iteration 17484 Loss: 0.9978846311569214\n",
      "Iteration 17485 Loss: 1.2745165824890137\n",
      "Iteration 17486 Loss: 1.095107078552246\n",
      "Iteration 17487 Loss: 1.1772092580795288\n",
      "Iteration 17488 Loss: 1.2443442344665527\n",
      "Iteration 17489 Loss: 1.1419682502746582\n",
      "Iteration 17489 Loss: 1.2230761051177979\n",
      "Iteration 17490 Loss: 1.3588097095489502\n",
      "Iteration 17491 Loss: 1.5918959379196167\n",
      "Iteration 17492 Loss: 1.4717538356781006\n",
      "Iteration 17493 Loss: 1.0388898849487305\n",
      "Iteration 17494 Loss: 1.1535398960113525\n",
      "Iteration 17495 Loss: 1.3050488233566284\n",
      "Iteration 17496 Loss: 1.6201112270355225\n",
      "Iteration 17497 Loss: 1.1946008205413818\n",
      "Iteration 17498 Loss: 0.9773334860801697\n",
      "Iteration 17499 Loss: 1.2507127523422241\n",
      "Iteration 17499 Loss: 1.2962696552276611\n",
      "Iteration 17500 Loss: 1.1554632186889648\n",
      "Iteration 17501 Loss: 1.0717567205429077\n",
      "Iteration 17502 Loss: 0.9744408130645752\n",
      "Iteration 17503 Loss: 0.8026999235153198\n",
      "Iteration 17504 Loss: 1.2902909517288208\n",
      "Iteration 17505 Loss: 1.1784251928329468\n",
      "Iteration 17506 Loss: 1.3155007362365723\n",
      "Iteration 17507 Loss: 1.1189050674438477\n",
      "Iteration 17508 Loss: 1.3374993801116943\n",
      "Iteration 17509 Loss: 1.1816329956054688\n",
      "Iteration 17509 Loss: 1.1426613330841064\n",
      "Iteration 17510 Loss: 1.4638619422912598\n",
      "Iteration 17511 Loss: 0.8938210606575012\n",
      "Iteration 17512 Loss: 1.363784909248352\n",
      "Iteration 17513 Loss: 0.7518502473831177\n",
      "Iteration 17514 Loss: 1.0373502969741821\n",
      "Iteration 17515 Loss: 1.0800213813781738\n",
      "Iteration 17516 Loss: 1.1858124732971191\n",
      "Iteration 17517 Loss: 1.261581540107727\n",
      "Iteration 17518 Loss: 0.7467344403266907\n",
      "Iteration 17519 Loss: 1.1807163953781128\n",
      "Iteration 17519 Loss: 1.0965534448623657\n",
      "Iteration 17520 Loss: 0.8740187287330627\n",
      "Iteration 17521 Loss: 1.589739203453064\n",
      "Iteration 17522 Loss: 1.1661056280136108\n",
      "Iteration 17523 Loss: 0.971095085144043\n",
      "Iteration 17524 Loss: 1.3968504667282104\n",
      "Iteration 17525 Loss: 1.0314993858337402\n",
      "Iteration 17526 Loss: 1.211411952972412\n",
      "Iteration 17527 Loss: 1.124021053314209\n",
      "Iteration 17528 Loss: 0.9008318781852722\n",
      "Iteration 17529 Loss: 1.321513056755066\n",
      "Iteration 17529 Loss: 1.1587088108062744\n",
      "Iteration 17530 Loss: 1.2262241840362549\n",
      "Iteration 17531 Loss: 1.4526773691177368\n",
      "Iteration 17532 Loss: 1.4475500583648682\n",
      "Iteration 17533 Loss: 0.6567175388336182\n",
      "Iteration 17534 Loss: 1.2230082750320435\n",
      "Iteration 17535 Loss: 1.23457670211792\n",
      "Iteration 17536 Loss: 1.4467558860778809\n",
      "Iteration 17537 Loss: 0.8207106590270996\n",
      "Iteration 17538 Loss: 1.316545009613037\n",
      "Iteration 17539 Loss: 1.3954986333847046\n",
      "Iteration 17539 Loss: 1.2220264673233032\n",
      "Iteration 17540 Loss: 1.1561243534088135\n",
      "Iteration 17541 Loss: 1.0778335332870483\n",
      "Iteration 17542 Loss: 1.4274760484695435\n",
      "Iteration 17543 Loss: 1.041023850440979\n",
      "Iteration 17544 Loss: 0.8582507967948914\n",
      "Iteration 17545 Loss: 1.2540723085403442\n",
      "Iteration 17546 Loss: 1.3274140357971191\n",
      "Iteration 17547 Loss: 1.268375277519226\n",
      "Iteration 17548 Loss: 0.7861005067825317\n",
      "Iteration 17549 Loss: 1.1432743072509766\n",
      "Iteration 17549 Loss: 1.133994460105896\n",
      "Iteration 17550 Loss: 1.0755558013916016\n",
      "Iteration 17551 Loss: 1.4059978723526\n",
      "Iteration 17552 Loss: 1.1188198328018188\n",
      "Iteration 17553 Loss: 0.9802247285842896\n",
      "Iteration 17554 Loss: 1.3982374668121338\n",
      "Iteration 17555 Loss: 1.1675138473510742\n",
      "Iteration 17556 Loss: 1.2914336919784546\n",
      "Iteration 17557 Loss: 1.5468454360961914\n",
      "Iteration 17558 Loss: 0.8516522645950317\n",
      "Iteration 17559 Loss: 1.3513474464416504\n",
      "Iteration 17559 Loss: 1.218762755393982\n",
      "Iteration 17560 Loss: 1.2239835262298584\n",
      "Iteration 17561 Loss: 1.1854472160339355\n",
      "Iteration 17562 Loss: 1.6104199886322021\n",
      "Iteration 17563 Loss: 1.0595378875732422\n",
      "Iteration 17564 Loss: 1.3279614448547363\n",
      "Iteration 17565 Loss: 0.9131569266319275\n",
      "Iteration 17566 Loss: 0.7601345777511597\n",
      "Iteration 17567 Loss: 0.7320433259010315\n",
      "Iteration 17568 Loss: 1.2116978168487549\n",
      "Iteration 17569 Loss: 1.238797664642334\n",
      "Iteration 17569 Loss: 1.1263179779052734\n",
      "Iteration 17570 Loss: 1.1708558797836304\n",
      "Iteration 17571 Loss: 1.055139422416687\n",
      "Iteration 17572 Loss: 1.2687774896621704\n",
      "Iteration 17573 Loss: 1.069427490234375\n",
      "Iteration 17574 Loss: 0.6612704992294312\n",
      "Iteration 17575 Loss: 1.0825600624084473\n",
      "Iteration 17576 Loss: 1.3303507566452026\n",
      "Iteration 17577 Loss: 0.7638632655143738\n",
      "Iteration 17578 Loss: 1.2029532194137573\n",
      "Iteration 17579 Loss: 0.9532464742660522\n",
      "Iteration 17579 Loss: 1.0558445453643799\n",
      "Iteration 17580 Loss: 1.2833144664764404\n",
      "Iteration 17581 Loss: 0.9643818736076355\n",
      "Iteration 17582 Loss: 1.427353024482727\n",
      "Iteration 17583 Loss: 0.9601465463638306\n",
      "Iteration 17584 Loss: 0.821925163269043\n",
      "Iteration 17585 Loss: 1.0392353534698486\n",
      "Iteration 17586 Loss: 0.7783100605010986\n",
      "Iteration 17587 Loss: 1.0440300703048706\n",
      "Iteration 17588 Loss: 1.0921814441680908\n",
      "Iteration 17589 Loss: 1.384657859802246\n",
      "Iteration 17589 Loss: 1.079553484916687\n",
      "Iteration 17590 Loss: 1.284592628479004\n",
      "Iteration 17591 Loss: 1.3760523796081543\n",
      "Iteration 17592 Loss: 1.207448959350586\n",
      "Iteration 17593 Loss: 0.9952053427696228\n",
      "Iteration 17594 Loss: 1.2501399517059326\n",
      "Iteration 17595 Loss: 0.8536777496337891\n",
      "Iteration 17596 Loss: 1.352020263671875\n",
      "Iteration 17597 Loss: 1.351110577583313\n",
      "Iteration 17598 Loss: 0.9338262677192688\n",
      "Iteration 17599 Loss: 0.9526134729385376\n",
      "Iteration 17599 Loss: 1.1556687355041504\n",
      "Iteration 17600 Loss: 1.2477983236312866\n",
      "Iteration 17601 Loss: 0.8515105247497559\n",
      "Iteration 17602 Loss: 1.282012939453125\n",
      "Iteration 17603 Loss: 1.2689716815948486\n",
      "Iteration 17604 Loss: 1.2767680883407593\n",
      "Iteration 17605 Loss: 1.2500615119934082\n",
      "Iteration 17606 Loss: 1.1621230840682983\n",
      "Iteration 17607 Loss: 1.2119914293289185\n",
      "Iteration 17608 Loss: 1.3658368587493896\n",
      "Iteration 17609 Loss: 1.0621215105056763\n",
      "Iteration 17609 Loss: 1.1979196071624756\n",
      "Iteration 17610 Loss: 1.0386041402816772\n",
      "Iteration 17611 Loss: 0.826840877532959\n",
      "Iteration 17612 Loss: 0.9635693430900574\n",
      "Iteration 17613 Loss: 1.1626801490783691\n",
      "Iteration 17614 Loss: 1.0868678092956543\n",
      "Iteration 17615 Loss: 1.3058700561523438\n",
      "Iteration 17616 Loss: 1.5215610265731812\n",
      "Iteration 17617 Loss: 1.409886360168457\n",
      "Iteration 17618 Loss: 1.3095240592956543\n",
      "Iteration 17619 Loss: 1.2685259580612183\n",
      "Iteration 17619 Loss: 1.1893929243087769\n",
      "Iteration 17620 Loss: 1.0398412942886353\n",
      "Iteration 17621 Loss: 1.3160673379898071\n",
      "Iteration 17622 Loss: 0.9634649753570557\n",
      "Iteration 17623 Loss: 0.9859510660171509\n",
      "Iteration 17624 Loss: 1.488415002822876\n",
      "Iteration 17625 Loss: 1.2041016817092896\n",
      "Iteration 17626 Loss: 1.0676217079162598\n",
      "Iteration 17627 Loss: 0.9438827633857727\n",
      "Iteration 17628 Loss: 0.9590182900428772\n",
      "Iteration 17629 Loss: 1.0939390659332275\n",
      "Iteration 17629 Loss: 1.1062302589416504\n",
      "Iteration 17630 Loss: 1.2609721422195435\n",
      "Iteration 17631 Loss: 0.8562652468681335\n",
      "Iteration 17632 Loss: 0.8794959187507629\n",
      "Iteration 17633 Loss: 1.1562777757644653\n",
      "Iteration 17634 Loss: 1.2230584621429443\n",
      "Iteration 17635 Loss: 1.6462345123291016\n",
      "Iteration 17636 Loss: 1.1751726865768433\n",
      "Iteration 17637 Loss: 1.058600664138794\n",
      "Iteration 17638 Loss: 1.3706190586090088\n",
      "Iteration 17639 Loss: 1.3119497299194336\n",
      "Iteration 17639 Loss: 1.1938645839691162\n",
      "Iteration 17640 Loss: 1.6328045129776\n",
      "Iteration 17641 Loss: 0.6953110098838806\n",
      "Iteration 17642 Loss: 0.8503754138946533\n",
      "Iteration 17643 Loss: 1.2592487335205078\n",
      "Iteration 17644 Loss: 0.8456509709358215\n",
      "Iteration 17645 Loss: 1.1602036952972412\n",
      "Iteration 17646 Loss: 1.0556542873382568\n",
      "Iteration 17647 Loss: 1.2000395059585571\n",
      "Iteration 17648 Loss: 1.5478028059005737\n",
      "Iteration 17649 Loss: 0.9937004446983337\n",
      "Iteration 17649 Loss: 1.1240792274475098\n",
      "Iteration 17650 Loss: 0.9572592973709106\n",
      "Iteration 17651 Loss: 1.195367693901062\n",
      "Iteration 17652 Loss: 1.090314507484436\n",
      "Iteration 17653 Loss: 0.9736659526824951\n",
      "Iteration 17654 Loss: 1.2674401998519897\n",
      "Iteration 17655 Loss: 1.5717058181762695\n",
      "Iteration 17656 Loss: 1.3133211135864258\n",
      "Iteration 17657 Loss: 1.0402767658233643\n",
      "Iteration 17658 Loss: 0.9992616176605225\n",
      "Iteration 17659 Loss: 0.7617087364196777\n",
      "Iteration 17659 Loss: 1.1170322895050049\n",
      "Iteration 17660 Loss: 1.421255350112915\n",
      "Iteration 17661 Loss: 1.1304491758346558\n",
      "Iteration 17662 Loss: 0.4421432316303253\n",
      "Iteration 17663 Loss: 1.4065183401107788\n",
      "Iteration 17664 Loss: 0.8943107724189758\n",
      "Iteration 17665 Loss: 1.230854868888855\n",
      "Iteration 17666 Loss: 0.880833089351654\n",
      "Iteration 17667 Loss: 0.974007248878479\n",
      "Iteration 17668 Loss: 0.9063642024993896\n",
      "Iteration 17669 Loss: 1.2874846458435059\n",
      "Iteration 17669 Loss: 1.057422161102295\n",
      "Iteration 17670 Loss: 0.8455164432525635\n",
      "Iteration 17671 Loss: 0.933343231678009\n",
      "Iteration 17672 Loss: 1.0904178619384766\n",
      "Iteration 17673 Loss: 0.9173539280891418\n",
      "Iteration 17674 Loss: 1.1511050462722778\n",
      "Iteration 17675 Loss: 1.0774956941604614\n",
      "Iteration 17676 Loss: 1.1311241388320923\n",
      "Iteration 17677 Loss: 1.3422316312789917\n",
      "Iteration 17678 Loss: 1.4062445163726807\n",
      "Iteration 17679 Loss: 1.0345659255981445\n",
      "Iteration 17679 Loss: 1.0929399728775024\n",
      "Iteration 17680 Loss: 1.3336530923843384\n",
      "Iteration 17681 Loss: 1.281083345413208\n",
      "Iteration 17682 Loss: 1.4003334045410156\n",
      "Iteration 17683 Loss: 1.3523989915847778\n",
      "Iteration 17684 Loss: 1.604506015777588\n",
      "Iteration 17685 Loss: 1.2546908855438232\n",
      "Iteration 17686 Loss: 1.3301119804382324\n",
      "Iteration 17687 Loss: 0.7412607669830322\n",
      "Iteration 17688 Loss: 1.267613172531128\n",
      "Iteration 17689 Loss: 0.8843600153923035\n",
      "Iteration 17689 Loss: 1.2450010776519775\n",
      "Iteration 17690 Loss: 1.389496088027954\n",
      "Iteration 17691 Loss: 1.1724770069122314\n",
      "Iteration 17692 Loss: 1.1191060543060303\n",
      "Iteration 17693 Loss: 1.3719900846481323\n",
      "Iteration 17694 Loss: 1.0762399435043335\n",
      "Iteration 17695 Loss: 1.23196280002594\n",
      "Iteration 17696 Loss: 1.2741082906723022\n",
      "Iteration 17697 Loss: 1.1374847888946533\n",
      "Iteration 17698 Loss: 0.6822971105575562\n",
      "Iteration 17699 Loss: 1.4293612241744995\n",
      "Iteration 17699 Loss: 1.1884523630142212\n",
      "Iteration 17700 Loss: 0.8064388632774353\n",
      "Iteration 17701 Loss: 1.1378997564315796\n",
      "Iteration 17702 Loss: 0.9642003774642944\n",
      "Iteration 17703 Loss: 0.962449848651886\n",
      "Iteration 17704 Loss: 1.0618647336959839\n",
      "Iteration 17705 Loss: 1.3685507774353027\n",
      "Iteration 17706 Loss: 0.9879839420318604\n",
      "Iteration 17707 Loss: 1.4188249111175537\n",
      "Iteration 17708 Loss: 1.082513451576233\n",
      "Iteration 17709 Loss: 1.1752841472625732\n",
      "Iteration 17709 Loss: 1.096601128578186\n",
      "Iteration 17710 Loss: 0.9044758081436157\n",
      "Iteration 17711 Loss: 0.8893819451332092\n",
      "Iteration 17712 Loss: 1.1398531198501587\n",
      "Iteration 17713 Loss: 1.2551896572113037\n",
      "Iteration 17714 Loss: 1.334405541419983\n",
      "Iteration 17715 Loss: 1.4117963314056396\n",
      "Iteration 17716 Loss: 1.3423271179199219\n",
      "Iteration 17717 Loss: 1.4126899242401123\n",
      "Iteration 17718 Loss: 1.1161361932754517\n",
      "Iteration 17719 Loss: 1.48695707321167\n",
      "Iteration 17719 Loss: 1.2293212413787842\n",
      "Iteration 17720 Loss: 1.2125496864318848\n",
      "Iteration 17721 Loss: 1.2367634773254395\n",
      "Iteration 17722 Loss: 0.8315957188606262\n",
      "Iteration 17723 Loss: 1.3845316171646118\n",
      "Iteration 17724 Loss: 1.0240075588226318\n",
      "Iteration 17725 Loss: 1.0271117687225342\n",
      "Iteration 17726 Loss: 0.9341433644294739\n",
      "Iteration 17727 Loss: 1.1526254415512085\n",
      "Iteration 17728 Loss: 1.4249807596206665\n",
      "Iteration 17729 Loss: 1.315881609916687\n",
      "Iteration 17729 Loss: 1.1544190645217896\n",
      "Iteration 17730 Loss: 1.0672497749328613\n",
      "Iteration 17731 Loss: 1.2251123189926147\n",
      "Iteration 17732 Loss: 1.2241798639297485\n",
      "Iteration 17733 Loss: 1.2140215635299683\n",
      "Iteration 17734 Loss: 1.4119148254394531\n",
      "Iteration 17735 Loss: 1.1898218393325806\n",
      "Iteration 17736 Loss: 1.1190228462219238\n",
      "Iteration 17737 Loss: 1.334641695022583\n",
      "Iteration 17738 Loss: 1.535535216331482\n",
      "Iteration 17739 Loss: 0.9111973643302917\n",
      "Iteration 17739 Loss: 1.2232697010040283\n",
      "Iteration 17740 Loss: 1.2572702169418335\n",
      "Iteration 17741 Loss: 1.2191247940063477\n",
      "Iteration 17742 Loss: 1.1354962587356567\n",
      "Iteration 17743 Loss: 0.9638043642044067\n",
      "Iteration 17744 Loss: 1.2211294174194336\n",
      "Iteration 17745 Loss: 1.5042924880981445\n",
      "Iteration 17746 Loss: 1.5903291702270508\n",
      "Iteration 17747 Loss: 1.1003869771957397\n",
      "Iteration 17748 Loss: 1.02572500705719\n",
      "Iteration 17749 Loss: 0.9306753277778625\n",
      "Iteration 17749 Loss: 1.1948233842849731\n",
      "Iteration 17750 Loss: 1.412282943725586\n",
      "Iteration 17751 Loss: 0.6257702112197876\n",
      "Iteration 17752 Loss: 0.9183676242828369\n",
      "Iteration 17753 Loss: 1.3460345268249512\n",
      "Iteration 17754 Loss: 0.8984013199806213\n",
      "Iteration 17755 Loss: 1.258669137954712\n",
      "Iteration 17756 Loss: 1.206516146659851\n",
      "Iteration 17757 Loss: 0.7969880700111389\n",
      "Iteration 17758 Loss: 0.6983117461204529\n",
      "Iteration 17759 Loss: 1.0169107913970947\n",
      "Iteration 17759 Loss: 1.0178253650665283\n",
      "Iteration 17760 Loss: 1.2768728733062744\n",
      "Iteration 17761 Loss: 1.2139874696731567\n",
      "Iteration 17762 Loss: 1.373984694480896\n",
      "Iteration 17763 Loss: 1.271346092224121\n",
      "Iteration 17764 Loss: 1.0658055543899536\n",
      "Iteration 17765 Loss: 1.321771264076233\n",
      "Iteration 17766 Loss: 0.9425899386405945\n",
      "Iteration 17767 Loss: 1.0859767198562622\n",
      "Iteration 17768 Loss: 0.9424646496772766\n",
      "Iteration 17769 Loss: 1.317749261856079\n",
      "Iteration 17769 Loss: 1.1812548637390137\n",
      "Iteration 17770 Loss: 1.1861417293548584\n",
      "Iteration 17771 Loss: 0.8355294466018677\n",
      "Iteration 17772 Loss: 1.171050786972046\n",
      "Iteration 17773 Loss: 1.3395037651062012\n",
      "Iteration 17774 Loss: 1.3564441204071045\n",
      "Iteration 17775 Loss: 1.1876707077026367\n",
      "Iteration 17776 Loss: 1.236413836479187\n",
      "Iteration 17777 Loss: 1.1708722114562988\n",
      "Iteration 17778 Loss: 1.2759548425674438\n",
      "Iteration 17779 Loss: 1.4772378206253052\n",
      "Iteration 17779 Loss: 1.2236818075180054\n",
      "Iteration 17780 Loss: 1.1741650104522705\n",
      "Iteration 17781 Loss: 1.0797005891799927\n",
      "Iteration 17782 Loss: 1.1939719915390015\n",
      "Iteration 17783 Loss: 1.0979135036468506\n",
      "Iteration 17784 Loss: 1.2871334552764893\n",
      "Iteration 17785 Loss: 1.2972592115402222\n",
      "Iteration 17786 Loss: 1.189493179321289\n",
      "Iteration 17787 Loss: 1.3740922212600708\n",
      "Iteration 17788 Loss: 1.2183080911636353\n",
      "Iteration 17789 Loss: 1.2676790952682495\n",
      "Iteration 17789 Loss: 1.2179715633392334\n",
      "Iteration 17790 Loss: 1.146732211112976\n",
      "Iteration 17791 Loss: 1.0311875343322754\n",
      "Iteration 17792 Loss: 0.6979495286941528\n",
      "Iteration 17793 Loss: 1.5236526727676392\n",
      "Iteration 17794 Loss: 1.1452339887619019\n",
      "Iteration 17795 Loss: 1.5171877145767212\n",
      "Iteration 17796 Loss: 0.7861292958259583\n",
      "Iteration 17797 Loss: 1.0616754293441772\n",
      "Iteration 17798 Loss: 0.9012513160705566\n",
      "Iteration 17799 Loss: 1.0044869184494019\n",
      "Iteration 17799 Loss: 1.0815485715866089\n",
      "Iteration 17800 Loss: 1.3135783672332764\n",
      "Iteration 17801 Loss: 0.953610360622406\n",
      "Iteration 17802 Loss: 1.5288593769073486\n",
      "Iteration 17803 Loss: 1.2807670831680298\n",
      "Iteration 17804 Loss: 0.7312403917312622\n",
      "Iteration 17805 Loss: 1.5918407440185547\n",
      "Iteration 17806 Loss: 1.6930564641952515\n",
      "Iteration 17807 Loss: 0.853569507598877\n",
      "Iteration 17808 Loss: 1.395630121231079\n",
      "Iteration 17809 Loss: 1.1370083093643188\n",
      "Iteration 17809 Loss: 1.2479159832000732\n",
      "Iteration 17810 Loss: 0.9154395461082458\n",
      "Iteration 17811 Loss: 0.9906625151634216\n",
      "Iteration 17812 Loss: 1.4734561443328857\n",
      "Iteration 17813 Loss: 1.2252064943313599\n",
      "Iteration 17814 Loss: 1.1065937280654907\n",
      "Iteration 17815 Loss: 1.3795732259750366\n",
      "Iteration 17816 Loss: 1.1778570413589478\n",
      "Iteration 17817 Loss: 1.389140248298645\n",
      "Iteration 17818 Loss: 1.1931335926055908\n",
      "Iteration 17819 Loss: 1.4378759860992432\n",
      "Iteration 17819 Loss: 1.2288938760757446\n",
      "Iteration 17820 Loss: 0.9373948574066162\n",
      "Iteration 17821 Loss: 1.066229224205017\n",
      "Iteration 17822 Loss: 0.9750865697860718\n",
      "Iteration 17823 Loss: 0.9736906290054321\n",
      "Iteration 17824 Loss: 0.9972965121269226\n",
      "Iteration 17825 Loss: 0.9333013296127319\n",
      "Iteration 17826 Loss: 1.4064539670944214\n",
      "Iteration 17827 Loss: 1.260679006576538\n",
      "Iteration 17828 Loss: 1.0050383806228638\n",
      "Iteration 17829 Loss: 1.4119608402252197\n",
      "Iteration 17829 Loss: 1.0967131853103638\n",
      "Iteration 17830 Loss: 1.176123857498169\n",
      "Iteration 17831 Loss: 1.0584499835968018\n",
      "Iteration 17832 Loss: 1.163619875907898\n",
      "Iteration 17833 Loss: 1.0017629861831665\n",
      "Iteration 17834 Loss: 0.7676184773445129\n",
      "Iteration 17835 Loss: 1.4880766868591309\n",
      "Iteration 17836 Loss: 0.8482560515403748\n",
      "Iteration 17837 Loss: 0.8268828988075256\n",
      "Iteration 17838 Loss: 1.3367493152618408\n",
      "Iteration 17839 Loss: 1.4203563928604126\n",
      "Iteration 17839 Loss: 1.1087896823883057\n",
      "Iteration 17840 Loss: 1.0062408447265625\n",
      "Iteration 17841 Loss: 1.2536205053329468\n",
      "Iteration 17842 Loss: 1.0211044549942017\n",
      "Iteration 17843 Loss: 1.0736645460128784\n",
      "Iteration 17844 Loss: 1.2426713705062866\n",
      "Iteration 17845 Loss: 1.0979048013687134\n",
      "Iteration 17846 Loss: 1.1951940059661865\n",
      "Iteration 17847 Loss: 1.0852224826812744\n",
      "Iteration 17848 Loss: 0.8745571970939636\n",
      "Iteration 17849 Loss: 1.6549198627471924\n",
      "Iteration 17849 Loss: 1.1505100727081299\n",
      "Iteration 17850 Loss: 1.452452540397644\n",
      "Iteration 17851 Loss: 1.1687158346176147\n",
      "Iteration 17852 Loss: 1.1849498748779297\n",
      "Iteration 17853 Loss: 1.072561264038086\n",
      "Iteration 17854 Loss: 0.9319007396697998\n",
      "Iteration 17855 Loss: 1.4627423286437988\n",
      "Iteration 17856 Loss: 1.4181424379348755\n",
      "Iteration 17857 Loss: 0.960291862487793\n",
      "Iteration 17858 Loss: 1.4657881259918213\n",
      "Iteration 17859 Loss: 0.9650037884712219\n",
      "Iteration 17859 Loss: 1.2082549333572388\n",
      "Iteration 17860 Loss: 1.0846238136291504\n",
      "Iteration 17861 Loss: 1.314119577407837\n",
      "Iteration 17862 Loss: 1.2979717254638672\n",
      "Iteration 17863 Loss: 1.1390573978424072\n",
      "Iteration 17864 Loss: 1.3026789426803589\n",
      "Iteration 17865 Loss: 1.5247372388839722\n",
      "Iteration 17866 Loss: 0.920599639415741\n",
      "Iteration 17867 Loss: 1.4102933406829834\n",
      "Iteration 17868 Loss: 1.2365976572036743\n",
      "Iteration 17869 Loss: 0.8574016690254211\n",
      "Iteration 17869 Loss: 1.208808183670044\n",
      "Iteration 17870 Loss: 1.4894030094146729\n",
      "Iteration 17871 Loss: 0.9986104965209961\n",
      "Iteration 17872 Loss: 1.3293641805648804\n",
      "Iteration 17873 Loss: 1.1614493131637573\n",
      "Iteration 17874 Loss: 1.0904415845870972\n",
      "Iteration 17875 Loss: 0.8400395512580872\n",
      "Iteration 17876 Loss: 1.1857198476791382\n",
      "Iteration 17877 Loss: 0.956762433052063\n",
      "Iteration 17878 Loss: 1.1228445768356323\n",
      "Iteration 17879 Loss: 1.2457733154296875\n",
      "Iteration 17879 Loss: 1.142040729522705\n",
      "Iteration 17880 Loss: 1.103137731552124\n",
      "Iteration 17881 Loss: 1.2769615650177002\n",
      "Iteration 17882 Loss: 1.2104339599609375\n",
      "Iteration 17883 Loss: 0.9305760860443115\n",
      "Iteration 17884 Loss: 1.4677846431732178\n",
      "Iteration 17885 Loss: 1.0925891399383545\n",
      "Iteration 17886 Loss: 1.1924850940704346\n",
      "Iteration 17887 Loss: 1.3308496475219727\n",
      "Iteration 17888 Loss: 1.2577286958694458\n",
      "Iteration 17889 Loss: 1.3307945728302002\n",
      "Iteration 17889 Loss: 1.2193341255187988\n",
      "Iteration 17890 Loss: 1.202860713005066\n",
      "Iteration 17891 Loss: 1.1907033920288086\n",
      "Iteration 17892 Loss: 1.190938115119934\n",
      "Iteration 17893 Loss: 1.2602447271347046\n",
      "Iteration 17894 Loss: 1.1398237943649292\n",
      "Iteration 17895 Loss: 1.2663160562515259\n",
      "Iteration 17896 Loss: 1.303877353668213\n",
      "Iteration 17897 Loss: 0.7262120842933655\n",
      "Iteration 17898 Loss: 1.1007404327392578\n",
      "Iteration 17899 Loss: 1.3943465948104858\n",
      "Iteration 17899 Loss: 1.177606463432312\n",
      "Iteration 17900 Loss: 1.4646109342575073\n",
      "Iteration 17901 Loss: 1.0758557319641113\n",
      "Iteration 17902 Loss: 1.232143759727478\n",
      "Iteration 17903 Loss: 0.9459442496299744\n",
      "Iteration 17904 Loss: 1.1921459436416626\n",
      "Iteration 17905 Loss: 1.3893024921417236\n",
      "Iteration 17906 Loss: 0.7823581695556641\n",
      "Iteration 17907 Loss: 1.2619434595108032\n",
      "Iteration 17908 Loss: 1.3916324377059937\n",
      "Iteration 17909 Loss: 1.1256928443908691\n",
      "Iteration 17909 Loss: 1.186163067817688\n",
      "Iteration 17910 Loss: 1.4108729362487793\n",
      "Iteration 17911 Loss: 1.1739866733551025\n",
      "Iteration 17912 Loss: 1.1676429510116577\n",
      "Iteration 17913 Loss: 1.1911611557006836\n",
      "Iteration 17914 Loss: 1.2340002059936523\n",
      "Iteration 17915 Loss: 1.4496532678604126\n",
      "Iteration 17916 Loss: 1.3497381210327148\n",
      "Iteration 17917 Loss: 1.053608775138855\n",
      "Iteration 17918 Loss: 1.2192249298095703\n",
      "Iteration 17919 Loss: 1.0540261268615723\n",
      "Iteration 17919 Loss: 1.230391502380371\n",
      "Iteration 17920 Loss: 0.8890151977539062\n",
      "Iteration 17921 Loss: 1.1624934673309326\n",
      "Iteration 17922 Loss: 0.8940340280532837\n",
      "Iteration 17923 Loss: 0.8144259452819824\n",
      "Iteration 17924 Loss: 1.4468510150909424\n",
      "Iteration 17925 Loss: 1.1250208616256714\n",
      "Iteration 17926 Loss: 0.7092651724815369\n",
      "Iteration 17927 Loss: 1.248369812965393\n",
      "Iteration 17928 Loss: 1.3313676118850708\n",
      "Iteration 17929 Loss: 1.3296369314193726\n",
      "Iteration 17929 Loss: 1.095047950744629\n",
      "Iteration 17930 Loss: 1.165045976638794\n",
      "Iteration 17931 Loss: 1.4532017707824707\n",
      "Iteration 17932 Loss: 1.037587285041809\n",
      "Iteration 17933 Loss: 1.0754808187484741\n",
      "Iteration 17934 Loss: 1.4973061084747314\n",
      "Iteration 17935 Loss: 0.6278973817825317\n",
      "Iteration 17936 Loss: 0.871249258518219\n",
      "Iteration 17937 Loss: 0.9017343521118164\n",
      "Iteration 17938 Loss: 1.2252252101898193\n",
      "Iteration 17939 Loss: 1.2026242017745972\n",
      "Iteration 17939 Loss: 1.105735182762146\n",
      "Iteration 17940 Loss: 1.2653000354766846\n",
      "Iteration 17941 Loss: 0.9926977157592773\n",
      "Iteration 17942 Loss: 0.9735362529754639\n",
      "Iteration 17943 Loss: 1.1546913385391235\n",
      "Iteration 17944 Loss: 1.1791164875030518\n",
      "Iteration 17945 Loss: 0.9008902311325073\n",
      "Iteration 17946 Loss: 1.099893569946289\n",
      "Iteration 17947 Loss: 1.0475882291793823\n",
      "Iteration 17948 Loss: 1.0845544338226318\n",
      "Iteration 17949 Loss: 1.0541396141052246\n",
      "Iteration 17949 Loss: 1.0752408504486084\n",
      "Iteration 17950 Loss: 1.2093404531478882\n",
      "Iteration 17951 Loss: 1.2027225494384766\n",
      "Iteration 17952 Loss: 1.0850532054901123\n",
      "Iteration 17953 Loss: 1.0158097743988037\n",
      "Iteration 17954 Loss: 1.2477269172668457\n",
      "Iteration 17955 Loss: 0.96905118227005\n",
      "Iteration 17956 Loss: 0.948217511177063\n",
      "Iteration 17957 Loss: 1.051531195640564\n",
      "Iteration 17958 Loss: 1.5821055173873901\n",
      "Iteration 17959 Loss: 1.2288265228271484\n",
      "Iteration 17959 Loss: 1.154038429260254\n",
      "Iteration 17960 Loss: 1.3367974758148193\n",
      "Iteration 17961 Loss: 0.9996251463890076\n",
      "Iteration 17962 Loss: 1.2079843282699585\n",
      "Iteration 17963 Loss: 1.6625488996505737\n",
      "Iteration 17964 Loss: 1.3457571268081665\n",
      "Iteration 17965 Loss: 1.2201122045516968\n",
      "Iteration 17966 Loss: 1.1033893823623657\n",
      "Iteration 17967 Loss: 0.9734022617340088\n",
      "Iteration 17968 Loss: 1.0239981412887573\n",
      "Iteration 17969 Loss: 1.2786732912063599\n",
      "Iteration 17969 Loss: 1.215228796005249\n",
      "Iteration 17970 Loss: 1.1457970142364502\n",
      "Iteration 17971 Loss: 1.4013283252716064\n",
      "Iteration 17972 Loss: 1.1696078777313232\n",
      "Iteration 17973 Loss: 0.9889999628067017\n",
      "Iteration 17974 Loss: 0.9361559748649597\n",
      "Iteration 17975 Loss: 0.8960840702056885\n",
      "Iteration 17976 Loss: 1.114413857460022\n",
      "Iteration 17977 Loss: 0.620476484298706\n",
      "Iteration 17978 Loss: 0.8248542547225952\n",
      "Iteration 17979 Loss: 0.8928784132003784\n",
      "Iteration 17979 Loss: 0.9990596771240234\n",
      "Iteration 17980 Loss: 1.0818252563476562\n",
      "Iteration 17981 Loss: 0.8970836997032166\n",
      "Iteration 17982 Loss: 1.71754789352417\n",
      "Iteration 17983 Loss: 0.9028939604759216\n",
      "Iteration 17984 Loss: 0.8935778141021729\n",
      "Iteration 17985 Loss: 0.9836196303367615\n",
      "Iteration 17986 Loss: 1.0303152799606323\n",
      "Iteration 17987 Loss: 1.0671383142471313\n",
      "Iteration 17988 Loss: 1.4712849855422974\n",
      "Iteration 17989 Loss: 1.124390721321106\n",
      "Iteration 17989 Loss: 1.116967797279358\n",
      "Iteration 17990 Loss: 1.091133952140808\n",
      "Iteration 17991 Loss: 1.303309679031372\n",
      "Iteration 17992 Loss: 1.0221978425979614\n",
      "Iteration 17993 Loss: 1.2822993993759155\n",
      "Iteration 17994 Loss: 1.1473885774612427\n",
      "Iteration 17995 Loss: 1.2415885925292969\n",
      "Iteration 17996 Loss: 1.5763237476348877\n",
      "Iteration 17997 Loss: 1.243590235710144\n",
      "Iteration 17998 Loss: 1.421151876449585\n",
      "Iteration 17999 Loss: 1.3740534782409668\n",
      "Iteration 17999 Loss: 1.270303726196289\n",
      "Iteration 18000 Loss: 0.8147968053817749\n",
      "Iteration 18001 Loss: 1.1552140712738037\n",
      "Iteration 18002 Loss: 1.17585289478302\n",
      "Iteration 18003 Loss: 0.9645227789878845\n",
      "Iteration 18004 Loss: 1.1041474342346191\n",
      "Iteration 18005 Loss: 1.4971833229064941\n",
      "Iteration 18006 Loss: 1.0889489650726318\n",
      "Iteration 18007 Loss: 1.3298308849334717\n",
      "Iteration 18008 Loss: 0.9686849117279053\n",
      "Iteration 18009 Loss: 1.1049919128417969\n",
      "Iteration 18009 Loss: 1.1204174757003784\n",
      "Iteration 18010 Loss: 0.9398965835571289\n",
      "Iteration 18011 Loss: 1.663001298904419\n",
      "Iteration 18012 Loss: 1.3120315074920654\n",
      "Iteration 18013 Loss: 1.3428781032562256\n",
      "Iteration 18014 Loss: 1.1261780261993408\n",
      "Iteration 18015 Loss: 0.9014933705329895\n",
      "Iteration 18016 Loss: 1.3269140720367432\n",
      "Iteration 18017 Loss: 0.9072463512420654\n",
      "Iteration 18018 Loss: 1.1017271280288696\n",
      "Iteration 18019 Loss: 1.253178358078003\n",
      "Iteration 18019 Loss: 1.1874544620513916\n",
      "Iteration 18020 Loss: 0.9278708100318909\n",
      "Iteration 18021 Loss: 0.9425118565559387\n",
      "Iteration 18022 Loss: 1.0715631246566772\n",
      "Iteration 18023 Loss: 1.42769455909729\n",
      "Iteration 18024 Loss: 1.1903339624404907\n",
      "Iteration 18025 Loss: 1.1614129543304443\n",
      "Iteration 18026 Loss: 1.508788824081421\n",
      "Iteration 18027 Loss: 1.7595590353012085\n",
      "Iteration 18028 Loss: 0.8545622825622559\n",
      "Iteration 18029 Loss: 1.1863824129104614\n",
      "Iteration 18029 Loss: 1.2030678987503052\n",
      "Iteration 18030 Loss: 1.3418458700180054\n",
      "Iteration 18031 Loss: 1.1777193546295166\n",
      "Iteration 18032 Loss: 0.9867421388626099\n",
      "Iteration 18033 Loss: 1.1575047969818115\n",
      "Iteration 18034 Loss: 1.3982499837875366\n",
      "Iteration 18035 Loss: 1.2507648468017578\n",
      "Iteration 18036 Loss: 0.958993673324585\n",
      "Iteration 18037 Loss: 0.9992729425430298\n",
      "Iteration 18038 Loss: 1.2507984638214111\n",
      "Iteration 18039 Loss: 1.1546874046325684\n",
      "Iteration 18039 Loss: 1.1676579713821411\n",
      "Iteration 18040 Loss: 1.4516650438308716\n",
      "Iteration 18041 Loss: 1.3515862226486206\n",
      "Iteration 18042 Loss: 1.151798129081726\n",
      "Iteration 18043 Loss: 1.2982368469238281\n",
      "Iteration 18044 Loss: 0.7578275799751282\n",
      "Iteration 18045 Loss: 1.2049837112426758\n",
      "Iteration 18046 Loss: 1.0555052757263184\n",
      "Iteration 18047 Loss: 1.2211960554122925\n",
      "Iteration 18048 Loss: 1.006739854812622\n",
      "Iteration 18049 Loss: 1.4655271768569946\n",
      "Iteration 18049 Loss: 1.1965066194534302\n",
      "Iteration 18050 Loss: 1.0590134859085083\n",
      "Iteration 18051 Loss: 1.2177428007125854\n",
      "Iteration 18052 Loss: 1.1123082637786865\n",
      "Iteration 18053 Loss: 1.1416609287261963\n",
      "Iteration 18054 Loss: 0.973680853843689\n",
      "Iteration 18055 Loss: 1.1847691535949707\n",
      "Iteration 18056 Loss: 1.0686492919921875\n",
      "Iteration 18057 Loss: 1.2883844375610352\n",
      "Iteration 18058 Loss: 0.9851697683334351\n",
      "Iteration 18059 Loss: 1.3619046211242676\n",
      "Iteration 18059 Loss: 1.1393283605575562\n",
      "Iteration 18060 Loss: 1.1075235605239868\n",
      "Iteration 18061 Loss: 1.0049399137496948\n",
      "Iteration 18062 Loss: 0.8492409586906433\n",
      "Iteration 18063 Loss: 0.9414187073707581\n",
      "Iteration 18064 Loss: 1.130534291267395\n",
      "Iteration 18065 Loss: 1.394175410270691\n",
      "Iteration 18066 Loss: 0.8182233572006226\n",
      "Iteration 18067 Loss: 1.6503856182098389\n",
      "Iteration 18068 Loss: 1.2936174869537354\n",
      "Iteration 18069 Loss: 1.0050630569458008\n",
      "Iteration 18069 Loss: 1.1195122003555298\n",
      "Iteration 18070 Loss: 1.0486371517181396\n",
      "Iteration 18071 Loss: 0.9170103669166565\n",
      "Iteration 18072 Loss: 1.0873358249664307\n",
      "Iteration 18073 Loss: 0.7152582406997681\n",
      "Iteration 18074 Loss: 1.1259835958480835\n",
      "Iteration 18075 Loss: 1.1817800998687744\n",
      "Iteration 18076 Loss: 1.3324685096740723\n",
      "Iteration 18077 Loss: 1.1610716581344604\n",
      "Iteration 18078 Loss: 1.227998971939087\n",
      "Iteration 18079 Loss: 1.0110039710998535\n",
      "Iteration 18079 Loss: 1.0808547735214233\n",
      "Iteration 18080 Loss: 0.9931356310844421\n",
      "Iteration 18081 Loss: 1.1615018844604492\n",
      "Iteration 18082 Loss: 1.1172163486480713\n",
      "Iteration 18083 Loss: 1.672929048538208\n",
      "Iteration 18084 Loss: 1.1086816787719727\n",
      "Iteration 18085 Loss: 1.3620818853378296\n",
      "Iteration 18086 Loss: 1.0701677799224854\n",
      "Iteration 18087 Loss: 1.1774197816848755\n",
      "Iteration 18088 Loss: 1.5855982303619385\n",
      "Iteration 18089 Loss: 1.0304861068725586\n",
      "Iteration 18089 Loss: 1.227921724319458\n",
      "Iteration 18090 Loss: 1.1165144443511963\n",
      "Iteration 18091 Loss: 1.3381130695343018\n",
      "Iteration 18092 Loss: 1.2588777542114258\n",
      "Iteration 18093 Loss: 1.2746148109436035\n",
      "Iteration 18094 Loss: 1.60629141330719\n",
      "Iteration 18095 Loss: 1.0559722185134888\n",
      "Iteration 18096 Loss: 1.06466543674469\n",
      "Iteration 18097 Loss: 1.2189230918884277\n",
      "Iteration 18098 Loss: 1.1206876039505005\n",
      "Iteration 18099 Loss: 1.0357952117919922\n",
      "Iteration 18099 Loss: 1.209045648574829\n",
      "Iteration 18100 Loss: 1.3450102806091309\n",
      "Iteration 18101 Loss: 1.1472277641296387\n",
      "Iteration 18102 Loss: 1.337313175201416\n",
      "Iteration 18103 Loss: 0.9773452877998352\n",
      "Iteration 18104 Loss: 1.2047979831695557\n",
      "Iteration 18105 Loss: 1.192737340927124\n",
      "Iteration 18106 Loss: 1.00281822681427\n",
      "Iteration 18107 Loss: 1.3449373245239258\n",
      "Iteration 18108 Loss: 1.498524785041809\n",
      "Iteration 18109 Loss: 0.8490133285522461\n",
      "Iteration 18109 Loss: 1.1899726390838623\n",
      "Iteration 18110 Loss: 1.521576166152954\n",
      "Iteration 18111 Loss: 1.071332573890686\n",
      "Iteration 18112 Loss: 1.5040394067764282\n",
      "Iteration 18113 Loss: 1.4010549783706665\n",
      "Iteration 18114 Loss: 1.1450519561767578\n",
      "Iteration 18115 Loss: 1.1181952953338623\n",
      "Iteration 18116 Loss: 1.294671654701233\n",
      "Iteration 18117 Loss: 1.0975085496902466\n",
      "Iteration 18118 Loss: 1.2533752918243408\n",
      "Iteration 18119 Loss: 0.9534406065940857\n",
      "Iteration 18119 Loss: 1.2360246181488037\n",
      "Iteration 18120 Loss: 1.3219541311264038\n",
      "Iteration 18121 Loss: 1.3474353551864624\n",
      "Iteration 18122 Loss: 1.2527167797088623\n",
      "Iteration 18123 Loss: 1.3832287788391113\n",
      "Iteration 18124 Loss: 1.0341134071350098\n",
      "Iteration 18125 Loss: 0.7582746744155884\n",
      "Iteration 18126 Loss: 1.3835220336914062\n",
      "Iteration 18127 Loss: 1.4455888271331787\n",
      "Iteration 18128 Loss: 1.186205506324768\n",
      "Iteration 18129 Loss: 1.0512951612472534\n",
      "Iteration 18129 Loss: 1.2164335250854492\n",
      "Iteration 18130 Loss: 1.1612902879714966\n",
      "Iteration 18131 Loss: 1.4292442798614502\n",
      "Iteration 18132 Loss: 1.3804571628570557\n",
      "Iteration 18133 Loss: 1.449879765510559\n",
      "Iteration 18134 Loss: 1.20204496383667\n",
      "Iteration 18135 Loss: 1.4032045602798462\n",
      "Iteration 18136 Loss: 1.201916217803955\n",
      "Iteration 18137 Loss: 1.5505257844924927\n",
      "Iteration 18138 Loss: 0.7504262328147888\n",
      "Iteration 18139 Loss: 1.1037521362304688\n",
      "Iteration 18139 Loss: 1.2632741928100586\n",
      "Iteration 18140 Loss: 1.148727297782898\n",
      "Iteration 18141 Loss: 0.8948834538459778\n",
      "Iteration 18142 Loss: 1.6355665922164917\n",
      "Iteration 18143 Loss: 1.125441074371338\n",
      "Iteration 18144 Loss: 1.0358425378799438\n",
      "Iteration 18145 Loss: 1.1179052591323853\n",
      "Iteration 18146 Loss: 1.4658408164978027\n",
      "Iteration 18147 Loss: 1.268074631690979\n",
      "Iteration 18148 Loss: 0.9744696021080017\n",
      "Iteration 18149 Loss: 1.0069884061813354\n",
      "Iteration 18149 Loss: 1.1673738956451416\n",
      "Iteration 18150 Loss: 1.3076331615447998\n",
      "Iteration 18151 Loss: 0.9189901351928711\n",
      "Iteration 18152 Loss: 0.8832388520240784\n",
      "Iteration 18153 Loss: 1.4713753461837769\n",
      "Iteration 18154 Loss: 1.3501089811325073\n",
      "Iteration 18155 Loss: 0.7877026796340942\n",
      "Iteration 18156 Loss: 0.9627740979194641\n",
      "Iteration 18157 Loss: 1.1093462705612183\n",
      "Iteration 18158 Loss: 1.0993717908859253\n",
      "Iteration 18159 Loss: 1.4396873712539673\n",
      "Iteration 18159 Loss: 1.1330229043960571\n",
      "Iteration 18160 Loss: 1.1858177185058594\n",
      "Iteration 18161 Loss: 1.338253140449524\n",
      "Iteration 18162 Loss: 0.9562922120094299\n",
      "Iteration 18163 Loss: 1.460877776145935\n",
      "Iteration 18164 Loss: 0.9225625395774841\n",
      "Iteration 18165 Loss: 1.0807589292526245\n",
      "Iteration 18166 Loss: 0.8910366892814636\n",
      "Iteration 18167 Loss: 1.0922176837921143\n",
      "Iteration 18168 Loss: 0.9819588661193848\n",
      "Iteration 18169 Loss: 0.7551369667053223\n",
      "Iteration 18169 Loss: 1.0664912462234497\n",
      "Iteration 18170 Loss: 0.937686562538147\n",
      "Iteration 18171 Loss: 0.5167585611343384\n",
      "Iteration 18172 Loss: 0.8164249062538147\n",
      "Iteration 18173 Loss: 1.1584584712982178\n",
      "Iteration 18174 Loss: 1.2164223194122314\n",
      "Iteration 18175 Loss: 1.221423625946045\n",
      "Iteration 18176 Loss: 1.3399300575256348\n",
      "Iteration 18177 Loss: 0.957854151725769\n",
      "Iteration 18178 Loss: 0.715275764465332\n",
      "Iteration 18179 Loss: 1.2302435636520386\n",
      "Iteration 18179 Loss: 1.0110477209091187\n",
      "Iteration 18180 Loss: 0.8925828337669373\n",
      "Iteration 18181 Loss: 1.3740540742874146\n",
      "Iteration 18182 Loss: 1.4398537874221802\n",
      "Iteration 18183 Loss: 1.1724437475204468\n",
      "Iteration 18184 Loss: 1.120271921157837\n",
      "Iteration 18185 Loss: 1.4673494100570679\n",
      "Iteration 18186 Loss: 0.7263686060905457\n",
      "Iteration 18187 Loss: 1.1831536293029785\n",
      "Iteration 18188 Loss: 1.1683967113494873\n",
      "Iteration 18189 Loss: 1.2152652740478516\n",
      "Iteration 18189 Loss: 1.1759741306304932\n",
      "Iteration 18190 Loss: 0.6461141109466553\n",
      "Iteration 18191 Loss: 1.1084169149398804\n",
      "Iteration 18192 Loss: 1.0376170873641968\n",
      "Iteration 18193 Loss: 1.113067626953125\n",
      "Iteration 18194 Loss: 1.3868879079818726\n",
      "Iteration 18195 Loss: 1.3615835905075073\n",
      "Iteration 18196 Loss: 1.460007667541504\n",
      "Iteration 18197 Loss: 1.069530963897705\n",
      "Iteration 18198 Loss: 0.8158483505249023\n",
      "Iteration 18199 Loss: 1.367313027381897\n",
      "Iteration 18199 Loss: 1.1366386413574219\n",
      "Iteration 18200 Loss: 1.0170543193817139\n",
      "Iteration 18201 Loss: 1.026530385017395\n",
      "Iteration 18202 Loss: 1.1391257047653198\n",
      "Iteration 18203 Loss: 1.3522999286651611\n",
      "Iteration 18204 Loss: 0.9261652231216431\n",
      "Iteration 18205 Loss: 1.0800259113311768\n",
      "Iteration 18206 Loss: 1.3668204545974731\n",
      "Iteration 18207 Loss: 0.9952497482299805\n",
      "Iteration 18208 Loss: 1.15345299243927\n",
      "Iteration 18209 Loss: 1.4568912982940674\n",
      "Iteration 18209 Loss: 1.1513617038726807\n",
      "Iteration 18210 Loss: 0.9729900360107422\n",
      "Iteration 18211 Loss: 1.0414631366729736\n",
      "Iteration 18212 Loss: 1.210192322731018\n",
      "Iteration 18213 Loss: 1.169142246246338\n",
      "Iteration 18214 Loss: 1.3237299919128418\n",
      "Iteration 18215 Loss: 1.1646270751953125\n",
      "Iteration 18216 Loss: 0.9566434621810913\n",
      "Iteration 18217 Loss: 1.1737443208694458\n",
      "Iteration 18218 Loss: 1.563398003578186\n",
      "Iteration 18219 Loss: 1.2386354207992554\n",
      "Iteration 18219 Loss: 1.181456446647644\n",
      "Iteration 18220 Loss: 0.993691623210907\n",
      "Iteration 18221 Loss: 1.2652041912078857\n",
      "Iteration 18222 Loss: 0.777536153793335\n",
      "Iteration 18223 Loss: 0.7270123958587646\n",
      "Iteration 18224 Loss: 1.3687547445297241\n",
      "Iteration 18225 Loss: 1.5366240739822388\n",
      "Iteration 18226 Loss: 1.1320641040802002\n",
      "Iteration 18227 Loss: 1.522886037826538\n",
      "Iteration 18228 Loss: 0.6466906070709229\n",
      "Iteration 18229 Loss: 1.2119303941726685\n",
      "Iteration 18229 Loss: 1.1182395219802856\n",
      "Iteration 18230 Loss: 1.0013266801834106\n",
      "Iteration 18231 Loss: 1.051958441734314\n",
      "Iteration 18232 Loss: 1.1145133972167969\n",
      "Iteration 18233 Loss: 1.2836885452270508\n",
      "Iteration 18234 Loss: 1.0616978406906128\n",
      "Iteration 18235 Loss: 1.2887964248657227\n",
      "Iteration 18236 Loss: 1.459484338760376\n",
      "Iteration 18237 Loss: 1.3236684799194336\n",
      "Iteration 18238 Loss: 1.181342363357544\n",
      "Iteration 18239 Loss: 1.5427227020263672\n",
      "Iteration 18239 Loss: 1.2309199571609497\n",
      "Iteration 18240 Loss: 1.2697358131408691\n",
      "Iteration 18241 Loss: 1.3557056188583374\n",
      "Iteration 18242 Loss: 1.125457525253296\n",
      "Iteration 18243 Loss: 1.3646854162216187\n",
      "Iteration 18244 Loss: 1.1858587265014648\n",
      "Iteration 18245 Loss: 0.8350326418876648\n",
      "Iteration 18246 Loss: 1.2006968259811401\n",
      "Iteration 18247 Loss: 0.9352335929870605\n",
      "Iteration 18248 Loss: 1.0338406562805176\n",
      "Iteration 18249 Loss: 1.2896084785461426\n",
      "Iteration 18249 Loss: 1.1595855951309204\n",
      "Iteration 18250 Loss: 1.6112406253814697\n",
      "Iteration 18251 Loss: 1.2756834030151367\n",
      "Iteration 18252 Loss: 1.176382064819336\n",
      "Iteration 18253 Loss: 1.4637268781661987\n",
      "Iteration 18254 Loss: 1.0418857336044312\n",
      "Iteration 18255 Loss: 1.1109704971313477\n",
      "Iteration 18256 Loss: 1.2558320760726929\n",
      "Iteration 18257 Loss: 1.0883792638778687\n",
      "Iteration 18258 Loss: 1.192376732826233\n",
      "Iteration 18259 Loss: 1.2288051843643188\n",
      "Iteration 18259 Loss: 1.2445281744003296\n",
      "Iteration 18260 Loss: 0.9948837757110596\n",
      "Iteration 18261 Loss: 0.9938901662826538\n",
      "Iteration 18262 Loss: 1.1450392007827759\n",
      "Iteration 18263 Loss: 1.4948064088821411\n",
      "Iteration 18264 Loss: 1.421286940574646\n",
      "Iteration 18265 Loss: 0.9727309346199036\n",
      "Iteration 18266 Loss: 1.4482892751693726\n",
      "Iteration 18267 Loss: 1.1382899284362793\n",
      "Iteration 18268 Loss: 0.9349801540374756\n",
      "Iteration 18269 Loss: 1.4410431385040283\n",
      "Iteration 18269 Loss: 1.1985238790512085\n",
      "Iteration 18270 Loss: 1.002388834953308\n",
      "Iteration 18271 Loss: 0.9710313677787781\n",
      "Iteration 18272 Loss: 1.1700191497802734\n",
      "Iteration 18273 Loss: 1.0779757499694824\n",
      "Iteration 18274 Loss: 0.9662690758705139\n",
      "Iteration 18275 Loss: 1.0922932624816895\n",
      "Iteration 18276 Loss: 1.2731842994689941\n",
      "Iteration 18277 Loss: 1.1757762432098389\n",
      "Iteration 18278 Loss: 1.0180344581604004\n",
      "Iteration 18279 Loss: 0.9180104732513428\n",
      "Iteration 18279 Loss: 1.0664982795715332\n",
      "Iteration 18280 Loss: 0.9970361590385437\n",
      "Iteration 18281 Loss: 1.0796912908554077\n",
      "Iteration 18282 Loss: 1.0134581327438354\n",
      "Iteration 18283 Loss: 1.2636820077896118\n",
      "Iteration 18284 Loss: 1.1127136945724487\n",
      "Iteration 18285 Loss: 1.3352354764938354\n",
      "Iteration 18286 Loss: 1.261616826057434\n",
      "Iteration 18287 Loss: 1.2429531812667847\n",
      "Iteration 18288 Loss: 1.3794937133789062\n",
      "Iteration 18289 Loss: 1.1746305227279663\n",
      "Iteration 18289 Loss: 1.1860511302947998\n",
      "Iteration 18290 Loss: 1.2334764003753662\n",
      "Iteration 18291 Loss: 0.8705264925956726\n",
      "Iteration 18292 Loss: 1.3843597173690796\n",
      "Iteration 18293 Loss: 1.5103354454040527\n",
      "Iteration 18294 Loss: 1.0342018604278564\n",
      "Iteration 18295 Loss: 1.0265538692474365\n",
      "Iteration 18296 Loss: 1.0619782209396362\n",
      "Iteration 18297 Loss: 1.2668362855911255\n",
      "Iteration 18298 Loss: 1.2333043813705444\n",
      "Iteration 18299 Loss: 0.9377344250679016\n",
      "Iteration 18299 Loss: 1.155930757522583\n",
      "Iteration 18300 Loss: 1.0549827814102173\n",
      "Iteration 18301 Loss: 1.0568976402282715\n",
      "Iteration 18302 Loss: 1.416316270828247\n",
      "Iteration 18303 Loss: 1.4162070751190186\n",
      "Iteration 18304 Loss: 1.2945197820663452\n",
      "Iteration 18305 Loss: 1.1466076374053955\n",
      "Iteration 18306 Loss: 1.7710729837417603\n",
      "Iteration 18307 Loss: 0.828595757484436\n",
      "Iteration 18308 Loss: 1.1954679489135742\n",
      "Iteration 18309 Loss: 1.1530325412750244\n",
      "Iteration 18309 Loss: 1.233370065689087\n",
      "Iteration 18310 Loss: 1.2971841096878052\n",
      "Iteration 18311 Loss: 0.7682485580444336\n",
      "Iteration 18312 Loss: 0.9930439591407776\n",
      "Iteration 18313 Loss: 1.137678623199463\n",
      "Iteration 18314 Loss: 1.1343815326690674\n",
      "Iteration 18315 Loss: 1.1937532424926758\n",
      "Iteration 18316 Loss: 1.02654230594635\n",
      "Iteration 18317 Loss: 1.0173826217651367\n",
      "Iteration 18318 Loss: 1.3183801174163818\n",
      "Iteration 18319 Loss: 1.1508601903915405\n",
      "Iteration 18319 Loss: 1.1037455797195435\n",
      "Iteration 18320 Loss: 1.0886744260787964\n",
      "Iteration 18321 Loss: 1.0317562818527222\n",
      "Iteration 18322 Loss: 1.1133618354797363\n",
      "Iteration 18323 Loss: 1.0648658275604248\n",
      "Iteration 18324 Loss: 1.108462929725647\n",
      "Iteration 18325 Loss: 0.8701925277709961\n",
      "Iteration 18326 Loss: 1.4580734968185425\n",
      "Iteration 18327 Loss: 1.1672818660736084\n",
      "Iteration 18328 Loss: 1.1043742895126343\n",
      "Iteration 18329 Loss: 1.3254119157791138\n",
      "Iteration 18329 Loss: 1.133245587348938\n",
      "Iteration 18330 Loss: 0.8760133385658264\n",
      "Iteration 18331 Loss: 1.1424086093902588\n",
      "Iteration 18332 Loss: 1.4090087413787842\n",
      "Iteration 18333 Loss: 1.6710857152938843\n",
      "Iteration 18334 Loss: 1.2069947719573975\n",
      "Iteration 18335 Loss: 1.2243906259536743\n",
      "Iteration 18336 Loss: 0.9380127191543579\n",
      "Iteration 18337 Loss: 0.860907793045044\n",
      "Iteration 18338 Loss: 1.3508687019348145\n",
      "Iteration 18339 Loss: 1.2414478063583374\n",
      "Iteration 18339 Loss: 1.192113995552063\n",
      "Iteration 18340 Loss: 1.2582910060882568\n",
      "Iteration 18341 Loss: 1.2496358156204224\n",
      "Iteration 18342 Loss: 1.3484948873519897\n",
      "Iteration 18343 Loss: 1.1230378150939941\n",
      "Iteration 18344 Loss: 0.9365690350532532\n",
      "Iteration 18345 Loss: 1.322217345237732\n",
      "Iteration 18346 Loss: 1.1816518306732178\n",
      "Iteration 18347 Loss: 0.8801576495170593\n",
      "Iteration 18348 Loss: 0.9504031538963318\n",
      "Iteration 18349 Loss: 1.1475516557693481\n",
      "Iteration 18349 Loss: 1.139801025390625\n",
      "Iteration 18350 Loss: 1.2008310556411743\n",
      "Iteration 18351 Loss: 0.8671161532402039\n",
      "Iteration 18352 Loss: 1.4672026634216309\n",
      "Iteration 18353 Loss: 0.9939532279968262\n",
      "Iteration 18354 Loss: 1.145254373550415\n",
      "Iteration 18355 Loss: 1.023336410522461\n",
      "Iteration 18356 Loss: 0.9122488498687744\n",
      "Iteration 18357 Loss: 0.9842633605003357\n",
      "Iteration 18358 Loss: 1.28203547000885\n",
      "Iteration 18359 Loss: 1.0478899478912354\n",
      "Iteration 18359 Loss: 1.0924131870269775\n",
      "Iteration 18360 Loss: 0.7316223382949829\n",
      "Iteration 18361 Loss: 1.499674916267395\n",
      "Iteration 18362 Loss: 1.4288642406463623\n",
      "Iteration 18363 Loss: 0.8692970275878906\n",
      "Iteration 18364 Loss: 1.625507116317749\n",
      "Iteration 18365 Loss: 1.3463670015335083\n",
      "Iteration 18366 Loss: 0.9403878450393677\n",
      "Iteration 18367 Loss: 1.1781606674194336\n",
      "Iteration 18368 Loss: 1.5589876174926758\n",
      "Iteration 18369 Loss: 0.9685568809509277\n",
      "Iteration 18369 Loss: 1.2147425413131714\n",
      "Iteration 18370 Loss: 0.8622716069221497\n",
      "Iteration 18371 Loss: 1.157402515411377\n",
      "Iteration 18372 Loss: 1.0306133031845093\n",
      "Iteration 18373 Loss: 1.2199150323867798\n",
      "Iteration 18374 Loss: 1.294756531715393\n",
      "Iteration 18375 Loss: 1.073381781578064\n",
      "Iteration 18376 Loss: 0.8304465413093567\n",
      "Iteration 18377 Loss: 1.2355581521987915\n",
      "Iteration 18378 Loss: 0.9545788764953613\n",
      "Iteration 18379 Loss: 1.5735673904418945\n",
      "Iteration 18379 Loss: 1.1232491731643677\n",
      "Iteration 18380 Loss: 0.9630506634712219\n",
      "Iteration 18381 Loss: 0.7620893120765686\n",
      "Iteration 18382 Loss: 0.9476422071456909\n",
      "Iteration 18383 Loss: 1.3881455659866333\n",
      "Iteration 18384 Loss: 1.2177174091339111\n",
      "Iteration 18385 Loss: 1.2426259517669678\n",
      "Iteration 18386 Loss: 0.9456223845481873\n",
      "Iteration 18387 Loss: 1.2622960805892944\n",
      "Iteration 18388 Loss: 1.4604902267456055\n",
      "Iteration 18389 Loss: 1.4611294269561768\n",
      "Iteration 18389 Loss: 1.1650809049606323\n",
      "Iteration 18390 Loss: 1.0903608798980713\n",
      "Iteration 18391 Loss: 0.9022701978683472\n",
      "Iteration 18392 Loss: 0.7934364676475525\n",
      "Iteration 18393 Loss: 1.2153176069259644\n",
      "Iteration 18394 Loss: 0.8456811904907227\n",
      "Iteration 18395 Loss: 1.625720500946045\n",
      "Iteration 18396 Loss: 1.0828527212142944\n",
      "Iteration 18397 Loss: 1.2228258848190308\n",
      "Iteration 18398 Loss: 1.5068552494049072\n",
      "Iteration 18399 Loss: 1.1529428958892822\n",
      "Iteration 18399 Loss: 1.1438263654708862\n",
      "Iteration 18400 Loss: 0.9707770347595215\n",
      "Iteration 18401 Loss: 1.0161125659942627\n",
      "Iteration 18402 Loss: 1.0160671472549438\n",
      "Iteration 18403 Loss: 0.9459655284881592\n",
      "Iteration 18404 Loss: 1.1194146871566772\n",
      "Iteration 18405 Loss: 0.9262739419937134\n",
      "Iteration 18406 Loss: 1.4643433094024658\n",
      "Iteration 18407 Loss: 0.9538950324058533\n",
      "Iteration 18408 Loss: 0.9165870547294617\n",
      "Iteration 18409 Loss: 1.1490871906280518\n",
      "Iteration 18409 Loss: 1.0478522777557373\n",
      "Iteration 18410 Loss: 0.9801256656646729\n",
      "Iteration 18411 Loss: 0.9973114132881165\n",
      "Iteration 18412 Loss: 1.8368011713027954\n",
      "Iteration 18413 Loss: 1.1799490451812744\n",
      "Iteration 18414 Loss: 0.8096779584884644\n",
      "Iteration 18415 Loss: 0.8938760757446289\n",
      "Iteration 18416 Loss: 1.050615668296814\n",
      "Iteration 18417 Loss: 1.0764268636703491\n",
      "Iteration 18418 Loss: 1.0390503406524658\n",
      "Iteration 18419 Loss: 1.218196988105774\n",
      "Iteration 18419 Loss: 1.1082030534744263\n",
      "Iteration 18420 Loss: 0.8356037139892578\n",
      "Iteration 18421 Loss: 1.2324632406234741\n",
      "Iteration 18422 Loss: 1.1318309307098389\n",
      "Iteration 18423 Loss: 1.179256796836853\n",
      "Iteration 18424 Loss: 1.2660330533981323\n",
      "Iteration 18425 Loss: 1.4680999517440796\n",
      "Iteration 18426 Loss: 1.3787965774536133\n",
      "Iteration 18427 Loss: 1.1231342554092407\n",
      "Iteration 18428 Loss: 1.0378514528274536\n",
      "Iteration 18429 Loss: 1.3989982604980469\n",
      "Iteration 18429 Loss: 1.2052067518234253\n",
      "Iteration 18430 Loss: 1.0525295734405518\n",
      "Iteration 18431 Loss: 1.173997163772583\n",
      "Iteration 18432 Loss: 1.193161964416504\n",
      "Iteration 18433 Loss: 1.1084169149398804\n",
      "Iteration 18434 Loss: 1.3348968029022217\n",
      "Iteration 18435 Loss: 1.1004475355148315\n",
      "Iteration 18436 Loss: 0.9872753024101257\n",
      "Iteration 18437 Loss: 0.7591347694396973\n",
      "Iteration 18438 Loss: 1.0469506978988647\n",
      "Iteration 18439 Loss: 1.1041449308395386\n",
      "Iteration 18439 Loss: 1.0860955715179443\n",
      "Iteration 18440 Loss: 0.9777294397354126\n",
      "Iteration 18441 Loss: 1.0326671600341797\n",
      "Iteration 18442 Loss: 1.2650269269943237\n",
      "Iteration 18443 Loss: 1.1707168817520142\n",
      "Iteration 18444 Loss: 1.1161341667175293\n",
      "Iteration 18445 Loss: 1.1501178741455078\n",
      "Iteration 18446 Loss: 1.3487831354141235\n",
      "Iteration 18447 Loss: 1.1652004718780518\n",
      "Iteration 18448 Loss: 1.6267409324645996\n",
      "Iteration 18449 Loss: 1.5726912021636963\n",
      "Iteration 18449 Loss: 1.2425808906555176\n",
      "Iteration 18450 Loss: 1.1768989562988281\n",
      "Iteration 18451 Loss: 1.3579853773117065\n",
      "Iteration 18452 Loss: 1.113905668258667\n",
      "Iteration 18453 Loss: 0.8269212245941162\n",
      "Iteration 18454 Loss: 0.9409608840942383\n",
      "Iteration 18455 Loss: 1.3008170127868652\n",
      "Iteration 18456 Loss: 1.0112384557724\n",
      "Iteration 18457 Loss: 1.0010645389556885\n",
      "Iteration 18458 Loss: 1.2761640548706055\n",
      "Iteration 18459 Loss: 1.100998044013977\n",
      "Iteration 18459 Loss: 1.110695481300354\n",
      "Iteration 18460 Loss: 1.151122808456421\n",
      "Iteration 18461 Loss: 1.2834138870239258\n",
      "Iteration 18462 Loss: 1.0753555297851562\n",
      "Iteration 18463 Loss: 1.0535529851913452\n",
      "Iteration 18464 Loss: 0.8481161594390869\n",
      "Iteration 18465 Loss: 1.1602357625961304\n",
      "Iteration 18466 Loss: 1.2219158411026\n",
      "Iteration 18467 Loss: 1.4764031171798706\n",
      "Iteration 18468 Loss: 1.0809274911880493\n",
      "Iteration 18469 Loss: 1.184661865234375\n",
      "Iteration 18469 Loss: 1.153570532798767\n",
      "Iteration 18470 Loss: 1.0456064939498901\n",
      "Iteration 18471 Loss: 1.29099440574646\n",
      "Iteration 18472 Loss: 1.2902779579162598\n",
      "Iteration 18473 Loss: 0.8398536443710327\n",
      "Iteration 18474 Loss: 1.2579200267791748\n",
      "Iteration 18475 Loss: 1.1823886632919312\n",
      "Iteration 18476 Loss: 1.023296594619751\n",
      "Iteration 18477 Loss: 1.2374858856201172\n",
      "Iteration 18478 Loss: 1.4885448217391968\n",
      "Iteration 18479 Loss: 1.0468285083770752\n",
      "Iteration 18479 Loss: 1.170319676399231\n",
      "Iteration 18480 Loss: 1.0957145690917969\n",
      "Iteration 18481 Loss: 1.3002915382385254\n",
      "Iteration 18482 Loss: 1.319717288017273\n",
      "Iteration 18483 Loss: 1.5008792877197266\n",
      "Iteration 18484 Loss: 1.3370999097824097\n",
      "Iteration 18485 Loss: 1.2212246656417847\n",
      "Iteration 18486 Loss: 1.0654776096343994\n",
      "Iteration 18487 Loss: 1.0501731634140015\n",
      "Iteration 18488 Loss: 1.2061736583709717\n",
      "Iteration 18489 Loss: 1.0194731950759888\n",
      "Iteration 18489 Loss: 1.2116224765777588\n",
      "Iteration 18490 Loss: 1.084301233291626\n",
      "Iteration 18491 Loss: 1.0446178913116455\n",
      "Iteration 18492 Loss: 1.1529711484909058\n",
      "Iteration 18493 Loss: 0.9904487729072571\n",
      "Iteration 18494 Loss: 1.0919607877731323\n",
      "Iteration 18495 Loss: 1.0958024263381958\n",
      "Iteration 18496 Loss: 1.3581894636154175\n",
      "Iteration 18497 Loss: 1.2297219038009644\n",
      "Iteration 18498 Loss: 0.9287030100822449\n",
      "Iteration 18499 Loss: 1.291353702545166\n",
      "Iteration 18499 Loss: 1.1268070936203003\n",
      "Iteration 18500 Loss: 1.2781838178634644\n",
      "Iteration 18501 Loss: 1.1468168497085571\n",
      "Iteration 18502 Loss: 1.0353915691375732\n",
      "Iteration 18503 Loss: 1.0728905200958252\n",
      "Iteration 18504 Loss: 0.7888389229774475\n",
      "Iteration 18505 Loss: 1.2630162239074707\n",
      "Iteration 18506 Loss: 1.271400809288025\n",
      "Iteration 18507 Loss: 0.7925060391426086\n",
      "Iteration 18508 Loss: 1.358205795288086\n",
      "Iteration 18509 Loss: 1.1680830717086792\n",
      "Iteration 18509 Loss: 1.1175333261489868\n",
      "Iteration 18510 Loss: 0.71375572681427\n",
      "Iteration 18511 Loss: 1.3734713792800903\n",
      "Iteration 18512 Loss: 0.8865488767623901\n",
      "Iteration 18513 Loss: 1.3633869886398315\n",
      "Iteration 18514 Loss: 1.5794501304626465\n",
      "Iteration 18515 Loss: 1.0818414688110352\n",
      "Iteration 18516 Loss: 0.8207228779792786\n",
      "Iteration 18517 Loss: 1.2893716096878052\n",
      "Iteration 18518 Loss: 0.9972162842750549\n",
      "Iteration 18519 Loss: 1.1573784351348877\n",
      "Iteration 18519 Loss: 1.126314401626587\n",
      "Iteration 18520 Loss: 1.3973324298858643\n",
      "Iteration 18521 Loss: 1.4984924793243408\n",
      "Iteration 18522 Loss: 1.3595354557037354\n",
      "Iteration 18523 Loss: 1.1964486837387085\n",
      "Iteration 18524 Loss: 1.1502684354782104\n",
      "Iteration 18525 Loss: 0.9088209867477417\n",
      "Iteration 18526 Loss: 1.1753911972045898\n",
      "Iteration 18527 Loss: 1.1988525390625\n",
      "Iteration 18528 Loss: 1.348880648612976\n",
      "Iteration 18529 Loss: 1.2822593450546265\n",
      "Iteration 18529 Loss: 1.2516281604766846\n",
      "Iteration 18530 Loss: 0.8652704954147339\n",
      "Iteration 18531 Loss: 1.0546661615371704\n",
      "Iteration 18532 Loss: 1.1767257452011108\n",
      "Iteration 18533 Loss: 1.2326264381408691\n",
      "Iteration 18534 Loss: 1.0651456117630005\n",
      "Iteration 18535 Loss: 1.1194792985916138\n",
      "Iteration 18536 Loss: 1.2700021266937256\n",
      "Iteration 18537 Loss: 1.2556005716323853\n",
      "Iteration 18538 Loss: 1.5860106945037842\n",
      "Iteration 18539 Loss: 1.0389776229858398\n",
      "Iteration 18539 Loss: 1.1664505004882812\n",
      "Iteration 18540 Loss: 0.9411845803260803\n",
      "Iteration 18541 Loss: 0.8933689594268799\n",
      "Iteration 18542 Loss: 1.1748783588409424\n",
      "Iteration 18543 Loss: 1.3583910465240479\n",
      "Iteration 18544 Loss: 1.237477421760559\n",
      "Iteration 18545 Loss: 1.0981541872024536\n",
      "Iteration 18546 Loss: 1.1081311702728271\n",
      "Iteration 18547 Loss: 1.272894263267517\n",
      "Iteration 18548 Loss: 1.6667805910110474\n",
      "Iteration 18549 Loss: 0.9519899487495422\n",
      "Iteration 18549 Loss: 1.1703250408172607\n",
      "Iteration 18550 Loss: 1.3860902786254883\n",
      "Iteration 18551 Loss: 1.064009189605713\n",
      "Iteration 18552 Loss: 1.1199654340744019\n",
      "Iteration 18553 Loss: 0.9405320882797241\n",
      "Iteration 18554 Loss: 0.9696367383003235\n",
      "Iteration 18555 Loss: 1.0137403011322021\n",
      "Iteration 18556 Loss: 1.390907645225525\n",
      "Iteration 18557 Loss: 1.086690902709961\n",
      "Iteration 18558 Loss: 1.3849880695343018\n",
      "Iteration 18559 Loss: 0.949897289276123\n",
      "Iteration 18559 Loss: 1.130645751953125\n",
      "Iteration 18560 Loss: 1.1256988048553467\n",
      "Iteration 18561 Loss: 1.0176299810409546\n",
      "Iteration 18562 Loss: 1.2168363332748413\n",
      "Iteration 18563 Loss: 1.0047537088394165\n",
      "Iteration 18564 Loss: 0.668021559715271\n",
      "Iteration 18565 Loss: 1.330254077911377\n",
      "Iteration 18566 Loss: 1.2500574588775635\n",
      "Iteration 18567 Loss: 1.2081859111785889\n",
      "Iteration 18568 Loss: 1.6506447792053223\n",
      "Iteration 18569 Loss: 1.3174755573272705\n",
      "Iteration 18569 Loss: 1.1789557933807373\n",
      "Iteration 18570 Loss: 1.0344343185424805\n",
      "Iteration 18571 Loss: 1.2775214910507202\n",
      "Iteration 18572 Loss: 1.243293046951294\n",
      "Iteration 18573 Loss: 1.5216020345687866\n",
      "Iteration 18574 Loss: 1.2463334798812866\n",
      "Iteration 18575 Loss: 0.7419814467430115\n",
      "Iteration 18576 Loss: 1.0535017251968384\n",
      "Iteration 18577 Loss: 0.786988377571106\n",
      "Iteration 18578 Loss: 1.0322577953338623\n",
      "Iteration 18579 Loss: 1.1774994134902954\n",
      "Iteration 18579 Loss: 1.1115413904190063\n",
      "Iteration 18580 Loss: 0.9534828066825867\n",
      "Iteration 18581 Loss: 1.2195124626159668\n",
      "Iteration 18582 Loss: 0.7345819473266602\n",
      "Iteration 18583 Loss: 1.3142592906951904\n",
      "Iteration 18584 Loss: 1.068405270576477\n",
      "Iteration 18585 Loss: 0.961653470993042\n",
      "Iteration 18586 Loss: 1.1374584436416626\n",
      "Iteration 18587 Loss: 0.9451592564582825\n",
      "Iteration 18588 Loss: 1.306697964668274\n",
      "Iteration 18589 Loss: 0.9698255062103271\n",
      "Iteration 18589 Loss: 1.0611037015914917\n",
      "Iteration 18590 Loss: 1.2606770992279053\n",
      "Iteration 18591 Loss: 0.7416118383407593\n",
      "Iteration 18592 Loss: 1.0054759979248047\n",
      "Iteration 18593 Loss: 1.2444437742233276\n",
      "Iteration 18594 Loss: 1.1059356927871704\n",
      "Iteration 18595 Loss: 1.304910659790039\n",
      "Iteration 18596 Loss: 0.9525588750839233\n",
      "Iteration 18597 Loss: 1.0807945728302002\n",
      "Iteration 18598 Loss: 1.2385151386260986\n",
      "Iteration 18599 Loss: 1.637679100036621\n",
      "Iteration 18599 Loss: 1.1572601795196533\n",
      "Iteration 18600 Loss: 0.7946470975875854\n",
      "Iteration 18601 Loss: 1.2257498502731323\n",
      "Iteration 18602 Loss: 1.2047418355941772\n",
      "Iteration 18603 Loss: 1.2107409238815308\n",
      "Iteration 18604 Loss: 1.2460216283798218\n",
      "Iteration 18605 Loss: 0.9320312738418579\n",
      "Iteration 18606 Loss: 1.5435075759887695\n",
      "Iteration 18607 Loss: 1.2590769529342651\n",
      "Iteration 18608 Loss: 1.0530811548233032\n",
      "Iteration 18609 Loss: 1.1245193481445312\n",
      "Iteration 18609 Loss: 1.1594117879867554\n",
      "Iteration 18610 Loss: 1.1164146661758423\n",
      "Iteration 18611 Loss: 1.423683762550354\n",
      "Iteration 18612 Loss: 0.9081913828849792\n",
      "Iteration 18613 Loss: 1.5145148038864136\n",
      "Iteration 18614 Loss: 1.2383733987808228\n",
      "Iteration 18615 Loss: 1.1662957668304443\n",
      "Iteration 18616 Loss: 1.0134823322296143\n",
      "Iteration 18617 Loss: 1.129011631011963\n",
      "Iteration 18618 Loss: 0.8678147196769714\n",
      "Iteration 18619 Loss: 1.2794582843780518\n",
      "Iteration 18619 Loss: 1.1657240390777588\n",
      "Iteration 18620 Loss: 1.065350890159607\n",
      "Iteration 18621 Loss: 1.0225026607513428\n",
      "Iteration 18622 Loss: 1.0614405870437622\n",
      "Iteration 18623 Loss: 0.9384493827819824\n",
      "Iteration 18624 Loss: 1.1797648668289185\n",
      "Iteration 18625 Loss: 1.3268810510635376\n",
      "Iteration 18626 Loss: 1.0022549629211426\n",
      "Iteration 18627 Loss: 1.290879249572754\n",
      "Iteration 18628 Loss: 0.8006037473678589\n",
      "Iteration 18629 Loss: 0.9532124400138855\n",
      "Iteration 18629 Loss: 1.064133882522583\n",
      "Iteration 18630 Loss: 1.4962469339370728\n",
      "Iteration 18631 Loss: 1.2001129388809204\n",
      "Iteration 18632 Loss: 1.020547866821289\n",
      "Iteration 18633 Loss: 1.123674988746643\n",
      "Iteration 18634 Loss: 1.2356528043746948\n",
      "Iteration 18635 Loss: 1.3478578329086304\n",
      "Iteration 18636 Loss: 1.1148231029510498\n",
      "Iteration 18637 Loss: 0.9910144209861755\n",
      "Iteration 18638 Loss: 1.4873236417770386\n",
      "Iteration 18639 Loss: 1.472478985786438\n",
      "Iteration 18639 Loss: 1.2489732503890991\n",
      "Iteration 18640 Loss: 1.0232481956481934\n",
      "Iteration 18641 Loss: 1.1957650184631348\n",
      "Iteration 18642 Loss: 1.330119252204895\n",
      "Iteration 18643 Loss: 1.0784469842910767\n",
      "Iteration 18644 Loss: 1.3950865268707275\n",
      "Iteration 18645 Loss: 1.2905322313308716\n",
      "Iteration 18646 Loss: 1.0824698209762573\n",
      "Iteration 18647 Loss: 1.211440920829773\n",
      "Iteration 18648 Loss: 1.1596994400024414\n",
      "Iteration 18649 Loss: 1.0654642581939697\n",
      "Iteration 18649 Loss: 1.183227300643921\n",
      "Iteration 18650 Loss: 0.9179807305335999\n",
      "Iteration 18651 Loss: 1.4192521572113037\n",
      "Iteration 18652 Loss: 1.2150871753692627\n",
      "Iteration 18653 Loss: 1.0674362182617188\n",
      "Iteration 18654 Loss: 0.7437474727630615\n",
      "Iteration 18655 Loss: 1.5265544652938843\n",
      "Iteration 18656 Loss: 1.344083309173584\n",
      "Iteration 18657 Loss: 1.2163270711898804\n",
      "Iteration 18658 Loss: 1.2879077196121216\n",
      "Iteration 18659 Loss: 0.9855117797851562\n",
      "Iteration 18659 Loss: 1.1723887920379639\n",
      "Iteration 18660 Loss: 1.1135518550872803\n",
      "Iteration 18661 Loss: 0.8685770034790039\n",
      "Iteration 18662 Loss: 1.3486016988754272\n",
      "Iteration 18663 Loss: 0.9270767569541931\n",
      "Iteration 18664 Loss: 1.2331750392913818\n",
      "Iteration 18665 Loss: 1.250697374343872\n",
      "Iteration 18666 Loss: 0.8425773978233337\n",
      "Iteration 18667 Loss: 1.441660761833191\n",
      "Iteration 18668 Loss: 0.7758198380470276\n",
      "Iteration 18669 Loss: 1.3836694955825806\n",
      "Iteration 18669 Loss: 1.118540644645691\n",
      "Iteration 18670 Loss: 1.0435590744018555\n",
      "Iteration 18671 Loss: 1.474229335784912\n",
      "Iteration 18672 Loss: 1.03828763961792\n",
      "Iteration 18673 Loss: 1.1960705518722534\n",
      "Iteration 18674 Loss: 0.9291642308235168\n",
      "Iteration 18675 Loss: 1.3844757080078125\n",
      "Iteration 18676 Loss: 1.1947239637374878\n",
      "Iteration 18677 Loss: 1.143056869506836\n",
      "Iteration 18678 Loss: 0.8724988698959351\n",
      "Iteration 18679 Loss: 1.0337995290756226\n",
      "Iteration 18679 Loss: 1.1309865713119507\n",
      "Iteration 18680 Loss: 0.9450296759605408\n",
      "Iteration 18681 Loss: 0.8310768604278564\n",
      "Iteration 18682 Loss: 1.4459644556045532\n",
      "Iteration 18683 Loss: 1.3492778539657593\n",
      "Iteration 18684 Loss: 1.553431749343872\n",
      "Iteration 18685 Loss: 1.2012522220611572\n",
      "Iteration 18686 Loss: 1.4496413469314575\n",
      "Iteration 18687 Loss: 1.4140605926513672\n",
      "Iteration 18688 Loss: 0.8937067985534668\n",
      "Iteration 18689 Loss: 1.275079607963562\n",
      "Iteration 18689 Loss: 1.2358521223068237\n",
      "Iteration 18690 Loss: 1.192941427230835\n",
      "Iteration 18691 Loss: 0.8212674856185913\n",
      "Iteration 18692 Loss: 0.9242501258850098\n",
      "Iteration 18693 Loss: 1.1472665071487427\n",
      "Iteration 18694 Loss: 1.2301795482635498\n",
      "Iteration 18695 Loss: 1.5797967910766602\n",
      "Iteration 18696 Loss: 0.9637072086334229\n",
      "Iteration 18697 Loss: 1.4028617143630981\n",
      "Iteration 18698 Loss: 1.0835808515548706\n",
      "Iteration 18699 Loss: 0.8314382433891296\n",
      "Iteration 18699 Loss: 1.1177289485931396\n",
      "Iteration 18700 Loss: 0.9306025505065918\n",
      "Iteration 18701 Loss: 0.848958432674408\n",
      "Iteration 18702 Loss: 1.0874285697937012\n",
      "Iteration 18703 Loss: 0.7516943216323853\n",
      "Iteration 18704 Loss: 1.2942936420440674\n",
      "Iteration 18705 Loss: 0.9548313021659851\n",
      "Iteration 18706 Loss: 1.0567665100097656\n",
      "Iteration 18707 Loss: 1.0511388778686523\n",
      "Iteration 18708 Loss: 1.1108169555664062\n",
      "Iteration 18709 Loss: 0.8199946284294128\n",
      "Iteration 18709 Loss: 0.9906525611877441\n",
      "Iteration 18710 Loss: 1.437754511833191\n",
      "Iteration 18711 Loss: 1.238512635231018\n",
      "Iteration 18712 Loss: 1.1166253089904785\n",
      "Iteration 18713 Loss: 1.1255195140838623\n",
      "Iteration 18714 Loss: 1.2698520421981812\n",
      "Iteration 18715 Loss: 1.4910078048706055\n",
      "Iteration 18716 Loss: 1.2526044845581055\n",
      "Iteration 18717 Loss: 1.298162579536438\n",
      "Iteration 18718 Loss: 0.9721710681915283\n",
      "Iteration 18719 Loss: 1.2467658519744873\n",
      "Iteration 18719 Loss: 1.2448976039886475\n",
      "Iteration 18720 Loss: 1.8216766119003296\n",
      "Iteration 18721 Loss: 1.1680560111999512\n",
      "Iteration 18722 Loss: 1.3864030838012695\n",
      "Iteration 18723 Loss: 0.9098875522613525\n",
      "Iteration 18724 Loss: 1.1144232749938965\n",
      "Iteration 18725 Loss: 1.3592381477355957\n",
      "Iteration 18726 Loss: 1.3265190124511719\n",
      "Iteration 18727 Loss: 1.4028841257095337\n",
      "Iteration 18728 Loss: 0.8800129294395447\n",
      "Iteration 18729 Loss: 1.2084012031555176\n",
      "Iteration 18729 Loss: 1.2577502727508545\n",
      "Iteration 18730 Loss: 1.2552499771118164\n",
      "Iteration 18731 Loss: 1.3531975746154785\n",
      "Iteration 18732 Loss: 0.9429006576538086\n",
      "Iteration 18733 Loss: 1.1292650699615479\n",
      "Iteration 18734 Loss: 1.0209473371505737\n",
      "Iteration 18735 Loss: 0.7872130274772644\n",
      "Iteration 18736 Loss: 1.1973761320114136\n",
      "Iteration 18737 Loss: 1.0667363405227661\n",
      "Iteration 18738 Loss: 1.5402400493621826\n",
      "Iteration 18739 Loss: 1.4826163053512573\n",
      "Iteration 18739 Loss: 1.1775742769241333\n",
      "Iteration 18740 Loss: 0.9806950688362122\n",
      "Iteration 18741 Loss: 1.4497652053833008\n",
      "Iteration 18742 Loss: 1.2519861459732056\n",
      "Iteration 18743 Loss: 0.8444333076477051\n",
      "Iteration 18744 Loss: 1.1036542654037476\n",
      "Iteration 18745 Loss: 1.1805663108825684\n",
      "Iteration 18746 Loss: 1.0502430200576782\n",
      "Iteration 18747 Loss: 1.1579681634902954\n",
      "Iteration 18748 Loss: 1.3722522258758545\n",
      "Iteration 18749 Loss: 1.3964004516601562\n",
      "Iteration 18749 Loss: 1.178796410560608\n",
      "Iteration 18750 Loss: 0.8415293097496033\n",
      "Iteration 18751 Loss: 1.1031041145324707\n",
      "Iteration 18752 Loss: 1.1546941995620728\n",
      "Iteration 18753 Loss: 0.78658527135849\n",
      "Iteration 18754 Loss: 1.253902554512024\n",
      "Iteration 18755 Loss: 0.7462272644042969\n",
      "Iteration 18756 Loss: 1.1592237949371338\n",
      "Iteration 18757 Loss: 1.1743959188461304\n",
      "Iteration 18758 Loss: 1.2018520832061768\n",
      "Iteration 18759 Loss: 0.9448167085647583\n",
      "Iteration 18759 Loss: 1.036633014678955\n",
      "Iteration 18760 Loss: 0.9179394245147705\n",
      "Iteration 18761 Loss: 1.1910231113433838\n",
      "Iteration 18762 Loss: 1.1891682147979736\n",
      "Iteration 18763 Loss: 1.3551174402236938\n",
      "Iteration 18764 Loss: 1.1340407133102417\n",
      "Iteration 18765 Loss: 1.3101650476455688\n",
      "Iteration 18766 Loss: 1.08832848072052\n",
      "Iteration 18767 Loss: 1.1154460906982422\n",
      "Iteration 18768 Loss: 1.1333173513412476\n",
      "Iteration 18769 Loss: 1.1115028858184814\n",
      "Iteration 18769 Loss: 1.1546047925949097\n",
      "Iteration 18770 Loss: 1.2314808368682861\n",
      "Iteration 18771 Loss: 1.0248591899871826\n",
      "Iteration 18772 Loss: 1.0916200876235962\n",
      "Iteration 18773 Loss: 0.8997339606285095\n",
      "Iteration 18774 Loss: 1.2274255752563477\n",
      "Iteration 18775 Loss: 1.0516353845596313\n",
      "Iteration 18776 Loss: 1.2945904731750488\n",
      "Iteration 18777 Loss: 0.669365644454956\n",
      "Iteration 18778 Loss: 1.145263433456421\n",
      "Iteration 18779 Loss: 1.1611148118972778\n",
      "Iteration 18779 Loss: 1.0797089338302612\n",
      "Iteration 18780 Loss: 0.9599673748016357\n",
      "Iteration 18781 Loss: 1.5288759469985962\n",
      "Iteration 18782 Loss: 0.9157779216766357\n",
      "Iteration 18783 Loss: 1.2939469814300537\n",
      "Iteration 18784 Loss: 1.322431206703186\n",
      "Iteration 18785 Loss: 1.5706919431686401\n",
      "Iteration 18786 Loss: 1.1005713939666748\n",
      "Iteration 18787 Loss: 1.373894214630127\n",
      "Iteration 18788 Loss: 0.9136351943016052\n",
      "Iteration 18789 Loss: 1.2295002937316895\n",
      "Iteration 18789 Loss: 1.2209293842315674\n",
      "Iteration 18790 Loss: 1.164494276046753\n",
      "Iteration 18791 Loss: 0.9796339273452759\n",
      "Iteration 18792 Loss: 1.1787724494934082\n",
      "Iteration 18793 Loss: 1.3356752395629883\n",
      "Iteration 18794 Loss: 1.0582715272903442\n",
      "Iteration 18795 Loss: 1.0858032703399658\n",
      "Iteration 18796 Loss: 0.9774467349052429\n",
      "Iteration 18797 Loss: 1.027543067932129\n",
      "Iteration 18798 Loss: 1.1366552114486694\n",
      "Iteration 18799 Loss: 1.347191333770752\n",
      "Iteration 18799 Loss: 1.1291487216949463\n",
      "Iteration 18800 Loss: 0.8125929236412048\n",
      "Iteration 18801 Loss: 1.1050338745117188\n",
      "Iteration 18802 Loss: 1.2429461479187012\n",
      "Iteration 18803 Loss: 1.0221608877182007\n",
      "Iteration 18804 Loss: 1.2078548669815063\n",
      "Iteration 18805 Loss: 1.1665990352630615\n",
      "Iteration 18806 Loss: 1.5699760913848877\n",
      "Iteration 18807 Loss: 1.0316389799118042\n",
      "Iteration 18808 Loss: 1.092426061630249\n",
      "Iteration 18809 Loss: 1.3886477947235107\n",
      "Iteration 18809 Loss: 1.163987636566162\n",
      "Iteration 18810 Loss: 0.7976843118667603\n",
      "Iteration 18811 Loss: 1.2101866006851196\n",
      "Iteration 18812 Loss: 0.9897672533988953\n",
      "Iteration 18813 Loss: 1.2157952785491943\n",
      "Iteration 18814 Loss: 1.3420888185501099\n",
      "Iteration 18815 Loss: 1.2047311067581177\n",
      "Iteration 18816 Loss: 1.1243681907653809\n",
      "Iteration 18817 Loss: 1.0496536493301392\n",
      "Iteration 18818 Loss: 0.957518458366394\n",
      "Iteration 18819 Loss: 1.3756963014602661\n",
      "Iteration 18819 Loss: 1.126749038696289\n",
      "Iteration 18820 Loss: 0.9769143462181091\n",
      "Iteration 18821 Loss: 1.0702317953109741\n",
      "Iteration 18822 Loss: 1.0429948568344116\n",
      "Iteration 18823 Loss: 0.8823215365409851\n",
      "Iteration 18824 Loss: 1.1038893461227417\n",
      "Iteration 18825 Loss: 1.1951091289520264\n",
      "Iteration 18826 Loss: 1.112725019454956\n",
      "Iteration 18827 Loss: 1.541380524635315\n",
      "Iteration 18828 Loss: 1.1168111562728882\n",
      "Iteration 18829 Loss: 1.1187316179275513\n",
      "Iteration 18829 Loss: 1.1161110401153564\n",
      "Iteration 18830 Loss: 1.404463768005371\n",
      "Iteration 18831 Loss: 1.5163568258285522\n",
      "Iteration 18832 Loss: 0.9213967323303223\n",
      "Iteration 18833 Loss: 1.042059302330017\n",
      "Iteration 18834 Loss: 0.9230068325996399\n",
      "Iteration 18835 Loss: 0.9343189597129822\n",
      "Iteration 18836 Loss: 1.2352244853973389\n",
      "Iteration 18837 Loss: 0.9825373291969299\n",
      "Iteration 18838 Loss: 1.1921535730361938\n",
      "Iteration 18839 Loss: 1.0352160930633545\n",
      "Iteration 18839 Loss: 1.1186734437942505\n",
      "Iteration 18840 Loss: 1.3817412853240967\n",
      "Iteration 18841 Loss: 1.0785516500473022\n",
      "Iteration 18842 Loss: 1.125123143196106\n",
      "Iteration 18843 Loss: 0.9432792663574219\n",
      "Iteration 18844 Loss: 1.0489810705184937\n",
      "Iteration 18845 Loss: 0.7575423717498779\n",
      "Iteration 18846 Loss: 0.8854358196258545\n",
      "Iteration 18847 Loss: 1.1886792182922363\n",
      "Iteration 18848 Loss: 1.0028159618377686\n",
      "Iteration 18849 Loss: 0.865699291229248\n",
      "Iteration 18849 Loss: 1.0277849435806274\n",
      "Iteration 18850 Loss: 1.0138447284698486\n",
      "Iteration 18851 Loss: 1.3440485000610352\n",
      "Iteration 18852 Loss: 0.9616979360580444\n",
      "Iteration 18853 Loss: 1.2087011337280273\n",
      "Iteration 18854 Loss: 1.3890355825424194\n",
      "Iteration 18855 Loss: 0.5089712142944336\n",
      "Iteration 18856 Loss: 0.8795990943908691\n",
      "Iteration 18857 Loss: 1.1587432622909546\n",
      "Iteration 18858 Loss: 1.049742579460144\n",
      "Iteration 18859 Loss: 1.1851613521575928\n",
      "Iteration 18859 Loss: 1.069954514503479\n",
      "Iteration 18860 Loss: 1.3888278007507324\n",
      "Iteration 18861 Loss: 0.8182724118232727\n",
      "Iteration 18862 Loss: 1.0998830795288086\n",
      "Iteration 18863 Loss: 1.2878113985061646\n",
      "Iteration 18864 Loss: 1.159544587135315\n",
      "Iteration 18865 Loss: 1.1303597688674927\n",
      "Iteration 18866 Loss: 1.7189185619354248\n",
      "Iteration 18867 Loss: 1.3210090398788452\n",
      "Iteration 18868 Loss: 0.9374945759773254\n",
      "Iteration 18869 Loss: 1.3557265996932983\n",
      "Iteration 18869 Loss: 1.2217848300933838\n",
      "Iteration 18870 Loss: 0.9034956693649292\n",
      "Iteration 18871 Loss: 0.9870198965072632\n",
      "Iteration 18872 Loss: 1.2305163145065308\n",
      "Iteration 18873 Loss: 1.1831036806106567\n",
      "Iteration 18874 Loss: 1.1595542430877686\n",
      "Iteration 18875 Loss: 1.3722419738769531\n",
      "Iteration 18876 Loss: 1.312995195388794\n",
      "Iteration 18877 Loss: 1.4166476726531982\n",
      "Iteration 18878 Loss: 1.0937442779541016\n",
      "Iteration 18879 Loss: 0.7892569899559021\n",
      "Iteration 18879 Loss: 1.14485764503479\n",
      "Iteration 18880 Loss: 1.407270073890686\n",
      "Iteration 18881 Loss: 1.0982705354690552\n",
      "Iteration 18882 Loss: 1.2020524740219116\n",
      "Iteration 18883 Loss: 1.445971965789795\n",
      "Iteration 18884 Loss: 1.2273789644241333\n",
      "Iteration 18885 Loss: 1.1388485431671143\n",
      "Iteration 18886 Loss: 1.270176887512207\n",
      "Iteration 18887 Loss: 0.9928346872329712\n",
      "Iteration 18888 Loss: 0.9604650735855103\n",
      "Iteration 18889 Loss: 1.2312520742416382\n",
      "Iteration 18889 Loss: 1.197452187538147\n",
      "Iteration 18890 Loss: 1.4523801803588867\n",
      "Iteration 18891 Loss: 1.35847008228302\n",
      "Iteration 18892 Loss: 0.8728364109992981\n",
      "Iteration 18893 Loss: 0.9880210757255554\n",
      "Iteration 18894 Loss: 1.4078888893127441\n",
      "Iteration 18895 Loss: 1.0580673217773438\n",
      "Iteration 18896 Loss: 1.318168044090271\n",
      "Iteration 18897 Loss: 0.8434258699417114\n",
      "Iteration 18898 Loss: 1.1130459308624268\n",
      "Iteration 18899 Loss: 1.0047060251235962\n",
      "Iteration 18899 Loss: 1.1417009830474854\n",
      "Iteration 18900 Loss: 1.14993417263031\n",
      "Iteration 18901 Loss: 0.7400128245353699\n",
      "Iteration 18902 Loss: 1.0091456174850464\n",
      "Iteration 18903 Loss: 1.114211082458496\n",
      "Iteration 18904 Loss: 1.40375554561615\n",
      "Iteration 18905 Loss: 0.9781237244606018\n",
      "Iteration 18906 Loss: 0.9601866602897644\n",
      "Iteration 18907 Loss: 0.8806573748588562\n",
      "Iteration 18908 Loss: 0.6779816746711731\n",
      "Iteration 18909 Loss: 1.2989091873168945\n",
      "Iteration 18909 Loss: 1.0212918519973755\n",
      "Iteration 18910 Loss: 1.1075319051742554\n",
      "Iteration 18911 Loss: 1.0300358533859253\n",
      "Iteration 18912 Loss: 1.2331198453903198\n",
      "Iteration 18913 Loss: 1.4127216339111328\n",
      "Iteration 18914 Loss: 1.359076976776123\n",
      "Iteration 18915 Loss: 1.2482271194458008\n",
      "Iteration 18916 Loss: 1.4606550931930542\n",
      "Iteration 18917 Loss: 0.8276002407073975\n",
      "Iteration 18918 Loss: 1.1384416818618774\n",
      "Iteration 18919 Loss: 1.0331097841262817\n",
      "Iteration 18919 Loss: 1.1850520372390747\n",
      "Iteration 18920 Loss: 1.0233616828918457\n",
      "Iteration 18921 Loss: 1.1070209741592407\n",
      "Iteration 18922 Loss: 0.7733199000358582\n",
      "Iteration 18923 Loss: 0.8465406894683838\n",
      "Iteration 18924 Loss: 1.0573991537094116\n",
      "Iteration 18925 Loss: 1.2115436792373657\n",
      "Iteration 18926 Loss: 1.433741807937622\n",
      "Iteration 18927 Loss: 1.1364532709121704\n",
      "Iteration 18928 Loss: 0.9196352362632751\n",
      "Iteration 18929 Loss: 1.258025884628296\n",
      "Iteration 18929 Loss: 1.0767042636871338\n",
      "Iteration 18930 Loss: 1.010331630706787\n",
      "Iteration 18931 Loss: 1.1567606925964355\n",
      "Iteration 18932 Loss: 0.8516262769699097\n",
      "Iteration 18933 Loss: 1.087952971458435\n",
      "Iteration 18934 Loss: 1.1986753940582275\n",
      "Iteration 18935 Loss: 0.9377230405807495\n",
      "Iteration 18936 Loss: 0.7896372079849243\n",
      "Iteration 18937 Loss: 0.5746520161628723\n",
      "Iteration 18938 Loss: 1.2040579319000244\n",
      "Iteration 18939 Loss: 0.9326867461204529\n",
      "Iteration 18939 Loss: 0.9744104146957397\n",
      "Iteration 18940 Loss: 1.0683894157409668\n",
      "Iteration 18941 Loss: 1.4244533777236938\n",
      "Iteration 18942 Loss: 0.807366132736206\n",
      "Iteration 18943 Loss: 1.1211352348327637\n",
      "Iteration 18944 Loss: 0.9052831530570984\n",
      "Iteration 18945 Loss: 1.1192982196807861\n",
      "Iteration 18946 Loss: 1.1094036102294922\n",
      "Iteration 18947 Loss: 0.9138219356536865\n",
      "Iteration 18948 Loss: 0.6948439478874207\n",
      "Iteration 18949 Loss: 1.2307116985321045\n",
      "Iteration 18949 Loss: 1.0394706726074219\n",
      "Iteration 18950 Loss: 1.2655287981033325\n",
      "Iteration 18951 Loss: 1.142769694328308\n",
      "Iteration 18952 Loss: 1.0636531114578247\n",
      "Iteration 18953 Loss: 1.2872756719589233\n",
      "Iteration 18954 Loss: 0.9080609679222107\n",
      "Iteration 18955 Loss: 0.9930537939071655\n",
      "Iteration 18956 Loss: 1.5352165699005127\n",
      "Iteration 18957 Loss: 1.058869481086731\n",
      "Iteration 18958 Loss: 1.2273362874984741\n",
      "Iteration 18959 Loss: 1.3084537982940674\n",
      "Iteration 18959 Loss: 1.1790217161178589\n",
      "Iteration 18960 Loss: 1.179736614227295\n",
      "Iteration 18961 Loss: 1.2193926572799683\n",
      "Iteration 18962 Loss: 0.7828723192214966\n",
      "Iteration 18963 Loss: 1.182705044746399\n",
      "Iteration 18964 Loss: 0.8988049030303955\n",
      "Iteration 18965 Loss: 1.3573782444000244\n",
      "Iteration 18966 Loss: 0.819233775138855\n",
      "Iteration 18967 Loss: 1.2987289428710938\n",
      "Iteration 18968 Loss: 1.1540523767471313\n",
      "Iteration 18969 Loss: 0.716763436794281\n",
      "Iteration 18969 Loss: 1.0609667301177979\n",
      "Iteration 18970 Loss: 0.9021305441856384\n",
      "Iteration 18971 Loss: 1.3356237411499023\n",
      "Iteration 18972 Loss: 1.0460362434387207\n",
      "Iteration 18973 Loss: 1.4012802839279175\n",
      "Iteration 18974 Loss: 0.7557060718536377\n",
      "Iteration 18975 Loss: 1.0298399925231934\n",
      "Iteration 18976 Loss: 1.1509193181991577\n",
      "Iteration 18977 Loss: 1.543068289756775\n",
      "Iteration 18978 Loss: 1.6416072845458984\n",
      "Iteration 18979 Loss: 0.9926110506057739\n",
      "Iteration 18979 Loss: 1.179882287979126\n",
      "Iteration 18980 Loss: 0.9035213589668274\n",
      "Iteration 18981 Loss: 0.8911238312721252\n",
      "Iteration 18982 Loss: 1.076370358467102\n",
      "Iteration 18983 Loss: 1.1741944551467896\n",
      "Iteration 18984 Loss: 1.18882155418396\n",
      "Iteration 18985 Loss: 0.7689732909202576\n",
      "Iteration 18986 Loss: 1.046421766281128\n",
      "Iteration 18987 Loss: 1.6037384271621704\n",
      "Iteration 18988 Loss: 1.0916461944580078\n",
      "Iteration 18989 Loss: 0.9570538401603699\n",
      "Iteration 18989 Loss: 1.0701864957809448\n",
      "Iteration 18990 Loss: 0.6336445212364197\n",
      "Iteration 18991 Loss: 1.0569767951965332\n",
      "Iteration 18992 Loss: 0.5941187739372253\n",
      "Iteration 18993 Loss: 0.9672719240188599\n",
      "Iteration 18994 Loss: 1.2503997087478638\n",
      "Iteration 18995 Loss: 1.3853638172149658\n",
      "Iteration 18996 Loss: 0.8308312892913818\n",
      "Iteration 18997 Loss: 1.5213251113891602\n",
      "Iteration 18998 Loss: 1.2217744588851929\n",
      "Iteration 18999 Loss: 1.1025093793869019\n",
      "Iteration 18999 Loss: 1.0564215183258057\n",
      "Iteration 19000 Loss: 1.0770471096038818\n",
      "Iteration 19001 Loss: 1.1001163721084595\n",
      "Iteration 19002 Loss: 1.378289818763733\n",
      "Iteration 19003 Loss: 1.0894476175308228\n",
      "Iteration 19004 Loss: 1.4889576435089111\n",
      "Iteration 19005 Loss: 1.133499264717102\n",
      "Iteration 19006 Loss: 1.552035927772522\n",
      "Iteration 19007 Loss: 1.3823187351226807\n",
      "Iteration 19008 Loss: 1.1539933681488037\n",
      "Iteration 19009 Loss: 1.2851428985595703\n",
      "Iteration 19009 Loss: 1.264084815979004\n",
      "Iteration 19010 Loss: 1.193162202835083\n",
      "Iteration 19011 Loss: 1.4127362966537476\n",
      "Iteration 19012 Loss: 1.1068100929260254\n",
      "Iteration 19013 Loss: 1.1455196142196655\n",
      "Iteration 19014 Loss: 1.0349589586257935\n",
      "Iteration 19015 Loss: 1.1291195154190063\n",
      "Iteration 19016 Loss: 0.9256651997566223\n",
      "Iteration 19017 Loss: 1.3761835098266602\n",
      "Iteration 19018 Loss: 1.2803927659988403\n",
      "Iteration 19019 Loss: 1.2136279344558716\n",
      "Iteration 19019 Loss: 1.181817650794983\n",
      "Iteration 19020 Loss: 1.2500357627868652\n",
      "Iteration 19021 Loss: 1.3572256565093994\n",
      "Iteration 19022 Loss: 1.0639480352401733\n",
      "Iteration 19023 Loss: 1.4721200466156006\n",
      "Iteration 19024 Loss: 1.2445605993270874\n",
      "Iteration 19025 Loss: 1.1867495775222778\n",
      "Iteration 19026 Loss: 0.8981379270553589\n",
      "Iteration 19027 Loss: 1.1212249994277954\n",
      "Iteration 19028 Loss: 0.874718189239502\n",
      "Iteration 19029 Loss: 0.8115865588188171\n",
      "Iteration 19029 Loss: 1.128030776977539\n",
      "Iteration 19030 Loss: 1.1927952766418457\n",
      "Iteration 19031 Loss: 1.5385252237319946\n",
      "Iteration 19032 Loss: 1.0664759874343872\n",
      "Iteration 19033 Loss: 1.2794300317764282\n",
      "Iteration 19034 Loss: 1.2710974216461182\n",
      "Iteration 19035 Loss: 0.9913122057914734\n",
      "Iteration 19036 Loss: 1.449532151222229\n",
      "Iteration 19037 Loss: 1.1830525398254395\n",
      "Iteration 19038 Loss: 1.3997656106948853\n",
      "Iteration 19039 Loss: 1.3290647268295288\n",
      "Iteration 19039 Loss: 1.2701051235198975\n",
      "Iteration 19040 Loss: 1.3359616994857788\n",
      "Iteration 19041 Loss: 1.1689658164978027\n",
      "Iteration 19042 Loss: 1.4723073244094849\n",
      "Iteration 19043 Loss: 1.3655422925949097\n",
      "Iteration 19044 Loss: 1.1074292659759521\n",
      "Iteration 19045 Loss: 1.186166524887085\n",
      "Iteration 19046 Loss: 1.1550759077072144\n",
      "Iteration 19047 Loss: 1.0740548372268677\n",
      "Iteration 19048 Loss: 1.0869269371032715\n",
      "Iteration 19049 Loss: 1.1405503749847412\n",
      "Iteration 19049 Loss: 1.2092981338500977\n",
      "Iteration 19050 Loss: 1.1431571245193481\n",
      "Iteration 19051 Loss: 1.2524312734603882\n",
      "Iteration 19052 Loss: 1.0588210821151733\n",
      "Iteration 19053 Loss: 1.2516309022903442\n",
      "Iteration 19054 Loss: 0.7780179977416992\n",
      "Iteration 19055 Loss: 0.9425494074821472\n",
      "Iteration 19056 Loss: 1.5322455167770386\n",
      "Iteration 19057 Loss: 1.2631096839904785\n",
      "Iteration 19058 Loss: 0.9972137808799744\n",
      "Iteration 19059 Loss: 1.396783709526062\n",
      "Iteration 19059 Loss: 1.1615960597991943\n",
      "Iteration 19060 Loss: 1.2757480144500732\n",
      "Iteration 19061 Loss: 1.2158397436141968\n",
      "Iteration 19062 Loss: 1.5674948692321777\n",
      "Iteration 19063 Loss: 1.184428334236145\n",
      "Iteration 19064 Loss: 1.4621304273605347\n",
      "Iteration 19065 Loss: 1.0336849689483643\n",
      "Iteration 19066 Loss: 0.8098517656326294\n",
      "Iteration 19067 Loss: 1.522053599357605\n",
      "Iteration 19068 Loss: 1.2500771284103394\n",
      "Iteration 19069 Loss: 1.5371965169906616\n",
      "Iteration 19069 Loss: 1.2858505249023438\n",
      "Iteration 19070 Loss: 1.188759207725525\n",
      "Iteration 19071 Loss: 1.262869954109192\n",
      "Iteration 19072 Loss: 0.9082929491996765\n",
      "Iteration 19073 Loss: 1.3581609725952148\n",
      "Iteration 19074 Loss: 1.1159965991973877\n",
      "Iteration 19075 Loss: 1.0071375370025635\n",
      "Iteration 19076 Loss: 1.0558507442474365\n",
      "Iteration 19077 Loss: 0.9872996211051941\n",
      "Iteration 19078 Loss: 0.8947933316230774\n",
      "Iteration 19079 Loss: 1.0943236351013184\n",
      "Iteration 19079 Loss: 1.087348461151123\n",
      "Iteration 19080 Loss: 1.248651146888733\n",
      "Iteration 19081 Loss: 1.1379557847976685\n",
      "Iteration 19082 Loss: 0.9447668790817261\n",
      "Iteration 19083 Loss: 1.1960501670837402\n",
      "Iteration 19084 Loss: 1.2952675819396973\n",
      "Iteration 19085 Loss: 0.9246195554733276\n",
      "Iteration 19086 Loss: 0.9788073897361755\n",
      "Iteration 19087 Loss: 0.9177206754684448\n",
      "Iteration 19088 Loss: 1.3025541305541992\n",
      "Iteration 19089 Loss: 1.3489245176315308\n",
      "Iteration 19089 Loss: 1.1295318603515625\n",
      "Iteration 19090 Loss: 1.3697857856750488\n",
      "Iteration 19091 Loss: 1.2278916835784912\n",
      "Iteration 19092 Loss: 1.2127124071121216\n",
      "Iteration 19093 Loss: 1.034829020500183\n",
      "Iteration 19094 Loss: 0.964346706867218\n",
      "Iteration 19095 Loss: 0.9744737148284912\n",
      "Iteration 19096 Loss: 1.203882098197937\n",
      "Iteration 19097 Loss: 1.4736223220825195\n",
      "Iteration 19098 Loss: 1.1006017923355103\n",
      "Iteration 19099 Loss: 1.3237488269805908\n",
      "Iteration 19099 Loss: 1.1885894536972046\n",
      "Iteration 19100 Loss: 1.0816881656646729\n",
      "Iteration 19101 Loss: 1.2692010402679443\n",
      "Iteration 19102 Loss: 1.001341700553894\n",
      "Iteration 19103 Loss: 1.1155116558074951\n",
      "Iteration 19104 Loss: 1.6587363481521606\n",
      "Iteration 19105 Loss: 0.8711528182029724\n",
      "Iteration 19106 Loss: 1.197967767715454\n",
      "Iteration 19107 Loss: 1.0471230745315552\n",
      "Iteration 19108 Loss: 1.1811162233352661\n",
      "Iteration 19109 Loss: 1.309653878211975\n",
      "Iteration 19109 Loss: 1.1733492612838745\n",
      "Iteration 19110 Loss: 0.9133172035217285\n",
      "Iteration 19111 Loss: 1.0341966152191162\n",
      "Iteration 19112 Loss: 1.0630536079406738\n",
      "Iteration 19113 Loss: 0.9313011169433594\n",
      "Iteration 19114 Loss: 1.2060954570770264\n",
      "Iteration 19115 Loss: 1.015453815460205\n",
      "Iteration 19116 Loss: 0.9905505180358887\n",
      "Iteration 19117 Loss: 1.2890535593032837\n",
      "Iteration 19118 Loss: 1.1563587188720703\n",
      "Iteration 19119 Loss: 1.0720775127410889\n",
      "Iteration 19119 Loss: 1.067145824432373\n",
      "Iteration 19120 Loss: 1.348371982574463\n",
      "Iteration 19121 Loss: 0.9163731336593628\n",
      "Iteration 19122 Loss: 1.0416324138641357\n",
      "Iteration 19123 Loss: 1.3195719718933105\n",
      "Iteration 19124 Loss: 1.1570314168930054\n",
      "Iteration 19125 Loss: 1.392525553703308\n",
      "Iteration 19126 Loss: 1.2375915050506592\n",
      "Iteration 19127 Loss: 1.399475336074829\n",
      "Iteration 19128 Loss: 0.9732409715652466\n",
      "Iteration 19129 Loss: 1.2079678773880005\n",
      "Iteration 19129 Loss: 1.199378252029419\n",
      "Iteration 19130 Loss: 1.331991195678711\n",
      "Iteration 19131 Loss: 0.865651547908783\n",
      "Iteration 19132 Loss: 1.158078670501709\n",
      "Iteration 19133 Loss: 1.2317149639129639\n",
      "Iteration 19134 Loss: 0.9709921479225159\n",
      "Iteration 19135 Loss: 1.3039462566375732\n",
      "Iteration 19136 Loss: 0.9905737638473511\n",
      "Iteration 19137 Loss: 1.4102623462677002\n",
      "Iteration 19138 Loss: 1.5416613817214966\n",
      "Iteration 19139 Loss: 1.45751953125\n",
      "Iteration 19139 Loss: 1.2262392044067383\n",
      "Iteration 19140 Loss: 1.1815084218978882\n",
      "Iteration 19141 Loss: 1.324942708015442\n",
      "Iteration 19142 Loss: 1.723783016204834\n",
      "Iteration 19143 Loss: 1.023969054222107\n",
      "Iteration 19144 Loss: 1.2011902332305908\n",
      "Iteration 19145 Loss: 1.121073603630066\n",
      "Iteration 19146 Loss: 1.3436182737350464\n",
      "Iteration 19147 Loss: 1.2673064470291138\n",
      "Iteration 19148 Loss: 1.0518383979797363\n",
      "Iteration 19149 Loss: 0.9558203816413879\n",
      "Iteration 19149 Loss: 1.2195050716400146\n",
      "Iteration 19150 Loss: 1.247613787651062\n",
      "Iteration 19151 Loss: 1.206433653831482\n",
      "Iteration 19152 Loss: 0.9734170436859131\n",
      "Iteration 19153 Loss: 1.0064553022384644\n",
      "Iteration 19154 Loss: 1.0121595859527588\n",
      "Iteration 19155 Loss: 1.176875352859497\n",
      "Iteration 19156 Loss: 1.374589443206787\n",
      "Iteration 19157 Loss: 1.2256443500518799\n",
      "Iteration 19158 Loss: 1.1915192604064941\n",
      "Iteration 19159 Loss: 0.9477822184562683\n",
      "Iteration 19159 Loss: 1.1362488269805908\n",
      "Iteration 19160 Loss: 1.0729235410690308\n",
      "Iteration 19161 Loss: 0.9911072850227356\n",
      "Iteration 19162 Loss: 0.778447151184082\n",
      "Iteration 19163 Loss: 1.359919548034668\n",
      "Iteration 19164 Loss: 1.3977961540222168\n",
      "Iteration 19165 Loss: 1.220133662223816\n",
      "Iteration 19166 Loss: 0.7736098766326904\n",
      "Iteration 19167 Loss: 1.2983006238937378\n",
      "Iteration 19168 Loss: 1.1095373630523682\n",
      "Iteration 19169 Loss: 1.3018523454666138\n",
      "Iteration 19169 Loss: 1.1303627490997314\n",
      "Iteration 19170 Loss: 1.1358402967453003\n",
      "Iteration 19171 Loss: 0.8895999789237976\n",
      "Iteration 19172 Loss: 1.1552499532699585\n",
      "Iteration 19173 Loss: 1.2370874881744385\n",
      "Iteration 19174 Loss: 0.9526585936546326\n",
      "Iteration 19175 Loss: 1.4460952281951904\n",
      "Iteration 19176 Loss: 1.3393067121505737\n",
      "Iteration 19177 Loss: 0.9387801289558411\n",
      "Iteration 19178 Loss: 1.1936053037643433\n",
      "Iteration 19179 Loss: 0.9511426687240601\n",
      "Iteration 19179 Loss: 1.123936653137207\n",
      "Iteration 19180 Loss: 1.0747148990631104\n",
      "Iteration 19181 Loss: 0.6924829483032227\n",
      "Iteration 19182 Loss: 0.9983030557632446\n",
      "Iteration 19183 Loss: 1.1253373622894287\n",
      "Iteration 19184 Loss: 1.3241307735443115\n",
      "Iteration 19185 Loss: 1.1116859912872314\n",
      "Iteration 19186 Loss: 1.1131150722503662\n",
      "Iteration 19187 Loss: 1.0807126760482788\n",
      "Iteration 19188 Loss: 1.1743961572647095\n",
      "Iteration 19189 Loss: 0.7812405228614807\n",
      "Iteration 19189 Loss: 1.047611951828003\n",
      "Iteration 19190 Loss: 1.2160307168960571\n",
      "Iteration 19191 Loss: 1.295756459236145\n",
      "Iteration 19192 Loss: 1.1183236837387085\n",
      "Iteration 19193 Loss: 1.3139498233795166\n",
      "Iteration 19194 Loss: 1.4236952066421509\n",
      "Iteration 19195 Loss: 1.1615080833435059\n",
      "Iteration 19196 Loss: 1.2724072933197021\n",
      "Iteration 19197 Loss: 1.4792968034744263\n",
      "Iteration 19198 Loss: 1.0926365852355957\n",
      "Iteration 19199 Loss: 1.3425061702728271\n",
      "Iteration 19199 Loss: 1.2716110944747925\n",
      "Iteration 19200 Loss: 1.402943730354309\n",
      "Iteration 19201 Loss: 0.967368483543396\n",
      "Iteration 19202 Loss: 1.050252079963684\n",
      "Iteration 19203 Loss: 1.3631011247634888\n",
      "Iteration 19204 Loss: 1.0041197538375854\n",
      "Iteration 19205 Loss: 1.1813381910324097\n",
      "Iteration 19206 Loss: 1.1996203660964966\n",
      "Iteration 19207 Loss: 1.062997817993164\n",
      "Iteration 19208 Loss: 0.9692417979240417\n",
      "Iteration 19209 Loss: 1.2315162420272827\n",
      "Iteration 19209 Loss: 1.1432498693466187\n",
      "Iteration 19210 Loss: 0.9683529734611511\n",
      "Iteration 19211 Loss: 1.4795881509780884\n",
      "Iteration 19212 Loss: 1.441866397857666\n",
      "Iteration 19213 Loss: 1.0564316511154175\n",
      "Iteration 19214 Loss: 1.458801507949829\n",
      "Iteration 19215 Loss: 1.4100425243377686\n",
      "Iteration 19216 Loss: 1.2281526327133179\n",
      "Iteration 19217 Loss: 1.429286003112793\n",
      "Iteration 19218 Loss: 1.2704797983169556\n",
      "Iteration 19219 Loss: 1.0434775352478027\n",
      "Iteration 19219 Loss: 1.2786478996276855\n",
      "Iteration 19220 Loss: 1.204543113708496\n",
      "Iteration 19221 Loss: 1.367955207824707\n",
      "Iteration 19222 Loss: 1.1581969261169434\n",
      "Iteration 19223 Loss: 1.3176287412643433\n",
      "Iteration 19224 Loss: 1.4995574951171875\n",
      "Iteration 19225 Loss: 1.1272609233856201\n",
      "Iteration 19226 Loss: 1.3424713611602783\n",
      "Iteration 19227 Loss: 0.7165805101394653\n",
      "Iteration 19228 Loss: 1.400407314300537\n",
      "Iteration 19229 Loss: 1.2181503772735596\n",
      "Iteration 19229 Loss: 1.235275149345398\n",
      "Iteration 19230 Loss: 1.0650851726531982\n",
      "Iteration 19231 Loss: 1.0424830913543701\n",
      "Iteration 19232 Loss: 1.4508126974105835\n",
      "Iteration 19233 Loss: 1.3540743589401245\n",
      "Iteration 19234 Loss: 0.9562021493911743\n",
      "Iteration 19235 Loss: 1.443452000617981\n",
      "Iteration 19236 Loss: 0.9684967398643494\n",
      "Iteration 19237 Loss: 0.9694671630859375\n",
      "Iteration 19238 Loss: 1.2024930715560913\n",
      "Iteration 19239 Loss: 1.5041507482528687\n",
      "Iteration 19239 Loss: 1.1956716775894165\n",
      "Iteration 19240 Loss: 0.8344115614891052\n",
      "Iteration 19241 Loss: 1.260337471961975\n",
      "Iteration 19242 Loss: 0.7489035129547119\n",
      "Iteration 19243 Loss: 0.9802749752998352\n",
      "Iteration 19244 Loss: 1.4901820421218872\n",
      "Iteration 19245 Loss: 1.3295420408248901\n",
      "Iteration 19246 Loss: 1.3151150941848755\n",
      "Iteration 19247 Loss: 1.3779923915863037\n",
      "Iteration 19248 Loss: 1.4566413164138794\n",
      "Iteration 19249 Loss: 1.2232364416122437\n",
      "Iteration 19249 Loss: 1.2016637325286865\n",
      "Iteration 19250 Loss: 1.0159242153167725\n",
      "Iteration 19251 Loss: 0.9554735422134399\n",
      "Iteration 19252 Loss: 1.039316177368164\n",
      "Iteration 19253 Loss: 0.8814101219177246\n",
      "Iteration 19254 Loss: 1.2373459339141846\n",
      "Iteration 19255 Loss: 0.845161497592926\n",
      "Iteration 19256 Loss: 1.2923815250396729\n",
      "Iteration 19257 Loss: 0.9492115378379822\n",
      "Iteration 19258 Loss: 0.887036144733429\n",
      "Iteration 19259 Loss: 0.6990804076194763\n",
      "Iteration 19259 Loss: 0.9802340269088745\n",
      "Iteration 19260 Loss: 0.9646128416061401\n",
      "Iteration 19261 Loss: 1.1724506616592407\n",
      "Iteration 19262 Loss: 1.2711437940597534\n",
      "Iteration 19263 Loss: 1.0900390148162842\n",
      "Iteration 19264 Loss: 1.2800352573394775\n",
      "Iteration 19265 Loss: 0.9956122636795044\n",
      "Iteration 19266 Loss: 0.7592833042144775\n",
      "Iteration 19267 Loss: 1.2486302852630615\n",
      "Iteration 19268 Loss: 1.092512607574463\n",
      "Iteration 19269 Loss: 1.1084415912628174\n",
      "Iteration 19269 Loss: 1.098276138305664\n",
      "Iteration 19270 Loss: 0.8349860310554504\n",
      "Iteration 19271 Loss: 0.8767203092575073\n",
      "Iteration 19272 Loss: 1.1048650741577148\n",
      "Iteration 19273 Loss: 1.2847520112991333\n",
      "Iteration 19274 Loss: 0.9242497682571411\n",
      "Iteration 19275 Loss: 1.217925786972046\n",
      "Iteration 19276 Loss: 0.9253253936767578\n",
      "Iteration 19277 Loss: 1.0672985315322876\n",
      "Iteration 19278 Loss: 0.9606419801712036\n",
      "Iteration 19279 Loss: 1.0851516723632812\n",
      "Iteration 19279 Loss: 1.0281916856765747\n",
      "Iteration 19280 Loss: 0.9160718321800232\n",
      "Iteration 19281 Loss: 1.1991323232650757\n",
      "Iteration 19282 Loss: 0.9414072632789612\n",
      "Iteration 19283 Loss: 0.8668676018714905\n",
      "Iteration 19284 Loss: 1.125177025794983\n",
      "Iteration 19285 Loss: 1.3936206102371216\n",
      "Iteration 19286 Loss: 1.0604033470153809\n",
      "Iteration 19287 Loss: 1.6863738298416138\n",
      "Iteration 19288 Loss: 1.2845942974090576\n",
      "Iteration 19289 Loss: 0.978019118309021\n",
      "Iteration 19289 Loss: 1.1451666355133057\n",
      "Iteration 19290 Loss: 1.11613929271698\n",
      "Iteration 19291 Loss: 0.9932351112365723\n",
      "Iteration 19292 Loss: 1.0040546655654907\n",
      "Iteration 19293 Loss: 1.2418197393417358\n",
      "Iteration 19294 Loss: 1.0029693841934204\n",
      "Iteration 19295 Loss: 1.0048587322235107\n",
      "Iteration 19296 Loss: 0.9350560903549194\n",
      "Iteration 19297 Loss: 1.0452786684036255\n",
      "Iteration 19298 Loss: 1.3027446269989014\n",
      "Iteration 19299 Loss: 0.8897071480751038\n",
      "Iteration 19299 Loss: 1.0535862445831299\n",
      "Iteration 19300 Loss: 0.9843006730079651\n",
      "Iteration 19301 Loss: 1.1104967594146729\n",
      "Iteration 19302 Loss: 0.7912453413009644\n",
      "Iteration 19303 Loss: 1.4459049701690674\n",
      "Iteration 19304 Loss: 1.0391501188278198\n",
      "Iteration 19305 Loss: 1.3865113258361816\n",
      "Iteration 19306 Loss: 1.1539684534072876\n",
      "Iteration 19307 Loss: 1.1914845705032349\n",
      "Iteration 19308 Loss: 0.8333609104156494\n",
      "Iteration 19309 Loss: 1.3063228130340576\n",
      "Iteration 19309 Loss: 1.1242746114730835\n",
      "Iteration 19310 Loss: 1.1832634210586548\n",
      "Iteration 19311 Loss: 1.3957033157348633\n",
      "Iteration 19312 Loss: 0.8742602467536926\n",
      "Iteration 19313 Loss: 1.3154934644699097\n",
      "Iteration 19314 Loss: 1.1916135549545288\n",
      "Iteration 19315 Loss: 1.221121072769165\n",
      "Iteration 19316 Loss: 1.2344142198562622\n",
      "Iteration 19317 Loss: 1.0315752029418945\n",
      "Iteration 19318 Loss: 0.9680802822113037\n",
      "Iteration 19319 Loss: 1.1687134504318237\n",
      "Iteration 19319 Loss: 1.1584237813949585\n",
      "Iteration 19320 Loss: 1.0145916938781738\n",
      "Iteration 19321 Loss: 1.516155481338501\n",
      "Iteration 19322 Loss: 1.2556544542312622\n",
      "Iteration 19323 Loss: 1.2257416248321533\n",
      "Iteration 19324 Loss: 1.526742935180664\n",
      "Iteration 19325 Loss: 0.7828183770179749\n",
      "Iteration 19326 Loss: 1.2458821535110474\n",
      "Iteration 19327 Loss: 1.1221295595169067\n",
      "Iteration 19328 Loss: 0.7555864453315735\n",
      "Iteration 19329 Loss: 0.851059079170227\n",
      "Iteration 19329 Loss: 1.1296361684799194\n",
      "Iteration 19330 Loss: 1.4827803373336792\n",
      "Iteration 19331 Loss: 1.0933527946472168\n",
      "Iteration 19332 Loss: 1.2533270120620728\n",
      "Iteration 19333 Loss: 0.8889079689979553\n",
      "Iteration 19334 Loss: 0.8622457981109619\n",
      "Iteration 19335 Loss: 1.124788522720337\n",
      "Iteration 19336 Loss: 1.2332040071487427\n",
      "Iteration 19337 Loss: 1.393611192703247\n",
      "Iteration 19338 Loss: 1.6182626485824585\n",
      "Iteration 19339 Loss: 1.384491205215454\n",
      "Iteration 19339 Loss: 1.2334970235824585\n",
      "Iteration 19340 Loss: 1.2142276763916016\n",
      "Iteration 19341 Loss: 1.1963282823562622\n",
      "Iteration 19342 Loss: 1.27248215675354\n",
      "Iteration 19343 Loss: 1.1992971897125244\n",
      "Iteration 19344 Loss: 0.8496156334877014\n",
      "Iteration 19345 Loss: 1.0798395872116089\n",
      "Iteration 19346 Loss: 0.6465898156166077\n",
      "Iteration 19347 Loss: 1.3244391679763794\n",
      "Iteration 19348 Loss: 1.276031255722046\n",
      "Iteration 19349 Loss: 1.1044881343841553\n",
      "Iteration 19349 Loss: 1.1163339614868164\n",
      "Iteration 19350 Loss: 1.1051549911499023\n",
      "Iteration 19351 Loss: 0.9225984215736389\n",
      "Iteration 19352 Loss: 1.204941987991333\n",
      "Iteration 19353 Loss: 1.066643238067627\n",
      "Iteration 19354 Loss: 1.1811766624450684\n",
      "Iteration 19355 Loss: 0.9644480347633362\n",
      "Iteration 19356 Loss: 1.2337095737457275\n",
      "Iteration 19357 Loss: 1.0054893493652344\n",
      "Iteration 19358 Loss: 1.0810377597808838\n",
      "Iteration 19359 Loss: 1.0620769262313843\n",
      "Iteration 19359 Loss: 1.0827276706695557\n",
      "Iteration 19360 Loss: 0.9495289921760559\n",
      "Iteration 19361 Loss: 1.004932165145874\n",
      "Iteration 19362 Loss: 0.9529760479927063\n",
      "Iteration 19363 Loss: 0.8059523105621338\n",
      "Iteration 19364 Loss: 0.904714822769165\n",
      "Iteration 19365 Loss: 1.1062371730804443\n",
      "Iteration 19366 Loss: 1.1490190029144287\n",
      "Iteration 19367 Loss: 1.3296743631362915\n",
      "Iteration 19368 Loss: 1.093761920928955\n",
      "Iteration 19369 Loss: 1.2548054456710815\n",
      "Iteration 19369 Loss: 1.0551602840423584\n",
      "Iteration 19370 Loss: 1.0170645713806152\n",
      "Iteration 19371 Loss: 0.9331804513931274\n",
      "Iteration 19372 Loss: 1.038860559463501\n",
      "Iteration 19373 Loss: 0.8750473260879517\n",
      "Iteration 19374 Loss: 1.1281415224075317\n",
      "Iteration 19375 Loss: 1.2561489343643188\n",
      "Iteration 19376 Loss: 0.7596432566642761\n",
      "Iteration 19377 Loss: 1.3751171827316284\n",
      "Iteration 19378 Loss: 1.3965498208999634\n",
      "Iteration 19379 Loss: 0.9500623345375061\n",
      "Iteration 19379 Loss: 1.072981595993042\n",
      "Iteration 19380 Loss: 0.8399631977081299\n",
      "Iteration 19381 Loss: 1.5368144512176514\n",
      "Iteration 19382 Loss: 1.0961081981658936\n",
      "Iteration 19383 Loss: 0.7532221078872681\n",
      "Iteration 19384 Loss: 1.0337151288986206\n",
      "Iteration 19385 Loss: 1.2739886045455933\n",
      "Iteration 19386 Loss: 0.8551148772239685\n",
      "Iteration 19387 Loss: 0.8928085565567017\n",
      "Iteration 19388 Loss: 0.7625779509544373\n",
      "Iteration 19389 Loss: 0.8757812976837158\n",
      "Iteration 19389 Loss: 0.9920094609260559\n",
      "Iteration 19390 Loss: 1.307571291923523\n",
      "Iteration 19391 Loss: 1.5320398807525635\n",
      "Iteration 19392 Loss: 1.1260201930999756\n",
      "Iteration 19393 Loss: 1.21497642993927\n",
      "Iteration 19394 Loss: 1.3675402402877808\n",
      "Iteration 19395 Loss: 1.1958388090133667\n",
      "Iteration 19396 Loss: 1.0444031953811646\n",
      "Iteration 19397 Loss: 0.924454391002655\n",
      "Iteration 19398 Loss: 1.5998306274414062\n",
      "Iteration 19399 Loss: 1.0678387880325317\n",
      "Iteration 19399 Loss: 1.238051414489746\n",
      "Iteration 19400 Loss: 1.1523947715759277\n",
      "Iteration 19401 Loss: 1.1754205226898193\n",
      "Iteration 19402 Loss: 0.8475260734558105\n",
      "Iteration 19403 Loss: 0.946474015712738\n",
      "Iteration 19404 Loss: 1.1895670890808105\n",
      "Iteration 19405 Loss: 1.1064209938049316\n",
      "Iteration 19406 Loss: 0.9710106253623962\n",
      "Iteration 19407 Loss: 1.0927799940109253\n",
      "Iteration 19408 Loss: 1.1854195594787598\n",
      "Iteration 19409 Loss: 1.1335910558700562\n",
      "Iteration 19409 Loss: 1.0800604820251465\n",
      "Iteration 19410 Loss: 1.4975918531417847\n",
      "Iteration 19411 Loss: 0.8605746030807495\n",
      "Iteration 19412 Loss: 1.127832293510437\n",
      "Iteration 19413 Loss: 1.518923044204712\n",
      "Iteration 19414 Loss: 1.1247644424438477\n",
      "Iteration 19415 Loss: 1.120666265487671\n",
      "Iteration 19416 Loss: 1.299980878829956\n",
      "Iteration 19417 Loss: 0.9373776912689209\n",
      "Iteration 19418 Loss: 1.185016393661499\n",
      "Iteration 19419 Loss: 0.8946099281311035\n",
      "Iteration 19419 Loss: 1.156733751296997\n",
      "Iteration 19420 Loss: 1.18819260597229\n",
      "Iteration 19421 Loss: 1.2071937322616577\n",
      "Iteration 19422 Loss: 0.7688640356063843\n",
      "Iteration 19423 Loss: 1.0115412473678589\n",
      "Iteration 19424 Loss: 0.9124184250831604\n",
      "Iteration 19425 Loss: 0.8912353515625\n",
      "Iteration 19426 Loss: 1.3991100788116455\n",
      "Iteration 19427 Loss: 0.8966427445411682\n",
      "Iteration 19428 Loss: 1.1414546966552734\n",
      "Iteration 19429 Loss: 1.0584620237350464\n",
      "Iteration 19429 Loss: 1.0475114583969116\n",
      "Iteration 19430 Loss: 1.5091547966003418\n",
      "Iteration 19431 Loss: 1.309190273284912\n",
      "Iteration 19432 Loss: 0.9663504362106323\n",
      "Iteration 19433 Loss: 1.4199753999710083\n",
      "Iteration 19434 Loss: 1.2518552541732788\n",
      "Iteration 19435 Loss: 1.7097280025482178\n",
      "Iteration 19436 Loss: 0.8931543827056885\n",
      "Iteration 19437 Loss: 0.9247919917106628\n",
      "Iteration 19438 Loss: 0.8723055124282837\n",
      "Iteration 19439 Loss: 0.8031868934631348\n",
      "Iteration 19439 Loss: 1.1659692525863647\n",
      "Iteration 19440 Loss: 1.283843755722046\n",
      "Iteration 19441 Loss: 1.2115778923034668\n",
      "Iteration 19442 Loss: 1.3820178508758545\n",
      "Iteration 19443 Loss: 1.5541788339614868\n",
      "Iteration 19444 Loss: 1.225950002670288\n",
      "Iteration 19445 Loss: 1.3119288682937622\n",
      "Iteration 19446 Loss: 1.3758652210235596\n",
      "Iteration 19447 Loss: 0.9847132563591003\n",
      "Iteration 19448 Loss: 0.9494386315345764\n",
      "Iteration 19449 Loss: 0.9613373875617981\n",
      "Iteration 19449 Loss: 1.2240850925445557\n",
      "Iteration 19450 Loss: 1.2906897068023682\n",
      "Iteration 19451 Loss: 1.005448579788208\n",
      "Iteration 19452 Loss: 1.0712506771087646\n",
      "Iteration 19453 Loss: 1.2427382469177246\n",
      "Iteration 19454 Loss: 1.1035726070404053\n",
      "Iteration 19455 Loss: 1.0001287460327148\n",
      "Iteration 19456 Loss: 1.3672771453857422\n",
      "Iteration 19457 Loss: 0.8085752725601196\n",
      "Iteration 19458 Loss: 1.6853586435317993\n",
      "Iteration 19459 Loss: 1.169443130493164\n",
      "Iteration 19459 Loss: 1.1744482517242432\n",
      "Iteration 19460 Loss: 1.2419521808624268\n",
      "Iteration 19461 Loss: 1.251549243927002\n",
      "Iteration 19462 Loss: 1.271620750427246\n",
      "Iteration 19463 Loss: 0.9618626236915588\n",
      "Iteration 19464 Loss: 1.1347575187683105\n",
      "Iteration 19465 Loss: 1.0516173839569092\n",
      "Iteration 19466 Loss: 1.3208661079406738\n",
      "Iteration 19467 Loss: 0.9785345792770386\n",
      "Iteration 19468 Loss: 1.087581992149353\n",
      "Iteration 19469 Loss: 0.8627468943595886\n",
      "Iteration 19469 Loss: 1.1163089275360107\n",
      "Iteration 19470 Loss: 0.9780794978141785\n",
      "Iteration 19471 Loss: 1.2101517915725708\n",
      "Iteration 19472 Loss: 1.1319371461868286\n",
      "Iteration 19473 Loss: 1.497488260269165\n",
      "Iteration 19474 Loss: 1.2267498970031738\n",
      "Iteration 19475 Loss: 1.262904167175293\n",
      "Iteration 19476 Loss: 1.592870831489563\n",
      "Iteration 19477 Loss: 1.2871365547180176\n",
      "Iteration 19478 Loss: 1.0584222078323364\n",
      "Iteration 19479 Loss: 1.339550256729126\n",
      "Iteration 19479 Loss: 1.2585290670394897\n",
      "Iteration 19480 Loss: 1.167570948600769\n",
      "Iteration 19481 Loss: 1.290174961090088\n",
      "Iteration 19482 Loss: 1.463449478149414\n",
      "Iteration 19483 Loss: 1.3488689661026\n",
      "Iteration 19484 Loss: 1.4771815538406372\n",
      "Iteration 19485 Loss: 1.1283687353134155\n",
      "Iteration 19486 Loss: 1.3940215110778809\n",
      "Iteration 19487 Loss: 1.4524563550949097\n",
      "Iteration 19488 Loss: 1.4635173082351685\n",
      "Iteration 19489 Loss: 1.4837849140167236\n",
      "Iteration 19489 Loss: 1.3669394254684448\n",
      "Iteration 19490 Loss: 0.9720077514648438\n",
      "Iteration 19491 Loss: 1.2571989297866821\n",
      "Iteration 19492 Loss: 1.2479628324508667\n",
      "Iteration 19493 Loss: 0.8901749849319458\n",
      "Iteration 19494 Loss: 1.596142292022705\n",
      "Iteration 19495 Loss: 1.1539440155029297\n",
      "Iteration 19496 Loss: 0.8854712843894958\n",
      "Iteration 19497 Loss: 1.1150455474853516\n",
      "Iteration 19498 Loss: 1.0335330963134766\n",
      "Iteration 19499 Loss: 1.3004904985427856\n",
      "Iteration 19499 Loss: 1.1451971530914307\n",
      "Iteration 19500 Loss: 0.933809220790863\n",
      "Iteration 19501 Loss: 1.2425860166549683\n",
      "Iteration 19502 Loss: 0.8848791122436523\n",
      "Iteration 19503 Loss: 1.0293818712234497\n",
      "Iteration 19504 Loss: 1.1488862037658691\n",
      "Iteration 19505 Loss: 1.2318154573440552\n",
      "Iteration 19506 Loss: 0.6857197284698486\n",
      "Iteration 19507 Loss: 0.730431079864502\n",
      "Iteration 19508 Loss: 0.9980224967002869\n",
      "Iteration 19509 Loss: 1.2542918920516968\n",
      "Iteration 19509 Loss: 1.0139821767807007\n",
      "Iteration 19510 Loss: 1.4110451936721802\n",
      "Iteration 19511 Loss: 1.231650948524475\n",
      "Iteration 19512 Loss: 1.0794434547424316\n",
      "Iteration 19513 Loss: 1.295423984527588\n",
      "Iteration 19514 Loss: 1.1772353649139404\n",
      "Iteration 19515 Loss: 0.8862301111221313\n",
      "Iteration 19516 Loss: 1.124613642692566\n",
      "Iteration 19517 Loss: 1.108055830001831\n",
      "Iteration 19518 Loss: 0.7524973750114441\n",
      "Iteration 19519 Loss: 1.1477118730545044\n",
      "Iteration 19519 Loss: 1.1213908195495605\n",
      "Iteration 19520 Loss: 1.1562912464141846\n",
      "Iteration 19521 Loss: 1.2957006692886353\n",
      "Iteration 19522 Loss: 1.0498274564743042\n",
      "Iteration 19523 Loss: 1.3174748420715332\n",
      "Iteration 19524 Loss: 1.7915855646133423\n",
      "Iteration 19525 Loss: 0.9613415598869324\n",
      "Iteration 19526 Loss: 1.215024709701538\n",
      "Iteration 19527 Loss: 1.5391446352005005\n",
      "Iteration 19528 Loss: 1.1206899881362915\n",
      "Iteration 19529 Loss: 1.1557674407958984\n",
      "Iteration 19529 Loss: 1.2602849006652832\n",
      "Iteration 19530 Loss: 1.278046727180481\n",
      "Iteration 19531 Loss: 1.1880155801773071\n",
      "Iteration 19532 Loss: 0.9589284658432007\n",
      "Iteration 19533 Loss: 0.9853209853172302\n",
      "Iteration 19534 Loss: 0.9633153676986694\n",
      "Iteration 19535 Loss: 0.8394355773925781\n",
      "Iteration 19536 Loss: 1.1800094842910767\n",
      "Iteration 19537 Loss: 1.3039079904556274\n",
      "Iteration 19538 Loss: 1.2290350198745728\n",
      "Iteration 19539 Loss: 1.4158623218536377\n",
      "Iteration 19539 Loss: 1.1341878175735474\n",
      "Iteration 19540 Loss: 1.0381627082824707\n",
      "Iteration 19541 Loss: 1.4482671022415161\n",
      "Iteration 19542 Loss: 1.4824795722961426\n",
      "Iteration 19543 Loss: 1.1599531173706055\n",
      "Iteration 19544 Loss: 1.4196995496749878\n",
      "Iteration 19545 Loss: 0.837909460067749\n",
      "Iteration 19546 Loss: 0.9559898972511292\n",
      "Iteration 19547 Loss: 1.2791255712509155\n",
      "Iteration 19548 Loss: 1.1140395402908325\n",
      "Iteration 19549 Loss: 1.1931846141815186\n",
      "Iteration 19549 Loss: 1.1928811073303223\n",
      "Iteration 19550 Loss: 1.3008816242218018\n",
      "Iteration 19551 Loss: 1.18325674533844\n",
      "Iteration 19552 Loss: 1.0045790672302246\n",
      "Iteration 19553 Loss: 0.8423559665679932\n",
      "Iteration 19554 Loss: 0.8351754546165466\n",
      "Iteration 19555 Loss: 0.5724942684173584\n",
      "Iteration 19556 Loss: 1.4130589962005615\n",
      "Iteration 19557 Loss: 1.2398942708969116\n",
      "Iteration 19558 Loss: 1.159183382987976\n",
      "Iteration 19559 Loss: 1.306588888168335\n",
      "Iteration 19559 Loss: 1.0857468843460083\n",
      "Iteration 19560 Loss: 1.3141534328460693\n",
      "Iteration 19561 Loss: 0.8984459638595581\n",
      "Iteration 19562 Loss: 1.188171625137329\n",
      "Iteration 19563 Loss: 1.1796112060546875\n",
      "Iteration 19564 Loss: 1.1761648654937744\n",
      "Iteration 19565 Loss: 0.6533833742141724\n",
      "Iteration 19566 Loss: 0.9632623791694641\n",
      "Iteration 19567 Loss: 1.0737751722335815\n",
      "Iteration 19568 Loss: 0.8964672684669495\n",
      "Iteration 19569 Loss: 1.205520749092102\n",
      "Iteration 19569 Loss: 1.0548956394195557\n",
      "Iteration 19570 Loss: 1.3424657583236694\n",
      "Iteration 19571 Loss: 1.0721126794815063\n",
      "Iteration 19572 Loss: 1.3534185886383057\n",
      "Iteration 19573 Loss: 1.215970754623413\n",
      "Iteration 19574 Loss: 1.095072627067566\n",
      "Iteration 19575 Loss: 1.0729289054870605\n",
      "Iteration 19576 Loss: 1.070935845375061\n",
      "Iteration 19577 Loss: 1.4628933668136597\n",
      "Iteration 19578 Loss: 0.9689464569091797\n",
      "Iteration 19579 Loss: 0.9683381915092468\n",
      "Iteration 19579 Loss: 1.1623084545135498\n",
      "Iteration 19580 Loss: 1.0282529592514038\n",
      "Iteration 19581 Loss: 0.9847426414489746\n",
      "Iteration 19582 Loss: 1.0293632745742798\n",
      "Iteration 19583 Loss: 1.0656623840332031\n",
      "Iteration 19584 Loss: 1.1554183959960938\n",
      "Iteration 19585 Loss: 0.9562051892280579\n",
      "Iteration 19586 Loss: 0.800321638584137\n",
      "Iteration 19587 Loss: 0.9489377737045288\n",
      "Iteration 19588 Loss: 1.0196542739868164\n",
      "Iteration 19589 Loss: 1.4791414737701416\n",
      "Iteration 19589 Loss: 1.0467698574066162\n",
      "Iteration 19590 Loss: 0.8698524236679077\n",
      "Iteration 19591 Loss: 1.061944842338562\n",
      "Iteration 19592 Loss: 0.9273446798324585\n",
      "Iteration 19593 Loss: 1.3727902173995972\n",
      "Iteration 19594 Loss: 0.9141733646392822\n",
      "Iteration 19595 Loss: 1.2094961404800415\n",
      "Iteration 19596 Loss: 1.1335846185684204\n",
      "Iteration 19597 Loss: 0.864951491355896\n",
      "Iteration 19598 Loss: 0.9517014026641846\n",
      "Iteration 19599 Loss: 1.1516038179397583\n",
      "Iteration 19599 Loss: 1.0457444190979004\n",
      "Iteration 19600 Loss: 0.7627549171447754\n",
      "Iteration 19601 Loss: 1.069615125656128\n",
      "Iteration 19602 Loss: 1.101784348487854\n",
      "Iteration 19603 Loss: 1.2187250852584839\n",
      "Iteration 19604 Loss: 1.128921389579773\n",
      "Iteration 19605 Loss: 0.8140878677368164\n",
      "Iteration 19606 Loss: 0.9179940223693848\n",
      "Iteration 19607 Loss: 1.2272111177444458\n",
      "Iteration 19608 Loss: 1.102198600769043\n",
      "Iteration 19609 Loss: 0.8771378993988037\n",
      "Iteration 19609 Loss: 1.022042989730835\n",
      "Iteration 19610 Loss: 0.8020723462104797\n",
      "Iteration 19611 Loss: 0.8476352691650391\n",
      "Iteration 19612 Loss: 1.458632230758667\n",
      "Iteration 19613 Loss: 0.511601984500885\n",
      "Iteration 19614 Loss: 1.0934308767318726\n",
      "Iteration 19615 Loss: 1.6055676937103271\n",
      "Iteration 19616 Loss: 1.0618829727172852\n",
      "Iteration 19617 Loss: 0.7907032370567322\n",
      "Iteration 19618 Loss: 0.8179646134376526\n",
      "Iteration 19619 Loss: 0.9926963448524475\n",
      "Iteration 19619 Loss: 0.9982187151908875\n",
      "Iteration 19620 Loss: 0.9147226810455322\n",
      "Iteration 19621 Loss: 1.1552740335464478\n",
      "Iteration 19622 Loss: 1.3139795064926147\n",
      "Iteration 19623 Loss: 1.2524055242538452\n",
      "Iteration 19624 Loss: 0.7029596567153931\n",
      "Iteration 19625 Loss: 1.1928296089172363\n",
      "Iteration 19626 Loss: 1.138791561126709\n",
      "Iteration 19627 Loss: 0.9303659796714783\n",
      "Iteration 19628 Loss: 1.315500259399414\n",
      "Iteration 19629 Loss: 1.1905957460403442\n",
      "Iteration 19629 Loss: 1.110742449760437\n",
      "Iteration 19630 Loss: 1.2183492183685303\n",
      "Iteration 19631 Loss: 1.2713887691497803\n",
      "Iteration 19632 Loss: 1.138853907585144\n",
      "Iteration 19633 Loss: 1.0267916917800903\n",
      "Iteration 19634 Loss: 1.2940881252288818\n",
      "Iteration 19635 Loss: 1.0656417608261108\n",
      "Iteration 19636 Loss: 1.3224155902862549\n",
      "Iteration 19637 Loss: 1.189421534538269\n",
      "Iteration 19638 Loss: 1.1218661069869995\n",
      "Iteration 19639 Loss: 1.0989240407943726\n",
      "Iteration 19639 Loss: 1.1747740507125854\n",
      "Iteration 19640 Loss: 1.2067381143569946\n",
      "Iteration 19641 Loss: 1.0015592575073242\n",
      "Iteration 19642 Loss: 1.2360855340957642\n",
      "Iteration 19643 Loss: 0.889299213886261\n",
      "Iteration 19644 Loss: 1.1718822717666626\n",
      "Iteration 19645 Loss: 1.1488614082336426\n",
      "Iteration 19646 Loss: 0.9030320048332214\n",
      "Iteration 19647 Loss: 1.3675298690795898\n",
      "Iteration 19648 Loss: 1.202986478805542\n",
      "Iteration 19649 Loss: 1.287259817123413\n",
      "Iteration 19649 Loss: 1.1415234804153442\n",
      "Iteration 19650 Loss: 1.0674474239349365\n",
      "Iteration 19651 Loss: 1.2515630722045898\n",
      "Iteration 19652 Loss: 1.5127112865447998\n",
      "Iteration 19653 Loss: 0.9965638518333435\n",
      "Iteration 19654 Loss: 0.8163660764694214\n",
      "Iteration 19655 Loss: 1.0110098123550415\n",
      "Iteration 19656 Loss: 1.0532770156860352\n",
      "Iteration 19657 Loss: 0.9997389316558838\n",
      "Iteration 19658 Loss: 1.2207971811294556\n",
      "Iteration 19659 Loss: 1.1646583080291748\n",
      "Iteration 19659 Loss: 1.1094133853912354\n",
      "Iteration 19660 Loss: 1.383872628211975\n",
      "Iteration 19661 Loss: 1.1635810136795044\n",
      "Iteration 19662 Loss: 0.7440665364265442\n",
      "Iteration 19663 Loss: 1.2673842906951904\n",
      "Iteration 19664 Loss: 1.3098394870758057\n",
      "Iteration 19665 Loss: 1.0840308666229248\n",
      "Iteration 19666 Loss: 1.273911952972412\n",
      "Iteration 19667 Loss: 1.224582552909851\n",
      "Iteration 19668 Loss: 0.9964761734008789\n",
      "Iteration 19669 Loss: 1.314316749572754\n",
      "Iteration 19669 Loss: 1.1762062311172485\n",
      "Iteration 19670 Loss: 0.8510415554046631\n",
      "Iteration 19671 Loss: 1.2527061700820923\n",
      "Iteration 19672 Loss: 1.053025245666504\n",
      "Iteration 19673 Loss: 1.1638880968093872\n",
      "Iteration 19674 Loss: 1.1154067516326904\n",
      "Iteration 19675 Loss: 1.2731783390045166\n",
      "Iteration 19676 Loss: 0.7912091612815857\n",
      "Iteration 19677 Loss: 0.9173230528831482\n",
      "Iteration 19678 Loss: 0.9500632882118225\n",
      "Iteration 19679 Loss: 1.0445971488952637\n",
      "Iteration 19679 Loss: 1.0412437915802002\n",
      "Iteration 19680 Loss: 1.0707340240478516\n",
      "Iteration 19681 Loss: 0.8973362445831299\n",
      "Iteration 19682 Loss: 1.1228766441345215\n",
      "Iteration 19683 Loss: 1.012158989906311\n",
      "Iteration 19684 Loss: 1.2583585977554321\n",
      "Iteration 19685 Loss: 1.3222334384918213\n",
      "Iteration 19686 Loss: 1.1244138479232788\n",
      "Iteration 19687 Loss: 1.24495530128479\n",
      "Iteration 19688 Loss: 1.3471416234970093\n",
      "Iteration 19689 Loss: 1.0660984516143799\n",
      "Iteration 19689 Loss: 1.1466306447982788\n",
      "Iteration 19690 Loss: 0.8591119647026062\n",
      "Iteration 19691 Loss: 1.208486557006836\n",
      "Iteration 19692 Loss: 1.3221341371536255\n",
      "Iteration 19693 Loss: 1.2228180170059204\n",
      "Iteration 19694 Loss: 1.3107787370681763\n",
      "Iteration 19695 Loss: 1.3879770040512085\n",
      "Iteration 19696 Loss: 1.3099908828735352\n",
      "Iteration 19697 Loss: 1.1662616729736328\n",
      "Iteration 19698 Loss: 1.290468454360962\n",
      "Iteration 19699 Loss: 0.58647221326828\n",
      "Iteration 19699 Loss: 1.1664499044418335\n",
      "Iteration 19700 Loss: 1.555110216140747\n",
      "Iteration 19701 Loss: 1.1027565002441406\n",
      "Iteration 19702 Loss: 1.1659985780715942\n",
      "Iteration 19703 Loss: 1.2047125101089478\n",
      "Iteration 19704 Loss: 1.1731582880020142\n",
      "Iteration 19705 Loss: 1.6245406866073608\n",
      "Iteration 19706 Loss: 1.044628381729126\n",
      "Iteration 19707 Loss: 1.0304465293884277\n",
      "Iteration 19708 Loss: 1.336741328239441\n",
      "Iteration 19709 Loss: 0.9681391716003418\n",
      "Iteration 19709 Loss: 1.220623254776001\n",
      "Iteration 19710 Loss: 1.369157314300537\n",
      "Iteration 19711 Loss: 1.1613396406173706\n",
      "Iteration 19712 Loss: 1.3253929615020752\n",
      "Iteration 19713 Loss: 1.051680088043213\n",
      "Iteration 19714 Loss: 1.2452685832977295\n",
      "Iteration 19715 Loss: 1.421221137046814\n",
      "Iteration 19716 Loss: 1.1361247301101685\n",
      "Iteration 19717 Loss: 1.0735503435134888\n",
      "Iteration 19718 Loss: 1.0604268312454224\n",
      "Iteration 19719 Loss: 1.086264729499817\n",
      "Iteration 19719 Loss: 1.193042516708374\n",
      "Iteration 19720 Loss: 0.8861080408096313\n",
      "Iteration 19721 Loss: 1.413406252861023\n",
      "Iteration 19722 Loss: 1.0664626359939575\n",
      "Iteration 19723 Loss: 1.1142773628234863\n",
      "Iteration 19724 Loss: 1.0440529584884644\n",
      "Iteration 19725 Loss: 0.8071375489234924\n",
      "Iteration 19726 Loss: 0.7338610887527466\n",
      "Iteration 19727 Loss: 1.051834225654602\n",
      "Iteration 19728 Loss: 1.0897403955459595\n",
      "Iteration 19729 Loss: 1.2533010244369507\n",
      "Iteration 19729 Loss: 1.046018123626709\n",
      "Iteration 19730 Loss: 1.0707298517227173\n",
      "Iteration 19731 Loss: 1.5932822227478027\n",
      "Iteration 19732 Loss: 0.97649747133255\n",
      "Iteration 19733 Loss: 0.8443878889083862\n",
      "Iteration 19734 Loss: 1.397157073020935\n",
      "Iteration 19735 Loss: 1.0608508586883545\n",
      "Iteration 19736 Loss: 1.1287847757339478\n",
      "Iteration 19737 Loss: 0.8562688231468201\n",
      "Iteration 19738 Loss: 0.7764605283737183\n",
      "Iteration 19739 Loss: 0.9841132164001465\n",
      "Iteration 19739 Loss: 1.0688533782958984\n",
      "Iteration 19740 Loss: 0.9318007230758667\n",
      "Iteration 19741 Loss: 0.9493566751480103\n",
      "Iteration 19742 Loss: 1.2789300680160522\n",
      "Iteration 19743 Loss: 1.4010425806045532\n",
      "Iteration 19744 Loss: 1.2361568212509155\n",
      "Iteration 19745 Loss: 1.5391488075256348\n",
      "Iteration 19746 Loss: 1.3912957906723022\n",
      "Iteration 19747 Loss: 1.4561768770217896\n",
      "Iteration 19748 Loss: 1.0589600801467896\n",
      "Iteration 19749 Loss: 1.351191759109497\n",
      "Iteration 19749 Loss: 1.2594059705734253\n",
      "Iteration 19750 Loss: 1.2571457624435425\n",
      "Iteration 19751 Loss: 1.180127501487732\n",
      "Iteration 19752 Loss: 1.232873558998108\n",
      "Iteration 19753 Loss: 1.0962663888931274\n",
      "Iteration 19754 Loss: 1.5067464113235474\n",
      "Iteration 19755 Loss: 1.1223647594451904\n",
      "Iteration 19756 Loss: 1.2222524881362915\n",
      "Iteration 19757 Loss: 1.4344911575317383\n",
      "Iteration 19758 Loss: 1.132839322090149\n",
      "Iteration 19759 Loss: 1.100724220275879\n",
      "Iteration 19759 Loss: 1.2285832166671753\n",
      "Iteration 19760 Loss: 1.1645578145980835\n",
      "Iteration 19761 Loss: 1.3723341226577759\n",
      "Iteration 19762 Loss: 1.2240864038467407\n",
      "Iteration 19763 Loss: 1.3222699165344238\n",
      "Iteration 19764 Loss: 1.1240497827529907\n",
      "Iteration 19765 Loss: 1.3847070932388306\n",
      "Iteration 19766 Loss: 0.9112561941146851\n",
      "Iteration 19767 Loss: 1.0804274082183838\n",
      "Iteration 19768 Loss: 0.621782660484314\n",
      "Iteration 19769 Loss: 0.8981929421424866\n",
      "Iteration 19769 Loss: 1.1103663444519043\n",
      "Iteration 19770 Loss: 1.419467806816101\n",
      "Iteration 19771 Loss: 0.9729803204536438\n",
      "Iteration 19772 Loss: 0.9651246070861816\n",
      "Iteration 19773 Loss: 0.9620046019554138\n",
      "Iteration 19774 Loss: 1.0534313917160034\n",
      "Iteration 19775 Loss: 1.0204201936721802\n",
      "Iteration 19776 Loss: 1.1264679431915283\n",
      "Iteration 19777 Loss: 1.1940526962280273\n",
      "Iteration 19778 Loss: 1.3108255863189697\n",
      "Iteration 19779 Loss: 1.4044910669326782\n",
      "Iteration 19779 Loss: 1.142926573753357\n",
      "Iteration 19780 Loss: 1.0812406539916992\n",
      "Iteration 19781 Loss: 1.1675959825515747\n",
      "Iteration 19782 Loss: 0.9019315242767334\n",
      "Iteration 19783 Loss: 1.3809951543807983\n",
      "Iteration 19784 Loss: 1.5919289588928223\n",
      "Iteration 19785 Loss: 1.0169817209243774\n",
      "Iteration 19786 Loss: 0.895723819732666\n",
      "Iteration 19787 Loss: 0.8768141269683838\n",
      "Iteration 19788 Loss: 0.9926856160163879\n",
      "Iteration 19789 Loss: 1.1524842977523804\n",
      "Iteration 19789 Loss: 1.1058381795883179\n",
      "Iteration 19790 Loss: 1.2910892963409424\n",
      "Iteration 19791 Loss: 1.056728482246399\n",
      "Iteration 19792 Loss: 0.990654468536377\n",
      "Iteration 19793 Loss: 0.8144012093544006\n",
      "Iteration 19794 Loss: 1.1218169927597046\n",
      "Iteration 19795 Loss: 1.5935850143432617\n",
      "Iteration 19796 Loss: 1.2110834121704102\n",
      "Iteration 19797 Loss: 1.1044076681137085\n",
      "Iteration 19798 Loss: 1.1008938550949097\n",
      "Iteration 19799 Loss: 1.0749645233154297\n",
      "Iteration 19799 Loss: 1.1359624862670898\n",
      "Iteration 19800 Loss: 1.0931280851364136\n",
      "Iteration 19801 Loss: 1.4152963161468506\n",
      "Iteration 19802 Loss: 1.2374191284179688\n",
      "Iteration 19803 Loss: 1.0331448316574097\n",
      "Iteration 19804 Loss: 0.8824372291564941\n",
      "Iteration 19805 Loss: 0.9086765646934509\n",
      "Iteration 19806 Loss: 1.2655714750289917\n",
      "Iteration 19807 Loss: 1.104667067527771\n",
      "Iteration 19808 Loss: 1.0258461236953735\n",
      "Iteration 19809 Loss: 1.4068570137023926\n",
      "Iteration 19809 Loss: 1.137304425239563\n",
      "Iteration 19810 Loss: 0.9707653522491455\n",
      "Iteration 19811 Loss: 1.3027639389038086\n",
      "Iteration 19812 Loss: 0.9428130984306335\n",
      "Iteration 19813 Loss: 1.6510899066925049\n",
      "Iteration 19814 Loss: 0.8554772138595581\n",
      "Iteration 19815 Loss: 1.2180612087249756\n",
      "Iteration 19816 Loss: 1.1738276481628418\n",
      "Iteration 19817 Loss: 0.9824614524841309\n",
      "Iteration 19818 Loss: 1.1666544675827026\n",
      "Iteration 19819 Loss: 1.2597020864486694\n",
      "Iteration 19819 Loss: 1.1523616313934326\n",
      "Iteration 19820 Loss: 1.022794246673584\n",
      "Iteration 19821 Loss: 1.4599521160125732\n",
      "Iteration 19822 Loss: 1.0902775526046753\n",
      "Iteration 19823 Loss: 0.7922011613845825\n",
      "Iteration 19824 Loss: 1.034301996231079\n",
      "Iteration 19825 Loss: 1.0997145175933838\n",
      "Iteration 19826 Loss: 0.9077520370483398\n",
      "Iteration 19827 Loss: 0.98380047082901\n",
      "Iteration 19828 Loss: 1.332891583442688\n",
      "Iteration 19829 Loss: 1.1864279508590698\n",
      "Iteration 19829 Loss: 1.0910112857818604\n",
      "Iteration 19830 Loss: 1.0602681636810303\n",
      "Iteration 19831 Loss: 1.3668748140335083\n",
      "Iteration 19832 Loss: 1.1676427125930786\n",
      "Iteration 19833 Loss: 0.9464156627655029\n",
      "Iteration 19834 Loss: 0.9634354114532471\n",
      "Iteration 19835 Loss: 1.3037238121032715\n",
      "Iteration 19836 Loss: 1.2853225469589233\n",
      "Iteration 19837 Loss: 1.1065239906311035\n",
      "Iteration 19838 Loss: 1.2432811260223389\n",
      "Iteration 19839 Loss: 1.1525343656539917\n",
      "Iteration 19839 Loss: 1.159602165222168\n",
      "Iteration 19840 Loss: 1.1211813688278198\n",
      "Iteration 19841 Loss: 0.795505166053772\n",
      "Iteration 19842 Loss: 1.147515058517456\n",
      "Iteration 19843 Loss: 1.263085126876831\n",
      "Iteration 19844 Loss: 0.8830305933952332\n",
      "Iteration 19845 Loss: 1.2030308246612549\n",
      "Iteration 19846 Loss: 0.6827069520950317\n",
      "Iteration 19847 Loss: 1.4007253646850586\n",
      "Iteration 19848 Loss: 1.4655730724334717\n",
      "Iteration 19849 Loss: 0.9465723633766174\n",
      "Iteration 19849 Loss: 1.0908925533294678\n",
      "Iteration 19850 Loss: 1.1248819828033447\n",
      "Iteration 19851 Loss: 1.4972232580184937\n",
      "Iteration 19852 Loss: 1.2558282613754272\n",
      "Iteration 19853 Loss: 1.0720542669296265\n",
      "Iteration 19854 Loss: 0.6757134199142456\n",
      "Iteration 19855 Loss: 0.8848103284835815\n",
      "Iteration 19856 Loss: 1.0441359281539917\n",
      "Iteration 19857 Loss: 1.3452768325805664\n",
      "Iteration 19858 Loss: 1.2622878551483154\n",
      "Iteration 19859 Loss: 0.9402528405189514\n",
      "Iteration 19859 Loss: 1.1102465391159058\n",
      "Iteration 19860 Loss: 1.3318794965744019\n",
      "Iteration 19861 Loss: 1.0230941772460938\n",
      "Iteration 19862 Loss: 1.6491572856903076\n",
      "Iteration 19863 Loss: 0.9123032093048096\n",
      "Iteration 19864 Loss: 0.9104393124580383\n",
      "Iteration 19865 Loss: 0.6459635496139526\n",
      "Iteration 19866 Loss: 0.870790958404541\n",
      "Iteration 19867 Loss: 1.1586112976074219\n",
      "Iteration 19868 Loss: 1.1648207902908325\n",
      "Iteration 19869 Loss: 1.2734607458114624\n",
      "Iteration 19869 Loss: 1.0940520763397217\n",
      "Iteration 19870 Loss: 1.4216922521591187\n",
      "Iteration 19871 Loss: 1.370830774307251\n",
      "Iteration 19872 Loss: 1.1458088159561157\n",
      "Iteration 19873 Loss: 1.3892461061477661\n",
      "Iteration 19874 Loss: 0.8522052764892578\n",
      "Iteration 19875 Loss: 1.219241738319397\n",
      "Iteration 19876 Loss: 1.0113438367843628\n",
      "Iteration 19877 Loss: 1.1363623142242432\n",
      "Iteration 19878 Loss: 1.0329084396362305\n",
      "Iteration 19879 Loss: 1.1363685131072998\n",
      "Iteration 19879 Loss: 1.1716008186340332\n",
      "Iteration 19880 Loss: 1.182932734489441\n",
      "Iteration 19881 Loss: 1.2008991241455078\n",
      "Iteration 19882 Loss: 1.085069179534912\n",
      "Iteration 19883 Loss: 1.1306874752044678\n",
      "Iteration 19884 Loss: 1.0697795152664185\n",
      "Iteration 19885 Loss: 1.172257900238037\n",
      "Iteration 19886 Loss: 0.9926651120185852\n",
      "Iteration 19887 Loss: 1.1084553003311157\n",
      "Iteration 19888 Loss: 1.137802243232727\n",
      "Iteration 19889 Loss: 1.0222570896148682\n",
      "Iteration 19889 Loss: 1.1102807521820068\n",
      "Iteration 19890 Loss: 1.3671116828918457\n",
      "Iteration 19891 Loss: 1.8124971389770508\n",
      "Iteration 19892 Loss: 0.5717430114746094\n",
      "Iteration 19893 Loss: 1.1977020502090454\n",
      "Iteration 19894 Loss: 1.2464807033538818\n",
      "Iteration 19895 Loss: 1.4902597665786743\n",
      "Iteration 19896 Loss: 1.0651865005493164\n",
      "Iteration 19897 Loss: 1.1121124029159546\n",
      "Iteration 19898 Loss: 1.2501962184906006\n",
      "Iteration 19899 Loss: 0.9770355820655823\n",
      "Iteration 19899 Loss: 1.209032416343689\n",
      "Iteration 19900 Loss: 1.1627250909805298\n",
      "Iteration 19901 Loss: 1.1204158067703247\n",
      "Iteration 19902 Loss: 1.3687494993209839\n",
      "Iteration 19903 Loss: 1.4067041873931885\n",
      "Iteration 19904 Loss: 1.4020538330078125\n",
      "Iteration 19905 Loss: 1.1489603519439697\n",
      "Iteration 19906 Loss: 1.2559771537780762\n",
      "Iteration 19907 Loss: 1.1509019136428833\n",
      "Iteration 19908 Loss: 1.4604406356811523\n",
      "Iteration 19909 Loss: 1.0805103778839111\n",
      "Iteration 19909 Loss: 1.2557437419891357\n",
      "Iteration 19910 Loss: 1.045950174331665\n",
      "Iteration 19911 Loss: 0.6276153922080994\n",
      "Iteration 19912 Loss: 0.6588951945304871\n",
      "Iteration 19913 Loss: 1.3158292770385742\n",
      "Iteration 19914 Loss: 1.2048358917236328\n",
      "Iteration 19915 Loss: 1.0006814002990723\n",
      "Iteration 19916 Loss: 1.0392038822174072\n",
      "Iteration 19917 Loss: 1.1247963905334473\n",
      "Iteration 19918 Loss: 0.7065613269805908\n",
      "Iteration 19919 Loss: 1.1718816757202148\n",
      "Iteration 19919 Loss: 0.9896249771118164\n",
      "Iteration 19920 Loss: 0.6749599575996399\n",
      "Iteration 19921 Loss: 1.1741365194320679\n",
      "Iteration 19922 Loss: 1.2397687435150146\n",
      "Iteration 19923 Loss: 1.4459036588668823\n",
      "Iteration 19924 Loss: 1.1229108572006226\n",
      "Iteration 19925 Loss: 0.9529556632041931\n",
      "Iteration 19926 Loss: 1.2042492628097534\n",
      "Iteration 19927 Loss: 1.058713674545288\n",
      "Iteration 19928 Loss: 1.428330421447754\n",
      "Iteration 19929 Loss: 1.2599563598632812\n",
      "Iteration 19929 Loss: 1.1561884880065918\n",
      "Iteration 19930 Loss: 1.0315948724746704\n",
      "Iteration 19931 Loss: 0.9251577854156494\n",
      "Iteration 19932 Loss: 1.1320103406906128\n",
      "Iteration 19933 Loss: 1.1865063905715942\n",
      "Iteration 19934 Loss: 1.3339407444000244\n",
      "Iteration 19935 Loss: 0.9350717663764954\n",
      "Iteration 19936 Loss: 1.2037899494171143\n",
      "Iteration 19937 Loss: 1.4196374416351318\n",
      "Iteration 19938 Loss: 1.1827423572540283\n",
      "Iteration 19939 Loss: 1.0836799144744873\n",
      "Iteration 19939 Loss: 1.1434131860733032\n",
      "Iteration 19940 Loss: 1.1537472009658813\n",
      "Iteration 19941 Loss: 0.8998124599456787\n",
      "Iteration 19942 Loss: 0.9631049633026123\n",
      "Iteration 19943 Loss: 1.0556881427764893\n",
      "Iteration 19944 Loss: 1.4706048965454102\n",
      "Iteration 19945 Loss: 0.7439814805984497\n",
      "Iteration 19946 Loss: 0.7342872619628906\n",
      "Iteration 19947 Loss: 1.1926907300949097\n",
      "Iteration 19948 Loss: 0.9377948641777039\n",
      "Iteration 19949 Loss: 1.210228681564331\n",
      "Iteration 19949 Loss: 1.036194086074829\n",
      "Iteration 19950 Loss: 0.9650459289550781\n",
      "Iteration 19951 Loss: 1.209977626800537\n",
      "Iteration 19952 Loss: 1.3600631952285767\n",
      "Iteration 19953 Loss: 1.1172821521759033\n",
      "Iteration 19954 Loss: 0.9056898355484009\n",
      "Iteration 19955 Loss: 1.5137337446212769\n",
      "Iteration 19956 Loss: 1.161165475845337\n",
      "Iteration 19957 Loss: 1.0700486898422241\n",
      "Iteration 19958 Loss: 1.231555700302124\n",
      "Iteration 19959 Loss: 1.4247679710388184\n",
      "Iteration 19959 Loss: 1.1959331035614014\n",
      "Iteration 19960 Loss: 1.1835300922393799\n",
      "Iteration 19961 Loss: 1.341587781906128\n",
      "Iteration 19962 Loss: 1.4004762172698975\n",
      "Iteration 19963 Loss: 1.0875053405761719\n",
      "Iteration 19964 Loss: 0.9276609420776367\n",
      "Iteration 19965 Loss: 1.1489161252975464\n",
      "Iteration 19966 Loss: 0.650030791759491\n",
      "Iteration 19967 Loss: 0.5347318649291992\n",
      "Iteration 19968 Loss: 1.2041889429092407\n",
      "Iteration 19969 Loss: 1.1933056116104126\n",
      "Iteration 19969 Loss: 1.0671933889389038\n",
      "Iteration 19970 Loss: 0.945565402507782\n",
      "Iteration 19971 Loss: 1.1022650003433228\n",
      "Iteration 19972 Loss: 1.3461356163024902\n",
      "Iteration 19973 Loss: 1.6125019788742065\n",
      "Iteration 19974 Loss: 1.2058953046798706\n",
      "Iteration 19975 Loss: 1.2615666389465332\n",
      "Iteration 19976 Loss: 1.0242379903793335\n",
      "Iteration 19977 Loss: 1.1039280891418457\n",
      "Iteration 19978 Loss: 1.079017996788025\n",
      "Iteration 19979 Loss: 1.3989598751068115\n",
      "Iteration 19979 Loss: 1.2080074548721313\n",
      "Iteration 19980 Loss: 1.038143277168274\n",
      "Iteration 19981 Loss: 1.306010365486145\n",
      "Iteration 19982 Loss: 0.950119137763977\n",
      "Iteration 19983 Loss: 0.9024205207824707\n",
      "Iteration 19984 Loss: 0.9263556599617004\n",
      "Iteration 19985 Loss: 1.3098976612091064\n",
      "Iteration 19986 Loss: 1.2286713123321533\n",
      "Iteration 19987 Loss: 1.2648203372955322\n",
      "Iteration 19988 Loss: 0.8505460619926453\n",
      "Iteration 19989 Loss: 1.0916340351104736\n",
      "Iteration 19989 Loss: 1.0868618488311768\n",
      "Iteration 19990 Loss: 1.12613844871521\n",
      "Iteration 19991 Loss: 0.5324462652206421\n",
      "Iteration 19992 Loss: 0.9794164896011353\n",
      "Iteration 19993 Loss: 0.9458533525466919\n",
      "Iteration 19994 Loss: 1.2186110019683838\n",
      "Iteration 19995 Loss: 1.5525107383728027\n",
      "Iteration 19996 Loss: 1.2883220911026\n",
      "Iteration 19997 Loss: 0.9425609111785889\n",
      "Iteration 19998 Loss: 0.6506929397583008\n",
      "Iteration 19999 Loss: 1.5139250755310059\n",
      "Iteration 19999 Loss: 1.0750477313995361\n",
      "Iteration 20000 Loss: 1.0066198110580444\n",
      "Iteration 20001 Loss: 1.0406713485717773\n",
      "Iteration 20002 Loss: 1.4293605089187622\n",
      "Iteration 20003 Loss: 0.9981461763381958\n",
      "Iteration 20004 Loss: 1.3245691061019897\n",
      "Iteration 20005 Loss: 1.2610485553741455\n",
      "Iteration 20006 Loss: 0.8814144730567932\n",
      "Iteration 20007 Loss: 0.981332540512085\n",
      "Iteration 20008 Loss: 0.8957273364067078\n",
      "Iteration 20009 Loss: 0.981321394443512\n",
      "Iteration 20009 Loss: 1.0800211429595947\n",
      "Iteration 20010 Loss: 0.9956222772598267\n",
      "Iteration 20011 Loss: 0.6788117289543152\n",
      "Iteration 20012 Loss: 1.4164022207260132\n",
      "Iteration 20013 Loss: 1.0795844793319702\n",
      "Iteration 20014 Loss: 0.986343502998352\n",
      "Iteration 20015 Loss: 1.1714733839035034\n",
      "Iteration 20016 Loss: 1.0941038131713867\n",
      "Iteration 20017 Loss: 1.2722254991531372\n",
      "Iteration 20018 Loss: 1.0575112104415894\n",
      "Iteration 20019 Loss: 1.4601776599884033\n",
      "Iteration 20019 Loss: 1.1212255954742432\n",
      "Iteration 20020 Loss: 1.265766978263855\n",
      "Iteration 20021 Loss: 1.1686433553695679\n",
      "Iteration 20022 Loss: 0.8326881527900696\n",
      "Iteration 20023 Loss: 1.3621058464050293\n",
      "Iteration 20024 Loss: 0.8840867877006531\n",
      "Iteration 20025 Loss: 1.1483303308486938\n",
      "Iteration 20026 Loss: 1.1646252870559692\n",
      "Iteration 20027 Loss: 1.4613161087036133\n",
      "Iteration 20028 Loss: 1.0972477197647095\n",
      "Iteration 20029 Loss: 0.7924880385398865\n",
      "Iteration 20029 Loss: 1.117729902267456\n",
      "Iteration 20030 Loss: 1.0108413696289062\n",
      "Iteration 20031 Loss: 0.9626674652099609\n",
      "Iteration 20032 Loss: 1.1629520654678345\n",
      "Iteration 20033 Loss: 1.5959748029708862\n",
      "Iteration 20034 Loss: 1.1471019983291626\n",
      "Iteration 20035 Loss: 0.8666278719902039\n",
      "Iteration 20036 Loss: 1.3111131191253662\n",
      "Iteration 20037 Loss: 1.393682837486267\n",
      "Iteration 20038 Loss: 1.0336920022964478\n",
      "Iteration 20039 Loss: 1.515763759613037\n",
      "Iteration 20039 Loss: 1.200041651725769\n",
      "Iteration 20040 Loss: 1.2511910200119019\n",
      "Iteration 20041 Loss: 0.9073247313499451\n",
      "Iteration 20042 Loss: 0.8348423838615417\n",
      "Iteration 20043 Loss: 0.7651617527008057\n",
      "Iteration 20044 Loss: 1.4151195287704468\n",
      "Iteration 20045 Loss: 1.1320616006851196\n",
      "Iteration 20046 Loss: 1.4680923223495483\n",
      "Iteration 20047 Loss: 1.1053048372268677\n",
      "Iteration 20048 Loss: 1.1198123693466187\n",
      "Iteration 20049 Loss: 1.0338555574417114\n",
      "Iteration 20049 Loss: 1.1032764911651611\n",
      "Iteration 20050 Loss: 0.9959146976470947\n",
      "Iteration 20051 Loss: 1.2140191793441772\n",
      "Iteration 20052 Loss: 0.9474460482597351\n",
      "Iteration 20053 Loss: 0.8128382563591003\n",
      "Iteration 20054 Loss: 0.7799304723739624\n",
      "Iteration 20055 Loss: 1.2713221311569214\n",
      "Iteration 20056 Loss: 1.1571327447891235\n",
      "Iteration 20057 Loss: 1.3386561870574951\n",
      "Iteration 20058 Loss: 1.1953203678131104\n",
      "Iteration 20059 Loss: 1.1322352886199951\n",
      "Iteration 20059 Loss: 1.0844814777374268\n",
      "Iteration 20060 Loss: 1.297360897064209\n",
      "Iteration 20061 Loss: 1.2407565116882324\n",
      "Iteration 20062 Loss: 1.2498271465301514\n",
      "Iteration 20063 Loss: 1.175119400024414\n",
      "Iteration 20064 Loss: 1.2277729511260986\n",
      "Iteration 20065 Loss: 1.6026928424835205\n",
      "Iteration 20066 Loss: 1.524284839630127\n",
      "Iteration 20067 Loss: 1.253956913948059\n",
      "Iteration 20068 Loss: 1.0676263570785522\n",
      "Iteration 20069 Loss: 1.119031310081482\n",
      "Iteration 20069 Loss: 1.2758429050445557\n",
      "Iteration 20070 Loss: 1.1978856325149536\n",
      "Iteration 20071 Loss: 1.1830774545669556\n",
      "Iteration 20072 Loss: 1.284328579902649\n",
      "Iteration 20073 Loss: 1.0276143550872803\n",
      "Iteration 20074 Loss: 1.0760396718978882\n",
      "Iteration 20075 Loss: 1.1934059858322144\n",
      "Iteration 20076 Loss: 0.8947412967681885\n",
      "Iteration 20077 Loss: 0.9264183640480042\n",
      "Iteration 20078 Loss: 0.9447160363197327\n",
      "Iteration 20079 Loss: 1.058233380317688\n",
      "Iteration 20079 Loss: 1.078645944595337\n",
      "Iteration 20080 Loss: 1.183833122253418\n",
      "Iteration 20081 Loss: 1.073607087135315\n",
      "Iteration 20082 Loss: 0.7858118414878845\n",
      "Iteration 20083 Loss: 0.9905447959899902\n",
      "Iteration 20084 Loss: 1.0619843006134033\n",
      "Iteration 20085 Loss: 1.4580693244934082\n",
      "Iteration 20086 Loss: 1.4388104677200317\n",
      "Iteration 20087 Loss: 0.7032967209815979\n",
      "Iteration 20088 Loss: 1.0794825553894043\n",
      "Iteration 20089 Loss: 1.450371503829956\n",
      "Iteration 20089 Loss: 1.122581124305725\n",
      "Iteration 20090 Loss: 1.5783641338348389\n",
      "Iteration 20091 Loss: 1.117226004600525\n",
      "Iteration 20092 Loss: 1.2528531551361084\n",
      "Iteration 20093 Loss: 1.1793173551559448\n",
      "Iteration 20094 Loss: 0.9238888025283813\n",
      "Iteration 20095 Loss: 1.4586960077285767\n",
      "Iteration 20096 Loss: 0.9588615894317627\n",
      "Iteration 20097 Loss: 1.0844941139221191\n",
      "Iteration 20098 Loss: 1.1336674690246582\n",
      "Iteration 20099 Loss: 1.3491764068603516\n",
      "Iteration 20099 Loss: 1.2036545276641846\n",
      "Iteration 20100 Loss: 1.1654000282287598\n",
      "Iteration 20101 Loss: 1.0700221061706543\n",
      "Iteration 20102 Loss: 0.8164357542991638\n",
      "Iteration 20103 Loss: 1.323536992073059\n",
      "Iteration 20104 Loss: 1.3488311767578125\n",
      "Iteration 20105 Loss: 1.0370699167251587\n",
      "Iteration 20106 Loss: 1.4159637689590454\n",
      "Iteration 20107 Loss: 1.111145257949829\n",
      "Iteration 20108 Loss: 1.0335519313812256\n",
      "Iteration 20109 Loss: 1.4790962934494019\n",
      "Iteration 20109 Loss: 1.1801053285598755\n",
      "Iteration 20110 Loss: 1.281125783920288\n",
      "Iteration 20111 Loss: 0.8257741332054138\n",
      "Iteration 20112 Loss: 0.9940758943557739\n",
      "Iteration 20113 Loss: 1.0670729875564575\n",
      "Iteration 20114 Loss: 0.9987717270851135\n",
      "Iteration 20115 Loss: 1.858224630355835\n",
      "Iteration 20116 Loss: 1.141919732093811\n",
      "Iteration 20117 Loss: 1.0678492784500122\n",
      "Iteration 20118 Loss: 1.1560192108154297\n",
      "Iteration 20119 Loss: 1.3156400918960571\n",
      "Iteration 20119 Loss: 1.170647382736206\n",
      "Iteration 20120 Loss: 1.0412583351135254\n",
      "Iteration 20121 Loss: 1.0527657270431519\n",
      "Iteration 20122 Loss: 0.6339378952980042\n",
      "Iteration 20123 Loss: 1.1029326915740967\n",
      "Iteration 20124 Loss: 1.0802628993988037\n",
      "Iteration 20125 Loss: 1.343131422996521\n",
      "Iteration 20126 Loss: 1.2965679168701172\n",
      "Iteration 20127 Loss: 1.0698037147521973\n",
      "Iteration 20128 Loss: 1.6012492179870605\n",
      "Iteration 20129 Loss: 1.1147171258926392\n",
      "Iteration 20129 Loss: 1.1336625814437866\n",
      "Iteration 20130 Loss: 0.8895372152328491\n",
      "Iteration 20131 Loss: 1.1422501802444458\n",
      "Iteration 20132 Loss: 0.7635951638221741\n",
      "Iteration 20133 Loss: 0.8944293856620789\n",
      "Iteration 20134 Loss: 1.2961246967315674\n",
      "Iteration 20135 Loss: 1.4048432111740112\n",
      "Iteration 20136 Loss: 1.1131328344345093\n",
      "Iteration 20137 Loss: 1.0159573554992676\n",
      "Iteration 20138 Loss: 1.115815281867981\n",
      "Iteration 20139 Loss: 0.9602401852607727\n",
      "Iteration 20139 Loss: 1.0595924854278564\n",
      "Iteration 20140 Loss: 0.9984999895095825\n",
      "Iteration 20141 Loss: 1.35719633102417\n",
      "Iteration 20142 Loss: 1.1211217641830444\n",
      "Iteration 20143 Loss: 0.716067910194397\n",
      "Iteration 20144 Loss: 1.3069865703582764\n",
      "Iteration 20145 Loss: 1.2963567972183228\n",
      "Iteration 20146 Loss: 1.2727280855178833\n",
      "Iteration 20147 Loss: 1.3075625896453857\n",
      "Iteration 20148 Loss: 1.4246147871017456\n",
      "Iteration 20149 Loss: 0.9893147349357605\n",
      "Iteration 20149 Loss: 1.1790449619293213\n",
      "Iteration 20150 Loss: 0.9038946628570557\n",
      "Iteration 20151 Loss: 1.1121186017990112\n",
      "Iteration 20152 Loss: 1.4517590999603271\n",
      "Iteration 20153 Loss: 1.1843860149383545\n",
      "Iteration 20154 Loss: 1.430936574935913\n",
      "Iteration 20155 Loss: 0.850881040096283\n",
      "Iteration 20156 Loss: 0.9790918231010437\n",
      "Iteration 20157 Loss: 1.2636693716049194\n",
      "Iteration 20158 Loss: 1.1075937747955322\n",
      "Iteration 20159 Loss: 0.9725377559661865\n",
      "Iteration 20159 Loss: 1.1256868839263916\n",
      "Iteration 20160 Loss: 0.9669108390808105\n",
      "Iteration 20161 Loss: 1.2447302341461182\n",
      "Iteration 20162 Loss: 1.15798819065094\n",
      "Iteration 20163 Loss: 0.9949008822441101\n",
      "Iteration 20164 Loss: 1.4784280061721802\n",
      "Iteration 20165 Loss: 1.4686603546142578\n",
      "Iteration 20166 Loss: 1.1913120746612549\n",
      "Iteration 20167 Loss: 0.8587565422058105\n",
      "Iteration 20168 Loss: 1.0005061626434326\n",
      "Iteration 20169 Loss: 0.9795517921447754\n",
      "Iteration 20169 Loss: 1.1341743469238281\n",
      "Iteration 20170 Loss: 1.0921889543533325\n",
      "Iteration 20171 Loss: 0.8295830488204956\n",
      "Iteration 20172 Loss: 1.0852082967758179\n",
      "Iteration 20173 Loss: 1.2013391256332397\n",
      "Iteration 20174 Loss: 0.9941681027412415\n",
      "Iteration 20175 Loss: 1.3552722930908203\n",
      "Iteration 20176 Loss: 1.3587770462036133\n",
      "Iteration 20177 Loss: 1.097629189491272\n",
      "Iteration 20178 Loss: 1.075447678565979\n",
      "Iteration 20179 Loss: 1.349076509475708\n",
      "Iteration 20179 Loss: 1.143869161605835\n",
      "Iteration 20180 Loss: 0.807379961013794\n",
      "Iteration 20181 Loss: 0.9709692597389221\n",
      "Iteration 20182 Loss: 0.9625431895256042\n",
      "Iteration 20183 Loss: 0.9376077055931091\n",
      "Iteration 20184 Loss: 1.28810453414917\n",
      "Iteration 20185 Loss: 1.2713334560394287\n",
      "Iteration 20186 Loss: 1.3814533948898315\n",
      "Iteration 20187 Loss: 1.1677371263504028\n",
      "Iteration 20188 Loss: 1.0302438735961914\n",
      "Iteration 20189 Loss: 1.2800852060317993\n",
      "Iteration 20189 Loss: 1.109745740890503\n",
      "Iteration 20190 Loss: 1.1780450344085693\n",
      "Iteration 20191 Loss: 0.9532884955406189\n",
      "Iteration 20192 Loss: 1.0740705728530884\n",
      "Iteration 20193 Loss: 1.2524776458740234\n",
      "Iteration 20194 Loss: 1.0762677192687988\n",
      "Iteration 20195 Loss: 1.531078577041626\n",
      "Iteration 20196 Loss: 1.2893328666687012\n",
      "Iteration 20197 Loss: 1.1034880876541138\n",
      "Iteration 20198 Loss: 1.2320051193237305\n",
      "Iteration 20199 Loss: 0.9592706561088562\n",
      "Iteration 20199 Loss: 1.1649324893951416\n",
      "Iteration 20200 Loss: 1.3769948482513428\n",
      "Iteration 20201 Loss: 0.9477336406707764\n",
      "Iteration 20202 Loss: 1.4806957244873047\n",
      "Iteration 20203 Loss: 1.3975557088851929\n",
      "Iteration 20204 Loss: 1.0722683668136597\n",
      "Iteration 20205 Loss: 0.9424241781234741\n",
      "Iteration 20206 Loss: 1.0067461729049683\n",
      "Iteration 20207 Loss: 1.3690427541732788\n",
      "Iteration 20208 Loss: 0.8488902449607849\n",
      "Iteration 20209 Loss: 0.9136013984680176\n",
      "Iteration 20209 Loss: 1.1355952024459839\n",
      "Iteration 20210 Loss: 1.0806406736373901\n",
      "Iteration 20211 Loss: 1.3109943866729736\n",
      "Iteration 20212 Loss: 1.1365766525268555\n",
      "Iteration 20213 Loss: 1.385939598083496\n",
      "Iteration 20214 Loss: 1.5773241519927979\n",
      "Iteration 20215 Loss: 1.1274757385253906\n",
      "Iteration 20216 Loss: 1.1929049491882324\n",
      "Iteration 20217 Loss: 1.477881908416748\n",
      "Iteration 20218 Loss: 1.3775124549865723\n",
      "Iteration 20219 Loss: 1.1462764739990234\n",
      "Iteration 20219 Loss: 1.2813527584075928\n",
      "Iteration 20220 Loss: 1.0318968296051025\n",
      "Iteration 20221 Loss: 0.9787230491638184\n",
      "Iteration 20222 Loss: 1.0177093744277954\n",
      "Iteration 20223 Loss: 1.2903409004211426\n",
      "Iteration 20224 Loss: 0.948198676109314\n",
      "Iteration 20225 Loss: 0.7758775949478149\n",
      "Iteration 20226 Loss: 1.275834321975708\n",
      "Iteration 20227 Loss: 1.1900752782821655\n",
      "Iteration 20228 Loss: 1.191836953163147\n",
      "Iteration 20229 Loss: 1.2929799556732178\n",
      "Iteration 20229 Loss: 1.0993472337722778\n",
      "Iteration 20230 Loss: 1.30142343044281\n",
      "Iteration 20231 Loss: 1.2912647724151611\n",
      "Iteration 20232 Loss: 1.2691681385040283\n",
      "Iteration 20233 Loss: 0.8106343150138855\n",
      "Iteration 20234 Loss: 1.1043798923492432\n",
      "Iteration 20235 Loss: 1.0650461912155151\n",
      "Iteration 20236 Loss: 0.7387601733207703\n",
      "Iteration 20237 Loss: 1.2129877805709839\n",
      "Iteration 20238 Loss: 1.154254674911499\n",
      "Iteration 20239 Loss: 1.0581990480422974\n",
      "Iteration 20239 Loss: 1.100611925125122\n",
      "Iteration 20240 Loss: 1.1053727865219116\n",
      "Iteration 20241 Loss: 0.9112739562988281\n",
      "Iteration 20242 Loss: 0.9157485961914062\n",
      "Iteration 20243 Loss: 1.0422154664993286\n",
      "Iteration 20244 Loss: 0.8123873472213745\n",
      "Iteration 20245 Loss: 1.3662463426589966\n",
      "Iteration 20246 Loss: 1.3360809087753296\n",
      "Iteration 20247 Loss: 1.3853261470794678\n",
      "Iteration 20248 Loss: 1.637306571006775\n",
      "Iteration 20249 Loss: 0.9556587934494019\n",
      "Iteration 20249 Loss: 1.1467616558074951\n",
      "Iteration 20250 Loss: 1.4312366247177124\n",
      "Iteration 20251 Loss: 1.4617664813995361\n",
      "Iteration 20252 Loss: 1.3710616827011108\n",
      "Iteration 20253 Loss: 1.1251535415649414\n",
      "Iteration 20254 Loss: 1.6276217699050903\n",
      "Iteration 20255 Loss: 1.349940299987793\n",
      "Iteration 20256 Loss: 1.294978141784668\n",
      "Iteration 20257 Loss: 0.9303540587425232\n",
      "Iteration 20258 Loss: 1.3609058856964111\n",
      "Iteration 20259 Loss: 1.0106679201126099\n",
      "Iteration 20259 Loss: 1.2963687181472778\n",
      "Iteration 20260 Loss: 1.3728803396224976\n",
      "Iteration 20261 Loss: 0.9833892583847046\n",
      "Iteration 20262 Loss: 1.3482609987258911\n",
      "Iteration 20263 Loss: 1.0656179189682007\n",
      "Iteration 20264 Loss: 1.0062251091003418\n",
      "Iteration 20265 Loss: 1.0137112140655518\n",
      "Iteration 20266 Loss: 1.5274522304534912\n",
      "Iteration 20267 Loss: 1.4296611547470093\n",
      "Iteration 20268 Loss: 1.1807405948638916\n",
      "Iteration 20269 Loss: 0.8001461029052734\n",
      "Iteration 20269 Loss: 1.1728084087371826\n",
      "Iteration 20270 Loss: 1.2019723653793335\n",
      "Iteration 20271 Loss: 1.1226089000701904\n",
      "Iteration 20272 Loss: 1.1239904165267944\n",
      "Iteration 20273 Loss: 1.0755949020385742\n",
      "Iteration 20274 Loss: 1.1247061491012573\n",
      "Iteration 20275 Loss: 1.0943447351455688\n",
      "Iteration 20276 Loss: 1.305998682975769\n",
      "Iteration 20277 Loss: 0.9542816281318665\n",
      "Iteration 20278 Loss: 1.2165465354919434\n",
      "Iteration 20279 Loss: 1.2372504472732544\n",
      "Iteration 20279 Loss: 1.1457295417785645\n",
      "Iteration 20280 Loss: 1.1329888105392456\n",
      "Iteration 20281 Loss: 1.0936856269836426\n",
      "Iteration 20282 Loss: 1.2233766317367554\n",
      "Iteration 20283 Loss: 1.0092321634292603\n",
      "Iteration 20284 Loss: 1.0038126707077026\n",
      "Iteration 20285 Loss: 1.6900794506072998\n",
      "Iteration 20286 Loss: 1.2866616249084473\n",
      "Iteration 20287 Loss: 1.1518319845199585\n",
      "Iteration 20288 Loss: 1.0137370824813843\n",
      "Iteration 20289 Loss: 1.1521269083023071\n",
      "Iteration 20289 Loss: 1.1757533550262451\n",
      "Iteration 20290 Loss: 1.5593138933181763\n",
      "Iteration 20291 Loss: 1.2320643663406372\n",
      "Iteration 20292 Loss: 1.2135686874389648\n",
      "Iteration 20293 Loss: 1.2931586503982544\n",
      "Iteration 20294 Loss: 1.029289960861206\n",
      "Iteration 20295 Loss: 1.105323076248169\n",
      "Iteration 20296 Loss: 1.1068456172943115\n",
      "Iteration 20297 Loss: 1.1875101327896118\n",
      "Iteration 20298 Loss: 0.9387907981872559\n",
      "Iteration 20299 Loss: 0.9815700054168701\n",
      "Iteration 20299 Loss: 1.1647435426712036\n",
      "Iteration 20300 Loss: 0.9886602163314819\n",
      "Iteration 20301 Loss: 0.6865797638893127\n",
      "Iteration 20302 Loss: 1.0621997117996216\n",
      "Iteration 20303 Loss: 0.901245653629303\n",
      "Iteration 20304 Loss: 0.8793646097183228\n",
      "Iteration 20305 Loss: 0.9129078984260559\n",
      "Iteration 20306 Loss: 1.1856895685195923\n",
      "Iteration 20307 Loss: 0.8402547836303711\n",
      "Iteration 20308 Loss: 1.1639596223831177\n",
      "Iteration 20309 Loss: 1.3703302145004272\n",
      "Iteration 20309 Loss: 0.9991191625595093\n",
      "Iteration 20310 Loss: 1.1585408449172974\n",
      "Iteration 20311 Loss: 1.3404655456542969\n",
      "Iteration 20312 Loss: 1.1444978713989258\n",
      "Iteration 20313 Loss: 1.2319393157958984\n",
      "Iteration 20314 Loss: 1.4255911111831665\n",
      "Iteration 20315 Loss: 0.9895709753036499\n",
      "Iteration 20316 Loss: 1.2717138528823853\n",
      "Iteration 20317 Loss: 1.2100498676300049\n",
      "Iteration 20318 Loss: 0.9703906774520874\n",
      "Iteration 20319 Loss: 0.9049147963523865\n",
      "Iteration 20319 Loss: 1.1647675037384033\n",
      "Iteration 20320 Loss: 1.153681755065918\n",
      "Iteration 20321 Loss: 1.3008790016174316\n",
      "Iteration 20322 Loss: 1.3388235569000244\n",
      "Iteration 20323 Loss: 1.1203563213348389\n",
      "Iteration 20324 Loss: 1.4115360975265503\n",
      "Iteration 20325 Loss: 1.2258714437484741\n",
      "Iteration 20326 Loss: 0.7848970890045166\n",
      "Iteration 20327 Loss: 0.9952174425125122\n",
      "Iteration 20328 Loss: 1.0834853649139404\n",
      "Iteration 20329 Loss: 0.831000566482544\n",
      "Iteration 20329 Loss: 1.124574899673462\n",
      "Iteration 20330 Loss: 1.1146104335784912\n",
      "Iteration 20331 Loss: 0.9821265339851379\n",
      "Iteration 20332 Loss: 0.9087410569190979\n",
      "Iteration 20333 Loss: 1.2020857334136963\n",
      "Iteration 20334 Loss: 1.1136205196380615\n",
      "Iteration 20335 Loss: 1.2341384887695312\n",
      "Iteration 20336 Loss: 0.7297758460044861\n",
      "Iteration 20337 Loss: 0.96235191822052\n",
      "Iteration 20338 Loss: 1.0304726362228394\n",
      "Iteration 20339 Loss: 1.5247821807861328\n",
      "Iteration 20339 Loss: 1.080270528793335\n",
      "Iteration 20340 Loss: 1.1438603401184082\n",
      "Iteration 20341 Loss: 1.2832931280136108\n",
      "Iteration 20342 Loss: 1.0569989681243896\n",
      "Iteration 20343 Loss: 1.3287254571914673\n",
      "Iteration 20344 Loss: 1.10491144657135\n",
      "Iteration 20345 Loss: 1.3361709117889404\n",
      "Iteration 20346 Loss: 1.4333239793777466\n",
      "Iteration 20347 Loss: 1.2499747276306152\n",
      "Iteration 20348 Loss: 1.0880045890808105\n",
      "Iteration 20349 Loss: 1.3769640922546387\n",
      "Iteration 20349 Loss: 1.240222692489624\n",
      "Iteration 20350 Loss: 0.8152769207954407\n",
      "Iteration 20351 Loss: 1.265036702156067\n",
      "Iteration 20352 Loss: 1.2750885486602783\n",
      "Iteration 20353 Loss: 1.1847320795059204\n",
      "Iteration 20354 Loss: 1.3578872680664062\n",
      "Iteration 20355 Loss: 1.0390424728393555\n",
      "Iteration 20356 Loss: 1.2256300449371338\n",
      "Iteration 20357 Loss: 1.0603251457214355\n",
      "Iteration 20358 Loss: 1.496976613998413\n",
      "Iteration 20359 Loss: 1.2860833406448364\n",
      "Iteration 20359 Loss: 1.2006078958511353\n",
      "Iteration 20360 Loss: 1.2920538187026978\n",
      "Iteration 20361 Loss: 1.4472651481628418\n",
      "Iteration 20362 Loss: 0.977963924407959\n",
      "Iteration 20363 Loss: 1.0262367725372314\n",
      "Iteration 20364 Loss: 0.8974728584289551\n",
      "Iteration 20365 Loss: 1.3405219316482544\n",
      "Iteration 20366 Loss: 0.7839314341545105\n",
      "Iteration 20367 Loss: 1.1256023645401\n",
      "Iteration 20368 Loss: 1.1289297342300415\n",
      "Iteration 20369 Loss: 1.0943180322647095\n",
      "Iteration 20369 Loss: 1.1114296913146973\n",
      "Iteration 20370 Loss: 0.7560915946960449\n",
      "Iteration 20371 Loss: 1.0227748155593872\n",
      "Iteration 20372 Loss: 1.0755375623703003\n",
      "Iteration 20373 Loss: 1.0716428756713867\n",
      "Iteration 20374 Loss: 1.2394963502883911\n",
      "Iteration 20375 Loss: 1.4302794933319092\n",
      "Iteration 20376 Loss: 0.942990779876709\n",
      "Iteration 20377 Loss: 1.0900517702102661\n",
      "Iteration 20378 Loss: 1.1315572261810303\n",
      "Iteration 20379 Loss: 1.190955400466919\n",
      "Iteration 20379 Loss: 1.0951378345489502\n",
      "Iteration 20380 Loss: 0.9609112739562988\n",
      "Iteration 20381 Loss: 1.1407947540283203\n",
      "Iteration 20382 Loss: 1.3507906198501587\n",
      "Iteration 20383 Loss: 1.0244252681732178\n",
      "Iteration 20384 Loss: 1.325296401977539\n",
      "Iteration 20385 Loss: 1.1818708181381226\n",
      "Iteration 20386 Loss: 0.8417592644691467\n",
      "Iteration 20387 Loss: 1.0115509033203125\n",
      "Iteration 20388 Loss: 1.0331159830093384\n",
      "Iteration 20389 Loss: 1.0660074949264526\n",
      "Iteration 20389 Loss: 1.0936522483825684\n",
      "Iteration 20390 Loss: 1.2044380903244019\n",
      "Iteration 20391 Loss: 1.2596917152404785\n",
      "Iteration 20392 Loss: 1.1105384826660156\n",
      "Iteration 20393 Loss: 0.8446150422096252\n",
      "Iteration 20394 Loss: 0.9615429043769836\n",
      "Iteration 20395 Loss: 1.5362439155578613\n",
      "Iteration 20396 Loss: 1.1650041341781616\n",
      "Iteration 20397 Loss: 1.2725518941879272\n",
      "Iteration 20398 Loss: 1.0045340061187744\n",
      "Iteration 20399 Loss: 1.1259534358978271\n",
      "Iteration 20399 Loss: 1.148511290550232\n",
      "Iteration 20400 Loss: 0.9929579496383667\n",
      "Iteration 20401 Loss: 1.2438857555389404\n",
      "Iteration 20402 Loss: 1.097379446029663\n",
      "Iteration 20403 Loss: 1.0525767803192139\n",
      "Iteration 20404 Loss: 1.3237050771713257\n",
      "Iteration 20405 Loss: 1.2131983041763306\n",
      "Iteration 20406 Loss: 1.0003865957260132\n",
      "Iteration 20407 Loss: 0.9198237061500549\n",
      "Iteration 20408 Loss: 1.130168080329895\n",
      "Iteration 20409 Loss: 0.9213463664054871\n",
      "Iteration 20409 Loss: 1.0895427465438843\n",
      "Iteration 20410 Loss: 1.069431185722351\n",
      "Iteration 20411 Loss: 1.2400108575820923\n",
      "Iteration 20412 Loss: 0.8794738054275513\n",
      "Iteration 20413 Loss: 1.01336669921875\n",
      "Iteration 20414 Loss: 1.1275370121002197\n",
      "Iteration 20415 Loss: 1.1295088529586792\n",
      "Iteration 20416 Loss: 0.8750835657119751\n",
      "Iteration 20417 Loss: 1.3198552131652832\n",
      "Iteration 20418 Loss: 1.154350757598877\n",
      "Iteration 20419 Loss: 0.8331717252731323\n",
      "Iteration 20419 Loss: 1.0641790628433228\n",
      "Iteration 20420 Loss: 0.7657893896102905\n",
      "Iteration 20421 Loss: 1.180821418762207\n",
      "Iteration 20422 Loss: 1.2745487689971924\n",
      "Iteration 20423 Loss: 1.2701374292373657\n",
      "Iteration 20424 Loss: 1.013118028640747\n",
      "Iteration 20425 Loss: 1.1604615449905396\n",
      "Iteration 20426 Loss: 1.1231328248977661\n",
      "Iteration 20427 Loss: 0.7491899132728577\n",
      "Iteration 20428 Loss: 1.278631329536438\n",
      "Iteration 20429 Loss: 1.2087515592575073\n",
      "Iteration 20429 Loss: 1.1024582386016846\n",
      "Iteration 20430 Loss: 0.8460245132446289\n",
      "Iteration 20431 Loss: 1.4149396419525146\n",
      "Iteration 20432 Loss: 1.3045614957809448\n",
      "Iteration 20433 Loss: 1.0735442638397217\n",
      "Iteration 20434 Loss: 1.1837201118469238\n",
      "Iteration 20435 Loss: 1.076324462890625\n",
      "Iteration 20436 Loss: 1.0022903680801392\n",
      "Iteration 20437 Loss: 1.0034157037734985\n",
      "Iteration 20438 Loss: 1.0365315675735474\n",
      "Iteration 20439 Loss: 1.212473750114441\n",
      "Iteration 20439 Loss: 1.1153827905654907\n",
      "Iteration 20440 Loss: 1.229529619216919\n",
      "Iteration 20441 Loss: 0.8475151658058167\n",
      "Iteration 20442 Loss: 1.192981243133545\n",
      "Iteration 20443 Loss: 1.3481296300888062\n",
      "Iteration 20444 Loss: 0.8173837065696716\n",
      "Iteration 20445 Loss: 0.8768057823181152\n",
      "Iteration 20446 Loss: 0.9644644856452942\n",
      "Iteration 20447 Loss: 1.1058140993118286\n",
      "Iteration 20448 Loss: 1.3473234176635742\n",
      "Iteration 20449 Loss: 1.1088860034942627\n",
      "Iteration 20449 Loss: 1.083883285522461\n",
      "Iteration 20450 Loss: 0.8903384804725647\n",
      "Iteration 20451 Loss: 0.7979161739349365\n",
      "Iteration 20452 Loss: 1.173741340637207\n",
      "Iteration 20453 Loss: 1.0060420036315918\n",
      "Iteration 20454 Loss: 0.5586969256401062\n",
      "Iteration 20455 Loss: 1.0131417512893677\n",
      "Iteration 20456 Loss: 1.1101036071777344\n",
      "Iteration 20457 Loss: 0.9527300596237183\n",
      "Iteration 20458 Loss: 0.8062891364097595\n",
      "Iteration 20459 Loss: 1.1509909629821777\n",
      "Iteration 20459 Loss: 0.9459989666938782\n",
      "Iteration 20460 Loss: 1.327738881111145\n",
      "Iteration 20461 Loss: 1.4206560850143433\n",
      "Iteration 20462 Loss: 1.3200457096099854\n",
      "Iteration 20463 Loss: 1.172313928604126\n",
      "Iteration 20464 Loss: 1.4011366367340088\n",
      "Iteration 20465 Loss: 1.0953725576400757\n",
      "Iteration 20466 Loss: 1.1594198942184448\n",
      "Iteration 20467 Loss: 1.3110589981079102\n",
      "Iteration 20468 Loss: 1.2462807893753052\n",
      "Iteration 20469 Loss: 0.9012390971183777\n",
      "Iteration 20469 Loss: 1.235526204109192\n",
      "Iteration 20470 Loss: 1.1297622919082642\n",
      "Iteration 20471 Loss: 1.1927790641784668\n",
      "Iteration 20472 Loss: 1.3819403648376465\n",
      "Iteration 20473 Loss: 1.3773443698883057\n",
      "Iteration 20474 Loss: 1.0412689447402954\n",
      "Iteration 20475 Loss: 0.81941819190979\n",
      "Iteration 20476 Loss: 0.8837864398956299\n",
      "Iteration 20477 Loss: 1.0934795141220093\n",
      "Iteration 20478 Loss: 1.3706084489822388\n",
      "Iteration 20479 Loss: 1.1789013147354126\n",
      "Iteration 20479 Loss: 1.1469289064407349\n",
      "Iteration 20480 Loss: 1.2274943590164185\n",
      "Iteration 20481 Loss: 1.2992550134658813\n",
      "Iteration 20482 Loss: 1.1116957664489746\n",
      "Iteration 20483 Loss: 1.2885154485702515\n",
      "Iteration 20484 Loss: 1.2114999294281006\n",
      "Iteration 20485 Loss: 1.3978222608566284\n",
      "Iteration 20486 Loss: 1.0686450004577637\n",
      "Iteration 20487 Loss: 0.7639482617378235\n",
      "Iteration 20488 Loss: 1.0198302268981934\n",
      "Iteration 20489 Loss: 1.0436471700668335\n",
      "Iteration 20489 Loss: 1.143235445022583\n",
      "Iteration 20490 Loss: 1.37959885597229\n",
      "Iteration 20491 Loss: 1.2217435836791992\n",
      "Iteration 20492 Loss: 1.0123684406280518\n",
      "Iteration 20493 Loss: 1.1507946252822876\n",
      "Iteration 20494 Loss: 1.610008716583252\n",
      "Iteration 20495 Loss: 1.0988527536392212\n",
      "Iteration 20496 Loss: 1.2390834093093872\n",
      "Iteration 20497 Loss: 1.3018699884414673\n",
      "Iteration 20498 Loss: 1.1111096143722534\n",
      "Iteration 20499 Loss: 1.415060043334961\n",
      "Iteration 20499 Loss: 1.2540490627288818\n",
      "Iteration 20500 Loss: 1.290961503982544\n",
      "Iteration 20501 Loss: 1.2527318000793457\n",
      "Iteration 20502 Loss: 0.7050886750221252\n",
      "Iteration 20503 Loss: 0.7921484112739563\n",
      "Iteration 20504 Loss: 1.1902605295181274\n",
      "Iteration 20505 Loss: 0.7755318880081177\n",
      "Iteration 20506 Loss: 1.0735238790512085\n",
      "Iteration 20507 Loss: 1.4280818700790405\n",
      "Iteration 20508 Loss: 1.3128032684326172\n",
      "Iteration 20509 Loss: 1.3980497121810913\n",
      "Iteration 20509 Loss: 1.1219180822372437\n",
      "Iteration 20510 Loss: 1.195984959602356\n",
      "Iteration 20511 Loss: 0.8645973205566406\n",
      "Iteration 20512 Loss: 1.084319829940796\n",
      "Iteration 20513 Loss: 1.118878960609436\n",
      "Iteration 20514 Loss: 0.9163618087768555\n",
      "Iteration 20515 Loss: 1.0925928354263306\n",
      "Iteration 20516 Loss: 1.2754403352737427\n",
      "Iteration 20517 Loss: 0.960366427898407\n",
      "Iteration 20518 Loss: 0.9128539562225342\n",
      "Iteration 20519 Loss: 1.1626167297363281\n",
      "Iteration 20519 Loss: 1.058401346206665\n",
      "Iteration 20520 Loss: 1.1588284969329834\n",
      "Iteration 20521 Loss: 1.110193133354187\n",
      "Iteration 20522 Loss: 1.0320805311203003\n",
      "Iteration 20523 Loss: 1.1987453699111938\n",
      "Iteration 20524 Loss: 1.1962497234344482\n",
      "Iteration 20525 Loss: 1.101912021636963\n",
      "Iteration 20526 Loss: 0.9754679799079895\n",
      "Iteration 20527 Loss: 1.2877893447875977\n",
      "Iteration 20528 Loss: 1.0426597595214844\n",
      "Iteration 20529 Loss: 0.81332927942276\n",
      "Iteration 20529 Loss: 1.091725468635559\n",
      "Iteration 20530 Loss: 0.8793735504150391\n",
      "Iteration 20531 Loss: 0.6702514886856079\n",
      "Iteration 20532 Loss: 0.8261215686798096\n",
      "Iteration 20533 Loss: 1.066158413887024\n",
      "Iteration 20534 Loss: 1.250961184501648\n",
      "Iteration 20535 Loss: 0.9990073442459106\n",
      "Iteration 20536 Loss: 1.3213595151901245\n",
      "Iteration 20537 Loss: 1.2710400819778442\n",
      "Iteration 20538 Loss: 0.9715386629104614\n",
      "Iteration 20539 Loss: 1.1978754997253418\n",
      "Iteration 20539 Loss: 1.0453686714172363\n",
      "Iteration 20540 Loss: 1.53189218044281\n",
      "Iteration 20541 Loss: 1.2277499437332153\n",
      "Iteration 20542 Loss: 1.1302850246429443\n",
      "Iteration 20543 Loss: 1.2230387926101685\n",
      "Iteration 20544 Loss: 1.2630795240402222\n",
      "Iteration 20545 Loss: 1.1136285066604614\n",
      "Iteration 20546 Loss: 1.0167396068572998\n",
      "Iteration 20547 Loss: 1.2930208444595337\n",
      "Iteration 20548 Loss: 1.0436058044433594\n",
      "Iteration 20549 Loss: 0.8287779688835144\n",
      "Iteration 20549 Loss: 1.1671818494796753\n",
      "Iteration 20550 Loss: 0.8066208362579346\n",
      "Iteration 20551 Loss: 1.0459848642349243\n",
      "Iteration 20552 Loss: 1.334283471107483\n",
      "Iteration 20553 Loss: 1.1753544807434082\n",
      "Iteration 20554 Loss: 0.9452998042106628\n",
      "Iteration 20555 Loss: 1.2065118551254272\n",
      "Iteration 20556 Loss: 0.865470290184021\n",
      "Iteration 20557 Loss: 1.3128923177719116\n",
      "Iteration 20558 Loss: 1.3795782327651978\n",
      "Iteration 20559 Loss: 0.9956567883491516\n",
      "Iteration 20559 Loss: 1.1067651510238647\n",
      "Iteration 20560 Loss: 0.8839988708496094\n",
      "Iteration 20561 Loss: 1.1492021083831787\n",
      "Iteration 20562 Loss: 1.3150545358657837\n",
      "Iteration 20563 Loss: 0.9934704303741455\n",
      "Iteration 20564 Loss: 1.4325674772262573\n",
      "Iteration 20565 Loss: 0.991019606590271\n",
      "Iteration 20566 Loss: 0.9552308917045593\n",
      "Iteration 20567 Loss: 1.0229530334472656\n",
      "Iteration 20568 Loss: 1.2995860576629639\n",
      "Iteration 20569 Loss: 1.1823327541351318\n",
      "Iteration 20569 Loss: 1.1225415468215942\n",
      "Iteration 20570 Loss: 1.1754486560821533\n",
      "Iteration 20571 Loss: 1.3743581771850586\n",
      "Iteration 20572 Loss: 1.1954684257507324\n",
      "Iteration 20573 Loss: 1.016898274421692\n",
      "Iteration 20574 Loss: 1.1044971942901611\n",
      "Iteration 20575 Loss: 1.0528546571731567\n",
      "Iteration 20576 Loss: 1.0836479663848877\n",
      "Iteration 20577 Loss: 1.01093327999115\n",
      "Iteration 20578 Loss: 1.1724196672439575\n",
      "Iteration 20579 Loss: 1.488820195198059\n",
      "Iteration 20579 Loss: 1.167534589767456\n",
      "Iteration 20580 Loss: 1.2314101457595825\n",
      "Iteration 20581 Loss: 1.1133990287780762\n",
      "Iteration 20582 Loss: 1.121218204498291\n",
      "Iteration 20583 Loss: 1.0556669235229492\n",
      "Iteration 20584 Loss: 1.3351294994354248\n",
      "Iteration 20585 Loss: 0.8799727559089661\n",
      "Iteration 20586 Loss: 0.8870203495025635\n",
      "Iteration 20587 Loss: 1.414384365081787\n",
      "Iteration 20588 Loss: 1.3356257677078247\n",
      "Iteration 20589 Loss: 1.077832579612732\n",
      "Iteration 20589 Loss: 1.1451658010482788\n",
      "Iteration 20590 Loss: 1.0989084243774414\n",
      "Iteration 20591 Loss: 1.4884607791900635\n",
      "Iteration 20592 Loss: 1.1636399030685425\n",
      "Iteration 20593 Loss: 1.3438242673873901\n",
      "Iteration 20594 Loss: 1.089637279510498\n",
      "Iteration 20595 Loss: 1.591678261756897\n",
      "Iteration 20596 Loss: 1.242579698562622\n",
      "Iteration 20597 Loss: 0.8080148696899414\n",
      "Iteration 20598 Loss: 0.9489824771881104\n",
      "Iteration 20599 Loss: 1.1754915714263916\n",
      "Iteration 20599 Loss: 1.1951217651367188\n",
      "Iteration 20600 Loss: 1.4503898620605469\n",
      "Iteration 20601 Loss: 1.5808650255203247\n",
      "Iteration 20602 Loss: 1.01300847530365\n",
      "Iteration 20603 Loss: 0.8530080318450928\n",
      "Iteration 20604 Loss: 1.068427324295044\n",
      "Iteration 20605 Loss: 0.8647261261940002\n",
      "Iteration 20606 Loss: 1.0837692022323608\n",
      "Iteration 20607 Loss: 1.0614328384399414\n",
      "Iteration 20608 Loss: 0.8941593170166016\n",
      "Iteration 20609 Loss: 1.0538337230682373\n",
      "Iteration 20609 Loss: 1.0923619270324707\n",
      "Iteration 20610 Loss: 1.305751085281372\n",
      "Iteration 20611 Loss: 0.6458017826080322\n",
      "Iteration 20612 Loss: 0.7301545143127441\n",
      "Iteration 20613 Loss: 0.9106448888778687\n",
      "Iteration 20614 Loss: 1.3841400146484375\n",
      "Iteration 20615 Loss: 1.1769102811813354\n",
      "Iteration 20616 Loss: 1.0615862607955933\n",
      "Iteration 20617 Loss: 1.031723141670227\n",
      "Iteration 20618 Loss: 1.1437119245529175\n",
      "Iteration 20619 Loss: 1.1831042766571045\n",
      "Iteration 20619 Loss: 1.0573527812957764\n",
      "Iteration 20620 Loss: 1.2088661193847656\n",
      "Iteration 20621 Loss: 0.7130562663078308\n",
      "Iteration 20622 Loss: 1.0278829336166382\n",
      "Iteration 20623 Loss: 1.0716534852981567\n",
      "Iteration 20624 Loss: 1.210086464881897\n",
      "Iteration 20625 Loss: 1.4488887786865234\n",
      "Iteration 20626 Loss: 0.92555171251297\n",
      "Iteration 20627 Loss: 1.3587669134140015\n",
      "Iteration 20628 Loss: 0.8379210829734802\n",
      "Iteration 20629 Loss: 1.3702740669250488\n",
      "Iteration 20629 Loss: 1.1172946691513062\n",
      "Iteration 20630 Loss: 1.0309276580810547\n",
      "Iteration 20631 Loss: 1.444036602973938\n",
      "Iteration 20632 Loss: 0.9928612112998962\n",
      "Iteration 20633 Loss: 0.8952392935752869\n",
      "Iteration 20634 Loss: 0.9211990833282471\n",
      "Iteration 20635 Loss: 1.1190500259399414\n",
      "Iteration 20636 Loss: 1.1502270698547363\n",
      "Iteration 20637 Loss: 1.1974050998687744\n",
      "Iteration 20638 Loss: 1.2267485857009888\n",
      "Iteration 20639 Loss: 0.8962200284004211\n",
      "Iteration 20639 Loss: 1.0873914957046509\n",
      "Iteration 20640 Loss: 1.1394307613372803\n",
      "Iteration 20641 Loss: 1.3339468240737915\n",
      "Iteration 20642 Loss: 0.72524094581604\n",
      "Iteration 20643 Loss: 0.983694314956665\n",
      "Iteration 20644 Loss: 0.8135350346565247\n",
      "Iteration 20645 Loss: 1.0318821668624878\n",
      "Iteration 20646 Loss: 1.2807612419128418\n",
      "Iteration 20647 Loss: 1.2917715311050415\n",
      "Iteration 20648 Loss: 0.8705163598060608\n",
      "Iteration 20649 Loss: 1.3596121072769165\n",
      "Iteration 20649 Loss: 1.0830391645431519\n",
      "Iteration 20650 Loss: 1.2147430181503296\n",
      "Iteration 20651 Loss: 1.3203141689300537\n",
      "Iteration 20652 Loss: 0.9895817041397095\n",
      "Iteration 20653 Loss: 1.3098938465118408\n",
      "Iteration 20654 Loss: 1.148996114730835\n",
      "Iteration 20655 Loss: 1.2571316957473755\n",
      "Iteration 20656 Loss: 1.4434242248535156\n",
      "Iteration 20657 Loss: 1.2641221284866333\n",
      "Iteration 20658 Loss: 1.1562238931655884\n",
      "Iteration 20659 Loss: 1.2679193019866943\n",
      "Iteration 20659 Loss: 1.2372350692749023\n",
      "Iteration 20660 Loss: 1.2578234672546387\n",
      "Iteration 20661 Loss: 1.0132031440734863\n",
      "Iteration 20662 Loss: 1.4135781526565552\n",
      "Iteration 20663 Loss: 1.4239715337753296\n",
      "Iteration 20664 Loss: 0.9005603790283203\n",
      "Iteration 20665 Loss: 1.2559114694595337\n",
      "Iteration 20666 Loss: 1.379539132118225\n",
      "Iteration 20667 Loss: 1.2862814664840698\n",
      "Iteration 20668 Loss: 1.0174708366394043\n",
      "Iteration 20669 Loss: 1.408597469329834\n",
      "Iteration 20669 Loss: 1.2356938123703003\n",
      "Iteration 20670 Loss: 1.4127763509750366\n",
      "Iteration 20671 Loss: 1.032052993774414\n",
      "Iteration 20672 Loss: 0.9025059342384338\n",
      "Iteration 20673 Loss: 1.0478479862213135\n",
      "Iteration 20674 Loss: 0.7943613529205322\n",
      "Iteration 20675 Loss: 1.3403048515319824\n",
      "Iteration 20676 Loss: 1.1935549974441528\n",
      "Iteration 20677 Loss: 1.1310945749282837\n",
      "Iteration 20678 Loss: 1.3196494579315186\n",
      "Iteration 20679 Loss: 0.9035391211509705\n",
      "Iteration 20679 Loss: 1.1077687740325928\n",
      "Iteration 20680 Loss: 1.1638885736465454\n",
      "Iteration 20681 Loss: 0.7844547629356384\n",
      "Iteration 20682 Loss: 1.1788828372955322\n",
      "Iteration 20683 Loss: 0.7824584245681763\n",
      "Iteration 20684 Loss: 0.9278759360313416\n",
      "Iteration 20685 Loss: 1.3453418016433716\n",
      "Iteration 20686 Loss: 1.0144197940826416\n",
      "Iteration 20687 Loss: 1.267850637435913\n",
      "Iteration 20688 Loss: 1.0742539167404175\n",
      "Iteration 20689 Loss: 1.0834277868270874\n",
      "Iteration 20689 Loss: 1.0622855424880981\n",
      "Iteration 20690 Loss: 1.0321705341339111\n",
      "Iteration 20691 Loss: 1.0195467472076416\n",
      "Iteration 20692 Loss: 0.7520071268081665\n",
      "Iteration 20693 Loss: 1.0095704793930054\n",
      "Iteration 20694 Loss: 1.1149733066558838\n",
      "Iteration 20695 Loss: 1.0446629524230957\n",
      "Iteration 20696 Loss: 1.1008371114730835\n",
      "Iteration 20697 Loss: 0.8472420573234558\n",
      "Iteration 20698 Loss: 1.2482253313064575\n",
      "Iteration 20699 Loss: 1.3819180727005005\n",
      "Iteration 20699 Loss: 1.0551154613494873\n",
      "Iteration 20700 Loss: 1.1026616096496582\n",
      "Iteration 20701 Loss: 1.1704976558685303\n",
      "Iteration 20702 Loss: 0.9536349773406982\n",
      "Iteration 20703 Loss: 1.2547632455825806\n",
      "Iteration 20704 Loss: 0.9837221503257751\n",
      "Iteration 20705 Loss: 0.9951150417327881\n",
      "Iteration 20706 Loss: 1.0273492336273193\n",
      "Iteration 20707 Loss: 0.9435577392578125\n",
      "Iteration 20708 Loss: 1.4112857580184937\n",
      "Iteration 20709 Loss: 1.2474366426467896\n",
      "Iteration 20709 Loss: 1.1090024709701538\n",
      "Iteration 20710 Loss: 1.1363211870193481\n",
      "Iteration 20711 Loss: 1.4931203126907349\n",
      "Iteration 20712 Loss: 0.9311792254447937\n",
      "Iteration 20713 Loss: 1.0382851362228394\n",
      "Iteration 20714 Loss: 1.1850368976593018\n",
      "Iteration 20715 Loss: 1.1481990814208984\n",
      "Iteration 20716 Loss: 1.1812564134597778\n",
      "Iteration 20717 Loss: 1.1505125761032104\n",
      "Iteration 20718 Loss: 0.9563754200935364\n",
      "Iteration 20719 Loss: 0.5977118611335754\n",
      "Iteration 20719 Loss: 1.0817997455596924\n",
      "Iteration 20720 Loss: 1.21891188621521\n",
      "Iteration 20721 Loss: 1.0905640125274658\n",
      "Iteration 20722 Loss: 0.9443077445030212\n",
      "Iteration 20723 Loss: 1.08175528049469\n",
      "Iteration 20724 Loss: 0.9712762236595154\n",
      "Iteration 20725 Loss: 0.8378473520278931\n",
      "Iteration 20726 Loss: 1.3345216512680054\n",
      "Iteration 20727 Loss: 0.8780562281608582\n",
      "Iteration 20728 Loss: 1.3212625980377197\n",
      "Iteration 20729 Loss: 1.1687463521957397\n",
      "Iteration 20729 Loss: 1.0847249031066895\n",
      "Iteration 20730 Loss: 0.6792435050010681\n",
      "Iteration 20731 Loss: 1.4810658693313599\n",
      "Iteration 20732 Loss: 0.8558897972106934\n",
      "Iteration 20733 Loss: 1.0215809345245361\n",
      "Iteration 20734 Loss: 1.029685139656067\n",
      "Iteration 20735 Loss: 0.8722778558731079\n",
      "Iteration 20736 Loss: 1.0444475412368774\n",
      "Iteration 20737 Loss: 1.1497279405593872\n",
      "Iteration 20738 Loss: 0.9707223773002625\n",
      "Iteration 20739 Loss: 1.0534155368804932\n",
      "Iteration 20739 Loss: 1.0158056020736694\n",
      "Iteration 20740 Loss: 1.0879712104797363\n",
      "Iteration 20741 Loss: 1.496876835823059\n",
      "Iteration 20742 Loss: 1.2626888751983643\n",
      "Iteration 20743 Loss: 1.1570323705673218\n",
      "Iteration 20744 Loss: 1.1917625665664673\n",
      "Iteration 20745 Loss: 1.2774266004562378\n",
      "Iteration 20746 Loss: 1.0488545894622803\n",
      "Iteration 20747 Loss: 1.248672366142273\n",
      "Iteration 20748 Loss: 1.1045016050338745\n",
      "Iteration 20749 Loss: 1.1877765655517578\n",
      "Iteration 20749 Loss: 1.206356406211853\n",
      "Iteration 20750 Loss: 1.0039026737213135\n",
      "Iteration 20751 Loss: 0.7923558950424194\n",
      "Iteration 20752 Loss: 1.084062099456787\n",
      "Iteration 20753 Loss: 1.189992904663086\n",
      "Iteration 20754 Loss: 1.0888928174972534\n",
      "Iteration 20755 Loss: 0.9918732643127441\n",
      "Iteration 20756 Loss: 0.9615983366966248\n",
      "Iteration 20757 Loss: 0.8488564491271973\n",
      "Iteration 20758 Loss: 0.9964033365249634\n",
      "Iteration 20759 Loss: 1.0158946514129639\n",
      "Iteration 20759 Loss: 0.9973832964897156\n",
      "Iteration 20760 Loss: 0.78535395860672\n",
      "Iteration 20761 Loss: 1.4336934089660645\n",
      "Iteration 20762 Loss: 1.40847647190094\n",
      "Iteration 20763 Loss: 1.448244333267212\n",
      "Iteration 20764 Loss: 1.1309009790420532\n",
      "Iteration 20765 Loss: 1.2452423572540283\n",
      "Iteration 20766 Loss: 1.2359188795089722\n",
      "Iteration 20767 Loss: 1.1304330825805664\n",
      "Iteration 20768 Loss: 1.034307837486267\n",
      "Iteration 20769 Loss: 0.8864220380783081\n",
      "Iteration 20769 Loss: 1.1738994121551514\n",
      "Iteration 20770 Loss: 1.1824467182159424\n",
      "Iteration 20771 Loss: 0.9467062950134277\n",
      "Iteration 20772 Loss: 0.7498268485069275\n",
      "Iteration 20773 Loss: 0.9633455276489258\n",
      "Iteration 20774 Loss: 1.1297259330749512\n",
      "Iteration 20775 Loss: 0.7063146233558655\n",
      "Iteration 20776 Loss: 1.1838948726654053\n",
      "Iteration 20777 Loss: 1.4169400930404663\n",
      "Iteration 20778 Loss: 1.3607020378112793\n",
      "Iteration 20779 Loss: 1.251100778579712\n",
      "Iteration 20779 Loss: 1.0891003608703613\n",
      "Iteration 20780 Loss: 0.8880503177642822\n",
      "Iteration 20781 Loss: 1.0166001319885254\n",
      "Iteration 20782 Loss: 1.085058331489563\n",
      "Iteration 20783 Loss: 0.7693447470664978\n",
      "Iteration 20784 Loss: 1.3733270168304443\n",
      "Iteration 20785 Loss: 0.6841959953308105\n",
      "Iteration 20786 Loss: 0.8836137652397156\n",
      "Iteration 20787 Loss: 0.7687981128692627\n",
      "Iteration 20788 Loss: 1.1828721761703491\n",
      "Iteration 20789 Loss: 0.9476295709609985\n",
      "Iteration 20789 Loss: 0.9599490165710449\n",
      "Iteration 20790 Loss: 1.3052172660827637\n",
      "Iteration 20791 Loss: 1.3865103721618652\n",
      "Iteration 20792 Loss: 0.9097586274147034\n",
      "Iteration 20793 Loss: 1.2804771661758423\n",
      "Iteration 20794 Loss: 1.250596523284912\n",
      "Iteration 20795 Loss: 0.7658824324607849\n",
      "Iteration 20796 Loss: 1.1272096633911133\n",
      "Iteration 20797 Loss: 1.0968568325042725\n",
      "Iteration 20798 Loss: 1.1944794654846191\n",
      "Iteration 20799 Loss: 0.9533923268318176\n",
      "Iteration 20799 Loss: 1.1270381212234497\n",
      "Iteration 20800 Loss: 1.281626582145691\n",
      "Iteration 20801 Loss: 0.9385397434234619\n",
      "Iteration 20802 Loss: 0.8856601119041443\n",
      "Iteration 20803 Loss: 1.0813363790512085\n",
      "Iteration 20804 Loss: 1.2596997022628784\n",
      "Iteration 20805 Loss: 1.8114991188049316\n",
      "Iteration 20806 Loss: 0.8736095428466797\n",
      "Iteration 20807 Loss: 1.7468249797821045\n",
      "Iteration 20808 Loss: 0.9766345024108887\n",
      "Iteration 20809 Loss: 1.142297625541687\n",
      "Iteration 20809 Loss: 1.1997729539871216\n",
      "Iteration 20810 Loss: 1.3393019437789917\n",
      "Iteration 20811 Loss: 1.1750911474227905\n",
      "Iteration 20812 Loss: 0.9922512769699097\n",
      "Iteration 20813 Loss: 1.0337077379226685\n",
      "Iteration 20814 Loss: 1.2381669282913208\n",
      "Iteration 20815 Loss: 0.9811159372329712\n",
      "Iteration 20816 Loss: 0.9057160019874573\n",
      "Iteration 20817 Loss: 1.469477653503418\n",
      "Iteration 20818 Loss: 1.1717140674591064\n",
      "Iteration 20819 Loss: 0.9837157130241394\n",
      "Iteration 20819 Loss: 1.1290258169174194\n",
      "Iteration 20820 Loss: 0.9627248048782349\n",
      "Iteration 20821 Loss: 0.8228901624679565\n",
      "Iteration 20822 Loss: 1.073156714439392\n",
      "Iteration 20823 Loss: 1.0334712266921997\n",
      "Iteration 20824 Loss: 0.8193787932395935\n",
      "Iteration 20825 Loss: 1.4161590337753296\n",
      "Iteration 20826 Loss: 1.2705183029174805\n",
      "Iteration 20827 Loss: 0.9741258025169373\n",
      "Iteration 20828 Loss: 1.2392183542251587\n",
      "Iteration 20829 Loss: 1.3592004776000977\n",
      "Iteration 20829 Loss: 1.0970842838287354\n",
      "Iteration 20830 Loss: 0.9629783630371094\n",
      "Iteration 20831 Loss: 1.164495587348938\n",
      "Iteration 20832 Loss: 1.210674524307251\n",
      "Iteration 20833 Loss: 1.0521800518035889\n",
      "Iteration 20834 Loss: 1.0226972103118896\n",
      "Iteration 20835 Loss: 1.0514671802520752\n",
      "Iteration 20836 Loss: 1.0021710395812988\n",
      "Iteration 20837 Loss: 0.857045590877533\n",
      "Iteration 20838 Loss: 1.1402174234390259\n",
      "Iteration 20839 Loss: 1.1222519874572754\n",
      "Iteration 20839 Loss: 1.0586178302764893\n",
      "Iteration 20840 Loss: 0.9817910194396973\n",
      "Iteration 20841 Loss: 1.5315648317337036\n",
      "Iteration 20842 Loss: 0.6164988875389099\n",
      "Iteration 20843 Loss: 0.8100711107254028\n",
      "Iteration 20844 Loss: 0.6098058223724365\n",
      "Iteration 20845 Loss: 1.1077582836151123\n",
      "Iteration 20846 Loss: 1.1504839658737183\n",
      "Iteration 20847 Loss: 1.3192845582962036\n",
      "Iteration 20848 Loss: 1.1784732341766357\n",
      "Iteration 20849 Loss: 1.15201735496521\n",
      "Iteration 20849 Loss: 1.0457749366760254\n",
      "Iteration 20850 Loss: 0.9518837928771973\n",
      "Iteration 20851 Loss: 1.618238925933838\n",
      "Iteration 20852 Loss: 0.9555197358131409\n",
      "Iteration 20853 Loss: 1.1030631065368652\n",
      "Iteration 20854 Loss: 1.3149538040161133\n",
      "Iteration 20855 Loss: 0.8719779253005981\n",
      "Iteration 20856 Loss: 0.9725199937820435\n",
      "Iteration 20857 Loss: 1.309579849243164\n",
      "Iteration 20858 Loss: 0.7606717944145203\n",
      "Iteration 20859 Loss: 1.0980132818222046\n",
      "Iteration 20859 Loss: 1.0956422090530396\n",
      "Iteration 20860 Loss: 0.9970654845237732\n",
      "Iteration 20861 Loss: 0.74835205078125\n",
      "Iteration 20862 Loss: 0.9417125582695007\n",
      "Iteration 20863 Loss: 1.5590852499008179\n",
      "Iteration 20864 Loss: 0.9464434385299683\n",
      "Iteration 20865 Loss: 1.1620402336120605\n",
      "Iteration 20866 Loss: 0.9381087422370911\n",
      "Iteration 20867 Loss: 1.3312022686004639\n",
      "Iteration 20868 Loss: 0.7887617945671082\n",
      "Iteration 20869 Loss: 1.274330735206604\n",
      "Iteration 20869 Loss: 1.068710207939148\n",
      "Iteration 20870 Loss: 1.2458999156951904\n",
      "Iteration 20871 Loss: 1.098118543624878\n",
      "Iteration 20872 Loss: 1.1642796993255615\n",
      "Iteration 20873 Loss: 1.4615874290466309\n",
      "Iteration 20874 Loss: 1.2854814529418945\n",
      "Iteration 20875 Loss: 1.3058935403823853\n",
      "Iteration 20876 Loss: 1.302319049835205\n",
      "Iteration 20877 Loss: 0.9355009198188782\n",
      "Iteration 20878 Loss: 1.0933754444122314\n",
      "Iteration 20879 Loss: 1.0360767841339111\n",
      "Iteration 20879 Loss: 1.192853331565857\n",
      "Iteration 20880 Loss: 1.1627998352050781\n",
      "Iteration 20881 Loss: 1.316253423690796\n",
      "Iteration 20882 Loss: 0.8734402656555176\n",
      "Iteration 20883 Loss: 0.7054746150970459\n",
      "Iteration 20884 Loss: 0.7080453038215637\n",
      "Iteration 20885 Loss: 0.37982866168022156\n",
      "Iteration 20886 Loss: 0.9820653200149536\n",
      "Iteration 20887 Loss: 0.9317277073860168\n",
      "Iteration 20888 Loss: 1.1207860708236694\n",
      "Iteration 20889 Loss: 0.9054572582244873\n",
      "Iteration 20889 Loss: 0.9085878133773804\n",
      "Iteration 20890 Loss: 0.9370152354240417\n",
      "Iteration 20891 Loss: 1.0193625688552856\n",
      "Iteration 20892 Loss: 1.2856950759887695\n",
      "Iteration 20893 Loss: 1.0991528034210205\n",
      "Iteration 20894 Loss: 1.0701223611831665\n",
      "Iteration 20895 Loss: 1.3109489679336548\n",
      "Iteration 20896 Loss: 1.1657546758651733\n",
      "Iteration 20897 Loss: 0.9642192721366882\n",
      "Iteration 20898 Loss: 1.2204174995422363\n",
      "Iteration 20899 Loss: 1.2238514423370361\n",
      "Iteration 20899 Loss: 1.1296539306640625\n",
      "Iteration 20900 Loss: 1.3374814987182617\n",
      "Iteration 20901 Loss: 1.2786310911178589\n",
      "Iteration 20902 Loss: 1.2010036706924438\n",
      "Iteration 20903 Loss: 1.5057141780853271\n",
      "Iteration 20904 Loss: 1.1278363466262817\n",
      "Iteration 20905 Loss: 1.3864463567733765\n",
      "Iteration 20906 Loss: 1.4038352966308594\n",
      "Iteration 20907 Loss: 1.236656904220581\n",
      "Iteration 20908 Loss: 1.3981677293777466\n",
      "Iteration 20909 Loss: 1.0338850021362305\n",
      "Iteration 20909 Loss: 1.2909657955169678\n",
      "Iteration 20910 Loss: 1.1478965282440186\n",
      "Iteration 20911 Loss: 1.0049363374710083\n",
      "Iteration 20912 Loss: 0.9625531435012817\n",
      "Iteration 20913 Loss: 0.9879079461097717\n",
      "Iteration 20914 Loss: 1.2812527418136597\n",
      "Iteration 20915 Loss: 1.1398897171020508\n",
      "Iteration 20916 Loss: 0.7843583822250366\n",
      "Iteration 20917 Loss: 1.4207932949066162\n",
      "Iteration 20918 Loss: 0.8776670098304749\n",
      "Iteration 20919 Loss: 1.2402939796447754\n",
      "Iteration 20919 Loss: 1.0847548246383667\n",
      "Iteration 20920 Loss: 0.8688352108001709\n",
      "Iteration 20921 Loss: 0.8960745930671692\n",
      "Iteration 20922 Loss: 0.968278169631958\n",
      "Iteration 20923 Loss: 1.2638341188430786\n",
      "Iteration 20924 Loss: 1.158054232597351\n",
      "Iteration 20925 Loss: 1.1998262405395508\n",
      "Iteration 20926 Loss: 1.0815482139587402\n",
      "Iteration 20927 Loss: 1.539251685142517\n",
      "Iteration 20928 Loss: 1.452394962310791\n",
      "Iteration 20929 Loss: 0.9171484708786011\n",
      "Iteration 20929 Loss: 1.1345245838165283\n",
      "Iteration 20930 Loss: 1.024711012840271\n",
      "Iteration 20931 Loss: 1.4255388975143433\n",
      "Iteration 20932 Loss: 1.0440280437469482\n",
      "Iteration 20933 Loss: 1.075972318649292\n",
      "Iteration 20934 Loss: 0.9925901293754578\n",
      "Iteration 20935 Loss: 0.927901566028595\n",
      "Iteration 20936 Loss: 0.9681104421615601\n",
      "Iteration 20937 Loss: 1.27953040599823\n",
      "Iteration 20938 Loss: 0.7843680381774902\n",
      "Iteration 20939 Loss: 1.1400028467178345\n",
      "Iteration 20939 Loss: 1.0662753582000732\n",
      "Iteration 20940 Loss: 1.1682149171829224\n",
      "Iteration 20941 Loss: 1.4797695875167847\n",
      "Iteration 20942 Loss: 1.2452844381332397\n",
      "Iteration 20943 Loss: 1.1334710121154785\n",
      "Iteration 20944 Loss: 0.9857020974159241\n",
      "Iteration 20945 Loss: 0.5415975451469421\n",
      "Iteration 20946 Loss: 0.9084163904190063\n",
      "Iteration 20947 Loss: 1.0652329921722412\n",
      "Iteration 20948 Loss: 1.1301285028457642\n",
      "Iteration 20949 Loss: 1.4767802953720093\n",
      "Iteration 20949 Loss: 1.113459825515747\n",
      "Iteration 20950 Loss: 1.2505868673324585\n",
      "Iteration 20951 Loss: 0.9742169976234436\n",
      "Iteration 20952 Loss: 0.8654089570045471\n",
      "Iteration 20953 Loss: 1.0149827003479004\n",
      "Iteration 20954 Loss: 1.2545135021209717\n",
      "Iteration 20955 Loss: 0.9227509498596191\n",
      "Iteration 20956 Loss: 1.1144047975540161\n",
      "Iteration 20957 Loss: 1.4395244121551514\n",
      "Iteration 20958 Loss: 1.0509541034698486\n",
      "Iteration 20959 Loss: 1.3160728216171265\n",
      "Iteration 20959 Loss: 1.1203415393829346\n",
      "Iteration 20960 Loss: 1.460503101348877\n",
      "Iteration 20961 Loss: 1.3755013942718506\n",
      "Iteration 20962 Loss: 0.9987709522247314\n",
      "Iteration 20963 Loss: 1.1802273988723755\n",
      "Iteration 20964 Loss: 1.1121128797531128\n",
      "Iteration 20965 Loss: 0.7636827826499939\n",
      "Iteration 20966 Loss: 1.5498403310775757\n",
      "Iteration 20967 Loss: 0.7610277533531189\n",
      "Iteration 20968 Loss: 1.0357933044433594\n",
      "Iteration 20969 Loss: 1.120609164237976\n",
      "Iteration 20969 Loss: 1.1358067989349365\n",
      "Iteration 20970 Loss: 1.1549978256225586\n",
      "Iteration 20971 Loss: 1.497880220413208\n",
      "Iteration 20972 Loss: 1.2378997802734375\n",
      "Iteration 20973 Loss: 1.3991354703903198\n",
      "Iteration 20974 Loss: 1.2334377765655518\n",
      "Iteration 20975 Loss: 0.9791608452796936\n",
      "Iteration 20976 Loss: 1.3138412237167358\n",
      "Iteration 20977 Loss: 0.8966730833053589\n",
      "Iteration 20978 Loss: 1.260575294494629\n",
      "Iteration 20979 Loss: 1.1026463508605957\n",
      "Iteration 20979 Loss: 1.2076247930526733\n",
      "Iteration 20980 Loss: 1.3023890256881714\n",
      "Iteration 20981 Loss: 0.9137855768203735\n",
      "Iteration 20982 Loss: 1.0914216041564941\n",
      "Iteration 20983 Loss: 1.0556296110153198\n",
      "Iteration 20984 Loss: 1.4055756330490112\n",
      "Iteration 20985 Loss: 1.5946706533432007\n",
      "Iteration 20986 Loss: 0.8094018697738647\n",
      "Iteration 20987 Loss: 1.0674601793289185\n",
      "Iteration 20988 Loss: 1.077310562133789\n",
      "Iteration 20989 Loss: 1.2096787691116333\n",
      "Iteration 20989 Loss: 1.152732253074646\n",
      "Iteration 20990 Loss: 1.0999356508255005\n",
      "Iteration 20991 Loss: 1.2195451259613037\n",
      "Iteration 20992 Loss: 1.0915486812591553\n",
      "Iteration 20993 Loss: 1.041149377822876\n",
      "Iteration 20994 Loss: 0.9677719473838806\n",
      "Iteration 20995 Loss: 1.0380734205245972\n",
      "Iteration 20996 Loss: 1.1374125480651855\n",
      "Iteration 20997 Loss: 0.9851942658424377\n",
      "Iteration 20998 Loss: 1.4567773342132568\n",
      "Iteration 20999 Loss: 0.9108363389968872\n",
      "Iteration 20999 Loss: 1.0948245525360107\n",
      "Iteration 21000 Loss: 1.1281061172485352\n",
      "Iteration 21001 Loss: 1.2683970928192139\n",
      "Iteration 21002 Loss: 1.0080032348632812\n",
      "Iteration 21003 Loss: 0.9486677050590515\n",
      "Iteration 21004 Loss: 1.2909544706344604\n",
      "Iteration 21005 Loss: 0.8636630773544312\n",
      "Iteration 21006 Loss: 1.282387375831604\n",
      "Iteration 21007 Loss: 1.0645819902420044\n",
      "Iteration 21008 Loss: 1.5172518491744995\n",
      "Iteration 21009 Loss: 1.1754921674728394\n",
      "Iteration 21009 Loss: 1.1547504663467407\n",
      "Iteration 21010 Loss: 1.7500507831573486\n",
      "Iteration 21011 Loss: 1.3173880577087402\n",
      "Iteration 21012 Loss: 0.919712245464325\n",
      "Iteration 21013 Loss: 1.1514753103256226\n",
      "Iteration 21014 Loss: 0.878805935382843\n",
      "Iteration 21015 Loss: 1.115959882736206\n",
      "Iteration 21016 Loss: 1.0141479969024658\n",
      "Iteration 21017 Loss: 1.2131890058517456\n",
      "Iteration 21018 Loss: 1.0789858102798462\n",
      "Iteration 21019 Loss: 1.1919714212417603\n",
      "Iteration 21019 Loss: 1.1631686687469482\n",
      "Iteration 21020 Loss: 1.2485711574554443\n",
      "Iteration 21021 Loss: 0.8454487919807434\n",
      "Iteration 21022 Loss: 1.2544986009597778\n",
      "Iteration 21023 Loss: 1.0638242959976196\n",
      "Iteration 21024 Loss: 1.0751835107803345\n",
      "Iteration 21025 Loss: 0.8299589157104492\n",
      "Iteration 21026 Loss: 1.134177327156067\n",
      "Iteration 21027 Loss: 1.0369340181350708\n",
      "Iteration 21028 Loss: 1.2598552703857422\n",
      "Iteration 21029 Loss: 0.9138006567955017\n",
      "Iteration 21029 Loss: 1.0662251710891724\n",
      "Iteration 21030 Loss: 1.5336980819702148\n",
      "Iteration 21031 Loss: 1.2450473308563232\n",
      "Iteration 21032 Loss: 0.9819451570510864\n",
      "Iteration 21033 Loss: 1.2153105735778809\n",
      "Iteration 21034 Loss: 1.0419262647628784\n",
      "Iteration 21035 Loss: 1.3965157270431519\n",
      "Iteration 21036 Loss: 1.433047890663147\n",
      "Iteration 21037 Loss: 1.426048994064331\n",
      "Iteration 21038 Loss: 1.0111348628997803\n",
      "Iteration 21039 Loss: 1.079487681388855\n",
      "Iteration 21039 Loss: 1.236416220664978\n",
      "Iteration 21040 Loss: 0.9858825206756592\n",
      "Iteration 21041 Loss: 0.8596106171607971\n",
      "Iteration 21042 Loss: 1.0514057874679565\n",
      "Iteration 21043 Loss: 1.2812796831130981\n",
      "Iteration 21044 Loss: 1.4600688219070435\n",
      "Iteration 21045 Loss: 1.2616456747055054\n",
      "Iteration 21046 Loss: 1.3740863800048828\n",
      "Iteration 21047 Loss: 1.1182717084884644\n",
      "Iteration 21048 Loss: 1.2746976613998413\n",
      "Iteration 21049 Loss: 1.364546298980713\n",
      "Iteration 21049 Loss: 1.2031495571136475\n",
      "Iteration 21050 Loss: 0.8209665417671204\n",
      "Iteration 21051 Loss: 0.8288475275039673\n",
      "Iteration 21052 Loss: 1.0678828954696655\n",
      "Iteration 21053 Loss: 1.1130309104919434\n",
      "Iteration 21054 Loss: 0.7504250407218933\n",
      "Iteration 21055 Loss: 0.9998247623443604\n",
      "Iteration 21056 Loss: 1.268283486366272\n",
      "Iteration 21057 Loss: 0.9035407304763794\n",
      "Iteration 21058 Loss: 1.1771541833877563\n",
      "Iteration 21059 Loss: 1.1478782892227173\n",
      "Iteration 21059 Loss: 1.0077834129333496\n",
      "Iteration 21060 Loss: 0.8553299903869629\n",
      "Iteration 21061 Loss: 1.00569486618042\n",
      "Iteration 21062 Loss: 1.263066053390503\n",
      "Iteration 21063 Loss: 1.0678603649139404\n",
      "Iteration 21064 Loss: 1.0918492078781128\n",
      "Iteration 21065 Loss: 0.6680566072463989\n",
      "Iteration 21066 Loss: 0.9941442608833313\n",
      "Iteration 21067 Loss: 1.133541464805603\n",
      "Iteration 21068 Loss: 1.3035622835159302\n",
      "Iteration 21069 Loss: 1.0052083730697632\n",
      "Iteration 21069 Loss: 1.038831353187561\n",
      "Iteration 21070 Loss: 1.395420789718628\n",
      "Iteration 21071 Loss: 0.7770717740058899\n",
      "Iteration 21072 Loss: 0.5814315676689148\n",
      "Iteration 21073 Loss: 1.1349549293518066\n",
      "Iteration 21074 Loss: 1.440739631652832\n",
      "Iteration 21075 Loss: 1.2053403854370117\n",
      "Iteration 21076 Loss: 1.0437812805175781\n",
      "Iteration 21077 Loss: 0.9265579581260681\n",
      "Iteration 21078 Loss: 1.2070680856704712\n",
      "Iteration 21079 Loss: 1.3132680654525757\n",
      "Iteration 21079 Loss: 1.1025633811950684\n",
      "Iteration 21080 Loss: 1.1775152683258057\n",
      "Iteration 21081 Loss: 0.8901684284210205\n",
      "Iteration 21082 Loss: 1.5433425903320312\n",
      "Iteration 21083 Loss: 1.1696938276290894\n",
      "Iteration 21084 Loss: 1.4891457557678223\n",
      "Iteration 21085 Loss: 1.107426643371582\n",
      "Iteration 21086 Loss: 1.2247647047042847\n",
      "Iteration 21087 Loss: 1.0244529247283936\n",
      "Iteration 21088 Loss: 1.2548046112060547\n",
      "Iteration 21089 Loss: 0.743361234664917\n",
      "Iteration 21089 Loss: 1.1624677181243896\n",
      "Iteration 21090 Loss: 1.265731930732727\n",
      "Iteration 21091 Loss: 1.1200323104858398\n",
      "Iteration 21092 Loss: 0.8519244194030762\n",
      "Iteration 21093 Loss: 1.2901806831359863\n",
      "Iteration 21094 Loss: 1.2640300989151\n",
      "Iteration 21095 Loss: 1.038281798362732\n",
      "Iteration 21096 Loss: 1.2148178815841675\n",
      "Iteration 21097 Loss: 0.6433565020561218\n",
      "Iteration 21098 Loss: 1.2298389673233032\n",
      "Iteration 21099 Loss: 1.0548679828643799\n",
      "Iteration 21099 Loss: 1.097306251525879\n",
      "Iteration 21100 Loss: 1.0739825963974\n",
      "Iteration 21101 Loss: 1.3253729343414307\n",
      "Iteration 21102 Loss: 1.2898439168930054\n",
      "Iteration 21103 Loss: 1.3221694231033325\n",
      "Iteration 21104 Loss: 1.5509469509124756\n",
      "Iteration 21105 Loss: 1.3365823030471802\n",
      "Iteration 21106 Loss: 0.7557769417762756\n",
      "Iteration 21107 Loss: 1.501736044883728\n",
      "Iteration 21108 Loss: 1.1157805919647217\n",
      "Iteration 21109 Loss: 1.3089805841445923\n",
      "Iteration 21109 Loss: 1.2581171989440918\n",
      "Iteration 21110 Loss: 1.2846308946609497\n",
      "Iteration 21111 Loss: 1.2326347827911377\n",
      "Iteration 21112 Loss: 1.4499541521072388\n",
      "Iteration 21113 Loss: 1.4720379114151\n",
      "Iteration 21114 Loss: 1.0785661935806274\n",
      "Iteration 21115 Loss: 0.9904541969299316\n",
      "Iteration 21116 Loss: 1.2510026693344116\n",
      "Iteration 21117 Loss: 1.2123453617095947\n",
      "Iteration 21118 Loss: 1.362553596496582\n",
      "Iteration 21119 Loss: 1.2351130247116089\n",
      "Iteration 21119 Loss: 1.2569292783737183\n",
      "Iteration 21120 Loss: 1.3531434535980225\n",
      "Iteration 21121 Loss: 1.243766188621521\n",
      "Iteration 21122 Loss: 0.8546372652053833\n",
      "Iteration 21123 Loss: 1.2514768838882446\n",
      "Iteration 21124 Loss: 1.2241401672363281\n",
      "Iteration 21125 Loss: 1.3292310237884521\n",
      "Iteration 21126 Loss: 1.231797456741333\n",
      "Iteration 21127 Loss: 1.03292715549469\n",
      "Iteration 21128 Loss: 1.130811095237732\n",
      "Iteration 21129 Loss: 1.197522521018982\n",
      "Iteration 21129 Loss: 1.1849453449249268\n",
      "Iteration 21130 Loss: 1.248944878578186\n",
      "Iteration 21131 Loss: 1.2292110919952393\n",
      "Iteration 21132 Loss: 1.4164315462112427\n",
      "Iteration 21133 Loss: 1.2148537635803223\n",
      "Iteration 21134 Loss: 1.1448445320129395\n",
      "Iteration 21135 Loss: 1.3300799131393433\n",
      "Iteration 21136 Loss: 1.1218494176864624\n",
      "Iteration 21137 Loss: 1.4725955724716187\n",
      "Iteration 21138 Loss: 0.9522915482521057\n",
      "Iteration 21139 Loss: 1.23561429977417\n",
      "Iteration 21139 Loss: 1.2366715669631958\n",
      "Iteration 21140 Loss: 1.0375397205352783\n",
      "Iteration 21141 Loss: 1.3734489679336548\n",
      "Iteration 21142 Loss: 0.6557364463806152\n",
      "Iteration 21143 Loss: 1.4591764211654663\n",
      "Iteration 21144 Loss: 1.148871660232544\n",
      "Iteration 21145 Loss: 1.5015037059783936\n",
      "Iteration 21146 Loss: 1.163967251777649\n",
      "Iteration 21147 Loss: 1.2768316268920898\n",
      "Iteration 21148 Loss: 1.199422001838684\n",
      "Iteration 21149 Loss: 1.2417720556259155\n",
      "Iteration 21149 Loss: 1.205826997756958\n",
      "Iteration 21150 Loss: 1.0778766870498657\n",
      "Iteration 21151 Loss: 1.224417805671692\n",
      "Iteration 21152 Loss: 1.066890835762024\n",
      "Iteration 21153 Loss: 0.7997871041297913\n",
      "Iteration 21154 Loss: 0.9167802929878235\n",
      "Iteration 21155 Loss: 1.4311140775680542\n",
      "Iteration 21156 Loss: 0.8974365592002869\n",
      "Iteration 21157 Loss: 1.5558785200119019\n",
      "Iteration 21158 Loss: 0.9258873462677002\n",
      "Iteration 21159 Loss: 1.0245106220245361\n",
      "Iteration 21159 Loss: 1.0920579433441162\n",
      "Iteration 21160 Loss: 1.1500635147094727\n",
      "Iteration 21161 Loss: 0.7271186709403992\n",
      "Iteration 21162 Loss: 1.3926752805709839\n",
      "Iteration 21163 Loss: 1.0427242517471313\n",
      "Iteration 21164 Loss: 1.0962862968444824\n",
      "Iteration 21165 Loss: 0.8352489471435547\n",
      "Iteration 21166 Loss: 1.1350003480911255\n",
      "Iteration 21167 Loss: 1.0655288696289062\n",
      "Iteration 21168 Loss: 1.2444452047348022\n",
      "Iteration 21169 Loss: 1.3628108501434326\n",
      "Iteration 21169 Loss: 1.1051901578903198\n",
      "Iteration 21170 Loss: 0.7902636528015137\n",
      "Iteration 21171 Loss: 1.2525854110717773\n",
      "Iteration 21172 Loss: 1.168567419052124\n",
      "Iteration 21173 Loss: 0.7720351815223694\n",
      "Iteration 21174 Loss: 0.9570829272270203\n",
      "Iteration 21175 Loss: 1.0420963764190674\n",
      "Iteration 21176 Loss: 0.8586323261260986\n",
      "Iteration 21177 Loss: 1.0476969480514526\n",
      "Iteration 21178 Loss: 1.4911803007125854\n",
      "Iteration 21179 Loss: 1.3506540060043335\n",
      "Iteration 21179 Loss: 1.0730794668197632\n",
      "Iteration 21180 Loss: 1.1669073104858398\n",
      "Iteration 21181 Loss: 1.1336455345153809\n",
      "Iteration 21182 Loss: 0.848059356212616\n",
      "Iteration 21183 Loss: 1.1392391920089722\n",
      "Iteration 21184 Loss: 1.393183708190918\n",
      "Iteration 21185 Loss: 1.3598703145980835\n",
      "Iteration 21186 Loss: 1.12026047706604\n",
      "Iteration 21187 Loss: 1.62955641746521\n",
      "Iteration 21188 Loss: 0.8796645402908325\n",
      "Iteration 21189 Loss: 0.9079495668411255\n",
      "Iteration 21189 Loss: 1.1578336954116821\n",
      "Iteration 21190 Loss: 1.3044997453689575\n",
      "Iteration 21191 Loss: 0.8397481441497803\n",
      "Iteration 21192 Loss: 1.1877177953720093\n",
      "Iteration 21193 Loss: 1.164380669593811\n",
      "Iteration 21194 Loss: 1.2531685829162598\n",
      "Iteration 21195 Loss: 1.364863395690918\n",
      "Iteration 21196 Loss: 1.057149887084961\n",
      "Iteration 21197 Loss: 0.4775058925151825\n",
      "Iteration 21198 Loss: 1.0247091054916382\n",
      "Iteration 21199 Loss: 0.9424082636833191\n",
      "Iteration 21199 Loss: 1.0616151094436646\n",
      "Iteration 21200 Loss: 1.044066309928894\n",
      "Iteration 21201 Loss: 0.9522751569747925\n",
      "Iteration 21202 Loss: 1.0930153131484985\n",
      "Iteration 21203 Loss: 1.4067292213439941\n",
      "Iteration 21204 Loss: 1.013525366783142\n",
      "Iteration 21205 Loss: 1.0888276100158691\n",
      "Iteration 21206 Loss: 0.8316054344177246\n",
      "Iteration 21207 Loss: 1.1471761465072632\n",
      "Iteration 21208 Loss: 1.011542797088623\n",
      "Iteration 21209 Loss: 0.9652798771858215\n",
      "Iteration 21209 Loss: 1.0554044246673584\n",
      "Iteration 21210 Loss: 1.6624095439910889\n",
      "Iteration 21211 Loss: 1.1017423868179321\n",
      "Iteration 21212 Loss: 0.8680449724197388\n",
      "Iteration 21213 Loss: 1.1037577390670776\n",
      "Iteration 21214 Loss: 0.7091174125671387\n",
      "Iteration 21215 Loss: 1.0765118598937988\n",
      "Iteration 21216 Loss: 1.1968073844909668\n",
      "Iteration 21217 Loss: 1.516304850578308\n",
      "Iteration 21218 Loss: 1.1582918167114258\n",
      "Iteration 21219 Loss: 0.8499037027359009\n",
      "Iteration 21219 Loss: 1.1242892742156982\n",
      "Iteration 21220 Loss: 1.312064290046692\n",
      "Iteration 21221 Loss: 0.8398668766021729\n",
      "Iteration 21222 Loss: 1.0660399198532104\n",
      "Iteration 21223 Loss: 1.016120195388794\n",
      "Iteration 21224 Loss: 1.1021074056625366\n",
      "Iteration 21225 Loss: 1.156313180923462\n",
      "Iteration 21226 Loss: 1.4854892492294312\n",
      "Iteration 21227 Loss: 1.059401512145996\n",
      "Iteration 21228 Loss: 1.2248387336730957\n",
      "Iteration 21229 Loss: 0.9722082018852234\n",
      "Iteration 21229 Loss: 1.12344491481781\n",
      "Iteration 21230 Loss: 1.4361521005630493\n",
      "Iteration 21231 Loss: 0.8036890625953674\n",
      "Iteration 21232 Loss: 1.3590199947357178\n",
      "Iteration 21233 Loss: 0.9171911478042603\n",
      "Iteration 21234 Loss: 1.0160233974456787\n",
      "Iteration 21235 Loss: 1.023247241973877\n",
      "Iteration 21236 Loss: 0.9278348684310913\n",
      "Iteration 21237 Loss: 1.1838219165802002\n",
      "Iteration 21238 Loss: 1.1853286027908325\n",
      "Iteration 21239 Loss: 1.0495500564575195\n",
      "Iteration 21239 Loss: 1.0901857614517212\n",
      "Iteration 21240 Loss: 1.2868626117706299\n",
      "Iteration 21241 Loss: 1.1136057376861572\n",
      "Iteration 21242 Loss: 1.1144949197769165\n",
      "Iteration 21243 Loss: 1.1385254859924316\n",
      "Iteration 21244 Loss: 0.7957482933998108\n",
      "Iteration 21245 Loss: 1.0155309438705444\n",
      "Iteration 21246 Loss: 1.1472316980361938\n",
      "Iteration 21247 Loss: 0.9472326636314392\n",
      "Iteration 21248 Loss: 1.1226940155029297\n",
      "Iteration 21249 Loss: 1.1295514106750488\n",
      "Iteration 21249 Loss: 1.0811477899551392\n",
      "Iteration 21250 Loss: 1.1724070310592651\n",
      "Iteration 21251 Loss: 0.7270157933235168\n",
      "Iteration 21252 Loss: 1.1131380796432495\n",
      "Iteration 21253 Loss: 1.3939945697784424\n",
      "Iteration 21254 Loss: 1.1705305576324463\n",
      "Iteration 21255 Loss: 0.975451648235321\n",
      "Iteration 21256 Loss: 1.1502984762191772\n",
      "Iteration 21257 Loss: 1.113681435585022\n",
      "Iteration 21258 Loss: 1.1212667226791382\n",
      "Iteration 21259 Loss: 1.0589288473129272\n",
      "Iteration 21259 Loss: 1.0996712446212769\n",
      "Iteration 21260 Loss: 1.1077513694763184\n",
      "Iteration 21261 Loss: 0.8654592037200928\n",
      "Iteration 21262 Loss: 1.0338603258132935\n",
      "Iteration 21263 Loss: 1.4674749374389648\n",
      "Iteration 21264 Loss: 1.346225380897522\n",
      "Iteration 21265 Loss: 1.5767955780029297\n",
      "Iteration 21266 Loss: 1.244398593902588\n",
      "Iteration 21267 Loss: 1.2651218175888062\n",
      "Iteration 21268 Loss: 1.0422395467758179\n",
      "Iteration 21269 Loss: 0.7108141183853149\n",
      "Iteration 21269 Loss: 1.1660140752792358\n",
      "Iteration 21270 Loss: 1.5189329385757446\n",
      "Iteration 21271 Loss: 1.1153138875961304\n",
      "Iteration 21272 Loss: 1.7575923204421997\n",
      "Iteration 21273 Loss: 1.1954807043075562\n",
      "Iteration 21274 Loss: 1.1455315351486206\n",
      "Iteration 21275 Loss: 1.2847634553909302\n",
      "Iteration 21276 Loss: 1.2614984512329102\n",
      "Iteration 21277 Loss: 1.457581639289856\n",
      "Iteration 21278 Loss: 0.8421311378479004\n",
      "Iteration 21279 Loss: 1.0818705558776855\n",
      "Iteration 21279 Loss: 1.2660696506500244\n",
      "Iteration 21280 Loss: 1.17624831199646\n",
      "Iteration 21281 Loss: 0.820708692073822\n",
      "Iteration 21282 Loss: 1.0404447317123413\n",
      "Iteration 21283 Loss: 1.0026885271072388\n",
      "Iteration 21284 Loss: 1.21688973903656\n",
      "Iteration 21285 Loss: 1.4182149171829224\n",
      "Iteration 21286 Loss: 1.198577642440796\n",
      "Iteration 21287 Loss: 0.9638891220092773\n",
      "Iteration 21288 Loss: 1.214531421661377\n",
      "Iteration 21289 Loss: 1.519185185432434\n",
      "Iteration 21289 Loss: 1.1571378707885742\n",
      "Iteration 21290 Loss: 1.2638109922409058\n",
      "Iteration 21291 Loss: 1.244262933731079\n",
      "Iteration 21292 Loss: 1.1718069314956665\n",
      "Iteration 21293 Loss: 0.9462050199508667\n",
      "Iteration 21294 Loss: 1.1004242897033691\n",
      "Iteration 21295 Loss: 1.0929447412490845\n",
      "Iteration 21296 Loss: 0.7808687686920166\n",
      "Iteration 21297 Loss: 0.8378317356109619\n",
      "Iteration 21298 Loss: 1.1145683526992798\n",
      "Iteration 21299 Loss: 1.314712643623352\n",
      "Iteration 21299 Loss: 1.0867435932159424\n",
      "Iteration 21300 Loss: 1.3617902994155884\n",
      "Iteration 21301 Loss: 0.6686770915985107\n",
      "Iteration 21302 Loss: 1.1058350801467896\n",
      "Iteration 21303 Loss: 1.1901249885559082\n",
      "Iteration 21304 Loss: 0.957508385181427\n",
      "Iteration 21305 Loss: 1.4900057315826416\n",
      "Iteration 21306 Loss: 0.7516515254974365\n",
      "Iteration 21307 Loss: 0.8138893842697144\n",
      "Iteration 21308 Loss: 0.7781124711036682\n",
      "Iteration 21309 Loss: 1.373779296875\n",
      "Iteration 21309 Loss: 1.0491373538970947\n",
      "Iteration 21310 Loss: 1.3255704641342163\n",
      "Iteration 21311 Loss: 1.2525312900543213\n",
      "Iteration 21312 Loss: 0.8118900656700134\n",
      "Iteration 21313 Loss: 0.831421971321106\n",
      "Iteration 21314 Loss: 1.2147246599197388\n",
      "Iteration 21315 Loss: 0.7820250988006592\n",
      "Iteration 21316 Loss: 1.5283900499343872\n",
      "Iteration 21317 Loss: 1.1146169900894165\n",
      "Iteration 21318 Loss: 0.780963122844696\n",
      "Iteration 21319 Loss: 0.823636531829834\n",
      "Iteration 21319 Loss: 1.046576976776123\n",
      "Iteration 21320 Loss: 0.7256138920783997\n",
      "Iteration 21321 Loss: 1.1498265266418457\n",
      "Iteration 21322 Loss: 1.0594686269760132\n",
      "Iteration 21323 Loss: 0.6650179028511047\n",
      "Iteration 21324 Loss: 0.8716509342193604\n",
      "Iteration 21325 Loss: 0.6564158797264099\n",
      "Iteration 21326 Loss: 1.095837116241455\n",
      "Iteration 21327 Loss: 0.8856292963027954\n",
      "Iteration 21328 Loss: 1.1764228343963623\n",
      "Iteration 21329 Loss: 1.41034734249115\n",
      "Iteration 21329 Loss: 0.9696232080459595\n",
      "Iteration 21330 Loss: 1.1649460792541504\n",
      "Iteration 21331 Loss: 1.22649085521698\n",
      "Iteration 21332 Loss: 1.0848524570465088\n",
      "Iteration 21333 Loss: 1.2863643169403076\n",
      "Iteration 21334 Loss: 1.2802660465240479\n",
      "Iteration 21335 Loss: 1.4308470487594604\n",
      "Iteration 21336 Loss: 1.1565276384353638\n",
      "Iteration 21337 Loss: 1.114556908607483\n",
      "Iteration 21338 Loss: 1.2125846147537231\n",
      "Iteration 21339 Loss: 0.8664795160293579\n",
      "Iteration 21339 Loss: 1.1823915243148804\n",
      "Iteration 21340 Loss: 0.8898645639419556\n",
      "Iteration 21341 Loss: 1.5393704175949097\n",
      "Iteration 21342 Loss: 0.9272432923316956\n",
      "Iteration 21343 Loss: 1.1001228094100952\n",
      "Iteration 21344 Loss: 0.946250319480896\n",
      "Iteration 21345 Loss: 1.1327271461486816\n",
      "Iteration 21346 Loss: 0.83297199010849\n",
      "Iteration 21347 Loss: 1.183316707611084\n",
      "Iteration 21348 Loss: 1.4656262397766113\n",
      "Iteration 21349 Loss: 1.12080717086792\n",
      "Iteration 21349 Loss: 1.1138299703598022\n",
      "Iteration 21350 Loss: 0.909762442111969\n",
      "Iteration 21351 Loss: 0.9518477916717529\n",
      "Iteration 21352 Loss: 1.3678559064865112\n",
      "Iteration 21353 Loss: 0.9795742034912109\n",
      "Iteration 21354 Loss: 1.2699825763702393\n",
      "Iteration 21355 Loss: 1.2949445247650146\n",
      "Iteration 21356 Loss: 1.4457273483276367\n",
      "Iteration 21357 Loss: 1.2681431770324707\n",
      "Iteration 21358 Loss: 1.221659541130066\n",
      "Iteration 21359 Loss: 1.4073021411895752\n",
      "Iteration 21359 Loss: 1.2116800546646118\n",
      "Iteration 21360 Loss: 1.2498283386230469\n",
      "Iteration 21361 Loss: 1.3361659049987793\n",
      "Iteration 21362 Loss: 0.8571437001228333\n",
      "Iteration 21363 Loss: 0.9502183198928833\n",
      "Iteration 21364 Loss: 1.414528727531433\n",
      "Iteration 21365 Loss: 0.9552980661392212\n",
      "Iteration 21366 Loss: 1.420596718788147\n",
      "Iteration 21367 Loss: 1.4980882406234741\n",
      "Iteration 21368 Loss: 0.9297904372215271\n",
      "Iteration 21369 Loss: 1.3259719610214233\n",
      "Iteration 21369 Loss: 1.193763017654419\n",
      "Iteration 21370 Loss: 1.310638666152954\n",
      "Iteration 21371 Loss: 1.1475067138671875\n",
      "Iteration 21372 Loss: 1.551780104637146\n",
      "Iteration 21373 Loss: 1.6151812076568604\n",
      "Iteration 21374 Loss: 1.5290329456329346\n",
      "Iteration 21375 Loss: 0.75005042552948\n",
      "Iteration 21376 Loss: 1.0909208059310913\n",
      "Iteration 21377 Loss: 1.2901420593261719\n",
      "Iteration 21378 Loss: 1.1689870357513428\n",
      "Iteration 21379 Loss: 0.968817412853241\n",
      "Iteration 21379 Loss: 1.2423057556152344\n",
      "Iteration 21380 Loss: 1.4106727838516235\n",
      "Iteration 21381 Loss: 1.1937865018844604\n",
      "Iteration 21382 Loss: 1.1187081336975098\n",
      "Iteration 21383 Loss: 1.2731589078903198\n",
      "Iteration 21384 Loss: 0.8509136438369751\n",
      "Iteration 21385 Loss: 0.9199397563934326\n",
      "Iteration 21386 Loss: 1.125442385673523\n",
      "Iteration 21387 Loss: 1.031941533088684\n",
      "Iteration 21388 Loss: 1.2546770572662354\n",
      "Iteration 21389 Loss: 1.4646326303482056\n",
      "Iteration 21389 Loss: 1.1643874645233154\n",
      "Iteration 21390 Loss: 1.0064218044281006\n",
      "Iteration 21391 Loss: 1.14883291721344\n",
      "Iteration 21392 Loss: 1.1060510873794556\n",
      "Iteration 21393 Loss: 1.2311360836029053\n",
      "Iteration 21394 Loss: 1.0154651403427124\n",
      "Iteration 21395 Loss: 1.0766186714172363\n",
      "Iteration 21396 Loss: 1.1153521537780762\n",
      "Iteration 21397 Loss: 1.008288860321045\n",
      "Iteration 21398 Loss: 1.2631340026855469\n",
      "Iteration 21399 Loss: 1.2656010389328003\n",
      "Iteration 21399 Loss: 1.1236902475357056\n",
      "Iteration 21400 Loss: 1.5963929891586304\n",
      "Iteration 21401 Loss: 1.1276193857192993\n",
      "Iteration 21402 Loss: 1.2993160486221313\n",
      "Iteration 21403 Loss: 1.0138530731201172\n",
      "Iteration 21404 Loss: 0.8733658194541931\n",
      "Iteration 21405 Loss: 1.2168413400650024\n",
      "Iteration 21406 Loss: 1.2214115858078003\n",
      "Iteration 21407 Loss: 1.1254178285598755\n",
      "Iteration 21408 Loss: 1.1847058534622192\n",
      "Iteration 21409 Loss: 1.0857114791870117\n",
      "Iteration 21409 Loss: 1.1744635105133057\n",
      "Iteration 21410 Loss: 0.8919461965560913\n",
      "Iteration 21411 Loss: 1.4624823331832886\n",
      "Iteration 21412 Loss: 0.957792341709137\n",
      "Iteration 21413 Loss: 1.2373979091644287\n",
      "Iteration 21414 Loss: 1.1787633895874023\n",
      "Iteration 21415 Loss: 0.8632194399833679\n",
      "Iteration 21416 Loss: 1.0226967334747314\n",
      "Iteration 21417 Loss: 0.9394195079803467\n",
      "Iteration 21418 Loss: 1.2292606830596924\n",
      "Iteration 21419 Loss: 1.0754499435424805\n",
      "Iteration 21419 Loss: 1.0858428478240967\n",
      "Iteration 21420 Loss: 1.1716105937957764\n",
      "Iteration 21421 Loss: 1.112926721572876\n",
      "Iteration 21422 Loss: 1.1561291217803955\n",
      "Iteration 21423 Loss: 0.8541437387466431\n",
      "Iteration 21424 Loss: 0.9424477219581604\n",
      "Iteration 21425 Loss: 1.0602558851242065\n",
      "Iteration 21426 Loss: 0.8866822719573975\n",
      "Iteration 21427 Loss: 0.8790736794471741\n",
      "Iteration 21428 Loss: 0.8071111440658569\n",
      "Iteration 21429 Loss: 0.7904441356658936\n",
      "Iteration 21429 Loss: 0.9660825729370117\n",
      "Iteration 21430 Loss: 1.4527438879013062\n",
      "Iteration 21431 Loss: 0.9219357371330261\n",
      "Iteration 21432 Loss: 1.0955193042755127\n",
      "Iteration 21433 Loss: 0.8609170317649841\n",
      "Iteration 21434 Loss: 1.091496229171753\n",
      "Iteration 21435 Loss: 0.9016047120094299\n",
      "Iteration 21436 Loss: 1.1745598316192627\n",
      "Iteration 21437 Loss: 1.0356215238571167\n",
      "Iteration 21438 Loss: 1.1427183151245117\n",
      "Iteration 21439 Loss: 0.6903029680252075\n",
      "Iteration 21439 Loss: 1.0367419719696045\n",
      "Iteration 21440 Loss: 1.286192774772644\n",
      "Iteration 21441 Loss: 1.1508994102478027\n",
      "Iteration 21442 Loss: 1.1468504667282104\n",
      "Iteration 21443 Loss: 1.1154190301895142\n",
      "Iteration 21444 Loss: 0.9890292286872864\n",
      "Iteration 21445 Loss: 1.1513887643814087\n",
      "Iteration 21446 Loss: 1.2644084692001343\n",
      "Iteration 21447 Loss: 1.0226800441741943\n",
      "Iteration 21448 Loss: 1.3841384649276733\n",
      "Iteration 21449 Loss: 1.040149211883545\n",
      "Iteration 21449 Loss: 1.1551156044006348\n",
      "Iteration 21450 Loss: 0.7599932551383972\n",
      "Iteration 21451 Loss: 1.1256135702133179\n",
      "Iteration 21452 Loss: 1.39826500415802\n",
      "Iteration 21453 Loss: 1.1869090795516968\n",
      "Iteration 21454 Loss: 1.257256031036377\n",
      "Iteration 21455 Loss: 1.1683650016784668\n",
      "Iteration 21456 Loss: 1.1279457807540894\n",
      "Iteration 21457 Loss: 0.9119675159454346\n",
      "Iteration 21458 Loss: 0.9984614849090576\n",
      "Iteration 21459 Loss: 0.8155330419540405\n",
      "Iteration 21459 Loss: 1.0750309228897095\n",
      "Iteration 21460 Loss: 1.1846777200698853\n",
      "Iteration 21461 Loss: 0.8777787685394287\n",
      "Iteration 21462 Loss: 1.229216456413269\n",
      "Iteration 21463 Loss: 1.1684587001800537\n",
      "Iteration 21464 Loss: 1.3100405931472778\n",
      "Iteration 21465 Loss: 1.3159242868423462\n",
      "Iteration 21466 Loss: 1.0219982862472534\n",
      "Iteration 21467 Loss: 1.0619395971298218\n",
      "Iteration 21468 Loss: 0.8315076231956482\n",
      "Iteration 21469 Loss: 1.0265039205551147\n",
      "Iteration 21469 Loss: 1.1028045415878296\n",
      "Iteration 21470 Loss: 0.8342615962028503\n",
      "Iteration 21471 Loss: 1.5068678855895996\n",
      "Iteration 21472 Loss: 1.1652733087539673\n",
      "Iteration 21473 Loss: 1.2340044975280762\n",
      "Iteration 21474 Loss: 1.1507351398468018\n",
      "Iteration 21475 Loss: 1.23396897315979\n",
      "Iteration 21476 Loss: 0.9850069880485535\n",
      "Iteration 21477 Loss: 1.1295807361602783\n",
      "Iteration 21478 Loss: 1.1645714044570923\n",
      "Iteration 21479 Loss: 1.2575300931930542\n",
      "Iteration 21479 Loss: 1.1661800146102905\n",
      "Iteration 21480 Loss: 0.9873418211936951\n",
      "Iteration 21481 Loss: 1.019813060760498\n",
      "Iteration 21482 Loss: 1.2039976119995117\n",
      "Iteration 21483 Loss: 1.189132571220398\n",
      "Iteration 21484 Loss: 1.2321131229400635\n",
      "Iteration 21485 Loss: 0.9529982805252075\n",
      "Iteration 21486 Loss: 0.7846203446388245\n",
      "Iteration 21487 Loss: 1.4407963752746582\n",
      "Iteration 21488 Loss: 1.3687865734100342\n",
      "Iteration 21489 Loss: 1.1258591413497925\n",
      "Iteration 21489 Loss: 1.1305458545684814\n",
      "Iteration 21490 Loss: 1.1174817085266113\n",
      "Iteration 21491 Loss: 1.4633139371871948\n",
      "Iteration 21492 Loss: 0.8359925150871277\n",
      "Iteration 21493 Loss: 1.4341928958892822\n",
      "Iteration 21494 Loss: 1.1044809818267822\n",
      "Iteration 21495 Loss: 0.9148139953613281\n",
      "Iteration 21496 Loss: 1.2973710298538208\n",
      "Iteration 21497 Loss: 1.1467267274856567\n",
      "Iteration 21498 Loss: 0.9743872284889221\n",
      "Iteration 21499 Loss: 0.7063631415367126\n",
      "Iteration 21499 Loss: 1.0995123386383057\n",
      "Iteration 21500 Loss: 0.8942893743515015\n",
      "Iteration 21501 Loss: 0.944034993648529\n",
      "Iteration 21502 Loss: 0.8496506214141846\n",
      "Iteration 21503 Loss: 1.1782827377319336\n",
      "Iteration 21504 Loss: 1.2651042938232422\n",
      "Iteration 21505 Loss: 1.0944873094558716\n",
      "Iteration 21506 Loss: 1.0303330421447754\n",
      "Iteration 21507 Loss: 0.8526743054389954\n",
      "Iteration 21508 Loss: 0.8652511835098267\n",
      "Iteration 21509 Loss: 1.3071255683898926\n",
      "Iteration 21509 Loss: 1.0281232595443726\n",
      "Iteration 21510 Loss: 1.3835045099258423\n",
      "Iteration 21511 Loss: 0.7042977213859558\n",
      "Iteration 21512 Loss: 0.9883344769477844\n",
      "Iteration 21513 Loss: 1.5160208940505981\n",
      "Iteration 21514 Loss: 1.1135101318359375\n",
      "Iteration 21515 Loss: 1.4042021036148071\n",
      "Iteration 21516 Loss: 1.1149399280548096\n",
      "Iteration 21517 Loss: 1.065360188484192\n",
      "Iteration 21518 Loss: 0.8721947073936462\n",
      "Iteration 21519 Loss: 0.790063738822937\n",
      "Iteration 21519 Loss: 1.0952427387237549\n",
      "Iteration 21520 Loss: 1.1499115228652954\n",
      "Iteration 21521 Loss: 1.1745569705963135\n",
      "Iteration 21522 Loss: 0.807162344455719\n",
      "Iteration 21523 Loss: 0.8594362139701843\n",
      "Iteration 21524 Loss: 0.9497162699699402\n",
      "Iteration 21525 Loss: 1.058093547821045\n",
      "Iteration 21526 Loss: 1.2171461582183838\n",
      "Iteration 21527 Loss: 1.3752034902572632\n",
      "Iteration 21528 Loss: 1.3036209344863892\n",
      "Iteration 21529 Loss: 1.0510447025299072\n",
      "Iteration 21529 Loss: 1.094589114189148\n",
      "Iteration 21530 Loss: 1.247226357460022\n",
      "Iteration 21531 Loss: 1.019544243812561\n",
      "Iteration 21532 Loss: 1.139643669128418\n",
      "Iteration 21533 Loss: 0.5772314667701721\n",
      "Iteration 21534 Loss: 1.0356833934783936\n",
      "Iteration 21535 Loss: 1.0258325338363647\n",
      "Iteration 21536 Loss: 1.3628740310668945\n",
      "Iteration 21537 Loss: 0.9696769118309021\n",
      "Iteration 21538 Loss: 0.9961566925048828\n",
      "Iteration 21539 Loss: 1.2917653322219849\n",
      "Iteration 21539 Loss: 1.066563367843628\n",
      "Iteration 21540 Loss: 0.8133282661437988\n",
      "Iteration 21541 Loss: 1.1078031063079834\n",
      "Iteration 21542 Loss: 0.8749616742134094\n",
      "Iteration 21543 Loss: 0.8874108791351318\n",
      "Iteration 21544 Loss: 1.062783122062683\n",
      "Iteration 21545 Loss: 1.4112101793289185\n",
      "Iteration 21546 Loss: 0.995973527431488\n",
      "Iteration 21547 Loss: 1.265662670135498\n",
      "Iteration 21548 Loss: 1.1449000835418701\n",
      "Iteration 21549 Loss: 0.9487887024879456\n",
      "Iteration 21549 Loss: 1.051282286643982\n",
      "Iteration 21550 Loss: 0.9710665345191956\n",
      "Iteration 21551 Loss: 1.284131407737732\n",
      "Iteration 21552 Loss: 1.0372138023376465\n",
      "Iteration 21553 Loss: 0.910418689250946\n",
      "Iteration 21554 Loss: 1.1388736963272095\n",
      "Iteration 21555 Loss: 1.1804591417312622\n",
      "Iteration 21556 Loss: 1.143440842628479\n",
      "Iteration 21557 Loss: 0.991558849811554\n",
      "Iteration 21558 Loss: 0.7351825833320618\n",
      "Iteration 21559 Loss: 1.33698570728302\n",
      "Iteration 21559 Loss: 1.0729331970214844\n",
      "Iteration 21560 Loss: 1.0190726518630981\n",
      "Iteration 21561 Loss: 1.189565896987915\n",
      "Iteration 21562 Loss: 0.8847238421440125\n",
      "Iteration 21563 Loss: 1.2340415716171265\n",
      "Iteration 21564 Loss: 1.019853949546814\n",
      "Iteration 21565 Loss: 0.7918713092803955\n",
      "Iteration 21566 Loss: 1.3798140287399292\n",
      "Iteration 21567 Loss: 1.0662676095962524\n",
      "Iteration 21568 Loss: 1.0576785802841187\n",
      "Iteration 21569 Loss: 1.084803819656372\n",
      "Iteration 21569 Loss: 1.0727694034576416\n",
      "Iteration 21570 Loss: 1.111092209815979\n",
      "Iteration 21571 Loss: 0.8082056045532227\n",
      "Iteration 21572 Loss: 1.248772144317627\n",
      "Iteration 21573 Loss: 1.1866533756256104\n",
      "Iteration 21574 Loss: 1.1520435810089111\n",
      "Iteration 21575 Loss: 1.1642812490463257\n",
      "Iteration 21576 Loss: 1.0038442611694336\n",
      "Iteration 21577 Loss: 1.1893725395202637\n",
      "Iteration 21578 Loss: 1.2341346740722656\n",
      "Iteration 21579 Loss: 1.0952978134155273\n",
      "Iteration 21579 Loss: 1.1193697452545166\n",
      "Iteration 21580 Loss: 1.003682017326355\n",
      "Iteration 21581 Loss: 0.8622089624404907\n",
      "Iteration 21582 Loss: 1.1457104682922363\n",
      "Iteration 21583 Loss: 1.0328683853149414\n",
      "Iteration 21584 Loss: 1.2751045227050781\n",
      "Iteration 21585 Loss: 1.3260334730148315\n",
      "Iteration 21586 Loss: 0.8506355881690979\n",
      "Iteration 21587 Loss: 1.1834181547164917\n",
      "Iteration 21588 Loss: 1.1871663331985474\n",
      "Iteration 21589 Loss: 1.0292630195617676\n",
      "Iteration 21589 Loss: 1.0896090269088745\n",
      "Iteration 21590 Loss: 1.100014328956604\n",
      "Iteration 21591 Loss: 1.4165990352630615\n",
      "Iteration 21592 Loss: 0.9476296901702881\n",
      "Iteration 21593 Loss: 1.1912490129470825\n",
      "Iteration 21594 Loss: 1.187026023864746\n",
      "Iteration 21595 Loss: 0.7991251945495605\n",
      "Iteration 21596 Loss: 0.9912641048431396\n",
      "Iteration 21597 Loss: 1.5412529706954956\n",
      "Iteration 21598 Loss: 1.0823832750320435\n",
      "Iteration 21599 Loss: 0.8645078539848328\n",
      "Iteration 21599 Loss: 1.1121052503585815\n",
      "Iteration 21600 Loss: 1.2293254137039185\n",
      "Iteration 21601 Loss: 1.1772297620773315\n",
      "Iteration 21602 Loss: 1.294754147529602\n",
      "Iteration 21603 Loss: 1.2312268018722534\n",
      "Iteration 21604 Loss: 0.8557412624359131\n",
      "Iteration 21605 Loss: 1.1453280448913574\n",
      "Iteration 21606 Loss: 0.8559118509292603\n",
      "Iteration 21607 Loss: 1.1024879217147827\n",
      "Iteration 21608 Loss: 1.3082255125045776\n",
      "Iteration 21609 Loss: 1.1812301874160767\n",
      "Iteration 21609 Loss: 1.138146162033081\n",
      "Iteration 21610 Loss: 0.8939601182937622\n",
      "Iteration 21611 Loss: 1.6599640846252441\n",
      "Iteration 21612 Loss: 1.5549968481063843\n",
      "Iteration 21613 Loss: 1.1287431716918945\n",
      "Iteration 21614 Loss: 1.3603800535202026\n",
      "Iteration 21615 Loss: 1.4821929931640625\n",
      "Iteration 21616 Loss: 1.3046643733978271\n",
      "Iteration 21617 Loss: 0.8517969250679016\n",
      "Iteration 21618 Loss: 1.2147047519683838\n",
      "Iteration 21619 Loss: 1.2526224851608276\n",
      "Iteration 21619 Loss: 1.2704026699066162\n",
      "Iteration 21620 Loss: 1.2635667324066162\n",
      "Iteration 21621 Loss: 1.0187373161315918\n",
      "Iteration 21622 Loss: 1.2742640972137451\n",
      "Iteration 21623 Loss: 1.372498631477356\n",
      "Iteration 21624 Loss: 1.0670727491378784\n",
      "Iteration 21625 Loss: 1.2158715724945068\n",
      "Iteration 21626 Loss: 1.351359486579895\n",
      "Iteration 21627 Loss: 1.0277740955352783\n",
      "Iteration 21628 Loss: 1.0725740194320679\n",
      "Iteration 21629 Loss: 1.1740046739578247\n",
      "Iteration 21629 Loss: 1.183772325515747\n",
      "Iteration 21630 Loss: 1.1505299806594849\n",
      "Iteration 21631 Loss: 0.8871175646781921\n",
      "Iteration 21632 Loss: 1.171126365661621\n",
      "Iteration 21633 Loss: 1.0410962104797363\n",
      "Iteration 21634 Loss: 0.9518659114837646\n",
      "Iteration 21635 Loss: 1.0282819271087646\n",
      "Iteration 21636 Loss: 1.4100133180618286\n",
      "Iteration 21637 Loss: 1.116654872894287\n",
      "Iteration 21638 Loss: 1.3692902326583862\n",
      "Iteration 21639 Loss: 1.1316436529159546\n",
      "Iteration 21639 Loss: 1.1257619857788086\n",
      "Iteration 21640 Loss: 0.7787150144577026\n",
      "Iteration 21641 Loss: 0.6647341847419739\n",
      "Iteration 21642 Loss: 0.855439305305481\n",
      "Iteration 21643 Loss: 0.6941026449203491\n",
      "Iteration 21644 Loss: 1.20277738571167\n",
      "Iteration 21645 Loss: 0.7005637884140015\n",
      "Iteration 21646 Loss: 1.3091965913772583\n",
      "Iteration 21647 Loss: 1.2250335216522217\n",
      "Iteration 21648 Loss: 1.1450556516647339\n",
      "Iteration 21649 Loss: 1.159296989440918\n",
      "Iteration 21649 Loss: 0.9734915494918823\n",
      "Iteration 21650 Loss: 1.245328426361084\n",
      "Iteration 21651 Loss: 1.25148344039917\n",
      "Iteration 21652 Loss: 1.0962859392166138\n",
      "Iteration 21653 Loss: 0.9353482723236084\n",
      "Iteration 21654 Loss: 1.4965168237686157\n",
      "Iteration 21655 Loss: 1.1855863332748413\n",
      "Iteration 21656 Loss: 0.7848735451698303\n",
      "Iteration 21657 Loss: 1.0151921510696411\n",
      "Iteration 21658 Loss: 1.261393666267395\n",
      "Iteration 21659 Loss: 0.7214288115501404\n",
      "Iteration 21659 Loss: 1.0993437767028809\n",
      "Iteration 21660 Loss: 1.1811745166778564\n",
      "Iteration 21661 Loss: 1.015541911125183\n",
      "Iteration 21662 Loss: 1.0851106643676758\n",
      "Iteration 21663 Loss: 1.2779834270477295\n",
      "Iteration 21664 Loss: 0.6919132471084595\n",
      "Iteration 21665 Loss: 1.1781222820281982\n",
      "Iteration 21666 Loss: 1.0796873569488525\n",
      "Iteration 21667 Loss: 0.9348897337913513\n",
      "Iteration 21668 Loss: 1.124219298362732\n",
      "Iteration 21669 Loss: 0.9391049742698669\n",
      "Iteration 21669 Loss: 1.0507746934890747\n",
      "Iteration 21670 Loss: 0.9190508127212524\n",
      "Iteration 21671 Loss: 1.3791768550872803\n",
      "Iteration 21672 Loss: 1.5279072523117065\n",
      "Iteration 21673 Loss: 1.0623269081115723\n",
      "Iteration 21674 Loss: 1.1607450246810913\n",
      "Iteration 21675 Loss: 0.9644821286201477\n",
      "Iteration 21676 Loss: 0.998162567615509\n",
      "Iteration 21677 Loss: 1.364154577255249\n",
      "Iteration 21678 Loss: 0.9274770021438599\n",
      "Iteration 21679 Loss: 1.3677797317504883\n",
      "Iteration 21679 Loss: 1.1671262979507446\n",
      "Iteration 21680 Loss: 1.1751874685287476\n",
      "Iteration 21681 Loss: 1.0972214937210083\n",
      "Iteration 21682 Loss: 1.0554931163787842\n",
      "Iteration 21683 Loss: 1.252655267715454\n",
      "Iteration 21684 Loss: 1.0939011573791504\n",
      "Iteration 21685 Loss: 0.8581326007843018\n",
      "Iteration 21686 Loss: 1.0792912244796753\n",
      "Iteration 21687 Loss: 1.0406255722045898\n",
      "Iteration 21688 Loss: 1.1840711832046509\n",
      "Iteration 21689 Loss: 1.0291037559509277\n",
      "Iteration 21689 Loss: 1.0865682363510132\n",
      "Iteration 21690 Loss: 0.7953730821609497\n",
      "Iteration 21691 Loss: 0.9318609237670898\n",
      "Iteration 21692 Loss: 1.2565596103668213\n",
      "Iteration 21693 Loss: 1.0031852722167969\n",
      "Iteration 21694 Loss: 1.231879711151123\n",
      "Iteration 21695 Loss: 0.9570921659469604\n",
      "Iteration 21696 Loss: 1.3186390399932861\n",
      "Iteration 21697 Loss: 0.7567473649978638\n",
      "Iteration 21698 Loss: 1.2634694576263428\n",
      "Iteration 21699 Loss: 1.1863127946853638\n",
      "Iteration 21699 Loss: 1.070111870765686\n",
      "Iteration 21700 Loss: 1.1755084991455078\n",
      "Iteration 21701 Loss: 1.0804001092910767\n",
      "Iteration 21702 Loss: 1.064610242843628\n",
      "Iteration 21703 Loss: 1.5795927047729492\n",
      "Iteration 21704 Loss: 1.213448166847229\n",
      "Iteration 21705 Loss: 1.1062058210372925\n",
      "Iteration 21706 Loss: 1.264000415802002\n",
      "Iteration 21707 Loss: 0.8947319388389587\n",
      "Iteration 21708 Loss: 1.3904005289077759\n",
      "Iteration 21709 Loss: 1.419409155845642\n",
      "Iteration 21709 Loss: 1.2188307046890259\n",
      "Iteration 21710 Loss: 0.9584048390388489\n",
      "Iteration 21711 Loss: 0.8915448784828186\n",
      "Iteration 21712 Loss: 1.3010822534561157\n",
      "Iteration 21713 Loss: 0.9965710639953613\n",
      "Iteration 21714 Loss: 1.1503690481185913\n",
      "Iteration 21715 Loss: 1.3135557174682617\n",
      "Iteration 21716 Loss: 1.0439141988754272\n",
      "Iteration 21717 Loss: 1.1226682662963867\n",
      "Iteration 21718 Loss: 0.9086831212043762\n",
      "Iteration 21719 Loss: 1.2610394954681396\n",
      "Iteration 21719 Loss: 1.0947831869125366\n",
      "Iteration 21720 Loss: 1.2718645334243774\n",
      "Iteration 21721 Loss: 1.0486477613449097\n",
      "Iteration 21722 Loss: 1.3961290121078491\n",
      "Iteration 21723 Loss: 1.054559588432312\n",
      "Iteration 21724 Loss: 1.1919562816619873\n",
      "Iteration 21725 Loss: 1.2441341876983643\n",
      "Iteration 21726 Loss: 1.1193097829818726\n",
      "Iteration 21727 Loss: 1.0823215246200562\n",
      "Iteration 21728 Loss: 1.6052333116531372\n",
      "Iteration 21729 Loss: 1.4684112071990967\n",
      "Iteration 21729 Loss: 1.2482566833496094\n",
      "Iteration 21730 Loss: 0.9026234745979309\n",
      "Iteration 21731 Loss: 1.2060812711715698\n",
      "Iteration 21732 Loss: 1.1411535739898682\n",
      "Iteration 21733 Loss: 1.0779578685760498\n",
      "Iteration 21734 Loss: 0.8854054808616638\n",
      "Iteration 21735 Loss: 1.0517419576644897\n",
      "Iteration 21736 Loss: 1.015040636062622\n",
      "Iteration 21737 Loss: 1.2031550407409668\n",
      "Iteration 21738 Loss: 1.325900912284851\n",
      "Iteration 21739 Loss: 1.221522569656372\n",
      "Iteration 21739 Loss: 1.1030582189559937\n",
      "Iteration 21740 Loss: 1.2812113761901855\n",
      "Iteration 21741 Loss: 1.1137231588363647\n",
      "Iteration 21742 Loss: 0.8898487091064453\n",
      "Iteration 21743 Loss: 1.489127516746521\n",
      "Iteration 21744 Loss: 1.7086565494537354\n",
      "Iteration 21745 Loss: 0.7784704566001892\n",
      "Iteration 21746 Loss: 1.3136435747146606\n",
      "Iteration 21747 Loss: 0.8765841126441956\n",
      "Iteration 21748 Loss: 0.9590865969657898\n",
      "Iteration 21749 Loss: 0.9957723617553711\n",
      "Iteration 21749 Loss: 1.1406123638153076\n",
      "Iteration 21750 Loss: 1.202797532081604\n",
      "Iteration 21751 Loss: 1.1128730773925781\n",
      "Iteration 21752 Loss: 1.2087832689285278\n",
      "Iteration 21753 Loss: 0.9421648979187012\n",
      "Iteration 21754 Loss: 1.2465604543685913\n",
      "Iteration 21755 Loss: 1.0869604349136353\n",
      "Iteration 21756 Loss: 1.092825174331665\n",
      "Iteration 21757 Loss: 0.9169196486473083\n",
      "Iteration 21758 Loss: 0.8497692346572876\n",
      "Iteration 21759 Loss: 1.4775148630142212\n",
      "Iteration 21759 Loss: 1.1137168407440186\n",
      "Iteration 21760 Loss: 0.6867797374725342\n",
      "Iteration 21761 Loss: 0.8404313325881958\n",
      "Iteration 21762 Loss: 1.1366466283798218\n",
      "Iteration 21763 Loss: 1.2194595336914062\n",
      "Iteration 21764 Loss: 1.1044261455535889\n",
      "Iteration 21765 Loss: 1.0083155632019043\n",
      "Iteration 21766 Loss: 1.1519701480865479\n",
      "Iteration 21767 Loss: 1.1665680408477783\n",
      "Iteration 21768 Loss: 0.8038464188575745\n",
      "Iteration 21769 Loss: 0.7585866451263428\n",
      "Iteration 21769 Loss: 0.9877030253410339\n",
      "Iteration 21770 Loss: 0.7732160091400146\n",
      "Iteration 21771 Loss: 1.437757134437561\n",
      "Iteration 21772 Loss: 1.2672621011734009\n",
      "Iteration 21773 Loss: 0.9971380829811096\n",
      "Iteration 21774 Loss: 1.0895648002624512\n",
      "Iteration 21775 Loss: 0.9248156547546387\n",
      "Iteration 21776 Loss: 1.0508356094360352\n",
      "Iteration 21777 Loss: 1.0827287435531616\n",
      "Iteration 21778 Loss: 1.4408634901046753\n",
      "Iteration 21779 Loss: 0.6293432712554932\n",
      "Iteration 21779 Loss: 1.069352388381958\n",
      "Iteration 21780 Loss: 0.9416781663894653\n",
      "Iteration 21781 Loss: 0.684881329536438\n",
      "Iteration 21782 Loss: 0.8487293124198914\n",
      "Iteration 21783 Loss: 1.185813069343567\n",
      "Iteration 21784 Loss: 0.972661018371582\n",
      "Iteration 21785 Loss: 0.9784651398658752\n",
      "Iteration 21786 Loss: 0.9186539649963379\n",
      "Iteration 21787 Loss: 1.573502779006958\n",
      "Iteration 21788 Loss: 1.1966843605041504\n",
      "Iteration 21789 Loss: 0.8747273087501526\n",
      "Iteration 21789 Loss: 1.0175795555114746\n",
      "Iteration 21790 Loss: 1.164126992225647\n",
      "Iteration 21791 Loss: 0.9134518504142761\n",
      "Iteration 21792 Loss: 1.239892840385437\n",
      "Iteration 21793 Loss: 1.598610281944275\n",
      "Iteration 21794 Loss: 0.9812098741531372\n",
      "Iteration 21795 Loss: 1.080824613571167\n",
      "Iteration 21796 Loss: 0.9337608814239502\n",
      "Iteration 21797 Loss: 1.2884092330932617\n",
      "Iteration 21798 Loss: 0.604558527469635\n",
      "Iteration 21799 Loss: 1.1490721702575684\n",
      "Iteration 21799 Loss: 1.0953916311264038\n",
      "Iteration 21800 Loss: 1.3652657270431519\n",
      "Iteration 21801 Loss: 1.4631876945495605\n",
      "Iteration 21802 Loss: 0.7035727500915527\n",
      "Iteration 21803 Loss: 0.9107093214988708\n",
      "Iteration 21804 Loss: 1.439475178718567\n",
      "Iteration 21805 Loss: 0.9907309412956238\n",
      "Iteration 21806 Loss: 1.281906247138977\n",
      "Iteration 21807 Loss: 0.7719278335571289\n",
      "Iteration 21808 Loss: 1.0714527368545532\n",
      "Iteration 21809 Loss: 1.3375827074050903\n",
      "Iteration 21809 Loss: 1.1335811614990234\n",
      "Iteration 21810 Loss: 1.1515295505523682\n",
      "Iteration 21811 Loss: 1.190502643585205\n",
      "Iteration 21812 Loss: 0.7948030233383179\n",
      "Iteration 21813 Loss: 0.9475864171981812\n",
      "Iteration 21814 Loss: 1.2759416103363037\n",
      "Iteration 21815 Loss: 1.0083485841751099\n",
      "Iteration 21816 Loss: 0.8645491003990173\n",
      "Iteration 21817 Loss: 0.8188959956169128\n",
      "Iteration 21818 Loss: 0.8838797807693481\n",
      "Iteration 21819 Loss: 1.3361748456954956\n",
      "Iteration 21819 Loss: 1.0272212028503418\n",
      "Iteration 21820 Loss: 0.8911058306694031\n",
      "Iteration 21821 Loss: 1.3996303081512451\n",
      "Iteration 21822 Loss: 1.290690302848816\n",
      "Iteration 21823 Loss: 1.4193419218063354\n",
      "Iteration 21824 Loss: 1.1485023498535156\n",
      "Iteration 21825 Loss: 1.5217622518539429\n",
      "Iteration 21826 Loss: 1.4013614654541016\n",
      "Iteration 21827 Loss: 1.1067683696746826\n",
      "Iteration 21828 Loss: 1.0076438188552856\n",
      "Iteration 21829 Loss: 1.1635862588882446\n",
      "Iteration 21829 Loss: 1.2350393533706665\n",
      "Iteration 21830 Loss: 1.1545482873916626\n",
      "Iteration 21831 Loss: 1.6064366102218628\n",
      "Iteration 21832 Loss: 0.8043933510780334\n",
      "Iteration 21833 Loss: 0.9900081753730774\n",
      "Iteration 21834 Loss: 1.037726879119873\n",
      "Iteration 21835 Loss: 1.1263161897659302\n",
      "Iteration 21836 Loss: 1.0805513858795166\n",
      "Iteration 21837 Loss: 1.0945725440979004\n",
      "Iteration 21838 Loss: 0.9892295002937317\n",
      "Iteration 21839 Loss: 1.338738203048706\n",
      "Iteration 21839 Loss: 1.122252106666565\n",
      "Iteration 21840 Loss: 1.458603024482727\n",
      "Iteration 21841 Loss: 1.1224545240402222\n",
      "Iteration 21842 Loss: 1.106629490852356\n",
      "Iteration 21843 Loss: 1.4313530921936035\n",
      "Iteration 21844 Loss: 0.7722965478897095\n",
      "Iteration 21845 Loss: 0.8722903728485107\n",
      "Iteration 21846 Loss: 1.1807775497436523\n",
      "Iteration 21847 Loss: 1.3256638050079346\n",
      "Iteration 21848 Loss: 1.1019264459609985\n",
      "Iteration 21849 Loss: 1.4225437641143799\n",
      "Iteration 21849 Loss: 1.1794538497924805\n",
      "Iteration 21850 Loss: 0.8931289315223694\n",
      "Iteration 21851 Loss: 1.0156632661819458\n",
      "Iteration 21852 Loss: 1.057355284690857\n",
      "Iteration 21853 Loss: 1.6573169231414795\n",
      "Iteration 21854 Loss: 0.9924855828285217\n",
      "Iteration 21855 Loss: 1.004148244857788\n",
      "Iteration 21856 Loss: 1.4946861267089844\n",
      "Iteration 21857 Loss: 1.201145052909851\n",
      "Iteration 21858 Loss: 1.168248176574707\n",
      "Iteration 21859 Loss: 1.0991759300231934\n",
      "Iteration 21859 Loss: 1.1583353281021118\n",
      "Iteration 21860 Loss: 1.294634222984314\n",
      "Iteration 21861 Loss: 0.9422878623008728\n",
      "Iteration 21862 Loss: 0.9945081472396851\n",
      "Iteration 21863 Loss: 0.9696173667907715\n",
      "Iteration 21864 Loss: 1.0890281200408936\n",
      "Iteration 21865 Loss: 1.2915977239608765\n",
      "Iteration 21866 Loss: 0.9294244050979614\n",
      "Iteration 21867 Loss: 0.9805245399475098\n",
      "Iteration 21868 Loss: 1.1880292892456055\n",
      "Iteration 21869 Loss: 1.2695257663726807\n",
      "Iteration 21869 Loss: 1.09491765499115\n",
      "Iteration 21870 Loss: 1.1347535848617554\n",
      "Iteration 21871 Loss: 0.9646244049072266\n",
      "Iteration 21872 Loss: 1.1686574220657349\n",
      "Iteration 21873 Loss: 1.074792742729187\n",
      "Iteration 21874 Loss: 0.9506461024284363\n",
      "Iteration 21875 Loss: 1.3566350936889648\n",
      "Iteration 21876 Loss: 1.278992772102356\n",
      "Iteration 21877 Loss: 1.1459903717041016\n",
      "Iteration 21878 Loss: 1.0910712480545044\n",
      "Iteration 21879 Loss: 1.0536472797393799\n",
      "Iteration 21879 Loss: 1.121981143951416\n",
      "Iteration 21880 Loss: 1.3197191953659058\n",
      "Iteration 21881 Loss: 1.1896841526031494\n",
      "Iteration 21882 Loss: 1.2527557611465454\n",
      "Iteration 21883 Loss: 0.8999597430229187\n",
      "Iteration 21884 Loss: 1.1237648725509644\n",
      "Iteration 21885 Loss: 1.270556092262268\n",
      "Iteration 21886 Loss: 0.768098771572113\n",
      "Iteration 21887 Loss: 1.4500155448913574\n",
      "Iteration 21888 Loss: 0.7541502714157104\n",
      "Iteration 21889 Loss: 1.4625979661941528\n",
      "Iteration 21889 Loss: 1.1491302251815796\n",
      "Iteration 21890 Loss: 1.120351791381836\n",
      "Iteration 21891 Loss: 1.2931357622146606\n",
      "Iteration 21892 Loss: 1.3021496534347534\n",
      "Iteration 21893 Loss: 0.9906426668167114\n",
      "Iteration 21894 Loss: 0.8455720543861389\n",
      "Iteration 21895 Loss: 1.1432921886444092\n",
      "Iteration 21896 Loss: 1.0415488481521606\n",
      "Iteration 21897 Loss: 1.0419225692749023\n",
      "Iteration 21898 Loss: 1.015793800354004\n",
      "Iteration 21899 Loss: 0.644908607006073\n",
      "Iteration 21899 Loss: 1.0439317226409912\n",
      "Iteration 21900 Loss: 1.1909425258636475\n",
      "Iteration 21901 Loss: 0.8400974869728088\n",
      "Iteration 21902 Loss: 1.1334538459777832\n",
      "Iteration 21903 Loss: 1.1933165788650513\n",
      "Iteration 21904 Loss: 1.2703156471252441\n",
      "Iteration 21905 Loss: 0.8368213176727295\n",
      "Iteration 21906 Loss: 1.4270144701004028\n",
      "Iteration 21907 Loss: 1.2371339797973633\n",
      "Iteration 21908 Loss: 1.287920594215393\n",
      "Iteration 21909 Loss: 0.955043613910675\n",
      "Iteration 21909 Loss: 1.137205958366394\n",
      "Iteration 21910 Loss: 0.8350821137428284\n",
      "Iteration 21911 Loss: 1.085300087928772\n",
      "Iteration 21912 Loss: 1.3953361511230469\n",
      "Iteration 21913 Loss: 1.0597302913665771\n",
      "Iteration 21914 Loss: 0.9337451457977295\n",
      "Iteration 21915 Loss: 1.3404583930969238\n",
      "Iteration 21916 Loss: 0.8618710041046143\n",
      "Iteration 21917 Loss: 1.0645333528518677\n",
      "Iteration 21918 Loss: 0.908193826675415\n",
      "Iteration 21919 Loss: 0.9588637948036194\n",
      "Iteration 21919 Loss: 1.0443114042282104\n",
      "Iteration 21920 Loss: 1.1601626873016357\n",
      "Iteration 21921 Loss: 1.0297213792800903\n",
      "Iteration 21922 Loss: 0.8569809794425964\n",
      "Iteration 21923 Loss: 1.054768681526184\n",
      "Iteration 21924 Loss: 1.1885982751846313\n",
      "Iteration 21925 Loss: 0.7309662103652954\n",
      "Iteration 21926 Loss: 1.267600417137146\n",
      "Iteration 21927 Loss: 1.5704389810562134\n",
      "Iteration 21928 Loss: 0.94392991065979\n",
      "Iteration 21929 Loss: 1.0759409666061401\n",
      "Iteration 21929 Loss: 1.0879108905792236\n",
      "Iteration 21930 Loss: 1.6080490350723267\n",
      "Iteration 21931 Loss: 1.3075491189956665\n",
      "Iteration 21932 Loss: 1.1325634717941284\n",
      "Iteration 21933 Loss: 0.7529298663139343\n",
      "Iteration 21934 Loss: 1.0328747034072876\n",
      "Iteration 21935 Loss: 1.47329580783844\n",
      "Iteration 21936 Loss: 0.8029680252075195\n",
      "Iteration 21937 Loss: 1.5663963556289673\n",
      "Iteration 21938 Loss: 1.0259902477264404\n",
      "Iteration 21939 Loss: 1.1540652513504028\n",
      "Iteration 21939 Loss: 1.1856682300567627\n",
      "Iteration 21940 Loss: 1.0887395143508911\n",
      "Iteration 21941 Loss: 1.276476502418518\n",
      "Iteration 21942 Loss: 1.215800166130066\n",
      "Iteration 21943 Loss: 1.1816049814224243\n",
      "Iteration 21944 Loss: 1.1181615591049194\n",
      "Iteration 21945 Loss: 1.092132329940796\n",
      "Iteration 21946 Loss: 1.1300262212753296\n",
      "Iteration 21947 Loss: 1.3955578804016113\n",
      "Iteration 21948 Loss: 0.8604736328125\n",
      "Iteration 21949 Loss: 0.996453046798706\n",
      "Iteration 21949 Loss: 1.1355425119400024\n",
      "Iteration 21950 Loss: 1.2338429689407349\n",
      "Iteration 21951 Loss: 1.0074117183685303\n",
      "Iteration 21952 Loss: 1.0603734254837036\n",
      "Iteration 21953 Loss: 1.5071467161178589\n",
      "Iteration 21954 Loss: 1.2390282154083252\n",
      "Iteration 21955 Loss: 1.2234671115875244\n",
      "Iteration 21956 Loss: 0.9543952941894531\n",
      "Iteration 21957 Loss: 1.1074308156967163\n",
      "Iteration 21958 Loss: 1.1855909824371338\n",
      "Iteration 21959 Loss: 0.9551120400428772\n",
      "Iteration 21959 Loss: 1.1473798751831055\n",
      "Iteration 21960 Loss: 0.8139727115631104\n",
      "Iteration 21961 Loss: 1.0758873224258423\n",
      "Iteration 21962 Loss: 1.4554166793823242\n",
      "Iteration 21963 Loss: 0.9754689931869507\n",
      "Iteration 21964 Loss: 1.29897940158844\n",
      "Iteration 21965 Loss: 1.2026318311691284\n",
      "Iteration 21966 Loss: 1.0697622299194336\n",
      "Iteration 21967 Loss: 1.0426489114761353\n",
      "Iteration 21968 Loss: 1.0597257614135742\n",
      "Iteration 21969 Loss: 1.2018852233886719\n",
      "Iteration 21969 Loss: 1.1196379661560059\n",
      "Iteration 21970 Loss: 1.1273388862609863\n",
      "Iteration 21971 Loss: 1.403358817100525\n",
      "Iteration 21972 Loss: 1.2277661561965942\n",
      "Iteration 21973 Loss: 1.2166789770126343\n",
      "Iteration 21974 Loss: 1.1396080255508423\n",
      "Iteration 21975 Loss: 1.6033109426498413\n",
      "Iteration 21976 Loss: 0.7527613639831543\n",
      "Iteration 21977 Loss: 1.3615938425064087\n",
      "Iteration 21978 Loss: 1.0918586254119873\n",
      "Iteration 21979 Loss: 1.1848489046096802\n",
      "Iteration 21979 Loss: 1.2109124660491943\n",
      "Iteration 21980 Loss: 1.3211021423339844\n",
      "Iteration 21981 Loss: 1.073948860168457\n",
      "Iteration 21982 Loss: 1.1835298538208008\n",
      "Iteration 21983 Loss: 0.8688839673995972\n",
      "Iteration 21984 Loss: 0.904038667678833\n",
      "Iteration 21985 Loss: 0.8003464937210083\n",
      "Iteration 21986 Loss: 1.2315263748168945\n",
      "Iteration 21987 Loss: 0.8983246088027954\n",
      "Iteration 21988 Loss: 1.2177070379257202\n",
      "Iteration 21989 Loss: 0.9544652104377747\n",
      "Iteration 21989 Loss: 1.0453873872756958\n",
      "Iteration 21990 Loss: 1.001299500465393\n",
      "Iteration 21991 Loss: 1.089590072631836\n",
      "Iteration 21992 Loss: 0.8426238298416138\n",
      "Iteration 21993 Loss: 1.1387012004852295\n",
      "Iteration 21994 Loss: 0.8604750037193298\n",
      "Iteration 21995 Loss: 1.1680220365524292\n",
      "Iteration 21996 Loss: 0.9147984385490417\n",
      "Iteration 21997 Loss: 1.1568033695220947\n",
      "Iteration 21998 Loss: 0.8201979398727417\n",
      "Iteration 21999 Loss: 1.1310980319976807\n",
      "Iteration 21999 Loss: 1.01236093044281\n",
      "Iteration 22000 Loss: 1.3691619634628296\n",
      "Iteration 22001 Loss: 1.1263210773468018\n",
      "Iteration 22002 Loss: 1.1224493980407715\n",
      "Iteration 22003 Loss: 0.9092866778373718\n",
      "Iteration 22004 Loss: 1.3332347869873047\n",
      "Iteration 22005 Loss: 1.1393344402313232\n",
      "Iteration 22006 Loss: 0.9101874828338623\n",
      "Iteration 22007 Loss: 1.4318063259124756\n",
      "Iteration 22008 Loss: 1.0418329238891602\n",
      "Iteration 22009 Loss: 1.1343051195144653\n",
      "Iteration 22009 Loss: 1.1517921686172485\n",
      "Iteration 22010 Loss: 1.2141408920288086\n",
      "Iteration 22011 Loss: 0.9768013954162598\n",
      "Iteration 22012 Loss: 1.2638951539993286\n",
      "Iteration 22013 Loss: 0.9145063757896423\n",
      "Iteration 22014 Loss: 1.147033452987671\n",
      "Iteration 22015 Loss: 0.9491319060325623\n",
      "Iteration 22016 Loss: 0.9773738384246826\n",
      "Iteration 22017 Loss: 1.4370187520980835\n",
      "Iteration 22018 Loss: 0.9900622963905334\n",
      "Iteration 22019 Loss: 1.1827915906906128\n",
      "Iteration 22019 Loss: 1.1052755117416382\n",
      "Iteration 22020 Loss: 0.9020587205886841\n",
      "Iteration 22021 Loss: 0.7945384979248047\n",
      "Iteration 22022 Loss: 0.6052170991897583\n",
      "Iteration 22023 Loss: 1.3463170528411865\n",
      "Iteration 22024 Loss: 0.9716140031814575\n",
      "Iteration 22025 Loss: 0.9107328057289124\n",
      "Iteration 22026 Loss: 1.0001938343048096\n",
      "Iteration 22027 Loss: 1.3929288387298584\n",
      "Iteration 22028 Loss: 1.3486919403076172\n",
      "Iteration 22029 Loss: 0.8958398699760437\n",
      "Iteration 22029 Loss: 1.0168132781982422\n",
      "Iteration 22030 Loss: 0.9914028644561768\n",
      "Iteration 22031 Loss: 1.197890281677246\n",
      "Iteration 22032 Loss: 1.3082853555679321\n",
      "Iteration 22033 Loss: 1.2829011678695679\n",
      "Iteration 22034 Loss: 0.6974175572395325\n",
      "Iteration 22035 Loss: 0.8321741819381714\n",
      "Iteration 22036 Loss: 1.0438824892044067\n",
      "Iteration 22037 Loss: 1.0329241752624512\n",
      "Iteration 22038 Loss: 1.2763158082962036\n",
      "Iteration 22039 Loss: 0.7570849061012268\n",
      "Iteration 22039 Loss: 1.0420278310775757\n",
      "Iteration 22040 Loss: 0.8862184286117554\n",
      "Iteration 22041 Loss: 1.0656347274780273\n",
      "Iteration 22042 Loss: 1.399660348892212\n",
      "Iteration 22043 Loss: 1.2721418142318726\n",
      "Iteration 22044 Loss: 0.9642242789268494\n",
      "Iteration 22045 Loss: 1.2965471744537354\n",
      "Iteration 22046 Loss: 1.3421072959899902\n",
      "Iteration 22047 Loss: 0.5549485087394714\n",
      "Iteration 22048 Loss: 0.7664703130722046\n",
      "Iteration 22049 Loss: 1.0213044881820679\n",
      "Iteration 22049 Loss: 1.0569257736206055\n",
      "Iteration 22050 Loss: 0.9943729639053345\n",
      "Iteration 22051 Loss: 1.1629340648651123\n",
      "Iteration 22052 Loss: 1.1567522287368774\n",
      "Iteration 22053 Loss: 1.2614192962646484\n",
      "Iteration 22054 Loss: 1.1412760019302368\n",
      "Iteration 22055 Loss: 0.7000898122787476\n",
      "Iteration 22056 Loss: 0.8130850791931152\n",
      "Iteration 22057 Loss: 1.0349180698394775\n",
      "Iteration 22058 Loss: 1.2479137182235718\n",
      "Iteration 22059 Loss: 1.208721399307251\n",
      "Iteration 22059 Loss: 1.072148084640503\n",
      "Iteration 22060 Loss: 0.9309461712837219\n",
      "Iteration 22061 Loss: 1.2417789697647095\n",
      "Iteration 22062 Loss: 1.0730841159820557\n",
      "Iteration 22063 Loss: 1.2418264150619507\n",
      "Iteration 22064 Loss: 1.3313560485839844\n",
      "Iteration 22065 Loss: 0.9748969078063965\n",
      "Iteration 22066 Loss: 0.8857132196426392\n",
      "Iteration 22067 Loss: 0.9323355555534363\n",
      "Iteration 22068 Loss: 1.2827452421188354\n",
      "Iteration 22069 Loss: 0.7104705572128296\n",
      "Iteration 22069 Loss: 1.0605154037475586\n",
      "Iteration 22070 Loss: 1.2002366781234741\n",
      "Iteration 22071 Loss: 0.8700670599937439\n",
      "Iteration 22072 Loss: 0.8610446453094482\n",
      "Iteration 22073 Loss: 0.986488938331604\n",
      "Iteration 22074 Loss: 1.0664300918579102\n",
      "Iteration 22075 Loss: 1.2240006923675537\n",
      "Iteration 22076 Loss: 0.9666606783866882\n",
      "Iteration 22077 Loss: 1.1301162242889404\n",
      "Iteration 22078 Loss: 1.2122200727462769\n",
      "Iteration 22079 Loss: 1.175526738166809\n",
      "Iteration 22079 Loss: 1.0692791938781738\n",
      "Iteration 22080 Loss: 1.493553638458252\n",
      "Iteration 22081 Loss: 1.0748028755187988\n",
      "Iteration 22082 Loss: 0.8614361882209778\n",
      "Iteration 22083 Loss: 1.0615015029907227\n",
      "Iteration 22084 Loss: 0.8467171788215637\n",
      "Iteration 22085 Loss: 1.0520421266555786\n",
      "Iteration 22086 Loss: 1.467267394065857\n",
      "Iteration 22087 Loss: 1.1296641826629639\n",
      "Iteration 22088 Loss: 1.0998218059539795\n",
      "Iteration 22089 Loss: 1.4152545928955078\n",
      "Iteration 22089 Loss: 1.150206208229065\n",
      "Iteration 22090 Loss: 1.07894766330719\n",
      "Iteration 22091 Loss: 1.389247179031372\n",
      "Iteration 22092 Loss: 1.0691522359848022\n",
      "Iteration 22093 Loss: 1.2215687036514282\n",
      "Iteration 22094 Loss: 0.94980388879776\n",
      "Iteration 22095 Loss: 1.2140467166900635\n",
      "Iteration 22096 Loss: 0.7520397901535034\n",
      "Iteration 22097 Loss: 1.0429996252059937\n",
      "Iteration 22098 Loss: 1.2365518808364868\n",
      "Iteration 22099 Loss: 1.073262333869934\n",
      "Iteration 22099 Loss: 1.10276198387146\n",
      "Iteration 22100 Loss: 1.2483348846435547\n",
      "Iteration 22101 Loss: 0.9229353070259094\n",
      "Iteration 22102 Loss: 1.131095290184021\n",
      "Iteration 22103 Loss: 0.9167855978012085\n",
      "Iteration 22104 Loss: 1.0977380275726318\n",
      "Iteration 22105 Loss: 1.1302518844604492\n",
      "Iteration 22106 Loss: 1.197413444519043\n",
      "Iteration 22107 Loss: 0.7582197785377502\n",
      "Iteration 22108 Loss: 0.7251293063163757\n",
      "Iteration 22109 Loss: 1.5256576538085938\n",
      "Iteration 22109 Loss: 1.0653561353683472\n",
      "Iteration 22110 Loss: 1.424170970916748\n",
      "Iteration 22111 Loss: 0.7134078145027161\n",
      "Iteration 22112 Loss: 1.3753466606140137\n",
      "Iteration 22113 Loss: 1.0387954711914062\n",
      "Iteration 22114 Loss: 0.885334849357605\n",
      "Iteration 22115 Loss: 1.1174745559692383\n",
      "Iteration 22116 Loss: 0.6918907761573792\n",
      "Iteration 22117 Loss: 1.2908241748809814\n",
      "Iteration 22118 Loss: 1.4974563121795654\n",
      "Iteration 22119 Loss: 1.3423391580581665\n",
      "Iteration 22119 Loss: 1.1377041339874268\n",
      "Iteration 22120 Loss: 0.7466433048248291\n",
      "Iteration 22121 Loss: 0.5255655646324158\n",
      "Iteration 22122 Loss: 0.9836677312850952\n",
      "Iteration 22123 Loss: 0.8156364560127258\n",
      "Iteration 22124 Loss: 1.2118194103240967\n",
      "Iteration 22125 Loss: 0.7956261038780212\n",
      "Iteration 22126 Loss: 1.2569934129714966\n",
      "Iteration 22127 Loss: 1.2990642786026\n",
      "Iteration 22128 Loss: 1.1133038997650146\n",
      "Iteration 22129 Loss: 1.2854145765304565\n",
      "Iteration 22129 Loss: 1.0033735036849976\n",
      "Iteration 22130 Loss: 1.4206552505493164\n",
      "Iteration 22131 Loss: 0.763485312461853\n",
      "Iteration 22132 Loss: 0.9594849944114685\n",
      "Iteration 22133 Loss: 0.852095901966095\n",
      "Iteration 22134 Loss: 0.9686647057533264\n",
      "Iteration 22135 Loss: 1.5109602212905884\n",
      "Iteration 22136 Loss: 0.8226719498634338\n",
      "Iteration 22137 Loss: 1.0158523321151733\n",
      "Iteration 22138 Loss: 1.1268913745880127\n",
      "Iteration 22139 Loss: 1.0991134643554688\n",
      "Iteration 22139 Loss: 1.0539875030517578\n",
      "Iteration 22140 Loss: 0.8369189500808716\n",
      "Iteration 22141 Loss: 1.1183726787567139\n",
      "Iteration 22142 Loss: 0.9545965194702148\n",
      "Iteration 22143 Loss: 0.8526816964149475\n",
      "Iteration 22144 Loss: 1.223710536956787\n",
      "Iteration 22145 Loss: 1.1553313732147217\n",
      "Iteration 22146 Loss: 1.3378428220748901\n",
      "Iteration 22147 Loss: 1.1220173835754395\n",
      "Iteration 22148 Loss: 1.1332939863204956\n",
      "Iteration 22149 Loss: 1.2605881690979004\n",
      "Iteration 22149 Loss: 1.099535346031189\n",
      "Iteration 22150 Loss: 1.3293997049331665\n",
      "Iteration 22151 Loss: 1.4201983213424683\n",
      "Iteration 22152 Loss: 0.9449486136436462\n",
      "Iteration 22153 Loss: 1.158187747001648\n",
      "Iteration 22154 Loss: 1.198704481124878\n",
      "Iteration 22155 Loss: 0.9360867738723755\n",
      "Iteration 22156 Loss: 1.0037044286727905\n",
      "Iteration 22157 Loss: 1.1513010263442993\n",
      "Iteration 22158 Loss: 1.6444475650787354\n",
      "Iteration 22159 Loss: 0.8483907580375671\n",
      "Iteration 22159 Loss: 1.1635369062423706\n",
      "Iteration 22160 Loss: 0.8487280607223511\n",
      "Iteration 22161 Loss: 1.3445723056793213\n",
      "Iteration 22162 Loss: 1.3478763103485107\n",
      "Iteration 22163 Loss: 1.0544748306274414\n",
      "Iteration 22164 Loss: 1.1259669065475464\n",
      "Iteration 22165 Loss: 1.1077245473861694\n",
      "Iteration 22166 Loss: 0.8781384825706482\n",
      "Iteration 22167 Loss: 0.93740314245224\n",
      "Iteration 22168 Loss: 1.0815538167953491\n",
      "Iteration 22169 Loss: 1.200292944908142\n",
      "Iteration 22169 Loss: 1.0926730632781982\n",
      "Iteration 22170 Loss: 1.4896135330200195\n",
      "Iteration 22171 Loss: 1.0040433406829834\n",
      "Iteration 22172 Loss: 1.0276495218276978\n",
      "Iteration 22173 Loss: 0.9386007189750671\n",
      "Iteration 22174 Loss: 1.2131232023239136\n",
      "Iteration 22175 Loss: 1.3105169534683228\n",
      "Iteration 22176 Loss: 0.9059337377548218\n",
      "Iteration 22177 Loss: 1.205544352531433\n",
      "Iteration 22178 Loss: 0.9802612662315369\n",
      "Iteration 22179 Loss: 1.315998911857605\n",
      "Iteration 22179 Loss: 1.1391284465789795\n",
      "Iteration 22180 Loss: 1.3595319986343384\n",
      "Iteration 22181 Loss: 1.0226283073425293\n",
      "Iteration 22182 Loss: 0.7828332781791687\n",
      "Iteration 22183 Loss: 1.095522403717041\n",
      "Iteration 22184 Loss: 1.1013364791870117\n",
      "Iteration 22185 Loss: 1.2893565893173218\n",
      "Iteration 22186 Loss: 1.3561127185821533\n",
      "Iteration 22187 Loss: 1.1139270067214966\n",
      "Iteration 22188 Loss: 1.1374878883361816\n",
      "Iteration 22189 Loss: 1.371024250984192\n",
      "Iteration 22189 Loss: 1.1629760265350342\n",
      "Iteration 22190 Loss: 1.1893317699432373\n",
      "Iteration 22191 Loss: 1.2085411548614502\n",
      "Iteration 22192 Loss: 0.852336585521698\n",
      "Iteration 22193 Loss: 0.9367998838424683\n",
      "Iteration 22194 Loss: 1.140088438987732\n",
      "Iteration 22195 Loss: 0.9472411274909973\n",
      "Iteration 22196 Loss: 1.3645540475845337\n",
      "Iteration 22197 Loss: 1.1400924921035767\n",
      "Iteration 22198 Loss: 1.044437289237976\n",
      "Iteration 22199 Loss: 0.898441731929779\n",
      "Iteration 22199 Loss: 1.0721864700317383\n",
      "Iteration 22200 Loss: 1.3212295770645142\n",
      "Iteration 22201 Loss: 1.1111047267913818\n",
      "Iteration 22202 Loss: 1.0561063289642334\n",
      "Iteration 22203 Loss: 1.0433133840560913\n",
      "Iteration 22204 Loss: 1.1689066886901855\n",
      "Iteration 22205 Loss: 1.401273488998413\n",
      "Iteration 22206 Loss: 0.7935879230499268\n",
      "Iteration 22207 Loss: 1.32856023311615\n",
      "Iteration 22208 Loss: 0.9027135372161865\n",
      "Iteration 22209 Loss: 1.0180950164794922\n",
      "Iteration 22209 Loss: 1.1144890785217285\n",
      "Iteration 22210 Loss: 1.312909483909607\n",
      "Iteration 22211 Loss: 1.2071079015731812\n",
      "Iteration 22212 Loss: 1.373093605041504\n",
      "Iteration 22213 Loss: 1.091376543045044\n",
      "Iteration 22214 Loss: 1.3293001651763916\n",
      "Iteration 22215 Loss: 1.337155818939209\n",
      "Iteration 22216 Loss: 1.1372514963150024\n",
      "Iteration 22217 Loss: 0.9569766521453857\n",
      "Iteration 22218 Loss: 0.8103718757629395\n",
      "Iteration 22219 Loss: 1.1603423357009888\n",
      "Iteration 22219 Loss: 1.171588659286499\n",
      "Iteration 22220 Loss: 0.7589414119720459\n",
      "Iteration 22221 Loss: 1.0687003135681152\n",
      "Iteration 22222 Loss: 0.8978855013847351\n",
      "Iteration 22223 Loss: 0.9362446069717407\n",
      "Iteration 22224 Loss: 1.0296108722686768\n",
      "Iteration 22225 Loss: 1.5049262046813965\n",
      "Iteration 22226 Loss: 0.9368323683738708\n",
      "Iteration 22227 Loss: 1.4912036657333374\n",
      "Iteration 22228 Loss: 1.2138588428497314\n",
      "Iteration 22229 Loss: 1.3685141801834106\n",
      "Iteration 22229 Loss: 1.1206718683242798\n",
      "Iteration 22230 Loss: 1.0214996337890625\n",
      "Iteration 22231 Loss: 1.080346941947937\n",
      "Iteration 22232 Loss: 1.1888173818588257\n",
      "Iteration 22233 Loss: 1.1066725254058838\n",
      "Iteration 22234 Loss: 1.1984559297561646\n",
      "Iteration 22235 Loss: 1.3210060596466064\n",
      "Iteration 22236 Loss: 1.053276538848877\n",
      "Iteration 22237 Loss: 1.3586736917495728\n",
      "Iteration 22238 Loss: 1.0161782503128052\n",
      "Iteration 22239 Loss: 0.8251941204071045\n",
      "Iteration 22239 Loss: 1.1170121431350708\n",
      "Iteration 22240 Loss: 1.3589012622833252\n",
      "Iteration 22241 Loss: 1.1025738716125488\n",
      "Iteration 22242 Loss: 1.0357294082641602\n",
      "Iteration 22243 Loss: 1.0269529819488525\n",
      "Iteration 22244 Loss: 1.0622230768203735\n",
      "Iteration 22245 Loss: 0.8139473795890808\n",
      "Iteration 22246 Loss: 1.0870312452316284\n",
      "Iteration 22247 Loss: 0.7634121179580688\n",
      "Iteration 22248 Loss: 0.9747299551963806\n",
      "Iteration 22249 Loss: 0.9799371361732483\n",
      "Iteration 22249 Loss: 1.0205438137054443\n",
      "Iteration 22250 Loss: 1.0692371129989624\n",
      "Iteration 22251 Loss: 1.0903486013412476\n",
      "Iteration 22252 Loss: 1.3379290103912354\n",
      "Iteration 22253 Loss: 1.0385979413986206\n",
      "Iteration 22254 Loss: 1.0576674938201904\n",
      "Iteration 22255 Loss: 1.0858955383300781\n",
      "Iteration 22256 Loss: 0.9806727766990662\n",
      "Iteration 22257 Loss: 1.3004274368286133\n",
      "Iteration 22258 Loss: 1.1143474578857422\n",
      "Iteration 22259 Loss: 1.1682075262069702\n",
      "Iteration 22259 Loss: 1.124333143234253\n",
      "Iteration 22260 Loss: 1.2643641233444214\n",
      "Iteration 22261 Loss: 0.8646216988563538\n",
      "Iteration 22262 Loss: 1.3545966148376465\n",
      "Iteration 22263 Loss: 1.1610838174819946\n",
      "Iteration 22264 Loss: 1.3437048196792603\n",
      "Iteration 22265 Loss: 1.084928274154663\n",
      "Iteration 22266 Loss: 1.421010136604309\n",
      "Iteration 22267 Loss: 0.8483911156654358\n",
      "Iteration 22268 Loss: 1.1314858198165894\n",
      "Iteration 22269 Loss: 0.883717954158783\n",
      "Iteration 22269 Loss: 1.135790467262268\n",
      "Iteration 22270 Loss: 0.8850406408309937\n",
      "Iteration 22271 Loss: 0.8642221093177795\n",
      "Iteration 22272 Loss: 1.2482959032058716\n",
      "Iteration 22273 Loss: 1.010382890701294\n",
      "Iteration 22274 Loss: 1.1492475271224976\n",
      "Iteration 22275 Loss: 0.855241596698761\n",
      "Iteration 22276 Loss: 1.0606505870819092\n",
      "Iteration 22277 Loss: 0.8842297792434692\n",
      "Iteration 22278 Loss: 1.4114912748336792\n",
      "Iteration 22279 Loss: 1.0129199028015137\n",
      "Iteration 22279 Loss: 1.0381722450256348\n",
      "Iteration 22280 Loss: 0.818387508392334\n",
      "Iteration 22281 Loss: 1.621573567390442\n",
      "Iteration 22282 Loss: 0.9331824779510498\n",
      "Iteration 22283 Loss: 0.8697609305381775\n",
      "Iteration 22284 Loss: 1.29780912399292\n",
      "Iteration 22285 Loss: 1.1244004964828491\n",
      "Iteration 22286 Loss: 1.1937837600708008\n",
      "Iteration 22287 Loss: 1.351575493812561\n",
      "Iteration 22288 Loss: 1.195303201675415\n",
      "Iteration 22289 Loss: 1.1066346168518066\n",
      "Iteration 22289 Loss: 1.1512410640716553\n",
      "Iteration 22290 Loss: 1.442779302597046\n",
      "Iteration 22291 Loss: 1.1166281700134277\n",
      "Iteration 22292 Loss: 0.99160236120224\n",
      "Iteration 22293 Loss: 0.9364164471626282\n",
      "Iteration 22294 Loss: 1.2704870700836182\n",
      "Iteration 22295 Loss: 1.2088563442230225\n",
      "Iteration 22296 Loss: 0.9670588970184326\n",
      "Iteration 22297 Loss: 0.9316892623901367\n",
      "Iteration 22298 Loss: 1.301845908164978\n",
      "Iteration 22299 Loss: 0.9625959396362305\n",
      "Iteration 22299 Loss: 1.1129961013793945\n",
      "Iteration 22300 Loss: 1.090442419052124\n",
      "Iteration 22301 Loss: 1.1715818643569946\n",
      "Iteration 22302 Loss: 0.9158363342285156\n",
      "Iteration 22303 Loss: 1.0339034795761108\n",
      "Iteration 22304 Loss: 1.1330286264419556\n",
      "Iteration 22305 Loss: 1.5822300910949707\n",
      "Iteration 22306 Loss: 0.9610947966575623\n",
      "Iteration 22307 Loss: 1.0062533617019653\n",
      "Iteration 22308 Loss: 1.2681621313095093\n",
      "Iteration 22309 Loss: 0.7969937920570374\n",
      "Iteration 22309 Loss: 1.0959527492523193\n",
      "Iteration 22310 Loss: 1.2669845819473267\n",
      "Iteration 22311 Loss: 1.1953366994857788\n",
      "Iteration 22312 Loss: 0.9683943390846252\n",
      "Iteration 22313 Loss: 1.3216379880905151\n",
      "Iteration 22314 Loss: 1.0138126611709595\n",
      "Iteration 22315 Loss: 1.0994633436203003\n",
      "Iteration 22316 Loss: 1.0607047080993652\n",
      "Iteration 22317 Loss: 1.2421927452087402\n",
      "Iteration 22318 Loss: 0.9998902082443237\n",
      "Iteration 22319 Loss: 1.3908692598342896\n",
      "Iteration 22319 Loss: 1.1559288501739502\n",
      "Iteration 22320 Loss: 1.0570045709609985\n",
      "Iteration 22321 Loss: 1.466381311416626\n",
      "Iteration 22322 Loss: 0.9958826899528503\n",
      "Iteration 22323 Loss: 1.1042436361312866\n",
      "Iteration 22324 Loss: 1.1376628875732422\n",
      "Iteration 22325 Loss: 1.1357299089431763\n",
      "Iteration 22326 Loss: 1.3445178270339966\n",
      "Iteration 22327 Loss: 1.288987636566162\n",
      "Iteration 22328 Loss: 1.215928316116333\n",
      "Iteration 22329 Loss: 1.2808338403701782\n",
      "Iteration 22329 Loss: 1.2027171850204468\n",
      "Iteration 22330 Loss: 1.0772207975387573\n",
      "Iteration 22331 Loss: 1.121181607246399\n",
      "Iteration 22332 Loss: 1.4642693996429443\n",
      "Iteration 22333 Loss: 0.5399853587150574\n",
      "Iteration 22334 Loss: 1.1977899074554443\n",
      "Iteration 22335 Loss: 1.0504261255264282\n",
      "Iteration 22336 Loss: 0.9772739410400391\n",
      "Iteration 22337 Loss: 1.0964601039886475\n",
      "Iteration 22338 Loss: 0.9557058811187744\n",
      "Iteration 22339 Loss: 1.117564082145691\n",
      "Iteration 22339 Loss: 1.0597877502441406\n",
      "Iteration 22340 Loss: 1.0991435050964355\n",
      "Iteration 22341 Loss: 1.227752923965454\n",
      "Iteration 22342 Loss: 1.5285708904266357\n",
      "Iteration 22343 Loss: 1.1164076328277588\n",
      "Iteration 22344 Loss: 0.8016831278800964\n",
      "Iteration 22345 Loss: 1.3013755083084106\n",
      "Iteration 22346 Loss: 0.9548734426498413\n",
      "Iteration 22347 Loss: 1.0694658756256104\n",
      "Iteration 22348 Loss: 1.1738240718841553\n",
      "Iteration 22349 Loss: 0.8292304873466492\n",
      "Iteration 22349 Loss: 1.1102327108383179\n",
      "Iteration 22350 Loss: 1.066153645515442\n",
      "Iteration 22351 Loss: 0.8347320556640625\n",
      "Iteration 22352 Loss: 1.1337443590164185\n",
      "Iteration 22353 Loss: 1.2140274047851562\n",
      "Iteration 22354 Loss: 1.0812411308288574\n",
      "Iteration 22355 Loss: 1.4580103158950806\n",
      "Iteration 22356 Loss: 0.8006338477134705\n",
      "Iteration 22357 Loss: 1.6022824048995972\n",
      "Iteration 22358 Loss: 1.0915303230285645\n",
      "Iteration 22359 Loss: 1.3916046619415283\n",
      "Iteration 22359 Loss: 1.1673959493637085\n",
      "Iteration 22360 Loss: 0.8216744661331177\n",
      "Iteration 22361 Loss: 0.7038665413856506\n",
      "Iteration 22362 Loss: 1.2944250106811523\n",
      "Iteration 22363 Loss: 1.0716925859451294\n",
      "Iteration 22364 Loss: 1.051712989807129\n",
      "Iteration 22365 Loss: 0.9554799795150757\n",
      "Iteration 22366 Loss: 0.93587726354599\n",
      "Iteration 22367 Loss: 1.650179386138916\n",
      "Iteration 22368 Loss: 1.1271361112594604\n",
      "Iteration 22369 Loss: 1.670572280883789\n",
      "Iteration 22369 Loss: 1.1282615661621094\n",
      "Iteration 22370 Loss: 1.1981291770935059\n",
      "Iteration 22371 Loss: 1.0742759704589844\n",
      "Iteration 22372 Loss: 1.1343114376068115\n",
      "Iteration 22373 Loss: 0.8109790086746216\n",
      "Iteration 22374 Loss: 0.850583016872406\n",
      "Iteration 22375 Loss: 1.1323803663253784\n",
      "Iteration 22376 Loss: 0.9310253858566284\n",
      "Iteration 22377 Loss: 1.202346682548523\n",
      "Iteration 22378 Loss: 1.2076431512832642\n",
      "Iteration 22379 Loss: 0.91737961769104\n",
      "Iteration 22379 Loss: 1.045905351638794\n",
      "Iteration 22380 Loss: 1.3264319896697998\n",
      "Iteration 22381 Loss: 1.2869688272476196\n",
      "Iteration 22382 Loss: 1.2916018962860107\n",
      "Iteration 22383 Loss: 0.5930048823356628\n",
      "Iteration 22384 Loss: 1.0601774454116821\n",
      "Iteration 22385 Loss: 0.938521683216095\n",
      "Iteration 22386 Loss: 1.4015917778015137\n",
      "Iteration 22387 Loss: 1.3309428691864014\n",
      "Iteration 22388 Loss: 1.3391671180725098\n",
      "Iteration 22389 Loss: 1.3051413297653198\n",
      "Iteration 22389 Loss: 1.1873549222946167\n",
      "Iteration 22390 Loss: 1.1692906618118286\n",
      "Iteration 22391 Loss: 0.7711309194564819\n",
      "Iteration 22392 Loss: 0.7727581858634949\n",
      "Iteration 22393 Loss: 1.2827826738357544\n",
      "Iteration 22394 Loss: 1.0987545251846313\n",
      "Iteration 22395 Loss: 0.8896064758300781\n",
      "Iteration 22396 Loss: 1.1080282926559448\n",
      "Iteration 22397 Loss: 0.953660249710083\n",
      "Iteration 22398 Loss: 1.213573694229126\n",
      "Iteration 22399 Loss: 1.1817563772201538\n",
      "Iteration 22399 Loss: 1.0441341400146484\n",
      "Iteration 22400 Loss: 0.945237398147583\n",
      "Iteration 22401 Loss: 0.7113443613052368\n",
      "Iteration 22402 Loss: 1.130021333694458\n",
      "Iteration 22403 Loss: 0.9867302179336548\n",
      "Iteration 22404 Loss: 0.6954880356788635\n",
      "Iteration 22405 Loss: 1.6002808809280396\n",
      "Iteration 22406 Loss: 0.8118893504142761\n",
      "Iteration 22407 Loss: 0.9063529372215271\n",
      "Iteration 22408 Loss: 1.0535458326339722\n",
      "Iteration 22409 Loss: 0.8546732068061829\n",
      "Iteration 22409 Loss: 0.9695563316345215\n",
      "Iteration 22410 Loss: 1.2739685773849487\n",
      "Iteration 22411 Loss: 1.007103443145752\n",
      "Iteration 22412 Loss: 0.8209908604621887\n",
      "Iteration 22413 Loss: 0.9945759177207947\n",
      "Iteration 22414 Loss: 0.9944185614585876\n",
      "Iteration 22415 Loss: 1.081499457359314\n",
      "Iteration 22416 Loss: 1.3084807395935059\n",
      "Iteration 22417 Loss: 0.5679180026054382\n",
      "Iteration 22418 Loss: 1.0188559293746948\n",
      "Iteration 22419 Loss: 1.0611062049865723\n",
      "Iteration 22419 Loss: 1.0128918886184692\n",
      "Iteration 22420 Loss: 0.5082442760467529\n",
      "Iteration 22421 Loss: 1.0104361772537231\n",
      "Iteration 22422 Loss: 0.8562873601913452\n",
      "Iteration 22423 Loss: 1.2187024354934692\n",
      "Iteration 22424 Loss: 1.1036146879196167\n",
      "Iteration 22425 Loss: 1.3193668127059937\n",
      "Iteration 22426 Loss: 0.8938348889350891\n",
      "Iteration 22427 Loss: 1.2295823097229004\n",
      "Iteration 22428 Loss: 0.9252145886421204\n",
      "Iteration 22429 Loss: 1.1996493339538574\n",
      "Iteration 22429 Loss: 1.0264933109283447\n",
      "Iteration 22430 Loss: 1.123583436012268\n",
      "Iteration 22431 Loss: 1.3387531042099\n",
      "Iteration 22432 Loss: 0.935994029045105\n",
      "Iteration 22433 Loss: 1.046653389930725\n",
      "Iteration 22434 Loss: 1.0863239765167236\n",
      "Iteration 22435 Loss: 1.1665130853652954\n",
      "Iteration 22436 Loss: 1.0886874198913574\n",
      "Iteration 22437 Loss: 0.8170694708824158\n",
      "Iteration 22438 Loss: 1.3289854526519775\n",
      "Iteration 22439 Loss: 0.8297542333602905\n",
      "Iteration 22439 Loss: 1.0762317180633545\n",
      "Iteration 22440 Loss: 1.0861859321594238\n",
      "Iteration 22441 Loss: 1.1565377712249756\n",
      "Iteration 22442 Loss: 1.113081455230713\n",
      "Iteration 22443 Loss: 1.2020816802978516\n",
      "Iteration 22444 Loss: 0.8787629008293152\n",
      "Iteration 22445 Loss: 1.1717233657836914\n",
      "Iteration 22446 Loss: 1.007792353630066\n",
      "Iteration 22447 Loss: 0.9614858627319336\n",
      "Iteration 22448 Loss: 1.2660844326019287\n",
      "Iteration 22449 Loss: 1.186003565788269\n",
      "Iteration 22449 Loss: 1.1029739379882812\n",
      "Iteration 22450 Loss: 1.4046050310134888\n",
      "Iteration 22451 Loss: 0.9953146576881409\n",
      "Iteration 22452 Loss: 1.7816025018692017\n",
      "Iteration 22453 Loss: 1.1651426553726196\n",
      "Iteration 22454 Loss: 1.0891221761703491\n",
      "Iteration 22455 Loss: 1.1739369630813599\n",
      "Iteration 22456 Loss: 1.0880980491638184\n",
      "Iteration 22457 Loss: 1.1878547668457031\n",
      "Iteration 22458 Loss: 1.1486198902130127\n",
      "Iteration 22459 Loss: 1.2689292430877686\n",
      "Iteration 22459 Loss: 1.2303224802017212\n",
      "Iteration 22460 Loss: 0.9673466086387634\n",
      "Iteration 22461 Loss: 1.0599459409713745\n",
      "Iteration 22462 Loss: 0.9985305070877075\n",
      "Iteration 22463 Loss: 0.7939205169677734\n",
      "Iteration 22464 Loss: 1.1254929304122925\n",
      "Iteration 22465 Loss: 1.0279629230499268\n",
      "Iteration 22466 Loss: 1.3272303342819214\n",
      "Iteration 22467 Loss: 1.0342196226119995\n",
      "Iteration 22468 Loss: 1.1156150102615356\n",
      "Iteration 22469 Loss: 1.0109726190567017\n",
      "Iteration 22469 Loss: 1.046123743057251\n",
      "Iteration 22470 Loss: 1.1917781829833984\n",
      "Iteration 22471 Loss: 1.564990520477295\n",
      "Iteration 22472 Loss: 0.7431339025497437\n",
      "Iteration 22473 Loss: 1.1210068464279175\n",
      "Iteration 22474 Loss: 0.6594228148460388\n",
      "Iteration 22475 Loss: 1.4271796941757202\n",
      "Iteration 22476 Loss: 0.7930761575698853\n",
      "Iteration 22477 Loss: 1.1154779195785522\n",
      "Iteration 22478 Loss: 1.3185971975326538\n",
      "Iteration 22479 Loss: 1.0940666198730469\n",
      "Iteration 22479 Loss: 1.1028730869293213\n",
      "Iteration 22480 Loss: 0.7120367884635925\n",
      "Iteration 22481 Loss: 1.2017927169799805\n",
      "Iteration 22482 Loss: 1.2546335458755493\n",
      "Iteration 22483 Loss: 0.8435248732566833\n",
      "Iteration 22484 Loss: 0.8596792817115784\n",
      "Iteration 22485 Loss: 0.8665338754653931\n",
      "Iteration 22486 Loss: 1.530526876449585\n",
      "Iteration 22487 Loss: 0.7974951863288879\n",
      "Iteration 22488 Loss: 1.2384467124938965\n",
      "Iteration 22489 Loss: 0.8599901795387268\n",
      "Iteration 22489 Loss: 1.0164659023284912\n",
      "Iteration 22490 Loss: 1.1422088146209717\n",
      "Iteration 22491 Loss: 1.2895795106887817\n",
      "Iteration 22492 Loss: 1.0748116970062256\n",
      "Iteration 22493 Loss: 1.2229623794555664\n",
      "Iteration 22494 Loss: 1.26472008228302\n",
      "Iteration 22495 Loss: 1.1713224649429321\n",
      "Iteration 22496 Loss: 1.025325894355774\n",
      "Iteration 22497 Loss: 1.1570643186569214\n",
      "Iteration 22498 Loss: 1.1015180349349976\n",
      "Iteration 22499 Loss: 1.2619255781173706\n",
      "Iteration 22499 Loss: 1.171143889427185\n",
      "Iteration 22500 Loss: 1.3747650384902954\n",
      "Iteration 22501 Loss: 1.2196290493011475\n",
      "Iteration 22502 Loss: 1.1597074270248413\n",
      "Iteration 22503 Loss: 0.9653119444847107\n",
      "Iteration 22504 Loss: 1.0326725244522095\n",
      "Iteration 22505 Loss: 1.3975534439086914\n",
      "Iteration 22506 Loss: 0.863130509853363\n",
      "Iteration 22507 Loss: 0.9785922765731812\n",
      "Iteration 22508 Loss: 0.7541897892951965\n",
      "Iteration 22509 Loss: 1.424761414527893\n",
      "Iteration 22509 Loss: 1.1170313358306885\n",
      "Iteration 22510 Loss: 1.2035934925079346\n",
      "Iteration 22511 Loss: 1.2180262804031372\n",
      "Iteration 22512 Loss: 1.2442370653152466\n",
      "Iteration 22513 Loss: 0.7694191336631775\n",
      "Iteration 22514 Loss: 1.0508424043655396\n",
      "Iteration 22515 Loss: 1.061508059501648\n",
      "Iteration 22516 Loss: 0.9856613874435425\n",
      "Iteration 22517 Loss: 1.1372408866882324\n",
      "Iteration 22518 Loss: 0.7390432953834534\n",
      "Iteration 22519 Loss: 1.3543747663497925\n",
      "Iteration 22519 Loss: 1.0763946771621704\n",
      "Iteration 22520 Loss: 1.0399397611618042\n",
      "Iteration 22521 Loss: 1.0465800762176514\n",
      "Iteration 22522 Loss: 1.0646002292633057\n",
      "Iteration 22523 Loss: 0.9608963131904602\n",
      "Iteration 22524 Loss: 0.7715874314308167\n",
      "Iteration 22525 Loss: 1.1017709970474243\n",
      "Iteration 22526 Loss: 1.271553635597229\n",
      "Iteration 22527 Loss: 0.8569732308387756\n",
      "Iteration 22528 Loss: 1.0094965696334839\n",
      "Iteration 22529 Loss: 1.0711644887924194\n",
      "Iteration 22529 Loss: 1.019456386566162\n",
      "Iteration 22530 Loss: 1.329084038734436\n",
      "Iteration 22531 Loss: 0.8602074384689331\n",
      "Iteration 22532 Loss: 1.059516429901123\n",
      "Iteration 22533 Loss: 1.4371980428695679\n",
      "Iteration 22534 Loss: 1.1553584337234497\n",
      "Iteration 22535 Loss: 1.2863112688064575\n",
      "Iteration 22536 Loss: 0.8727076649665833\n",
      "Iteration 22537 Loss: 0.9378449320793152\n",
      "Iteration 22538 Loss: 0.6855263113975525\n",
      "Iteration 22539 Loss: 0.8989501595497131\n",
      "Iteration 22539 Loss: 1.052270531654358\n",
      "Iteration 22540 Loss: 1.237493634223938\n",
      "Iteration 22541 Loss: 0.9180848598480225\n",
      "Iteration 22542 Loss: 1.1140409708023071\n",
      "Iteration 22543 Loss: 1.590175986289978\n",
      "Iteration 22544 Loss: 1.0132651329040527\n",
      "Iteration 22545 Loss: 1.3203333616256714\n",
      "Iteration 22546 Loss: 1.1258009672164917\n",
      "Iteration 22547 Loss: 0.9439207911491394\n",
      "Iteration 22548 Loss: 0.8981167674064636\n",
      "Iteration 22549 Loss: 0.8253145217895508\n",
      "Iteration 22549 Loss: 1.0986547470092773\n",
      "Iteration 22550 Loss: 0.9950217008590698\n",
      "Iteration 22551 Loss: 0.9415959119796753\n",
      "Iteration 22552 Loss: 0.9926877617835999\n",
      "Iteration 22553 Loss: 0.9927436113357544\n",
      "Iteration 22554 Loss: 1.3461607694625854\n",
      "Iteration 22555 Loss: 1.0417269468307495\n",
      "Iteration 22556 Loss: 0.760574460029602\n",
      "Iteration 22557 Loss: 1.2276966571807861\n",
      "Iteration 22558 Loss: 1.1552608013153076\n",
      "Iteration 22559 Loss: 1.16257905960083\n",
      "Iteration 22559 Loss: 1.0616047382354736\n",
      "Iteration 22560 Loss: 0.9396328926086426\n",
      "Iteration 22561 Loss: 0.8603493571281433\n",
      "Iteration 22562 Loss: 1.226111888885498\n",
      "Iteration 22563 Loss: 1.0489294528961182\n",
      "Iteration 22564 Loss: 1.2043741941452026\n",
      "Iteration 22565 Loss: 0.693029522895813\n",
      "Iteration 22566 Loss: 0.9191944003105164\n",
      "Iteration 22567 Loss: 1.2363038063049316\n",
      "Iteration 22568 Loss: 1.1787039041519165\n",
      "Iteration 22569 Loss: 1.455474853515625\n",
      "Iteration 22569 Loss: 1.076210379600525\n",
      "Iteration 22570 Loss: 0.9826366901397705\n",
      "Iteration 22571 Loss: 1.3153469562530518\n",
      "Iteration 22572 Loss: 1.2416865825653076\n",
      "Iteration 22573 Loss: 1.5290989875793457\n",
      "Iteration 22574 Loss: 0.9777281284332275\n",
      "Iteration 22575 Loss: 1.184606909751892\n",
      "Iteration 22576 Loss: 1.012426495552063\n",
      "Iteration 22577 Loss: 1.2633850574493408\n",
      "Iteration 22578 Loss: 1.2624973058700562\n",
      "Iteration 22579 Loss: 1.3308508396148682\n",
      "Iteration 22579 Loss: 1.2100263833999634\n",
      "Iteration 22580 Loss: 1.1545817852020264\n",
      "Iteration 22581 Loss: 1.196367859840393\n",
      "Iteration 22582 Loss: 1.2978882789611816\n",
      "Iteration 22583 Loss: 1.174069881439209\n",
      "Iteration 22584 Loss: 1.149969458580017\n",
      "Iteration 22585 Loss: 1.3617472648620605\n",
      "Iteration 22586 Loss: 1.1653820276260376\n",
      "Iteration 22587 Loss: 0.9273157119750977\n",
      "Iteration 22588 Loss: 0.7646536231040955\n",
      "Iteration 22589 Loss: 0.9670456647872925\n",
      "Iteration 22589 Loss: 1.1159021854400635\n",
      "Iteration 22590 Loss: 1.3075222969055176\n",
      "Iteration 22591 Loss: 1.2163127660751343\n",
      "Iteration 22592 Loss: 0.686602771282196\n",
      "Iteration 22593 Loss: 1.144600749015808\n",
      "Iteration 22594 Loss: 0.8882647156715393\n",
      "Iteration 22595 Loss: 0.8467046022415161\n",
      "Iteration 22596 Loss: 1.4306162595748901\n",
      "Iteration 22597 Loss: 1.1870783567428589\n",
      "Iteration 22598 Loss: 0.9788143038749695\n",
      "Iteration 22599 Loss: 1.0154495239257812\n",
      "Iteration 22599 Loss: 1.0701966285705566\n",
      "Iteration 22600 Loss: 1.123926043510437\n",
      "Iteration 22601 Loss: 1.1651782989501953\n",
      "Iteration 22602 Loss: 1.1970794200897217\n",
      "Iteration 22603 Loss: 0.8694157004356384\n",
      "Iteration 22604 Loss: 0.841465950012207\n",
      "Iteration 22605 Loss: 1.2583330869674683\n",
      "Iteration 22606 Loss: 1.6569589376449585\n",
      "Iteration 22607 Loss: 1.0611380338668823\n",
      "Iteration 22608 Loss: 1.106399416923523\n",
      "Iteration 22609 Loss: 1.168748378753662\n",
      "Iteration 22609 Loss: 1.1448643207550049\n",
      "Iteration 22610 Loss: 1.4261136054992676\n",
      "Iteration 22611 Loss: 0.9141009449958801\n",
      "Iteration 22612 Loss: 1.3211764097213745\n",
      "Iteration 22613 Loss: 0.9356706738471985\n",
      "Iteration 22614 Loss: 1.1499582529067993\n",
      "Iteration 22615 Loss: 1.12337064743042\n",
      "Iteration 22616 Loss: 1.0513043403625488\n",
      "Iteration 22617 Loss: 1.4225138425827026\n",
      "Iteration 22618 Loss: 1.3088479042053223\n",
      "Iteration 22619 Loss: 1.0210552215576172\n",
      "Iteration 22619 Loss: 1.167411208152771\n",
      "Iteration 22620 Loss: 0.948211669921875\n",
      "Iteration 22621 Loss: 1.3308587074279785\n",
      "Iteration 22622 Loss: 1.3347455263137817\n",
      "Iteration 22623 Loss: 1.1723459959030151\n",
      "Iteration 22624 Loss: 0.8850418329238892\n",
      "Iteration 22625 Loss: 1.3840060234069824\n",
      "Iteration 22626 Loss: 1.4223906993865967\n",
      "Iteration 22627 Loss: 1.0895488262176514\n",
      "Iteration 22628 Loss: 1.2074660062789917\n",
      "Iteration 22629 Loss: 1.1081544160842896\n",
      "Iteration 22629 Loss: 1.188277006149292\n",
      "Iteration 22630 Loss: 1.2225335836410522\n",
      "Iteration 22631 Loss: 0.9492347240447998\n",
      "Iteration 22632 Loss: 1.3150498867034912\n",
      "Iteration 22633 Loss: 1.0760984420776367\n",
      "Iteration 22634 Loss: 1.1181483268737793\n",
      "Iteration 22635 Loss: 0.7824828028678894\n",
      "Iteration 22636 Loss: 1.192384958267212\n",
      "Iteration 22637 Loss: 1.020951271057129\n",
      "Iteration 22638 Loss: 0.8927150964736938\n",
      "Iteration 22639 Loss: 1.118119478225708\n",
      "Iteration 22639 Loss: 1.0687718391418457\n",
      "Iteration 22640 Loss: 0.9363297820091248\n",
      "Iteration 22641 Loss: 1.037855625152588\n",
      "Iteration 22642 Loss: 1.0149775743484497\n",
      "Iteration 22643 Loss: 0.9761574268341064\n",
      "Iteration 22644 Loss: 1.2818102836608887\n",
      "Iteration 22645 Loss: 1.4481209516525269\n",
      "Iteration 22646 Loss: 1.2468323707580566\n",
      "Iteration 22647 Loss: 0.8107056617736816\n",
      "Iteration 22648 Loss: 0.6564883589744568\n",
      "Iteration 22649 Loss: 1.2645635604858398\n",
      "Iteration 22649 Loss: 1.067384123802185\n",
      "Iteration 22650 Loss: 0.9099881052970886\n",
      "Iteration 22651 Loss: 1.0065102577209473\n",
      "Iteration 22652 Loss: 0.8807210326194763\n",
      "Iteration 22653 Loss: 0.8448597192764282\n",
      "Iteration 22654 Loss: 1.3081271648406982\n",
      "Iteration 22655 Loss: 0.963630735874176\n",
      "Iteration 22656 Loss: 0.8641572594642639\n",
      "Iteration 22657 Loss: 0.8569934964179993\n",
      "Iteration 22658 Loss: 1.1719343662261963\n",
      "Iteration 22659 Loss: 1.277980923652649\n",
      "Iteration 22659 Loss: 1.0084903240203857\n",
      "Iteration 22660 Loss: 1.3567003011703491\n",
      "Iteration 22661 Loss: 1.1579210758209229\n",
      "Iteration 22662 Loss: 1.1965781450271606\n",
      "Iteration 22663 Loss: 1.1650328636169434\n",
      "Iteration 22664 Loss: 1.1324504613876343\n",
      "Iteration 22665 Loss: 1.1488358974456787\n",
      "Iteration 22666 Loss: 1.2296066284179688\n",
      "Iteration 22667 Loss: 1.3930375576019287\n",
      "Iteration 22668 Loss: 0.7863029837608337\n",
      "Iteration 22669 Loss: 0.766750693321228\n",
      "Iteration 22669 Loss: 1.1333216428756714\n",
      "Iteration 22670 Loss: 0.9170455932617188\n",
      "Iteration 22671 Loss: 1.175337314605713\n",
      "Iteration 22672 Loss: 1.0665236711502075\n",
      "Iteration 22673 Loss: 1.4811303615570068\n",
      "Iteration 22674 Loss: 1.275120735168457\n",
      "Iteration 22675 Loss: 1.2620031833648682\n",
      "Iteration 22676 Loss: 1.3388772010803223\n",
      "Iteration 22677 Loss: 1.1529099941253662\n",
      "Iteration 22678 Loss: 1.6425319910049438\n",
      "Iteration 22679 Loss: 0.8963828682899475\n",
      "Iteration 22679 Loss: 1.2207863330841064\n",
      "Iteration 22680 Loss: 1.0328086614608765\n",
      "Iteration 22681 Loss: 1.380543828010559\n",
      "Iteration 22682 Loss: 0.9038202166557312\n",
      "Iteration 22683 Loss: 1.3640395402908325\n",
      "Iteration 22684 Loss: 1.150776743888855\n",
      "Iteration 22685 Loss: 0.9061035513877869\n",
      "Iteration 22686 Loss: 0.6592888832092285\n",
      "Iteration 22687 Loss: 1.0512787103652954\n",
      "Iteration 22688 Loss: 0.8145391345024109\n",
      "Iteration 22689 Loss: 1.083911418914795\n",
      "Iteration 22689 Loss: 1.0347111225128174\n",
      "Iteration 22690 Loss: 0.9820677042007446\n",
      "Iteration 22691 Loss: 0.9909064769744873\n",
      "Iteration 22692 Loss: 0.9863242506980896\n",
      "Iteration 22693 Loss: 1.0484915971755981\n",
      "Iteration 22694 Loss: 1.1327852010726929\n",
      "Iteration 22695 Loss: 1.0073554515838623\n",
      "Iteration 22696 Loss: 1.046123743057251\n",
      "Iteration 22697 Loss: 0.9824049472808838\n",
      "Iteration 22698 Loss: 1.5033252239227295\n",
      "Iteration 22699 Loss: 0.9322525858879089\n",
      "Iteration 22699 Loss: 1.0612037181854248\n",
      "Iteration 22700 Loss: 1.3054078817367554\n",
      "Iteration 22701 Loss: 0.9284420013427734\n",
      "Iteration 22702 Loss: 1.364188551902771\n",
      "Iteration 22703 Loss: 1.0298913717269897\n",
      "Iteration 22704 Loss: 0.8124265074729919\n",
      "Iteration 22705 Loss: 1.2044464349746704\n",
      "Iteration 22706 Loss: 0.7641758322715759\n",
      "Iteration 22707 Loss: 0.9398584365844727\n",
      "Iteration 22708 Loss: 1.2100290060043335\n",
      "Iteration 22709 Loss: 0.8662245273590088\n",
      "Iteration 22709 Loss: 1.0425090789794922\n",
      "Iteration 22710 Loss: 0.6494297981262207\n",
      "Iteration 22711 Loss: 1.0890406370162964\n",
      "Iteration 22712 Loss: 1.2235488891601562\n",
      "Iteration 22713 Loss: 1.2425427436828613\n",
      "Iteration 22714 Loss: 0.8233442902565002\n",
      "Iteration 22715 Loss: 1.0795539617538452\n",
      "Iteration 22716 Loss: 1.013343334197998\n",
      "Iteration 22717 Loss: 1.3192133903503418\n",
      "Iteration 22718 Loss: 1.3092033863067627\n",
      "Iteration 22719 Loss: 1.175776481628418\n",
      "Iteration 22719 Loss: 1.0924997329711914\n",
      "Iteration 22720 Loss: 1.0009161233901978\n",
      "Iteration 22721 Loss: 1.3727834224700928\n",
      "Iteration 22722 Loss: 1.262063980102539\n",
      "Iteration 22723 Loss: 1.4360864162445068\n",
      "Iteration 22724 Loss: 1.2370818853378296\n",
      "Iteration 22725 Loss: 1.2911393642425537\n",
      "Iteration 22726 Loss: 1.470369815826416\n",
      "Iteration 22727 Loss: 0.9001845121383667\n",
      "Iteration 22728 Loss: 1.0226588249206543\n",
      "Iteration 22729 Loss: 0.7422184944152832\n",
      "Iteration 22729 Loss: 1.1735502481460571\n",
      "Iteration 22730 Loss: 1.01439368724823\n",
      "Iteration 22731 Loss: 1.356404423713684\n",
      "Iteration 22732 Loss: 0.9948006272315979\n",
      "Iteration 22733 Loss: 1.1870927810668945\n",
      "Iteration 22734 Loss: 1.090075135231018\n",
      "Iteration 22735 Loss: 1.3472360372543335\n",
      "Iteration 22736 Loss: 0.8285844326019287\n",
      "Iteration 22737 Loss: 0.9831533432006836\n",
      "Iteration 22738 Loss: 1.167372465133667\n",
      "Iteration 22739 Loss: 0.8153378963470459\n",
      "Iteration 22739 Loss: 1.0784450769424438\n",
      "Iteration 22740 Loss: 1.2991091012954712\n",
      "Iteration 22741 Loss: 1.5438106060028076\n",
      "Iteration 22742 Loss: 1.0454130172729492\n",
      "Iteration 22743 Loss: 0.9144413471221924\n",
      "Iteration 22744 Loss: 1.5224617719650269\n",
      "Iteration 22745 Loss: 1.1799191236495972\n",
      "Iteration 22746 Loss: 1.2033088207244873\n",
      "Iteration 22747 Loss: 0.9050643444061279\n",
      "Iteration 22748 Loss: 1.288400411605835\n",
      "Iteration 22749 Loss: 0.7866079211235046\n",
      "Iteration 22749 Loss: 1.1688536405563354\n",
      "Iteration 22750 Loss: 1.1493945121765137\n",
      "Iteration 22751 Loss: 0.9555233716964722\n",
      "Iteration 22752 Loss: 1.0534824132919312\n",
      "Iteration 22753 Loss: 1.0844531059265137\n",
      "Iteration 22754 Loss: 0.8522903323173523\n",
      "Iteration 22755 Loss: 0.9918457269668579\n",
      "Iteration 22756 Loss: 1.3361533880233765\n",
      "Iteration 22757 Loss: 1.4564508199691772\n",
      "Iteration 22758 Loss: 0.8464739322662354\n",
      "Iteration 22759 Loss: 1.0243573188781738\n",
      "Iteration 22759 Loss: 1.075042486190796\n",
      "Iteration 22760 Loss: 1.0956984758377075\n",
      "Iteration 22761 Loss: 1.4083271026611328\n",
      "Iteration 22762 Loss: 1.4520542621612549\n",
      "Iteration 22763 Loss: 1.1612238883972168\n",
      "Iteration 22764 Loss: 0.966953694820404\n",
      "Iteration 22765 Loss: 0.8827559947967529\n",
      "Iteration 22766 Loss: 0.8809190988540649\n",
      "Iteration 22767 Loss: 1.4416297674179077\n",
      "Iteration 22768 Loss: 1.3470814228057861\n",
      "Iteration 22769 Loss: 0.8907593488693237\n",
      "Iteration 22769 Loss: 1.152740240097046\n",
      "Iteration 22770 Loss: 1.1271270513534546\n",
      "Iteration 22771 Loss: 1.0862751007080078\n",
      "Iteration 22772 Loss: 1.401611566543579\n",
      "Iteration 22773 Loss: 0.8150916695594788\n",
      "Iteration 22774 Loss: 1.3224678039550781\n",
      "Iteration 22775 Loss: 1.1476938724517822\n",
      "Iteration 22776 Loss: 1.300577998161316\n",
      "Iteration 22777 Loss: 1.2469401359558105\n",
      "Iteration 22778 Loss: 0.7719573378562927\n",
      "Iteration 22779 Loss: 1.2330445051193237\n",
      "Iteration 22779 Loss: 1.1452786922454834\n",
      "Iteration 22780 Loss: 1.0522164106369019\n",
      "Iteration 22781 Loss: 1.1311520338058472\n",
      "Iteration 22782 Loss: 1.0926518440246582\n",
      "Iteration 22783 Loss: 1.1623013019561768\n",
      "Iteration 22784 Loss: 0.806126594543457\n",
      "Iteration 22785 Loss: 1.0725922584533691\n",
      "Iteration 22786 Loss: 0.8075042366981506\n",
      "Iteration 22787 Loss: 1.1726469993591309\n",
      "Iteration 22788 Loss: 1.2077027559280396\n",
      "Iteration 22789 Loss: 1.2084852457046509\n",
      "Iteration 22789 Loss: 1.0713380575180054\n",
      "Iteration 22790 Loss: 0.9832732677459717\n",
      "Iteration 22791 Loss: 1.364527702331543\n",
      "Iteration 22792 Loss: 1.0846699476242065\n",
      "Iteration 22793 Loss: 1.3713948726654053\n",
      "Iteration 22794 Loss: 1.0720372200012207\n",
      "Iteration 22795 Loss: 0.6306235790252686\n",
      "Iteration 22796 Loss: 1.2116270065307617\n",
      "Iteration 22797 Loss: 1.0059138536453247\n",
      "Iteration 22798 Loss: 1.0072851181030273\n",
      "Iteration 22799 Loss: 1.1347419023513794\n",
      "Iteration 22799 Loss: 1.0866096019744873\n",
      "Iteration 22800 Loss: 0.9011745452880859\n",
      "Iteration 22801 Loss: 1.0114068984985352\n",
      "Iteration 22802 Loss: 1.2810957431793213\n",
      "Iteration 22803 Loss: 1.0170284509658813\n",
      "Iteration 22804 Loss: 1.4083008766174316\n",
      "Iteration 22805 Loss: 0.958297848701477\n",
      "Iteration 22806 Loss: 1.3193002939224243\n",
      "Iteration 22807 Loss: 1.3079031705856323\n",
      "Iteration 22808 Loss: 1.0491106510162354\n",
      "Iteration 22809 Loss: 1.1008998155593872\n",
      "Iteration 22809 Loss: 1.1354519128799438\n",
      "Iteration 22810 Loss: 1.3644057512283325\n",
      "Iteration 22811 Loss: 1.2820993661880493\n",
      "Iteration 22812 Loss: 1.7102205753326416\n",
      "Iteration 22813 Loss: 1.3154696226119995\n",
      "Iteration 22814 Loss: 1.0592517852783203\n",
      "Iteration 22815 Loss: 1.297662377357483\n",
      "Iteration 22816 Loss: 1.1778411865234375\n",
      "Iteration 22817 Loss: 1.3812298774719238\n",
      "Iteration 22818 Loss: 0.9508814215660095\n",
      "Iteration 22819 Loss: 1.1791595220565796\n",
      "Iteration 22819 Loss: 1.271822214126587\n",
      "Iteration 22820 Loss: 1.2430387735366821\n",
      "Iteration 22821 Loss: 0.9575920701026917\n",
      "Iteration 22822 Loss: 1.2937124967575073\n",
      "Iteration 22823 Loss: 1.328456163406372\n",
      "Iteration 22824 Loss: 1.0226413011550903\n",
      "Iteration 22825 Loss: 1.2134283781051636\n",
      "Iteration 22826 Loss: 1.2480589151382446\n",
      "Iteration 22827 Loss: 0.887938380241394\n",
      "Iteration 22828 Loss: 1.231573462486267\n",
      "Iteration 22829 Loss: 0.7834528684616089\n",
      "Iteration 22829 Loss: 1.1209893226623535\n",
      "Iteration 22830 Loss: 1.2017472982406616\n",
      "Iteration 22831 Loss: 1.2508995532989502\n",
      "Iteration 22832 Loss: 1.521490454673767\n",
      "Iteration 22833 Loss: 1.1125268936157227\n",
      "Iteration 22834 Loss: 0.9118906855583191\n",
      "Iteration 22835 Loss: 1.0747449398040771\n",
      "Iteration 22836 Loss: 1.0146151781082153\n",
      "Iteration 22837 Loss: 1.3266918659210205\n",
      "Iteration 22838 Loss: 1.2388324737548828\n",
      "Iteration 22839 Loss: 1.016994833946228\n",
      "Iteration 22839 Loss: 1.1670434474945068\n",
      "Iteration 22840 Loss: 1.0066020488739014\n",
      "Iteration 22841 Loss: 1.2036374807357788\n",
      "Iteration 22842 Loss: 1.3541977405548096\n",
      "Iteration 22843 Loss: 1.195481300354004\n",
      "Iteration 22844 Loss: 1.1361687183380127\n",
      "Iteration 22845 Loss: 1.1838895082473755\n",
      "Iteration 22846 Loss: 1.0276566743850708\n",
      "Iteration 22847 Loss: 1.0849792957305908\n",
      "Iteration 22848 Loss: 0.9517053365707397\n",
      "Iteration 22849 Loss: 1.0274605751037598\n",
      "Iteration 22849 Loss: 1.1171777248382568\n",
      "Iteration 22850 Loss: 1.1258158683776855\n",
      "Iteration 22851 Loss: 1.1166224479675293\n",
      "Iteration 22852 Loss: 0.9928151369094849\n",
      "Iteration 22853 Loss: 0.9333440661430359\n",
      "Iteration 22854 Loss: 1.1907135248184204\n",
      "Iteration 22855 Loss: 0.9815657734870911\n",
      "Iteration 22856 Loss: 0.911874532699585\n",
      "Iteration 22857 Loss: 1.2038931846618652\n",
      "Iteration 22858 Loss: 1.0371198654174805\n",
      "Iteration 22859 Loss: 1.1054301261901855\n",
      "Iteration 22859 Loss: 1.0599193572998047\n",
      "Iteration 22860 Loss: 1.0411568880081177\n",
      "Iteration 22861 Loss: 0.9766411185264587\n",
      "Iteration 22862 Loss: 1.2600771188735962\n",
      "Iteration 22863 Loss: 0.94891756772995\n",
      "Iteration 22864 Loss: 0.8549346327781677\n",
      "Iteration 22865 Loss: 1.1750569343566895\n",
      "Iteration 22866 Loss: 0.9163253307342529\n",
      "Iteration 22867 Loss: 1.0400985479354858\n",
      "Iteration 22868 Loss: 0.9601054191589355\n",
      "Iteration 22869 Loss: 1.0616565942764282\n",
      "Iteration 22869 Loss: 1.0234968662261963\n",
      "Iteration 22870 Loss: 1.0449843406677246\n",
      "Iteration 22871 Loss: 0.6635832786560059\n",
      "Iteration 22872 Loss: 1.2052356004714966\n",
      "Iteration 22873 Loss: 0.8389097452163696\n",
      "Iteration 22874 Loss: 0.6253523230552673\n",
      "Iteration 22875 Loss: 1.196366310119629\n",
      "Iteration 22876 Loss: 0.6893938779830933\n",
      "Iteration 22877 Loss: 1.338808298110962\n",
      "Iteration 22878 Loss: 1.091475248336792\n",
      "Iteration 22879 Loss: 0.9076544046401978\n",
      "Iteration 22879 Loss: 0.9601762890815735\n",
      "Iteration 22880 Loss: 0.8780775666236877\n",
      "Iteration 22881 Loss: 1.1774204969406128\n",
      "Iteration 22882 Loss: 1.2095516920089722\n",
      "Iteration 22883 Loss: 1.502423644065857\n",
      "Iteration 22884 Loss: 1.110865831375122\n",
      "Iteration 22885 Loss: 0.7430981993675232\n",
      "Iteration 22886 Loss: 1.015257716178894\n",
      "Iteration 22887 Loss: 0.8921985030174255\n",
      "Iteration 22888 Loss: 1.4851100444793701\n",
      "Iteration 22889 Loss: 1.0182313919067383\n",
      "Iteration 22889 Loss: 1.1032235622406006\n",
      "Iteration 22890 Loss: 1.0364246368408203\n",
      "Iteration 22891 Loss: 0.871558427810669\n",
      "Iteration 22892 Loss: 0.8417802453041077\n",
      "Iteration 22893 Loss: 1.4153289794921875\n",
      "Iteration 22894 Loss: 1.192381501197815\n",
      "Iteration 22895 Loss: 1.3872861862182617\n",
      "Iteration 22896 Loss: 0.8091204166412354\n",
      "Iteration 22897 Loss: 0.9420340657234192\n",
      "Iteration 22898 Loss: 1.0977725982666016\n",
      "Iteration 22899 Loss: 1.1195248365402222\n",
      "Iteration 22899 Loss: 1.0713211297988892\n",
      "Iteration 22900 Loss: 0.9460955262184143\n",
      "Iteration 22901 Loss: 1.024907112121582\n",
      "Iteration 22902 Loss: 1.240825891494751\n",
      "Iteration 22903 Loss: 0.7236459851264954\n",
      "Iteration 22904 Loss: 1.160829782485962\n",
      "Iteration 22905 Loss: 1.0626338720321655\n",
      "Iteration 22906 Loss: 1.1256818771362305\n",
      "Iteration 22907 Loss: 1.3487268686294556\n",
      "Iteration 22908 Loss: 1.2261219024658203\n",
      "Iteration 22909 Loss: 1.0292028188705444\n",
      "Iteration 22909 Loss: 1.0888671875\n",
      "Iteration 22910 Loss: 1.111493706703186\n",
      "Iteration 22911 Loss: 1.733327031135559\n",
      "Iteration 22912 Loss: 1.1271367073059082\n",
      "Iteration 22913 Loss: 1.0284801721572876\n",
      "Iteration 22914 Loss: 0.968585193157196\n",
      "Iteration 22915 Loss: 1.0540738105773926\n",
      "Iteration 22916 Loss: 0.8945017457008362\n",
      "Iteration 22917 Loss: 1.2520785331726074\n",
      "Iteration 22918 Loss: 1.6220109462738037\n",
      "Iteration 22919 Loss: 1.3644509315490723\n",
      "Iteration 22919 Loss: 1.2156137228012085\n",
      "Iteration 22920 Loss: 0.8767750263214111\n",
      "Iteration 22921 Loss: 1.0023140907287598\n",
      "Iteration 22922 Loss: 1.0244535207748413\n",
      "Iteration 22923 Loss: 0.8273618817329407\n",
      "Iteration 22924 Loss: 1.0147069692611694\n",
      "Iteration 22925 Loss: 1.0728834867477417\n",
      "Iteration 22926 Loss: 1.1742491722106934\n",
      "Iteration 22927 Loss: 1.1518772840499878\n",
      "Iteration 22928 Loss: 1.1478720903396606\n",
      "Iteration 22929 Loss: 0.8812694549560547\n",
      "Iteration 22929 Loss: 1.0173763036727905\n",
      "Iteration 22930 Loss: 0.845412015914917\n",
      "Iteration 22931 Loss: 0.808246374130249\n",
      "Iteration 22932 Loss: 1.1485376358032227\n",
      "Iteration 22933 Loss: 1.150485634803772\n",
      "Iteration 22934 Loss: 0.6840583086013794\n",
      "Iteration 22935 Loss: 1.7619174718856812\n",
      "Iteration 22936 Loss: 1.2807362079620361\n",
      "Iteration 22937 Loss: 1.0294431447982788\n",
      "Iteration 22938 Loss: 1.0415517091751099\n",
      "Iteration 22939 Loss: 1.5267744064331055\n",
      "Iteration 22939 Loss: 1.1277161836624146\n",
      "Iteration 22940 Loss: 1.2680705785751343\n",
      "Iteration 22941 Loss: 1.4905472993850708\n",
      "Iteration 22942 Loss: 1.0774877071380615\n",
      "Iteration 22943 Loss: 0.8419848680496216\n",
      "Iteration 22944 Loss: 1.3401130437850952\n",
      "Iteration 22945 Loss: 1.1495964527130127\n",
      "Iteration 22946 Loss: 1.2080044746398926\n",
      "Iteration 22947 Loss: 1.1968655586242676\n",
      "Iteration 22948 Loss: 0.8488027453422546\n",
      "Iteration 22949 Loss: 0.9605910778045654\n",
      "Iteration 22949 Loss: 1.1382062435150146\n",
      "Iteration 22950 Loss: 0.6870837807655334\n",
      "Iteration 22951 Loss: 1.3565349578857422\n",
      "Iteration 22952 Loss: 0.9972779154777527\n",
      "Iteration 22953 Loss: 1.2615346908569336\n",
      "Iteration 22954 Loss: 1.0883440971374512\n",
      "Iteration 22955 Loss: 0.9859597682952881\n",
      "Iteration 22956 Loss: 0.8702298402786255\n",
      "Iteration 22957 Loss: 1.2799700498580933\n",
      "Iteration 22958 Loss: 0.822421133518219\n",
      "Iteration 22959 Loss: 1.440963625907898\n",
      "Iteration 22959 Loss: 1.0790319442749023\n",
      "Iteration 22960 Loss: 1.3220967054367065\n",
      "Iteration 22961 Loss: 0.868126392364502\n",
      "Iteration 22962 Loss: 1.3037843704223633\n",
      "Iteration 22963 Loss: 0.8120732307434082\n",
      "Iteration 22964 Loss: 1.0266005992889404\n",
      "Iteration 22965 Loss: 0.8917407393455505\n",
      "Iteration 22966 Loss: 1.0758352279663086\n",
      "Iteration 22967 Loss: 1.0862494707107544\n",
      "Iteration 22968 Loss: 0.9656341075897217\n",
      "Iteration 22969 Loss: 1.2076953649520874\n",
      "Iteration 22969 Loss: 1.0559836626052856\n",
      "Iteration 22970 Loss: 1.2735843658447266\n",
      "Iteration 22971 Loss: 1.120797872543335\n",
      "Iteration 22972 Loss: 1.3338395357131958\n",
      "Iteration 22973 Loss: 1.0616004467010498\n",
      "Iteration 22974 Loss: 1.0720694065093994\n",
      "Iteration 22975 Loss: 1.3942146301269531\n",
      "Iteration 22976 Loss: 0.7541201710700989\n",
      "Iteration 22977 Loss: 1.3650033473968506\n",
      "Iteration 22978 Loss: 1.0113083124160767\n",
      "Iteration 22979 Loss: 0.7994398474693298\n",
      "Iteration 22979 Loss: 1.1185977458953857\n",
      "Iteration 22980 Loss: 1.1430354118347168\n",
      "Iteration 22981 Loss: 0.8736211061477661\n",
      "Iteration 22982 Loss: 1.2447012662887573\n",
      "Iteration 22983 Loss: 1.3658679723739624\n",
      "Iteration 22984 Loss: 0.9938874244689941\n",
      "Iteration 22985 Loss: 0.8997796773910522\n",
      "Iteration 22986 Loss: 0.9839395880699158\n",
      "Iteration 22987 Loss: 1.184754729270935\n",
      "Iteration 22988 Loss: 1.0483208894729614\n",
      "Iteration 22989 Loss: 1.6604344844818115\n",
      "Iteration 22989 Loss: 1.1398341655731201\n",
      "Iteration 22990 Loss: 1.1440932750701904\n",
      "Iteration 22991 Loss: 1.1765213012695312\n",
      "Iteration 22992 Loss: 1.0987187623977661\n",
      "Iteration 22993 Loss: 1.022746205329895\n",
      "Iteration 22994 Loss: 1.0594145059585571\n",
      "Iteration 22995 Loss: 1.4016706943511963\n",
      "Iteration 22996 Loss: 0.7777795791625977\n",
      "Iteration 22997 Loss: 0.7617554664611816\n",
      "Iteration 22998 Loss: 1.010062336921692\n",
      "Iteration 22999 Loss: 1.2663766145706177\n",
      "Iteration 22999 Loss: 1.0719139575958252\n",
      "Iteration 23000 Loss: 1.3814703226089478\n",
      "Iteration 23001 Loss: 1.5277659893035889\n",
      "Iteration 23002 Loss: 0.8889358639717102\n",
      "Iteration 23003 Loss: 1.0944792032241821\n",
      "Iteration 23004 Loss: 0.7226349711418152\n",
      "Iteration 23005 Loss: 1.1683849096298218\n",
      "Iteration 23006 Loss: 0.7723199129104614\n",
      "Iteration 23007 Loss: 1.1312891244888306\n",
      "Iteration 23008 Loss: 1.1312861442565918\n",
      "Iteration 23009 Loss: 1.1043829917907715\n",
      "Iteration 23009 Loss: 1.0922949314117432\n",
      "Iteration 23010 Loss: 0.7148444652557373\n",
      "Iteration 23011 Loss: 1.4570488929748535\n",
      "Iteration 23012 Loss: 1.1001427173614502\n",
      "Iteration 23013 Loss: 0.9699900150299072\n",
      "Iteration 23014 Loss: 1.2359105348587036\n",
      "Iteration 23015 Loss: 1.21199369430542\n",
      "Iteration 23016 Loss: 0.8060852885246277\n",
      "Iteration 23017 Loss: 1.1941497325897217\n",
      "Iteration 23018 Loss: 1.1669613122940063\n",
      "Iteration 23019 Loss: 0.8962393403053284\n",
      "Iteration 23019 Loss: 1.0753365755081177\n",
      "Iteration 23020 Loss: 1.248256802558899\n",
      "Iteration 23021 Loss: 1.0818077325820923\n",
      "Iteration 23022 Loss: 0.812556266784668\n",
      "Iteration 23023 Loss: 1.152564287185669\n",
      "Iteration 23024 Loss: 1.4267418384552002\n",
      "Iteration 23025 Loss: 1.0313353538513184\n",
      "Iteration 23026 Loss: 1.1093541383743286\n",
      "Iteration 23027 Loss: 0.9836491942405701\n",
      "Iteration 23028 Loss: 0.9018685221672058\n",
      "Iteration 23029 Loss: 1.1530663967132568\n",
      "Iteration 23029 Loss: 1.0901200771331787\n",
      "Iteration 23030 Loss: 1.0931789875030518\n",
      "Iteration 23031 Loss: 0.9100686311721802\n",
      "Iteration 23032 Loss: 0.9880790114402771\n",
      "Iteration 23033 Loss: 1.1473125219345093\n",
      "Iteration 23034 Loss: 1.698528528213501\n",
      "Iteration 23035 Loss: 1.1186950206756592\n",
      "Iteration 23036 Loss: 1.348374843597412\n",
      "Iteration 23037 Loss: 0.9926843047142029\n",
      "Iteration 23038 Loss: 1.0477041006088257\n",
      "Iteration 23039 Loss: 1.2451682090759277\n",
      "Iteration 23039 Loss: 1.1589795351028442\n",
      "Iteration 23040 Loss: 1.356937289237976\n",
      "Iteration 23041 Loss: 0.801021933555603\n",
      "Iteration 23042 Loss: 0.8783655166625977\n",
      "Iteration 23043 Loss: 0.991811215877533\n",
      "Iteration 23044 Loss: 1.486898422241211\n",
      "Iteration 23045 Loss: 1.07734215259552\n",
      "Iteration 23046 Loss: 0.9798200726509094\n",
      "Iteration 23047 Loss: 1.4996544122695923\n",
      "Iteration 23048 Loss: 1.6377360820770264\n",
      "Iteration 23049 Loss: 1.5053954124450684\n",
      "Iteration 23049 Loss: 1.2214982509613037\n",
      "Iteration 23050 Loss: 1.4515706300735474\n",
      "Iteration 23051 Loss: 0.8899728059768677\n",
      "Iteration 23052 Loss: 1.095489263534546\n",
      "Iteration 23053 Loss: 0.8722419738769531\n",
      "Iteration 23054 Loss: 1.0388574600219727\n",
      "Iteration 23055 Loss: 1.0780178308486938\n",
      "Iteration 23056 Loss: 1.1696540117263794\n",
      "Iteration 23057 Loss: 0.5454689860343933\n",
      "Iteration 23058 Loss: 1.0558704137802124\n",
      "Iteration 23059 Loss: 0.9609323143959045\n",
      "Iteration 23059 Loss: 1.0158076286315918\n",
      "Iteration 23060 Loss: 0.8480700254440308\n",
      "Iteration 23061 Loss: 0.8804676532745361\n",
      "Iteration 23062 Loss: 1.0677118301391602\n",
      "Iteration 23063 Loss: 0.8049745559692383\n",
      "Iteration 23064 Loss: 0.8373032212257385\n",
      "Iteration 23065 Loss: 1.137281060218811\n",
      "Iteration 23066 Loss: 0.9676095247268677\n",
      "Iteration 23067 Loss: 1.5916337966918945\n",
      "Iteration 23068 Loss: 1.4459197521209717\n",
      "Iteration 23069 Loss: 0.8490718603134155\n",
      "Iteration 23069 Loss: 1.0430042743682861\n",
      "Iteration 23070 Loss: 1.09376859664917\n",
      "Iteration 23071 Loss: 1.0492181777954102\n",
      "Iteration 23072 Loss: 1.0551382303237915\n",
      "Iteration 23073 Loss: 1.3592883348464966\n",
      "Iteration 23074 Loss: 1.1207417249679565\n",
      "Iteration 23075 Loss: 1.4591174125671387\n",
      "Iteration 23076 Loss: 0.79569411277771\n",
      "Iteration 23077 Loss: 1.271115779876709\n",
      "Iteration 23078 Loss: 0.8154613971710205\n",
      "Iteration 23079 Loss: 1.1195480823516846\n",
      "Iteration 23079 Loss: 1.113909125328064\n",
      "Iteration 23080 Loss: 1.0657546520233154\n",
      "Iteration 23081 Loss: 0.8227264285087585\n",
      "Iteration 23082 Loss: 1.4397964477539062\n",
      "Iteration 23083 Loss: 1.0610129833221436\n",
      "Iteration 23084 Loss: 1.452187418937683\n",
      "Iteration 23085 Loss: 0.9740653038024902\n",
      "Iteration 23086 Loss: 1.1152255535125732\n",
      "Iteration 23087 Loss: 1.152563214302063\n",
      "Iteration 23088 Loss: 0.9222137928009033\n",
      "Iteration 23089 Loss: 1.4041545391082764\n",
      "Iteration 23089 Loss: 1.1409701108932495\n",
      "Iteration 23090 Loss: 1.3946621417999268\n",
      "Iteration 23091 Loss: 0.8993005752563477\n",
      "Iteration 23092 Loss: 1.0087056159973145\n",
      "Iteration 23093 Loss: 1.0764212608337402\n",
      "Iteration 23094 Loss: 1.035345196723938\n",
      "Iteration 23095 Loss: 1.011448860168457\n",
      "Iteration 23096 Loss: 1.0021898746490479\n",
      "Iteration 23097 Loss: 1.0930495262145996\n",
      "Iteration 23098 Loss: 1.4441839456558228\n",
      "Iteration 23099 Loss: 1.0566997528076172\n",
      "Iteration 23099 Loss: 1.1022007465362549\n",
      "Iteration 23100 Loss: 1.5125267505645752\n",
      "Iteration 23101 Loss: 1.058272123336792\n",
      "Iteration 23102 Loss: 1.0339365005493164\n",
      "Iteration 23103 Loss: 0.7457448840141296\n",
      "Iteration 23104 Loss: 0.6601030230522156\n",
      "Iteration 23105 Loss: 0.9692809581756592\n",
      "Iteration 23106 Loss: 1.1048641204833984\n",
      "Iteration 23107 Loss: 1.2529700994491577\n",
      "Iteration 23108 Loss: 0.8663944602012634\n",
      "Iteration 23109 Loss: 1.0529237985610962\n",
      "Iteration 23109 Loss: 1.025701642036438\n",
      "Iteration 23110 Loss: 1.0072327852249146\n",
      "Iteration 23111 Loss: 1.0959495306015015\n",
      "Iteration 23112 Loss: 1.1982290744781494\n",
      "Iteration 23113 Loss: 1.0448112487792969\n",
      "Iteration 23114 Loss: 1.4592385292053223\n",
      "Iteration 23115 Loss: 1.1344413757324219\n",
      "Iteration 23116 Loss: 1.0609725713729858\n",
      "Iteration 23117 Loss: 1.4504555463790894\n",
      "Iteration 23118 Loss: 1.1093430519104004\n",
      "Iteration 23119 Loss: 1.0886181592941284\n",
      "Iteration 23119 Loss: 1.1649291515350342\n",
      "Iteration 23120 Loss: 1.2324668169021606\n",
      "Iteration 23121 Loss: 1.22383451461792\n",
      "Iteration 23122 Loss: 0.8784189820289612\n",
      "Iteration 23123 Loss: 1.0207903385162354\n",
      "Iteration 23124 Loss: 0.9200832843780518\n",
      "Iteration 23125 Loss: 1.1395630836486816\n",
      "Iteration 23126 Loss: 0.831379234790802\n",
      "Iteration 23127 Loss: 1.4209917783737183\n",
      "Iteration 23128 Loss: 0.7823929786682129\n",
      "Iteration 23129 Loss: 1.1069003343582153\n",
      "Iteration 23129 Loss: 1.0556819438934326\n",
      "Iteration 23130 Loss: 1.0498566627502441\n",
      "Iteration 23131 Loss: 1.1314246654510498\n",
      "Iteration 23132 Loss: 1.2924340963363647\n",
      "Iteration 23133 Loss: 0.9897075295448303\n",
      "Iteration 23134 Loss: 1.168267846107483\n",
      "Iteration 23135 Loss: 1.130828857421875\n",
      "Iteration 23136 Loss: 0.9734808802604675\n",
      "Iteration 23137 Loss: 0.9716539978981018\n",
      "Iteration 23138 Loss: 1.0324077606201172\n",
      "Iteration 23139 Loss: 0.8294639587402344\n",
      "Iteration 23139 Loss: 1.056952714920044\n",
      "Iteration 23140 Loss: 1.2516361474990845\n",
      "Iteration 23141 Loss: 1.6056785583496094\n",
      "Iteration 23142 Loss: 1.072116732597351\n",
      "Iteration 23143 Loss: 0.830310583114624\n",
      "Iteration 23144 Loss: 0.7378464341163635\n",
      "Iteration 23145 Loss: 0.9406579732894897\n",
      "Iteration 23146 Loss: 0.7967133522033691\n",
      "Iteration 23147 Loss: 0.8998876810073853\n",
      "Iteration 23148 Loss: 1.302441120147705\n",
      "Iteration 23149 Loss: 1.056892991065979\n",
      "Iteration 23149 Loss: 1.0494182109832764\n",
      "Iteration 23150 Loss: 0.8928080797195435\n",
      "Iteration 23151 Loss: 1.1971306800842285\n",
      "Iteration 23152 Loss: 1.179854393005371\n",
      "Iteration 23153 Loss: 0.8570160269737244\n",
      "Iteration 23154 Loss: 1.5691382884979248\n",
      "Iteration 23155 Loss: 0.8842054009437561\n",
      "Iteration 23156 Loss: 1.1984460353851318\n",
      "Iteration 23157 Loss: 1.4229931831359863\n",
      "Iteration 23158 Loss: 0.9957578778266907\n",
      "Iteration 23159 Loss: 0.8081458210945129\n",
      "Iteration 23159 Loss: 1.1005496978759766\n",
      "Iteration 23160 Loss: 0.828824520111084\n",
      "Iteration 23161 Loss: 1.125505805015564\n",
      "Iteration 23162 Loss: 1.1636841297149658\n",
      "Iteration 23163 Loss: 0.8599303364753723\n",
      "Iteration 23164 Loss: 1.1387252807617188\n",
      "Iteration 23165 Loss: 1.145046353340149\n",
      "Iteration 23166 Loss: 1.294266700744629\n",
      "Iteration 23167 Loss: 0.9360396862030029\n",
      "Iteration 23168 Loss: 0.7590888142585754\n",
      "Iteration 23169 Loss: 1.085267186164856\n",
      "Iteration 23169 Loss: 1.0336380004882812\n",
      "Iteration 23170 Loss: 0.8717113137245178\n",
      "Iteration 23171 Loss: 1.2027010917663574\n",
      "Iteration 23172 Loss: 1.0840226411819458\n",
      "Iteration 23173 Loss: 0.8447932600975037\n",
      "Iteration 23174 Loss: 0.6052610874176025\n",
      "Iteration 23175 Loss: 1.0860720872879028\n",
      "Iteration 23176 Loss: 1.1133747100830078\n",
      "Iteration 23177 Loss: 1.4620311260223389\n",
      "Iteration 23178 Loss: 0.8408485651016235\n",
      "Iteration 23179 Loss: 0.7622496485710144\n",
      "Iteration 23179 Loss: 0.9873065948486328\n",
      "Iteration 23180 Loss: 0.994603157043457\n",
      "Iteration 23181 Loss: 1.1115306615829468\n",
      "Iteration 23182 Loss: 1.3789687156677246\n",
      "Iteration 23183 Loss: 1.0125502347946167\n",
      "Iteration 23184 Loss: 0.9096637964248657\n",
      "Iteration 23185 Loss: 1.0428102016448975\n",
      "Iteration 23186 Loss: 0.9291679263114929\n",
      "Iteration 23187 Loss: 1.3836311101913452\n",
      "Iteration 23188 Loss: 1.1562690734863281\n",
      "Iteration 23189 Loss: 1.1967145204544067\n",
      "Iteration 23189 Loss: 1.11159086227417\n",
      "Iteration 23190 Loss: 1.0731892585754395\n",
      "Iteration 23191 Loss: 1.4231398105621338\n",
      "Iteration 23192 Loss: 1.1407787799835205\n",
      "Iteration 23193 Loss: 0.7438613176345825\n",
      "Iteration 23194 Loss: 1.0955435037612915\n",
      "Iteration 23195 Loss: 0.872576117515564\n",
      "Iteration 23196 Loss: 0.991186261177063\n",
      "Iteration 23197 Loss: 1.301446557044983\n",
      "Iteration 23198 Loss: 1.2780784368515015\n",
      "Iteration 23199 Loss: 1.3619930744171143\n",
      "Iteration 23199 Loss: 1.1281793117523193\n",
      "Iteration 23200 Loss: 1.1401396989822388\n",
      "Iteration 23201 Loss: 1.2137489318847656\n",
      "Iteration 23202 Loss: 1.2485424280166626\n",
      "Iteration 23203 Loss: 1.066434621810913\n",
      "Iteration 23204 Loss: 1.3205944299697876\n",
      "Iteration 23205 Loss: 1.06974458694458\n",
      "Iteration 23206 Loss: 0.8712822198867798\n",
      "Iteration 23207 Loss: 1.101846694946289\n",
      "Iteration 23208 Loss: 0.8649999499320984\n",
      "Iteration 23209 Loss: 1.2828257083892822\n",
      "Iteration 23209 Loss: 1.118016004562378\n",
      "Iteration 23210 Loss: 0.8500760793685913\n",
      "Iteration 23211 Loss: 1.4420511722564697\n",
      "Iteration 23212 Loss: 1.4592032432556152\n",
      "Iteration 23213 Loss: 1.2198622226715088\n",
      "Iteration 23214 Loss: 0.8440712094306946\n",
      "Iteration 23215 Loss: 1.2852513790130615\n",
      "Iteration 23216 Loss: 1.4467973709106445\n",
      "Iteration 23217 Loss: 1.6605623960494995\n",
      "Iteration 23218 Loss: 1.1246256828308105\n",
      "Iteration 23219 Loss: 0.899547815322876\n",
      "Iteration 23219 Loss: 1.2232048511505127\n",
      "Iteration 23220 Loss: 1.0121068954467773\n",
      "Iteration 23221 Loss: 0.609778881072998\n",
      "Iteration 23222 Loss: 1.3293299674987793\n",
      "Iteration 23223 Loss: 0.7968069911003113\n",
      "Iteration 23224 Loss: 0.5212652087211609\n",
      "Iteration 23225 Loss: 1.2362371683120728\n",
      "Iteration 23226 Loss: 0.8914116621017456\n",
      "Iteration 23227 Loss: 0.9012897610664368\n",
      "Iteration 23228 Loss: 1.191535234451294\n",
      "Iteration 23229 Loss: 0.5967490673065186\n",
      "Iteration 23229 Loss: 0.9086510539054871\n",
      "Iteration 23230 Loss: 1.48552668094635\n",
      "Iteration 23231 Loss: 0.689306378364563\n",
      "Iteration 23232 Loss: 1.0821644067764282\n",
      "Iteration 23233 Loss: 1.0385724306106567\n",
      "Iteration 23234 Loss: 1.1464953422546387\n",
      "Iteration 23235 Loss: 1.1016188859939575\n",
      "Iteration 23236 Loss: 0.9225167632102966\n",
      "Iteration 23237 Loss: 1.057570219039917\n",
      "Iteration 23238 Loss: 0.7952339053153992\n",
      "Iteration 23239 Loss: 0.9584403038024902\n",
      "Iteration 23239 Loss: 1.0277445316314697\n",
      "Iteration 23240 Loss: 0.9705118536949158\n",
      "Iteration 23241 Loss: 0.8411900997161865\n",
      "Iteration 23242 Loss: 1.3450698852539062\n",
      "Iteration 23243 Loss: 0.85552579164505\n",
      "Iteration 23244 Loss: 0.8584924936294556\n",
      "Iteration 23245 Loss: 1.3079769611358643\n",
      "Iteration 23246 Loss: 1.1219404935836792\n",
      "Iteration 23247 Loss: 1.0572725534439087\n",
      "Iteration 23248 Loss: 1.1376917362213135\n",
      "Iteration 23249 Loss: 0.9276132583618164\n",
      "Iteration 23249 Loss: 1.0423285961151123\n",
      "Iteration 23250 Loss: 0.9780568480491638\n",
      "Iteration 23251 Loss: 1.026507019996643\n",
      "Iteration 23252 Loss: 0.7033692598342896\n",
      "Iteration 23253 Loss: 1.2303133010864258\n",
      "Iteration 23254 Loss: 0.9129977822303772\n",
      "Iteration 23255 Loss: 1.3720027208328247\n",
      "Iteration 23256 Loss: 1.1056264638900757\n",
      "Iteration 23257 Loss: 1.115431785583496\n",
      "Iteration 23258 Loss: 0.6665937304496765\n",
      "Iteration 23259 Loss: 0.7237240076065063\n",
      "Iteration 23259 Loss: 0.9834622144699097\n",
      "Iteration 23260 Loss: 1.1136318445205688\n",
      "Iteration 23261 Loss: 1.0600467920303345\n",
      "Iteration 23262 Loss: 1.1269307136535645\n",
      "Iteration 23263 Loss: 0.9986112713813782\n",
      "Iteration 23264 Loss: 0.8607996702194214\n",
      "Iteration 23265 Loss: 0.9499105215072632\n",
      "Iteration 23266 Loss: 0.9334930181503296\n",
      "Iteration 23267 Loss: 0.8274776339530945\n",
      "Iteration 23268 Loss: 1.2898749113082886\n",
      "Iteration 23269 Loss: 0.8341037631034851\n",
      "Iteration 23269 Loss: 0.9994879961013794\n",
      "Iteration 23270 Loss: 0.9015786647796631\n",
      "Iteration 23271 Loss: 0.9880012273788452\n",
      "Iteration 23272 Loss: 1.227286458015442\n",
      "Iteration 23273 Loss: 1.168830394744873\n",
      "Iteration 23274 Loss: 0.8400335311889648\n",
      "Iteration 23275 Loss: 0.9807385802268982\n",
      "Iteration 23276 Loss: 1.0818662643432617\n",
      "Iteration 23277 Loss: 1.5228756666183472\n",
      "Iteration 23278 Loss: 0.7191441059112549\n",
      "Iteration 23279 Loss: 1.3105181455612183\n",
      "Iteration 23279 Loss: 1.0740872621536255\n",
      "Iteration 23280 Loss: 0.9075742363929749\n",
      "Iteration 23281 Loss: 1.401920199394226\n",
      "Iteration 23282 Loss: 1.0338785648345947\n",
      "Iteration 23283 Loss: 1.308741807937622\n",
      "Iteration 23284 Loss: 0.531107485294342\n",
      "Iteration 23285 Loss: 1.4586583375930786\n",
      "Iteration 23286 Loss: 1.7559484243392944\n",
      "Iteration 23287 Loss: 0.8652991056442261\n",
      "Iteration 23288 Loss: 1.057698130607605\n",
      "Iteration 23289 Loss: 1.2059539556503296\n",
      "Iteration 23289 Loss: 1.1526780128479004\n",
      "Iteration 23290 Loss: 0.8044013381004333\n",
      "Iteration 23291 Loss: 1.2647438049316406\n",
      "Iteration 23292 Loss: 1.3099420070648193\n",
      "Iteration 23293 Loss: 1.0422836542129517\n",
      "Iteration 23294 Loss: 1.053797960281372\n",
      "Iteration 23295 Loss: 0.9389204978942871\n",
      "Iteration 23296 Loss: 1.0140246152877808\n",
      "Iteration 23297 Loss: 1.1153310537338257\n",
      "Iteration 23298 Loss: 1.2413591146469116\n",
      "Iteration 23299 Loss: 0.9796611666679382\n",
      "Iteration 23299 Loss: 1.0764464139938354\n",
      "Iteration 23300 Loss: 0.9405234456062317\n",
      "Iteration 23301 Loss: 1.2604670524597168\n",
      "Iteration 23302 Loss: 0.9926406145095825\n",
      "Iteration 23303 Loss: 1.2318553924560547\n",
      "Iteration 23304 Loss: 1.0483582019805908\n",
      "Iteration 23305 Loss: 0.8823161125183105\n",
      "Iteration 23306 Loss: 1.3169909715652466\n",
      "Iteration 23307 Loss: 0.9551509022712708\n",
      "Iteration 23308 Loss: 0.9100260734558105\n",
      "Iteration 23309 Loss: 0.7265148758888245\n",
      "Iteration 23309 Loss: 1.0264843702316284\n",
      "Iteration 23310 Loss: 0.8138228058815002\n",
      "Iteration 23311 Loss: 1.0836026668548584\n",
      "Iteration 23312 Loss: 1.011468768119812\n",
      "Iteration 23313 Loss: 1.3547861576080322\n",
      "Iteration 23314 Loss: 1.17122220993042\n",
      "Iteration 23315 Loss: 1.4127568006515503\n",
      "Iteration 23316 Loss: 1.3729279041290283\n",
      "Iteration 23317 Loss: 1.3421375751495361\n",
      "Iteration 23318 Loss: 1.1949732303619385\n",
      "Iteration 23319 Loss: 0.8299395442008972\n",
      "Iteration 23319 Loss: 1.1587636470794678\n",
      "Iteration 23320 Loss: 1.5616450309753418\n",
      "Iteration 23321 Loss: 1.391681432723999\n",
      "Iteration 23322 Loss: 1.1733031272888184\n",
      "Iteration 23323 Loss: 0.9742259979248047\n",
      "Iteration 23324 Loss: 0.8189062476158142\n",
      "Iteration 23325 Loss: 1.386626958847046\n",
      "Iteration 23326 Loss: 1.4130747318267822\n",
      "Iteration 23327 Loss: 1.1166129112243652\n",
      "Iteration 23328 Loss: 0.8794242739677429\n",
      "Iteration 23329 Loss: 0.9977424144744873\n",
      "Iteration 23329 Loss: 1.171324372291565\n",
      "Iteration 23330 Loss: 1.1779695749282837\n",
      "Iteration 23331 Loss: 0.9472557902336121\n",
      "Iteration 23332 Loss: 0.870349645614624\n",
      "Iteration 23333 Loss: 1.093735694885254\n",
      "Iteration 23334 Loss: 1.2466524839401245\n",
      "Iteration 23335 Loss: 1.2030410766601562\n",
      "Iteration 23336 Loss: 0.7940022945404053\n",
      "Iteration 23337 Loss: 1.1290665864944458\n",
      "Iteration 23338 Loss: 1.156286597251892\n",
      "Iteration 23339 Loss: 1.526606798171997\n",
      "Iteration 23339 Loss: 1.1144965887069702\n",
      "Iteration 23340 Loss: 0.9717593789100647\n",
      "Iteration 23341 Loss: 0.9548089504241943\n",
      "Iteration 23342 Loss: 1.3067851066589355\n",
      "Iteration 23343 Loss: 0.9431429505348206\n",
      "Iteration 23344 Loss: 0.9988346695899963\n",
      "Iteration 23345 Loss: 1.4235759973526\n",
      "Iteration 23346 Loss: 1.3318930864334106\n",
      "Iteration 23347 Loss: 1.2549891471862793\n",
      "Iteration 23348 Loss: 1.1891305446624756\n",
      "Iteration 23349 Loss: 0.7801839113235474\n",
      "Iteration 23349 Loss: 1.11551034450531\n",
      "Iteration 23350 Loss: 1.0885432958602905\n",
      "Iteration 23351 Loss: 1.340152621269226\n",
      "Iteration 23352 Loss: 1.2173916101455688\n",
      "Iteration 23353 Loss: 1.2976547479629517\n",
      "Iteration 23354 Loss: 1.2369184494018555\n",
      "Iteration 23355 Loss: 0.72246915102005\n",
      "Iteration 23356 Loss: 1.090790033340454\n",
      "Iteration 23357 Loss: 1.0260436534881592\n",
      "Iteration 23358 Loss: 1.261738657951355\n",
      "Iteration 23359 Loss: 1.1304067373275757\n",
      "Iteration 23359 Loss: 1.1412107944488525\n",
      "Iteration 23360 Loss: 1.0830880403518677\n",
      "Iteration 23361 Loss: 1.1759449243545532\n",
      "Iteration 23362 Loss: 1.4216030836105347\n",
      "Iteration 23363 Loss: 0.5618806481361389\n",
      "Iteration 23364 Loss: 1.12477707862854\n",
      "Iteration 23365 Loss: 1.1673356294631958\n",
      "Iteration 23366 Loss: 1.1472687721252441\n",
      "Iteration 23367 Loss: 1.0444860458374023\n",
      "Iteration 23368 Loss: 0.7895890474319458\n",
      "Iteration 23369 Loss: 1.231663465499878\n",
      "Iteration 23369 Loss: 1.074763536453247\n",
      "Iteration 23370 Loss: 1.3026326894760132\n",
      "Iteration 23371 Loss: 0.7894442081451416\n",
      "Iteration 23372 Loss: 1.4434764385223389\n",
      "Iteration 23373 Loss: 1.1566065549850464\n",
      "Iteration 23374 Loss: 1.1766985654830933\n",
      "Iteration 23375 Loss: 0.8659250736236572\n",
      "Iteration 23376 Loss: 1.1288261413574219\n",
      "Iteration 23377 Loss: 1.1324800252914429\n",
      "Iteration 23378 Loss: 1.0046356916427612\n",
      "Iteration 23379 Loss: 1.0358390808105469\n",
      "Iteration 23379 Loss: 1.103656530380249\n",
      "Iteration 23380 Loss: 1.1214202642440796\n",
      "Iteration 23381 Loss: 0.6592429280281067\n",
      "Iteration 23382 Loss: 0.9441999793052673\n",
      "Iteration 23383 Loss: 1.0336161851882935\n",
      "Iteration 23384 Loss: 0.8068220615386963\n",
      "Iteration 23385 Loss: 1.478223443031311\n",
      "Iteration 23386 Loss: 0.9601470232009888\n",
      "Iteration 23387 Loss: 1.3254508972167969\n",
      "Iteration 23388 Loss: 0.9638422727584839\n",
      "Iteration 23389 Loss: 1.2889316082000732\n",
      "Iteration 23389 Loss: 1.0581896305084229\n",
      "Iteration 23390 Loss: 1.3007789850234985\n",
      "Iteration 23391 Loss: 1.2672878503799438\n",
      "Iteration 23392 Loss: 0.9457650184631348\n",
      "Iteration 23393 Loss: 1.0403048992156982\n",
      "Iteration 23394 Loss: 0.7335584163665771\n",
      "Iteration 23395 Loss: 0.9070472717285156\n",
      "Iteration 23396 Loss: 1.3969151973724365\n",
      "Iteration 23397 Loss: 1.037558913230896\n",
      "Iteration 23398 Loss: 1.2848396301269531\n",
      "Iteration 23399 Loss: 0.848394513130188\n",
      "Iteration 23399 Loss: 1.0762450695037842\n",
      "Iteration 23400 Loss: 1.0603702068328857\n",
      "Iteration 23401 Loss: 1.7823582887649536\n",
      "Iteration 23402 Loss: 0.9982109665870667\n",
      "Iteration 23403 Loss: 1.2673602104187012\n",
      "Iteration 23404 Loss: 1.0711939334869385\n",
      "Iteration 23405 Loss: 0.8842427134513855\n",
      "Iteration 23406 Loss: 0.9738591313362122\n",
      "Iteration 23407 Loss: 0.7692983746528625\n",
      "Iteration 23408 Loss: 1.1179850101470947\n",
      "Iteration 23409 Loss: 0.997216522693634\n",
      "Iteration 23409 Loss: 1.0922095775604248\n",
      "Iteration 23410 Loss: 1.297364592552185\n",
      "Iteration 23411 Loss: 0.8867665529251099\n",
      "Iteration 23412 Loss: 1.0880051851272583\n",
      "Iteration 23413 Loss: 1.0231035947799683\n",
      "Iteration 23414 Loss: 0.9871604442596436\n",
      "Iteration 23415 Loss: 1.2058480978012085\n",
      "Iteration 23416 Loss: 0.9197038412094116\n",
      "Iteration 23417 Loss: 1.0621751546859741\n",
      "Iteration 23418 Loss: 1.041390061378479\n",
      "Iteration 23419 Loss: 1.2384330034255981\n",
      "Iteration 23419 Loss: 1.0749949216842651\n",
      "Iteration 23420 Loss: 0.8579923510551453\n",
      "Iteration 23421 Loss: 1.625813364982605\n",
      "Iteration 23422 Loss: 1.0988019704818726\n",
      "Iteration 23423 Loss: 0.7611849308013916\n",
      "Iteration 23424 Loss: 1.1271679401397705\n",
      "Iteration 23425 Loss: 0.9507395625114441\n",
      "Iteration 23426 Loss: 1.1884759664535522\n",
      "Iteration 23427 Loss: 0.9001413583755493\n",
      "Iteration 23428 Loss: 1.3483285903930664\n",
      "Iteration 23429 Loss: 0.9954010844230652\n",
      "Iteration 23429 Loss: 1.0854047536849976\n",
      "Iteration 23430 Loss: 1.1735533475875854\n",
      "Iteration 23431 Loss: 1.1933746337890625\n",
      "Iteration 23432 Loss: 1.1569671630859375\n",
      "Iteration 23433 Loss: 1.0656712055206299\n",
      "Iteration 23434 Loss: 1.3205558061599731\n",
      "Iteration 23435 Loss: 1.3455781936645508\n",
      "Iteration 23436 Loss: 1.4679173231124878\n",
      "Iteration 23437 Loss: 1.5594990253448486\n",
      "Iteration 23438 Loss: 1.3784568309783936\n",
      "Iteration 23439 Loss: 1.2415403127670288\n",
      "Iteration 23439 Loss: 1.290311336517334\n",
      "Iteration 23440 Loss: 0.7965658903121948\n",
      "Iteration 23441 Loss: 1.043713092803955\n",
      "Iteration 23442 Loss: 1.0287801027297974\n",
      "Iteration 23443 Loss: 1.2833969593048096\n",
      "Iteration 23444 Loss: 1.1710866689682007\n",
      "Iteration 23445 Loss: 1.1446073055267334\n",
      "Iteration 23446 Loss: 1.0531588792800903\n",
      "Iteration 23447 Loss: 1.3169281482696533\n",
      "Iteration 23448 Loss: 0.9868562817573547\n",
      "Iteration 23449 Loss: 1.3927698135375977\n",
      "Iteration 23449 Loss: 1.1217862367630005\n",
      "Iteration 23450 Loss: 0.7741756439208984\n",
      "Iteration 23451 Loss: 1.0270462036132812\n",
      "Iteration 23452 Loss: 1.4831446409225464\n",
      "Iteration 23453 Loss: 1.0085710287094116\n",
      "Iteration 23454 Loss: 1.1471222639083862\n",
      "Iteration 23455 Loss: 1.1691017150878906\n",
      "Iteration 23456 Loss: 1.295351505279541\n",
      "Iteration 23457 Loss: 0.6957249045372009\n",
      "Iteration 23458 Loss: 1.5993068218231201\n",
      "Iteration 23459 Loss: 1.1344189643859863\n",
      "Iteration 23459 Loss: 1.1333963871002197\n",
      "Iteration 23460 Loss: 0.7086830139160156\n",
      "Iteration 23461 Loss: 1.1166421175003052\n",
      "Iteration 23462 Loss: 0.7832067608833313\n",
      "Iteration 23463 Loss: 1.328090786933899\n",
      "Iteration 23464 Loss: 1.1004141569137573\n",
      "Iteration 23465 Loss: 0.811244010925293\n",
      "Iteration 23466 Loss: 1.390545129776001\n",
      "Iteration 23467 Loss: 0.9661095142364502\n",
      "Iteration 23468 Loss: 1.327622413635254\n",
      "Iteration 23469 Loss: 0.995323121547699\n",
      "Iteration 23469 Loss: 1.0527880191802979\n",
      "Iteration 23470 Loss: 1.3484253883361816\n",
      "Iteration 23471 Loss: 0.8541646003723145\n",
      "Iteration 23472 Loss: 1.3038898706436157\n",
      "Iteration 23473 Loss: 0.8923873901367188\n",
      "Iteration 23474 Loss: 1.00348699092865\n",
      "Iteration 23475 Loss: 0.8186513185501099\n",
      "Iteration 23476 Loss: 1.3412580490112305\n",
      "Iteration 23477 Loss: 1.247807502746582\n",
      "Iteration 23478 Loss: 1.149146318435669\n",
      "Iteration 23479 Loss: 0.8929076194763184\n",
      "Iteration 23479 Loss: 1.0852124691009521\n",
      "Iteration 23480 Loss: 0.8764489889144897\n",
      "Iteration 23481 Loss: 1.1269851922988892\n",
      "Iteration 23482 Loss: 1.0938597917556763\n",
      "Iteration 23483 Loss: 1.1483368873596191\n",
      "Iteration 23484 Loss: 1.2857494354248047\n",
      "Iteration 23485 Loss: 1.0432976484298706\n",
      "Iteration 23486 Loss: 0.9439671635627747\n",
      "Iteration 23487 Loss: 0.9957630634307861\n",
      "Iteration 23488 Loss: 0.9319710731506348\n",
      "Iteration 23489 Loss: 1.1850706338882446\n",
      "Iteration 23489 Loss: 1.0631449222564697\n",
      "Iteration 23490 Loss: 0.7925925254821777\n",
      "Iteration 23491 Loss: 0.9596977233886719\n",
      "Iteration 23492 Loss: 0.8899825811386108\n",
      "Iteration 23493 Loss: 1.1365010738372803\n",
      "Iteration 23494 Loss: 1.1259335279464722\n",
      "Iteration 23495 Loss: 1.003497838973999\n",
      "Iteration 23496 Loss: 0.7605288028717041\n",
      "Iteration 23497 Loss: 1.214664340019226\n",
      "Iteration 23498 Loss: 1.0768624544143677\n",
      "Iteration 23499 Loss: 1.285938024520874\n",
      "Iteration 23499 Loss: 1.0246199369430542\n",
      "Iteration 23500 Loss: 1.3377629518508911\n",
      "Iteration 23501 Loss: 1.3524070978164673\n",
      "Iteration 23502 Loss: 0.6602745056152344\n",
      "Iteration 23503 Loss: 1.241033673286438\n",
      "Iteration 23504 Loss: 0.8372149467468262\n",
      "Iteration 23505 Loss: 0.8014195561408997\n",
      "Iteration 23506 Loss: 1.01220703125\n",
      "Iteration 23507 Loss: 1.068843126296997\n",
      "Iteration 23508 Loss: 0.6817389726638794\n",
      "Iteration 23509 Loss: 1.1978230476379395\n",
      "Iteration 23509 Loss: 1.019072413444519\n",
      "Iteration 23510 Loss: 1.1952838897705078\n",
      "Iteration 23511 Loss: 1.0737541913986206\n",
      "Iteration 23512 Loss: 1.1062322854995728\n",
      "Iteration 23513 Loss: 1.125709056854248\n",
      "Iteration 23514 Loss: 0.7370100021362305\n",
      "Iteration 23515 Loss: 1.3915272951126099\n",
      "Iteration 23516 Loss: 1.0457996129989624\n",
      "Iteration 23517 Loss: 1.2181795835494995\n",
      "Iteration 23518 Loss: 1.2464680671691895\n",
      "Iteration 23519 Loss: 1.0509095191955566\n",
      "Iteration 23519 Loss: 1.1190873384475708\n",
      "Iteration 23520 Loss: 1.287612795829773\n",
      "Iteration 23521 Loss: 1.247239351272583\n",
      "Iteration 23522 Loss: 1.4250093698501587\n",
      "Iteration 23523 Loss: 0.9960342049598694\n",
      "Iteration 23524 Loss: 0.9852256774902344\n",
      "Iteration 23525 Loss: 1.072632074356079\n",
      "Iteration 23526 Loss: 1.003808617591858\n",
      "Iteration 23527 Loss: 1.1013599634170532\n",
      "Iteration 23528 Loss: 1.2646560668945312\n",
      "Iteration 23529 Loss: 0.7239010334014893\n",
      "Iteration 23529 Loss: 1.1107479333877563\n",
      "Iteration 23530 Loss: 0.7775000333786011\n",
      "Iteration 23531 Loss: 1.0361769199371338\n",
      "Iteration 23532 Loss: 1.061720609664917\n",
      "Iteration 23533 Loss: 0.9089046120643616\n",
      "Iteration 23534 Loss: 1.4855189323425293\n",
      "Iteration 23535 Loss: 0.8842675685882568\n",
      "Iteration 23536 Loss: 1.3716611862182617\n",
      "Iteration 23537 Loss: 0.9192512035369873\n",
      "Iteration 23538 Loss: 0.6556230783462524\n",
      "Iteration 23539 Loss: 1.3781952857971191\n",
      "Iteration 23539 Loss: 1.0478819608688354\n",
      "Iteration 23540 Loss: 1.4046552181243896\n",
      "Iteration 23541 Loss: 1.2846378087997437\n",
      "Iteration 23542 Loss: 0.8005504012107849\n",
      "Iteration 23543 Loss: 1.0702953338623047\n",
      "Iteration 23544 Loss: 0.8760376572608948\n",
      "Iteration 23545 Loss: 0.9131640791893005\n",
      "Iteration 23546 Loss: 0.7663893103599548\n",
      "Iteration 23547 Loss: 1.582702398300171\n",
      "Iteration 23548 Loss: 1.0825634002685547\n",
      "Iteration 23549 Loss: 1.2550833225250244\n",
      "Iteration 23549 Loss: 1.1036078929901123\n",
      "Iteration 23550 Loss: 1.0422831773757935\n",
      "Iteration 23551 Loss: 1.2214518785476685\n",
      "Iteration 23552 Loss: 1.4323911666870117\n",
      "Iteration 23553 Loss: 1.293284296989441\n",
      "Iteration 23554 Loss: 1.2003874778747559\n",
      "Iteration 23555 Loss: 1.1796382665634155\n",
      "Iteration 23556 Loss: 0.7477653622627258\n",
      "Iteration 23557 Loss: 1.413932204246521\n",
      "Iteration 23558 Loss: 1.1234469413757324\n",
      "Iteration 23559 Loss: 1.008743405342102\n",
      "Iteration 23559 Loss: 1.1663323640823364\n",
      "Iteration 23560 Loss: 1.108900785446167\n",
      "Iteration 23561 Loss: 0.8547194004058838\n",
      "Iteration 23562 Loss: 0.8116933107376099\n",
      "Iteration 23563 Loss: 1.097961187362671\n",
      "Iteration 23564 Loss: 1.2344062328338623\n",
      "Iteration 23565 Loss: 0.9881370067596436\n",
      "Iteration 23566 Loss: 1.2078299522399902\n",
      "Iteration 23567 Loss: 0.5649034976959229\n",
      "Iteration 23568 Loss: 1.0322890281677246\n",
      "Iteration 23569 Loss: 0.9006476998329163\n",
      "Iteration 23569 Loss: 0.9801489114761353\n",
      "Iteration 23570 Loss: 0.7979823350906372\n",
      "Iteration 23571 Loss: 0.6393603682518005\n",
      "Iteration 23572 Loss: 1.0431650876998901\n",
      "Iteration 23573 Loss: 1.3359622955322266\n",
      "Iteration 23574 Loss: 1.1361424922943115\n",
      "Iteration 23575 Loss: 0.6976396441459656\n",
      "Iteration 23576 Loss: 1.154028296470642\n",
      "Iteration 23577 Loss: 1.4242815971374512\n",
      "Iteration 23578 Loss: 1.3070229291915894\n",
      "Iteration 23579 Loss: 1.2042062282562256\n",
      "Iteration 23579 Loss: 1.0739790201187134\n",
      "Iteration 23580 Loss: 0.9048014879226685\n",
      "Iteration 23581 Loss: 0.7813818454742432\n",
      "Iteration 23582 Loss: 1.0458556413650513\n",
      "Iteration 23583 Loss: 1.2701530456542969\n",
      "Iteration 23584 Loss: 1.089131474494934\n",
      "Iteration 23585 Loss: 1.2059886455535889\n",
      "Iteration 23586 Loss: 1.2333124876022339\n",
      "Iteration 23587 Loss: 0.9270793795585632\n",
      "Iteration 23588 Loss: 0.8479846119880676\n",
      "Iteration 23589 Loss: 1.0375480651855469\n",
      "Iteration 23589 Loss: 1.0343236923217773\n",
      "Iteration 23590 Loss: 1.0667990446090698\n",
      "Iteration 23591 Loss: 1.3318074941635132\n",
      "Iteration 23592 Loss: 1.2836217880249023\n",
      "Iteration 23593 Loss: 1.0999099016189575\n",
      "Iteration 23594 Loss: 1.319140076637268\n",
      "Iteration 23595 Loss: 1.3494846820831299\n",
      "Iteration 23596 Loss: 0.7626027464866638\n",
      "Iteration 23597 Loss: 1.2195279598236084\n",
      "Iteration 23598 Loss: 1.1750706434249878\n",
      "Iteration 23599 Loss: 0.8346009254455566\n",
      "Iteration 23599 Loss: 1.144256591796875\n",
      "Iteration 23600 Loss: 0.9162569046020508\n",
      "Iteration 23601 Loss: 0.9692845344543457\n",
      "Iteration 23602 Loss: 1.3179222345352173\n",
      "Iteration 23603 Loss: 1.3396248817443848\n",
      "Iteration 23604 Loss: 0.9644964337348938\n",
      "Iteration 23605 Loss: 1.250920295715332\n",
      "Iteration 23606 Loss: 1.0017178058624268\n",
      "Iteration 23607 Loss: 1.4765994548797607\n",
      "Iteration 23608 Loss: 1.3281729221343994\n",
      "Iteration 23609 Loss: 1.3144524097442627\n",
      "Iteration 23609 Loss: 1.187944769859314\n",
      "Iteration 23610 Loss: 1.3766603469848633\n",
      "Iteration 23611 Loss: 0.7188677191734314\n",
      "Iteration 23612 Loss: 1.0474448204040527\n",
      "Iteration 23613 Loss: 1.0296673774719238\n",
      "Iteration 23614 Loss: 1.0025922060012817\n",
      "Iteration 23615 Loss: 1.0657466650009155\n",
      "Iteration 23616 Loss: 0.9421945810317993\n",
      "Iteration 23617 Loss: 1.2722738981246948\n",
      "Iteration 23618 Loss: 0.9570422768592834\n",
      "Iteration 23619 Loss: 1.0016443729400635\n",
      "Iteration 23619 Loss: 1.041413426399231\n",
      "Iteration 23620 Loss: 1.4107565879821777\n",
      "Iteration 23621 Loss: 1.3305686712265015\n",
      "Iteration 23622 Loss: 1.2686868906021118\n",
      "Iteration 23623 Loss: 0.8909333348274231\n",
      "Iteration 23624 Loss: 1.4175688028335571\n",
      "Iteration 23625 Loss: 1.1100130081176758\n",
      "Iteration 23626 Loss: 0.9463421106338501\n",
      "Iteration 23627 Loss: 1.3007919788360596\n",
      "Iteration 23628 Loss: 1.0592790842056274\n",
      "Iteration 23629 Loss: 1.3914369344711304\n",
      "Iteration 23629 Loss: 1.2126377820968628\n",
      "Iteration 23630 Loss: 1.0542632341384888\n",
      "Iteration 23631 Loss: 1.0521897077560425\n",
      "Iteration 23632 Loss: 0.7670587301254272\n",
      "Iteration 23633 Loss: 1.1226273775100708\n",
      "Iteration 23634 Loss: 1.4084656238555908\n",
      "Iteration 23635 Loss: 1.1922410726547241\n",
      "Iteration 23636 Loss: 1.452377438545227\n",
      "Iteration 23637 Loss: 1.4349054098129272\n",
      "Iteration 23638 Loss: 1.3568105697631836\n",
      "Iteration 23639 Loss: 1.2757394313812256\n",
      "Iteration 23639 Loss: 1.211667776107788\n",
      "Iteration 23640 Loss: 1.0911389589309692\n",
      "Iteration 23641 Loss: 0.9002674221992493\n",
      "Iteration 23642 Loss: 1.1003376245498657\n",
      "Iteration 23643 Loss: 1.1088606119155884\n",
      "Iteration 23644 Loss: 1.1543304920196533\n",
      "Iteration 23645 Loss: 0.8518001437187195\n",
      "Iteration 23646 Loss: 1.141059160232544\n",
      "Iteration 23647 Loss: 1.2423840761184692\n",
      "Iteration 23648 Loss: 1.2996413707733154\n",
      "Iteration 23649 Loss: 1.0752559900283813\n",
      "Iteration 23649 Loss: 1.0965075492858887\n",
      "Iteration 23650 Loss: 0.9370782375335693\n",
      "Iteration 23651 Loss: 1.1100634336471558\n",
      "Iteration 23652 Loss: 1.194169044494629\n",
      "Iteration 23653 Loss: 1.064680814743042\n",
      "Iteration 23654 Loss: 1.3077301979064941\n",
      "Iteration 23655 Loss: 1.089897632598877\n",
      "Iteration 23656 Loss: 0.8456938862800598\n",
      "Iteration 23657 Loss: 1.1186763048171997\n",
      "Iteration 23658 Loss: 0.8664325475692749\n",
      "Iteration 23659 Loss: 0.7480880618095398\n",
      "Iteration 23659 Loss: 1.0282509326934814\n",
      "Iteration 23660 Loss: 1.2239519357681274\n",
      "Iteration 23661 Loss: 1.2240689992904663\n",
      "Iteration 23662 Loss: 0.7448945045471191\n",
      "Iteration 23663 Loss: 1.0279114246368408\n",
      "Iteration 23664 Loss: 1.1265982389450073\n",
      "Iteration 23665 Loss: 1.1969640254974365\n",
      "Iteration 23666 Loss: 1.17698335647583\n",
      "Iteration 23667 Loss: 0.992672324180603\n",
      "Iteration 23668 Loss: 1.2925156354904175\n",
      "Iteration 23669 Loss: 0.7257052063941956\n",
      "Iteration 23669 Loss: 1.0732265710830688\n",
      "Iteration 23670 Loss: 1.1646156311035156\n",
      "Iteration 23671 Loss: 0.9427406191825867\n",
      "Iteration 23672 Loss: 1.4924030303955078\n",
      "Iteration 23673 Loss: 1.102577805519104\n",
      "Iteration 23674 Loss: 1.4267544746398926\n",
      "Iteration 23675 Loss: 1.19316565990448\n",
      "Iteration 23676 Loss: 0.8699625730514526\n",
      "Iteration 23677 Loss: 0.8768450617790222\n",
      "Iteration 23678 Loss: 0.8362889289855957\n",
      "Iteration 23679 Loss: 1.431253433227539\n",
      "Iteration 23679 Loss: 1.1336606740951538\n",
      "Iteration 23680 Loss: 0.9486647844314575\n",
      "Iteration 23681 Loss: 1.509135127067566\n",
      "Iteration 23682 Loss: 0.9100942015647888\n",
      "Iteration 23683 Loss: 1.2665473222732544\n",
      "Iteration 23684 Loss: 1.1973844766616821\n",
      "Iteration 23685 Loss: 1.4865223169326782\n",
      "Iteration 23686 Loss: 0.9277441501617432\n",
      "Iteration 23687 Loss: 1.402631402015686\n",
      "Iteration 23688 Loss: 1.187882900238037\n",
      "Iteration 23689 Loss: 1.3151781558990479\n",
      "Iteration 23689 Loss: 1.2151786088943481\n",
      "Iteration 23690 Loss: 0.7612653970718384\n",
      "Iteration 23691 Loss: 0.8751959204673767\n",
      "Iteration 23692 Loss: 1.2453806400299072\n",
      "Iteration 23693 Loss: 0.8219344615936279\n",
      "Iteration 23694 Loss: 0.9422420859336853\n",
      "Iteration 23695 Loss: 1.0571287870407104\n",
      "Iteration 23696 Loss: 1.0520614385604858\n",
      "Iteration 23697 Loss: 1.1348545551300049\n",
      "Iteration 23698 Loss: 0.9041128754615784\n",
      "Iteration 23699 Loss: 1.1570212841033936\n",
      "Iteration 23699 Loss: 0.9951197504997253\n",
      "Iteration 23700 Loss: 1.3778481483459473\n",
      "Iteration 23701 Loss: 1.3664584159851074\n",
      "Iteration 23702 Loss: 1.5371090173721313\n",
      "Iteration 23703 Loss: 1.0601845979690552\n",
      "Iteration 23704 Loss: 1.0016013383865356\n",
      "Iteration 23705 Loss: 1.2060216665267944\n",
      "Iteration 23706 Loss: 1.007574439048767\n",
      "Iteration 23707 Loss: 0.9277624487876892\n",
      "Iteration 23708 Loss: 1.0355510711669922\n",
      "Iteration 23709 Loss: 1.3765946626663208\n",
      "Iteration 23709 Loss: 1.189670443534851\n",
      "Iteration 23710 Loss: 0.977554976940155\n",
      "Iteration 23711 Loss: 0.9826666712760925\n",
      "Iteration 23712 Loss: 1.2737380266189575\n",
      "Iteration 23713 Loss: 1.3324774503707886\n",
      "Iteration 23714 Loss: 1.055657148361206\n",
      "Iteration 23715 Loss: 0.9894803166389465\n",
      "Iteration 23716 Loss: 1.0674171447753906\n",
      "Iteration 23717 Loss: 1.1578142642974854\n",
      "Iteration 23718 Loss: 1.3508732318878174\n",
      "Iteration 23719 Loss: 1.021478295326233\n",
      "Iteration 23719 Loss: 1.1209156513214111\n",
      "Iteration 23720 Loss: 0.9243162870407104\n",
      "Iteration 23721 Loss: 1.085071325302124\n",
      "Iteration 23722 Loss: 1.018099308013916\n",
      "Iteration 23723 Loss: 1.0043643712997437\n",
      "Iteration 23724 Loss: 1.043268084526062\n",
      "Iteration 23725 Loss: 1.0950676202774048\n",
      "Iteration 23726 Loss: 1.234456181526184\n",
      "Iteration 23727 Loss: 0.9671465158462524\n",
      "Iteration 23728 Loss: 1.150941014289856\n",
      "Iteration 23729 Loss: 0.8301699161529541\n",
      "Iteration 23729 Loss: 1.0352901220321655\n",
      "Iteration 23730 Loss: 1.2462265491485596\n",
      "Iteration 23731 Loss: 0.9783349633216858\n",
      "Iteration 23732 Loss: 0.8839675188064575\n",
      "Iteration 23733 Loss: 0.9699795246124268\n",
      "Iteration 23734 Loss: 1.2241978645324707\n",
      "Iteration 23735 Loss: 1.056517481803894\n",
      "Iteration 23736 Loss: 1.4594910144805908\n",
      "Iteration 23737 Loss: 1.1532810926437378\n",
      "Iteration 23738 Loss: 1.504103183746338\n",
      "Iteration 23739 Loss: 1.4306532144546509\n",
      "Iteration 23739 Loss: 1.1906752586364746\n",
      "Iteration 23740 Loss: 1.1539185047149658\n",
      "Iteration 23741 Loss: 0.8820286989212036\n",
      "Iteration 23742 Loss: 1.0863415002822876\n",
      "Iteration 23743 Loss: 1.0633097887039185\n",
      "Iteration 23744 Loss: 0.8217387795448303\n",
      "Iteration 23745 Loss: 0.6886405944824219\n",
      "Iteration 23746 Loss: 0.9904894828796387\n",
      "Iteration 23747 Loss: 1.0852420330047607\n",
      "Iteration 23748 Loss: 1.131562352180481\n",
      "Iteration 23749 Loss: 1.2136949300765991\n",
      "Iteration 23749 Loss: 1.011696696281433\n",
      "Iteration 23750 Loss: 1.0413126945495605\n",
      "Iteration 23751 Loss: 1.4807353019714355\n",
      "Iteration 23752 Loss: 0.8188406825065613\n",
      "Iteration 23753 Loss: 1.1884651184082031\n",
      "Iteration 23754 Loss: 1.1603147983551025\n",
      "Iteration 23755 Loss: 0.905950665473938\n",
      "Iteration 23756 Loss: 1.1326007843017578\n",
      "Iteration 23757 Loss: 1.0364845991134644\n",
      "Iteration 23758 Loss: 0.8060320019721985\n",
      "Iteration 23759 Loss: 1.306872010231018\n",
      "Iteration 23759 Loss: 1.0877608060836792\n",
      "Iteration 23760 Loss: 1.1318422555923462\n",
      "Iteration 23761 Loss: 1.0857093334197998\n",
      "Iteration 23762 Loss: 1.0648905038833618\n",
      "Iteration 23763 Loss: 0.604034423828125\n",
      "Iteration 23764 Loss: 1.6852072477340698\n",
      "Iteration 23765 Loss: 0.8352128267288208\n",
      "Iteration 23766 Loss: 1.1187784671783447\n",
      "Iteration 23767 Loss: 0.9996915459632874\n",
      "Iteration 23768 Loss: 0.6469742655754089\n",
      "Iteration 23769 Loss: 0.9136630892753601\n",
      "Iteration 23769 Loss: 1.0086004734039307\n",
      "Iteration 23770 Loss: 0.9835296869277954\n",
      "Iteration 23771 Loss: 0.9331200122833252\n",
      "Iteration 23772 Loss: 0.8802984952926636\n",
      "Iteration 23773 Loss: 1.296960473060608\n",
      "Iteration 23774 Loss: 1.0378462076187134\n",
      "Iteration 23775 Loss: 0.9754236340522766\n",
      "Iteration 23776 Loss: 0.9927859306335449\n",
      "Iteration 23777 Loss: 1.0786126852035522\n",
      "Iteration 23778 Loss: 1.1504478454589844\n",
      "Iteration 23779 Loss: 1.2754616737365723\n",
      "Iteration 23779 Loss: 1.0604485273361206\n",
      "Iteration 23780 Loss: 0.7339027523994446\n",
      "Iteration 23781 Loss: 1.454093098640442\n",
      "Iteration 23782 Loss: 1.193995475769043\n",
      "Iteration 23783 Loss: 1.0544459819793701\n",
      "Iteration 23784 Loss: 1.3191447257995605\n",
      "Iteration 23785 Loss: 1.1331515312194824\n",
      "Iteration 23786 Loss: 1.4836770296096802\n",
      "Iteration 23787 Loss: 1.0026681423187256\n",
      "Iteration 23788 Loss: 1.409135341644287\n",
      "Iteration 23789 Loss: 0.9986103773117065\n",
      "Iteration 23789 Loss: 1.1782824993133545\n",
      "Iteration 23790 Loss: 0.7400052547454834\n",
      "Iteration 23791 Loss: 1.1181811094284058\n",
      "Iteration 23792 Loss: 0.9706780314445496\n",
      "Iteration 23793 Loss: 1.3032028675079346\n",
      "Iteration 23794 Loss: 1.034766435623169\n",
      "Iteration 23795 Loss: 1.0236737728118896\n",
      "Iteration 23796 Loss: 1.0056382417678833\n",
      "Iteration 23797 Loss: 0.8834976553916931\n",
      "Iteration 23798 Loss: 1.071434497833252\n",
      "Iteration 23799 Loss: 1.4624476432800293\n",
      "Iteration 23799 Loss: 1.0613524913787842\n",
      "Iteration 23800 Loss: 1.0931655168533325\n",
      "Iteration 23801 Loss: 1.0810999870300293\n",
      "Iteration 23802 Loss: 1.156693696975708\n",
      "Iteration 23803 Loss: 0.9817802906036377\n",
      "Iteration 23804 Loss: 1.2510569095611572\n",
      "Iteration 23805 Loss: 1.2372186183929443\n",
      "Iteration 23806 Loss: 1.4439902305603027\n",
      "Iteration 23807 Loss: 1.002267599105835\n",
      "Iteration 23808 Loss: 1.2445348501205444\n",
      "Iteration 23809 Loss: 1.3131777048110962\n",
      "Iteration 23809 Loss: 1.180498480796814\n",
      "Iteration 23810 Loss: 1.26252281665802\n",
      "Iteration 23811 Loss: 1.5050783157348633\n",
      "Iteration 23812 Loss: 1.037226915359497\n",
      "Iteration 23813 Loss: 1.3570605516433716\n",
      "Iteration 23814 Loss: 0.7765706181526184\n",
      "Iteration 23815 Loss: 1.2678509950637817\n",
      "Iteration 23816 Loss: 1.0228838920593262\n",
      "Iteration 23817 Loss: 1.2390131950378418\n",
      "Iteration 23818 Loss: 1.315251111984253\n",
      "Iteration 23819 Loss: 0.8525302410125732\n",
      "Iteration 23819 Loss: 1.1635987758636475\n",
      "Iteration 23820 Loss: 1.0650672912597656\n",
      "Iteration 23821 Loss: 1.8552112579345703\n",
      "Iteration 23822 Loss: 1.0975172519683838\n",
      "Iteration 23823 Loss: 1.2701478004455566\n",
      "Iteration 23824 Loss: 1.1354050636291504\n",
      "Iteration 23825 Loss: 1.0005168914794922\n",
      "Iteration 23826 Loss: 0.9129833579063416\n",
      "Iteration 23827 Loss: 1.020032525062561\n",
      "Iteration 23828 Loss: 1.3065813779830933\n",
      "Iteration 23829 Loss: 1.1678764820098877\n",
      "Iteration 23829 Loss: 1.183133840560913\n",
      "Iteration 23830 Loss: 1.083265781402588\n",
      "Iteration 23831 Loss: 0.8615230917930603\n",
      "Iteration 23832 Loss: 0.8425171971321106\n",
      "Iteration 23833 Loss: 0.7517728209495544\n",
      "Iteration 23834 Loss: 0.9948992729187012\n",
      "Iteration 23835 Loss: 1.6083858013153076\n",
      "Iteration 23836 Loss: 0.9426888823509216\n",
      "Iteration 23837 Loss: 1.0468149185180664\n",
      "Iteration 23838 Loss: 1.1077425479888916\n",
      "Iteration 23839 Loss: 1.1787528991699219\n",
      "Iteration 23839 Loss: 1.0418363809585571\n",
      "Iteration 23840 Loss: 1.3095604181289673\n",
      "Iteration 23841 Loss: 0.9788877367973328\n",
      "Iteration 23842 Loss: 1.4107764959335327\n",
      "Iteration 23843 Loss: 0.6124463677406311\n",
      "Iteration 23844 Loss: 1.2562389373779297\n",
      "Iteration 23845 Loss: 0.9949609041213989\n",
      "Iteration 23846 Loss: 0.7782933115959167\n",
      "Iteration 23847 Loss: 1.1117768287658691\n",
      "Iteration 23848 Loss: 0.9884074330329895\n",
      "Iteration 23849 Loss: 0.9871949553489685\n",
      "Iteration 23849 Loss: 1.0428543090820312\n",
      "Iteration 23850 Loss: 1.5348109006881714\n",
      "Iteration 23851 Loss: 1.0020203590393066\n",
      "Iteration 23852 Loss: 1.0709726810455322\n",
      "Iteration 23853 Loss: 1.129227638244629\n",
      "Iteration 23854 Loss: 1.4236036539077759\n",
      "Iteration 23855 Loss: 1.110257863998413\n",
      "Iteration 23856 Loss: 0.9770755767822266\n",
      "Iteration 23857 Loss: 1.1784632205963135\n",
      "Iteration 23858 Loss: 1.2085072994232178\n",
      "Iteration 23859 Loss: 1.203087329864502\n",
      "Iteration 23859 Loss: 1.1838027238845825\n",
      "Iteration 23860 Loss: 1.0427254438400269\n",
      "Iteration 23861 Loss: 1.0666866302490234\n",
      "Iteration 23862 Loss: 1.1958435773849487\n",
      "Iteration 23863 Loss: 0.9010114073753357\n",
      "Iteration 23864 Loss: 1.2923742532730103\n",
      "Iteration 23865 Loss: 1.0499215126037598\n",
      "Iteration 23866 Loss: 1.0124489068984985\n",
      "Iteration 23867 Loss: 0.8395288586616516\n",
      "Iteration 23868 Loss: 1.214279294013977\n",
      "Iteration 23869 Loss: 0.7631672620773315\n",
      "Iteration 23869 Loss: 1.0377987623214722\n",
      "Iteration 23870 Loss: 1.3728582859039307\n",
      "Iteration 23871 Loss: 1.1648324728012085\n",
      "Iteration 23872 Loss: 0.9039385914802551\n",
      "Iteration 23873 Loss: 1.1388705968856812\n",
      "Iteration 23874 Loss: 1.0742326974868774\n",
      "Iteration 23875 Loss: 0.8220754861831665\n",
      "Iteration 23876 Loss: 1.4123533964157104\n",
      "Iteration 23877 Loss: 1.0696635246276855\n",
      "Iteration 23878 Loss: 1.3401490449905396\n",
      "Iteration 23879 Loss: 1.1196494102478027\n",
      "Iteration 23879 Loss: 1.1418625116348267\n",
      "Iteration 23880 Loss: 1.1942124366760254\n",
      "Iteration 23881 Loss: 1.0760507583618164\n",
      "Iteration 23882 Loss: 1.1215728521347046\n",
      "Iteration 23883 Loss: 0.8606666922569275\n",
      "Iteration 23884 Loss: 1.0892229080200195\n",
      "Iteration 23885 Loss: 1.1119648218154907\n",
      "Iteration 23886 Loss: 1.2301411628723145\n",
      "Iteration 23887 Loss: 1.6035857200622559\n",
      "Iteration 23888 Loss: 0.9966345429420471\n",
      "Iteration 23889 Loss: 1.1096693277359009\n",
      "Iteration 23889 Loss: 1.1393722295761108\n",
      "Iteration 23890 Loss: 0.8619243502616882\n",
      "Iteration 23891 Loss: 1.1575658321380615\n",
      "Iteration 23892 Loss: 1.234910488128662\n",
      "Iteration 23893 Loss: 0.8260443210601807\n",
      "Iteration 23894 Loss: 1.250796914100647\n",
      "Iteration 23895 Loss: 1.0198427438735962\n",
      "Iteration 23896 Loss: 1.5339707136154175\n",
      "Iteration 23897 Loss: 1.080406904220581\n",
      "Iteration 23898 Loss: 1.5863858461380005\n",
      "Iteration 23899 Loss: 0.7075661420822144\n",
      "Iteration 23899 Loss: 1.125941514968872\n",
      "Iteration 23900 Loss: 1.1147416830062866\n",
      "Iteration 23901 Loss: 0.9152923226356506\n",
      "Iteration 23902 Loss: 0.7153215408325195\n",
      "Iteration 23903 Loss: 1.3996092081069946\n",
      "Iteration 23904 Loss: 1.2838373184204102\n",
      "Iteration 23905 Loss: 1.1127926111221313\n",
      "Iteration 23906 Loss: 1.385008454322815\n",
      "Iteration 23907 Loss: 1.122467279434204\n",
      "Iteration 23908 Loss: 1.211437702178955\n",
      "Iteration 23909 Loss: 1.0174381732940674\n",
      "Iteration 23909 Loss: 1.127794623374939\n",
      "Iteration 23910 Loss: 0.7448076605796814\n",
      "Iteration 23911 Loss: 0.8969757556915283\n",
      "Iteration 23912 Loss: 0.6916329860687256\n",
      "Iteration 23913 Loss: 0.846494197845459\n",
      "Iteration 23914 Loss: 1.1486495733261108\n",
      "Iteration 23915 Loss: 0.9108915328979492\n",
      "Iteration 23916 Loss: 1.3537911176681519\n",
      "Iteration 23917 Loss: 1.6988035440444946\n",
      "Iteration 23918 Loss: 1.0641863346099854\n",
      "Iteration 23919 Loss: 0.5520239472389221\n",
      "Iteration 23919 Loss: 0.9908256530761719\n",
      "Iteration 23920 Loss: 1.3225816488265991\n",
      "Iteration 23921 Loss: 0.9867688417434692\n",
      "Iteration 23922 Loss: 1.4140270948410034\n",
      "Iteration 23923 Loss: 0.9273921847343445\n",
      "Iteration 23924 Loss: 1.0802584886550903\n",
      "Iteration 23925 Loss: 1.0798460245132446\n",
      "Iteration 23926 Loss: 1.2541004419326782\n",
      "Iteration 23927 Loss: 1.1778303384780884\n",
      "Iteration 23928 Loss: 1.0673048496246338\n",
      "Iteration 23929 Loss: 0.9044604897499084\n",
      "Iteration 23929 Loss: 1.1214570999145508\n",
      "Iteration 23930 Loss: 1.1596171855926514\n",
      "Iteration 23931 Loss: 0.9783725738525391\n",
      "Iteration 23932 Loss: 0.5878417491912842\n",
      "Iteration 23933 Loss: 1.382297396659851\n",
      "Iteration 23934 Loss: 1.0128583908081055\n",
      "Iteration 23935 Loss: 0.8047149181365967\n",
      "Iteration 23936 Loss: 1.0520774126052856\n",
      "Iteration 23937 Loss: 0.8300225138664246\n",
      "Iteration 23938 Loss: 1.1404281854629517\n",
      "Iteration 23939 Loss: 0.8953261375427246\n",
      "Iteration 23939 Loss: 0.9843557476997375\n",
      "Iteration 23940 Loss: 1.064736247062683\n",
      "Iteration 23941 Loss: 0.7378926873207092\n",
      "Iteration 23942 Loss: 1.2875462770462036\n",
      "Iteration 23943 Loss: 0.9357921481132507\n",
      "Iteration 23944 Loss: 1.1662763357162476\n",
      "Iteration 23945 Loss: 1.0760173797607422\n",
      "Iteration 23946 Loss: 1.0384167432785034\n",
      "Iteration 23947 Loss: 1.1376127004623413\n",
      "Iteration 23948 Loss: 1.179246187210083\n",
      "Iteration 23949 Loss: 0.9878703355789185\n",
      "Iteration 23949 Loss: 1.0611406564712524\n",
      "Iteration 23950 Loss: 1.2567129135131836\n",
      "Iteration 23951 Loss: 1.0674574375152588\n",
      "Iteration 23952 Loss: 0.8477974534034729\n",
      "Iteration 23953 Loss: 0.752815842628479\n",
      "Iteration 23954 Loss: 0.8692089319229126\n",
      "Iteration 23955 Loss: 0.7293689250946045\n",
      "Iteration 23956 Loss: 1.309002161026001\n",
      "Iteration 23957 Loss: 1.0624557733535767\n",
      "Iteration 23958 Loss: 1.0519697666168213\n",
      "Iteration 23959 Loss: 1.2949879169464111\n",
      "Iteration 23959 Loss: 1.0241777896881104\n",
      "Iteration 23960 Loss: 1.0207258462905884\n",
      "Iteration 23961 Loss: 1.1950416564941406\n",
      "Iteration 23962 Loss: 1.0748257637023926\n",
      "Iteration 23963 Loss: 1.5716831684112549\n",
      "Iteration 23964 Loss: 1.1648104190826416\n",
      "Iteration 23965 Loss: 0.7565209269523621\n",
      "Iteration 23966 Loss: 1.0039491653442383\n",
      "Iteration 23967 Loss: 1.2645447254180908\n",
      "Iteration 23968 Loss: 0.8364447951316833\n",
      "Iteration 23969 Loss: 1.1826131343841553\n",
      "Iteration 23969 Loss: 1.1071159839630127\n",
      "Iteration 23970 Loss: 1.1908636093139648\n",
      "Iteration 23971 Loss: 1.139575481414795\n",
      "Iteration 23972 Loss: 1.2141164541244507\n",
      "Iteration 23973 Loss: 1.1696127653121948\n",
      "Iteration 23974 Loss: 1.4676952362060547\n",
      "Iteration 23975 Loss: 1.409815788269043\n",
      "Iteration 23976 Loss: 1.170445203781128\n",
      "Iteration 23977 Loss: 1.274672508239746\n",
      "Iteration 23978 Loss: 1.1538976430892944\n",
      "Iteration 23979 Loss: 0.9740347266197205\n",
      "Iteration 23979 Loss: 1.216472864151001\n",
      "Iteration 23980 Loss: 0.9976075887680054\n",
      "Iteration 23981 Loss: 1.3533296585083008\n",
      "Iteration 23982 Loss: 1.0209074020385742\n",
      "Iteration 23983 Loss: 0.8339994549751282\n",
      "Iteration 23984 Loss: 1.2139486074447632\n",
      "Iteration 23985 Loss: 1.1504493951797485\n",
      "Iteration 23986 Loss: 1.0967717170715332\n",
      "Iteration 23987 Loss: 0.9002799391746521\n",
      "Iteration 23988 Loss: 1.1835476160049438\n",
      "Iteration 23989 Loss: 1.1885453462600708\n",
      "Iteration 23989 Loss: 1.0939385890960693\n",
      "Iteration 23990 Loss: 0.9883724451065063\n",
      "Iteration 23991 Loss: 1.2918440103530884\n",
      "Iteration 23992 Loss: 0.8815470337867737\n",
      "Iteration 23993 Loss: 1.026595115661621\n",
      "Iteration 23994 Loss: 0.9336742162704468\n",
      "Iteration 23995 Loss: 0.9010420441627502\n",
      "Iteration 23996 Loss: 1.2553279399871826\n",
      "Iteration 23997 Loss: 0.978945791721344\n",
      "Iteration 23998 Loss: 0.992135763168335\n",
      "Iteration 23999 Loss: 1.2611688375473022\n",
      "Iteration 23999 Loss: 1.0510653257369995\n",
      "Iteration 24000 Loss: 1.1947295665740967\n",
      "Iteration 24001 Loss: 0.8947595953941345\n",
      "Iteration 24002 Loss: 1.2558640241622925\n",
      "Iteration 24003 Loss: 1.4352799654006958\n",
      "Iteration 24004 Loss: 1.1295130252838135\n",
      "Iteration 24005 Loss: 0.7850067615509033\n",
      "Iteration 24006 Loss: 0.8876769542694092\n",
      "Iteration 24007 Loss: 1.0500752925872803\n",
      "Iteration 24008 Loss: 0.8695746064186096\n",
      "Iteration 24009 Loss: 0.9102964997291565\n",
      "Iteration 24009 Loss: 1.0412776470184326\n",
      "Iteration 24010 Loss: 1.2437671422958374\n",
      "Iteration 24011 Loss: 1.3301869630813599\n",
      "Iteration 24012 Loss: 0.9453102946281433\n",
      "Iteration 24013 Loss: 1.2186957597732544\n",
      "Iteration 24014 Loss: 1.4965317249298096\n",
      "Iteration 24015 Loss: 1.0099087953567505\n",
      "Iteration 24016 Loss: 0.8332286477088928\n",
      "Iteration 24017 Loss: 0.8391850590705872\n",
      "Iteration 24018 Loss: 1.0797017812728882\n",
      "Iteration 24019 Loss: 1.2084953784942627\n",
      "Iteration 24019 Loss: 1.120501160621643\n",
      "Iteration 24020 Loss: 0.9529423713684082\n",
      "Iteration 24021 Loss: 1.2184183597564697\n",
      "Iteration 24022 Loss: 1.1139001846313477\n",
      "Iteration 24023 Loss: 0.969682514667511\n",
      "Iteration 24024 Loss: 0.7265397310256958\n",
      "Iteration 24025 Loss: 1.0751465559005737\n",
      "Iteration 24026 Loss: 0.6965085864067078\n",
      "Iteration 24027 Loss: 1.269784688949585\n",
      "Iteration 24028 Loss: 1.4644601345062256\n",
      "Iteration 24029 Loss: 1.0423334836959839\n",
      "Iteration 24029 Loss: 1.052971601486206\n",
      "Iteration 24030 Loss: 0.9582740068435669\n",
      "Iteration 24031 Loss: 1.212009072303772\n",
      "Iteration 24032 Loss: 1.2943463325500488\n",
      "Iteration 24033 Loss: 0.6344621181488037\n",
      "Iteration 24034 Loss: 0.9262994527816772\n",
      "Iteration 24035 Loss: 0.8188748359680176\n",
      "Iteration 24036 Loss: 1.3374240398406982\n",
      "Iteration 24037 Loss: 0.9321842789649963\n",
      "Iteration 24038 Loss: 1.14149808883667\n",
      "Iteration 24039 Loss: 1.1053093671798706\n",
      "Iteration 24039 Loss: 1.0360682010650635\n",
      "Iteration 24040 Loss: 0.9968316555023193\n",
      "Iteration 24041 Loss: 1.3490155935287476\n",
      "Iteration 24042 Loss: 1.224931001663208\n",
      "Iteration 24043 Loss: 1.1764978170394897\n",
      "Iteration 24044 Loss: 0.9337154626846313\n",
      "Iteration 24045 Loss: 1.176520586013794\n",
      "Iteration 24046 Loss: 1.1021548509597778\n",
      "Iteration 24047 Loss: 0.9915534853935242\n",
      "Iteration 24048 Loss: 0.8468542098999023\n",
      "Iteration 24049 Loss: 1.1133085489273071\n",
      "Iteration 24049 Loss: 1.091138243675232\n",
      "Iteration 24050 Loss: 1.2514595985412598\n",
      "Iteration 24051 Loss: 1.2448904514312744\n",
      "Iteration 24052 Loss: 1.0662026405334473\n",
      "Iteration 24053 Loss: 0.6838870048522949\n",
      "Iteration 24054 Loss: 1.212425947189331\n",
      "Iteration 24055 Loss: 1.0550118684768677\n",
      "Iteration 24056 Loss: 1.2731484174728394\n",
      "Iteration 24057 Loss: 1.4131946563720703\n",
      "Iteration 24058 Loss: 0.712399423122406\n",
      "Iteration 24059 Loss: 1.2037348747253418\n",
      "Iteration 24059 Loss: 1.111635446548462\n",
      "Iteration 24060 Loss: 1.200990915298462\n",
      "Iteration 24061 Loss: 1.0801362991333008\n",
      "Iteration 24062 Loss: 0.9120147228240967\n",
      "Iteration 24063 Loss: 1.1644632816314697\n",
      "Iteration 24064 Loss: 1.6047216653823853\n",
      "Iteration 24065 Loss: 1.2943875789642334\n",
      "Iteration 24066 Loss: 1.1645634174346924\n",
      "Iteration 24067 Loss: 1.2882779836654663\n",
      "Iteration 24068 Loss: 0.9903531670570374\n",
      "Iteration 24069 Loss: 0.791892945766449\n",
      "Iteration 24069 Loss: 1.1491801738739014\n",
      "Iteration 24070 Loss: 1.1072417497634888\n",
      "Iteration 24071 Loss: 1.088015079498291\n",
      "Iteration 24072 Loss: 1.3242371082305908\n",
      "Iteration 24073 Loss: 1.0201478004455566\n",
      "Iteration 24074 Loss: 1.3512704372406006\n",
      "Iteration 24075 Loss: 1.0536150932312012\n",
      "Iteration 24076 Loss: 1.146196722984314\n",
      "Iteration 24077 Loss: 0.9073097109794617\n",
      "Iteration 24078 Loss: 0.9556533098220825\n",
      "Iteration 24079 Loss: 1.0590306520462036\n",
      "Iteration 24079 Loss: 1.1012717485427856\n",
      "Iteration 24080 Loss: 0.9730197191238403\n",
      "Iteration 24081 Loss: 1.2913159132003784\n",
      "Iteration 24082 Loss: 0.932359516620636\n",
      "Iteration 24083 Loss: 1.1244359016418457\n",
      "Iteration 24084 Loss: 1.2152897119522095\n",
      "Iteration 24085 Loss: 1.0485976934432983\n",
      "Iteration 24086 Loss: 0.7706276178359985\n",
      "Iteration 24087 Loss: 1.3761152029037476\n",
      "Iteration 24088 Loss: 1.0649309158325195\n",
      "Iteration 24089 Loss: 0.7616465091705322\n",
      "Iteration 24089 Loss: 1.0558339357376099\n",
      "Iteration 24090 Loss: 0.8539021015167236\n",
      "Iteration 24091 Loss: 1.1480050086975098\n",
      "Iteration 24092 Loss: 1.2427855730056763\n",
      "Iteration 24093 Loss: 1.0010687112808228\n",
      "Iteration 24094 Loss: 1.3136869668960571\n",
      "Iteration 24095 Loss: 1.000001311302185\n",
      "Iteration 24096 Loss: 1.0104453563690186\n",
      "Iteration 24097 Loss: 0.9449196457862854\n",
      "Iteration 24098 Loss: 0.7279350757598877\n",
      "Iteration 24099 Loss: 1.2608386278152466\n",
      "Iteration 24099 Loss: 1.050358772277832\n",
      "Iteration 24100 Loss: 0.990602433681488\n",
      "Iteration 24101 Loss: 1.0159215927124023\n",
      "Iteration 24102 Loss: 1.1217334270477295\n",
      "Iteration 24103 Loss: 1.160843849182129\n",
      "Iteration 24104 Loss: 0.730375349521637\n",
      "Iteration 24105 Loss: 1.0839276313781738\n",
      "Iteration 24106 Loss: 0.9847766757011414\n",
      "Iteration 24107 Loss: 1.1189616918563843\n",
      "Iteration 24108 Loss: 1.151798129081726\n",
      "Iteration 24109 Loss: 1.241837739944458\n",
      "Iteration 24109 Loss: 1.0600777864456177\n",
      "Iteration 24110 Loss: 1.491963505744934\n",
      "Iteration 24111 Loss: 0.8804565072059631\n",
      "Iteration 24112 Loss: 0.9968069195747375\n",
      "Iteration 24113 Loss: 0.9745138883590698\n",
      "Iteration 24114 Loss: 0.9902263879776001\n",
      "Iteration 24115 Loss: 1.348771095275879\n",
      "Iteration 24116 Loss: 1.3796262741088867\n",
      "Iteration 24117 Loss: 1.3317712545394897\n",
      "Iteration 24118 Loss: 1.0597506761550903\n",
      "Iteration 24119 Loss: 0.82391756772995\n",
      "Iteration 24119 Loss: 1.1277803182601929\n",
      "Iteration 24120 Loss: 1.6782339811325073\n",
      "Iteration 24121 Loss: 1.0097841024398804\n",
      "Iteration 24122 Loss: 1.1033142805099487\n",
      "Iteration 24123 Loss: 1.5644745826721191\n",
      "Iteration 24124 Loss: 0.9951542615890503\n",
      "Iteration 24125 Loss: 0.7083200216293335\n",
      "Iteration 24126 Loss: 1.0204617977142334\n",
      "Iteration 24127 Loss: 1.0840080976486206\n",
      "Iteration 24128 Loss: 1.4545865058898926\n",
      "Iteration 24129 Loss: 1.0592714548110962\n",
      "Iteration 24129 Loss: 1.167760968208313\n",
      "Iteration 24130 Loss: 0.7792043089866638\n",
      "Iteration 24131 Loss: 0.8871678113937378\n",
      "Iteration 24132 Loss: 0.9962366223335266\n",
      "Iteration 24133 Loss: 1.1632369756698608\n",
      "Iteration 24134 Loss: 1.0536437034606934\n",
      "Iteration 24135 Loss: 1.253392219543457\n",
      "Iteration 24136 Loss: 1.1589362621307373\n",
      "Iteration 24137 Loss: 0.9554025530815125\n",
      "Iteration 24138 Loss: 1.0722341537475586\n",
      "Iteration 24139 Loss: 0.9467396140098572\n",
      "Iteration 24139 Loss: 1.0266194343566895\n",
      "Iteration 24140 Loss: 1.3434163331985474\n",
      "Iteration 24141 Loss: 1.0012755393981934\n",
      "Iteration 24142 Loss: 1.1413323879241943\n",
      "Iteration 24143 Loss: 1.1028884649276733\n",
      "Iteration 24144 Loss: 1.2479103803634644\n",
      "Iteration 24145 Loss: 1.0333024263381958\n",
      "Iteration 24146 Loss: 1.1183875799179077\n",
      "Iteration 24147 Loss: 0.8266854882240295\n",
      "Iteration 24148 Loss: 0.9832313060760498\n",
      "Iteration 24149 Loss: 1.0836142301559448\n",
      "Iteration 24149 Loss: 1.0882045030593872\n",
      "Iteration 24150 Loss: 0.9055094718933105\n",
      "Iteration 24151 Loss: 1.1661908626556396\n",
      "Iteration 24152 Loss: 1.042494773864746\n",
      "Iteration 24153 Loss: 1.2591590881347656\n",
      "Iteration 24154 Loss: 0.9128908514976501\n",
      "Iteration 24155 Loss: 0.9767865538597107\n",
      "Iteration 24156 Loss: 1.025707721710205\n",
      "Iteration 24157 Loss: 1.1295095682144165\n",
      "Iteration 24158 Loss: 0.8326091766357422\n",
      "Iteration 24159 Loss: 1.2282339334487915\n",
      "Iteration 24159 Loss: 1.0479092597961426\n",
      "Iteration 24160 Loss: 1.024253249168396\n",
      "Iteration 24161 Loss: 0.7820712924003601\n",
      "Iteration 24162 Loss: 1.2498674392700195\n",
      "Iteration 24163 Loss: 1.1103110313415527\n",
      "Iteration 24164 Loss: 1.5509134531021118\n",
      "Iteration 24165 Loss: 1.271108627319336\n",
      "Iteration 24166 Loss: 1.0738354921340942\n",
      "Iteration 24167 Loss: 1.0883411169052124\n",
      "Iteration 24168 Loss: 1.312216877937317\n",
      "Iteration 24169 Loss: 1.154440999031067\n",
      "Iteration 24169 Loss: 1.1617358922958374\n",
      "Iteration 24170 Loss: 1.1613645553588867\n",
      "Iteration 24171 Loss: 1.0335687398910522\n",
      "Iteration 24172 Loss: 0.9072003960609436\n",
      "Iteration 24173 Loss: 1.0741851329803467\n",
      "Iteration 24174 Loss: 0.5437793135643005\n",
      "Iteration 24175 Loss: 1.1788744926452637\n",
      "Iteration 24176 Loss: 1.4968292713165283\n",
      "Iteration 24177 Loss: 0.8660041689872742\n",
      "Iteration 24178 Loss: 1.1322699785232544\n",
      "Iteration 24179 Loss: 1.1718941926956177\n",
      "Iteration 24179 Loss: 1.0565969944000244\n",
      "Iteration 24180 Loss: 0.6619259119033813\n",
      "Iteration 24181 Loss: 1.2244731187820435\n",
      "Iteration 24182 Loss: 1.3631330728530884\n",
      "Iteration 24183 Loss: 0.9882996082305908\n",
      "Iteration 24184 Loss: 1.0743930339813232\n",
      "Iteration 24185 Loss: 1.2526715993881226\n",
      "Iteration 24186 Loss: 1.369740605354309\n",
      "Iteration 24187 Loss: 0.9855782985687256\n",
      "Iteration 24188 Loss: 0.8829891085624695\n",
      "Iteration 24189 Loss: 1.4697363376617432\n",
      "Iteration 24189 Loss: 1.1272940635681152\n",
      "Iteration 24190 Loss: 1.0787900686264038\n",
      "Iteration 24191 Loss: 1.0798143148422241\n",
      "Iteration 24192 Loss: 1.176939845085144\n",
      "Iteration 24193 Loss: 0.7559586763381958\n",
      "Iteration 24194 Loss: 0.9192942976951599\n",
      "Iteration 24195 Loss: 0.9757437705993652\n",
      "Iteration 24196 Loss: 1.1554735898971558\n",
      "Iteration 24197 Loss: 1.4390859603881836\n",
      "Iteration 24198 Loss: 1.004340648651123\n",
      "Iteration 24199 Loss: 1.4267475605010986\n",
      "Iteration 24199 Loss: 1.1012189388275146\n",
      "Iteration 24200 Loss: 0.8796889185905457\n",
      "Iteration 24201 Loss: 1.1557108163833618\n",
      "Iteration 24202 Loss: 1.0693974494934082\n",
      "Iteration 24203 Loss: 1.2965059280395508\n",
      "Iteration 24204 Loss: 1.2002971172332764\n",
      "Iteration 24205 Loss: 1.518554925918579\n",
      "Iteration 24206 Loss: 1.046774983406067\n",
      "Iteration 24207 Loss: 1.4243018627166748\n",
      "Iteration 24208 Loss: 1.15215003490448\n",
      "Iteration 24209 Loss: 1.6905555725097656\n",
      "Iteration 24209 Loss: 1.2433937788009644\n",
      "Iteration 24210 Loss: 0.9190772175788879\n",
      "Iteration 24211 Loss: 1.0335949659347534\n",
      "Iteration 24212 Loss: 0.8981148600578308\n",
      "Iteration 24213 Loss: 1.0077873468399048\n",
      "Iteration 24214 Loss: 1.206061840057373\n",
      "Iteration 24215 Loss: 1.3166797161102295\n",
      "Iteration 24216 Loss: 1.169980525970459\n",
      "Iteration 24217 Loss: 1.3520383834838867\n",
      "Iteration 24218 Loss: 0.9539514183998108\n",
      "Iteration 24219 Loss: 1.0605878829956055\n",
      "Iteration 24219 Loss: 1.0917874574661255\n",
      "Iteration 24220 Loss: 1.0238436460494995\n",
      "Iteration 24221 Loss: 1.0517427921295166\n",
      "Iteration 24222 Loss: 1.1249972581863403\n",
      "Iteration 24223 Loss: 0.9785348773002625\n",
      "Iteration 24224 Loss: 1.1722054481506348\n",
      "Iteration 24225 Loss: 1.319250226020813\n",
      "Iteration 24226 Loss: 1.3641785383224487\n",
      "Iteration 24227 Loss: 1.098083734512329\n",
      "Iteration 24228 Loss: 1.20943284034729\n",
      "Iteration 24229 Loss: 1.4708133935928345\n",
      "Iteration 24229 Loss: 1.1813082695007324\n",
      "Iteration 24230 Loss: 0.8596228957176208\n",
      "Iteration 24231 Loss: 1.118349552154541\n",
      "Iteration 24232 Loss: 1.0440014600753784\n",
      "Iteration 24233 Loss: 1.0140630006790161\n",
      "Iteration 24234 Loss: 1.269695520401001\n",
      "Iteration 24235 Loss: 1.0447869300842285\n",
      "Iteration 24236 Loss: 1.0501692295074463\n",
      "Iteration 24237 Loss: 1.0262160301208496\n",
      "Iteration 24238 Loss: 1.1107540130615234\n",
      "Iteration 24239 Loss: 1.2973663806915283\n",
      "Iteration 24239 Loss: 1.0835025310516357\n",
      "Iteration 24240 Loss: 0.6829165816307068\n",
      "Iteration 24241 Loss: 0.8932381868362427\n",
      "Iteration 24242 Loss: 1.206019401550293\n",
      "Iteration 24243 Loss: 0.7341896295547485\n",
      "Iteration 24244 Loss: 0.9000980854034424\n",
      "Iteration 24245 Loss: 1.0515073537826538\n",
      "Iteration 24246 Loss: 0.6066474318504333\n",
      "Iteration 24247 Loss: 1.0976810455322266\n",
      "Iteration 24248 Loss: 1.3208333253860474\n",
      "Iteration 24249 Loss: 0.7647325396537781\n",
      "Iteration 24249 Loss: 0.9257863163948059\n",
      "Iteration 24250 Loss: 1.1606260538101196\n",
      "Iteration 24251 Loss: 1.0410616397857666\n",
      "Iteration 24252 Loss: 1.0028345584869385\n",
      "Iteration 24253 Loss: 0.7406527996063232\n",
      "Iteration 24254 Loss: 1.318504810333252\n",
      "Iteration 24255 Loss: 1.1096457242965698\n",
      "Iteration 24256 Loss: 0.9644743204116821\n",
      "Iteration 24257 Loss: 1.1238861083984375\n",
      "Iteration 24258 Loss: 1.131691575050354\n",
      "Iteration 24259 Loss: 1.2395023107528687\n",
      "Iteration 24259 Loss: 1.0832879543304443\n",
      "Iteration 24260 Loss: 1.2849594354629517\n",
      "Iteration 24261 Loss: 0.9742305874824524\n",
      "Iteration 24262 Loss: 1.0165371894836426\n",
      "Iteration 24263 Loss: 1.017208456993103\n",
      "Iteration 24264 Loss: 0.9732890725135803\n",
      "Iteration 24265 Loss: 0.9997677206993103\n",
      "Iteration 24266 Loss: 1.115189790725708\n",
      "Iteration 24267 Loss: 1.2431696653366089\n",
      "Iteration 24268 Loss: 1.2991960048675537\n",
      "Iteration 24269 Loss: 0.7459412813186646\n",
      "Iteration 24269 Loss: 1.0669488906860352\n",
      "Iteration 24270 Loss: 0.8122642040252686\n",
      "Iteration 24271 Loss: 0.9019253253936768\n",
      "Iteration 24272 Loss: 1.1207300424575806\n",
      "Iteration 24273 Loss: 0.853941023349762\n",
      "Iteration 24274 Loss: 1.1727142333984375\n",
      "Iteration 24275 Loss: 1.2291959524154663\n",
      "Iteration 24276 Loss: 1.2755166292190552\n",
      "Iteration 24277 Loss: 1.0934574604034424\n",
      "Iteration 24278 Loss: 0.9182025194168091\n",
      "Iteration 24279 Loss: 0.9909337759017944\n",
      "Iteration 24279 Loss: 1.0368881225585938\n",
      "Iteration 24280 Loss: 0.7619824409484863\n",
      "Iteration 24281 Loss: 1.004220962524414\n",
      "Iteration 24282 Loss: 1.2011091709136963\n",
      "Iteration 24283 Loss: 1.0742435455322266\n",
      "Iteration 24284 Loss: 0.7786785960197449\n",
      "Iteration 24285 Loss: 1.2285823822021484\n",
      "Iteration 24286 Loss: 0.9225707054138184\n",
      "Iteration 24287 Loss: 1.096132755279541\n",
      "Iteration 24288 Loss: 0.9861282110214233\n",
      "Iteration 24289 Loss: 1.213192343711853\n",
      "Iteration 24289 Loss: 1.026684045791626\n",
      "Iteration 24290 Loss: 1.0397849082946777\n",
      "Iteration 24291 Loss: 1.399426817893982\n",
      "Iteration 24292 Loss: 0.8319007754325867\n",
      "Iteration 24293 Loss: 0.9453452825546265\n",
      "Iteration 24294 Loss: 0.9149556756019592\n",
      "Iteration 24295 Loss: 1.0598335266113281\n",
      "Iteration 24296 Loss: 1.0147989988327026\n",
      "Iteration 24297 Loss: 1.1285163164138794\n",
      "Iteration 24298 Loss: 1.1268707513809204\n",
      "Iteration 24299 Loss: 1.0588343143463135\n",
      "Iteration 24299 Loss: 1.0520267486572266\n",
      "Iteration 24300 Loss: 1.3465207815170288\n",
      "Iteration 24301 Loss: 1.2791041135787964\n",
      "Iteration 24302 Loss: 1.412076473236084\n",
      "Iteration 24303 Loss: 0.9873140454292297\n",
      "Iteration 24304 Loss: 1.0727226734161377\n",
      "Iteration 24305 Loss: 1.243267297744751\n",
      "Iteration 24306 Loss: 0.8416983485221863\n",
      "Iteration 24307 Loss: 1.064940094947815\n",
      "Iteration 24308 Loss: 1.0465039014816284\n",
      "Iteration 24309 Loss: 1.1596078872680664\n",
      "Iteration 24309 Loss: 1.1453756093978882\n",
      "Iteration 24310 Loss: 1.289238691329956\n",
      "Iteration 24311 Loss: 0.8881424069404602\n",
      "Iteration 24312 Loss: 1.4253714084625244\n",
      "Iteration 24313 Loss: 0.840562105178833\n",
      "Iteration 24314 Loss: 1.3315391540527344\n",
      "Iteration 24315 Loss: 0.9900175333023071\n",
      "Iteration 24316 Loss: 1.3209075927734375\n",
      "Iteration 24317 Loss: 1.2245500087738037\n",
      "Iteration 24318 Loss: 1.460984230041504\n",
      "Iteration 24319 Loss: 0.8528499603271484\n",
      "Iteration 24319 Loss: 1.1624163389205933\n",
      "Iteration 24320 Loss: 0.8190995454788208\n",
      "Iteration 24321 Loss: 0.8231385350227356\n",
      "Iteration 24322 Loss: 1.2152438163757324\n",
      "Iteration 24323 Loss: 1.1276379823684692\n",
      "Iteration 24324 Loss: 1.331269383430481\n",
      "Iteration 24325 Loss: 1.1071113348007202\n",
      "Iteration 24326 Loss: 1.0026757717132568\n",
      "Iteration 24327 Loss: 1.2697073221206665\n",
      "Iteration 24328 Loss: 1.0847469568252563\n",
      "Iteration 24329 Loss: 0.6967033743858337\n",
      "Iteration 24329 Loss: 1.0477334260940552\n",
      "Iteration 24330 Loss: 0.9334931969642639\n",
      "Iteration 24331 Loss: 1.006266474723816\n",
      "Iteration 24332 Loss: 1.437902808189392\n",
      "Iteration 24333 Loss: 1.0307469367980957\n",
      "Iteration 24334 Loss: 0.8083693385124207\n",
      "Iteration 24335 Loss: 1.0273302793502808\n",
      "Iteration 24336 Loss: 1.033104419708252\n",
      "Iteration 24337 Loss: 0.9137347340583801\n",
      "Iteration 24338 Loss: 1.0641733407974243\n",
      "Iteration 24339 Loss: 0.7558916807174683\n",
      "Iteration 24339 Loss: 1.0011012554168701\n",
      "Iteration 24340 Loss: 1.3058278560638428\n",
      "Iteration 24341 Loss: 0.8827703595161438\n",
      "Iteration 24342 Loss: 0.8508966565132141\n",
      "Iteration 24343 Loss: 1.2106897830963135\n",
      "Iteration 24344 Loss: 1.070948600769043\n",
      "Iteration 24345 Loss: 0.7295716404914856\n",
      "Iteration 24346 Loss: 0.6810378432273865\n",
      "Iteration 24347 Loss: 1.1974334716796875\n",
      "Iteration 24348 Loss: 0.7312154173851013\n",
      "Iteration 24349 Loss: 0.7087464928627014\n",
      "Iteration 24349 Loss: 0.936913788318634\n",
      "Iteration 24350 Loss: 1.3440656661987305\n",
      "Iteration 24351 Loss: 1.2189635038375854\n",
      "Iteration 24352 Loss: 1.2585457563400269\n",
      "Iteration 24353 Loss: 1.1337130069732666\n",
      "Iteration 24354 Loss: 0.8796693682670593\n",
      "Iteration 24355 Loss: 1.0300934314727783\n",
      "Iteration 24356 Loss: 1.0107303857803345\n",
      "Iteration 24357 Loss: 1.1616771221160889\n",
      "Iteration 24358 Loss: 1.2363276481628418\n",
      "Iteration 24359 Loss: 1.0776338577270508\n",
      "Iteration 24359 Loss: 1.1351420879364014\n",
      "Iteration 24360 Loss: 1.220219612121582\n",
      "Iteration 24361 Loss: 1.0953768491744995\n",
      "Iteration 24362 Loss: 0.7987837195396423\n",
      "Iteration 24363 Loss: 0.9328014254570007\n",
      "Iteration 24364 Loss: 0.9413686394691467\n",
      "Iteration 24365 Loss: 1.0163599252700806\n",
      "Iteration 24366 Loss: 1.0816537141799927\n",
      "Iteration 24367 Loss: 1.1934393644332886\n",
      "Iteration 24368 Loss: 1.1163792610168457\n",
      "Iteration 24369 Loss: 1.3450580835342407\n",
      "Iteration 24369 Loss: 1.0741441249847412\n",
      "Iteration 24370 Loss: 1.0176804065704346\n",
      "Iteration 24371 Loss: 0.9284972548484802\n",
      "Iteration 24372 Loss: 0.9439679384231567\n",
      "Iteration 24373 Loss: 0.9246294498443604\n",
      "Iteration 24374 Loss: 0.8227242231369019\n",
      "Iteration 24375 Loss: 1.1555373668670654\n",
      "Iteration 24376 Loss: 0.9515762329101562\n",
      "Iteration 24377 Loss: 1.1636929512023926\n",
      "Iteration 24378 Loss: 1.0191090106964111\n",
      "Iteration 24379 Loss: 1.2367918491363525\n",
      "Iteration 24379 Loss: 1.0164207220077515\n",
      "Iteration 24380 Loss: 1.0180277824401855\n",
      "Iteration 24381 Loss: 0.9513622522354126\n",
      "Iteration 24382 Loss: 1.4420490264892578\n",
      "Iteration 24383 Loss: 0.7952793836593628\n",
      "Iteration 24384 Loss: 1.165311574935913\n",
      "Iteration 24385 Loss: 1.360483169555664\n",
      "Iteration 24386 Loss: 0.9318254590034485\n",
      "Iteration 24387 Loss: 1.27520751953125\n",
      "Iteration 24388 Loss: 1.2232708930969238\n",
      "Iteration 24389 Loss: 1.4042131900787354\n",
      "Iteration 24389 Loss: 1.1567031145095825\n",
      "Iteration 24390 Loss: 1.1155085563659668\n",
      "Iteration 24391 Loss: 1.2968722581863403\n",
      "Iteration 24392 Loss: 1.127299427986145\n",
      "Iteration 24393 Loss: 0.7487836480140686\n",
      "Iteration 24394 Loss: 1.0422831773757935\n",
      "Iteration 24395 Loss: 1.1060832738876343\n",
      "Iteration 24396 Loss: 1.1941397190093994\n",
      "Iteration 24397 Loss: 0.7470824718475342\n",
      "Iteration 24398 Loss: 0.7823123931884766\n",
      "Iteration 24399 Loss: 1.0590776205062866\n",
      "Iteration 24399 Loss: 1.021944284439087\n",
      "Iteration 24400 Loss: 1.3384665250778198\n",
      "Iteration 24401 Loss: 0.9725965261459351\n",
      "Iteration 24402 Loss: 1.2684589624404907\n",
      "Iteration 24403 Loss: 1.1637895107269287\n",
      "Iteration 24404 Loss: 0.7543875575065613\n",
      "Iteration 24405 Loss: 0.9067724943161011\n",
      "Iteration 24406 Loss: 1.1676006317138672\n",
      "Iteration 24407 Loss: 0.8179576396942139\n",
      "Iteration 24408 Loss: 1.0427358150482178\n",
      "Iteration 24409 Loss: 1.0176901817321777\n",
      "Iteration 24409 Loss: 1.0450456142425537\n",
      "Iteration 24410 Loss: 1.0158666372299194\n",
      "Iteration 24411 Loss: 1.3868986368179321\n",
      "Iteration 24412 Loss: 1.0368815660476685\n",
      "Iteration 24413 Loss: 1.2002495527267456\n",
      "Iteration 24414 Loss: 0.9962384700775146\n",
      "Iteration 24415 Loss: 1.0855674743652344\n",
      "Iteration 24416 Loss: 1.1145838499069214\n",
      "Iteration 24417 Loss: 1.4104353189468384\n",
      "Iteration 24418 Loss: 1.2499430179595947\n",
      "Iteration 24419 Loss: 1.0536444187164307\n",
      "Iteration 24419 Loss: 1.1550309658050537\n",
      "Iteration 24420 Loss: 0.9250501990318298\n",
      "Iteration 24421 Loss: 0.8932787179946899\n",
      "Iteration 24422 Loss: 1.0024523735046387\n",
      "Iteration 24423 Loss: 1.1166441440582275\n",
      "Iteration 24424 Loss: 1.214098334312439\n",
      "Iteration 24425 Loss: 1.2064619064331055\n",
      "Iteration 24426 Loss: 1.3480958938598633\n",
      "Iteration 24427 Loss: 1.1885395050048828\n",
      "Iteration 24428 Loss: 0.7712879776954651\n",
      "Iteration 24429 Loss: 1.276320219039917\n",
      "Iteration 24429 Loss: 1.094222903251648\n",
      "Iteration 24430 Loss: 0.7900581359863281\n",
      "Iteration 24431 Loss: 1.1043004989624023\n",
      "Iteration 24432 Loss: 1.238224744796753\n",
      "Iteration 24433 Loss: 1.1331781148910522\n",
      "Iteration 24434 Loss: 0.8528813719749451\n",
      "Iteration 24435 Loss: 1.3507815599441528\n",
      "Iteration 24436 Loss: 1.4413461685180664\n",
      "Iteration 24437 Loss: 1.390557885169983\n",
      "Iteration 24438 Loss: 1.301581621170044\n",
      "Iteration 24439 Loss: 0.8798943758010864\n",
      "Iteration 24439 Loss: 1.1482805013656616\n",
      "Iteration 24440 Loss: 1.2487552165985107\n",
      "Iteration 24441 Loss: 1.4240756034851074\n",
      "Iteration 24442 Loss: 0.7423968315124512\n",
      "Iteration 24443 Loss: 1.1670678853988647\n",
      "Iteration 24444 Loss: 1.1337424516677856\n",
      "Iteration 24445 Loss: 1.0879205465316772\n",
      "Iteration 24446 Loss: 0.6916910409927368\n",
      "Iteration 24447 Loss: 1.0456621646881104\n",
      "Iteration 24448 Loss: 1.0323799848556519\n",
      "Iteration 24449 Loss: 1.0604628324508667\n",
      "Iteration 24449 Loss: 1.0634154081344604\n",
      "Iteration 24450 Loss: 1.5581612586975098\n",
      "Iteration 24451 Loss: 0.8974975943565369\n",
      "Iteration 24452 Loss: 0.9832106232643127\n",
      "Iteration 24453 Loss: 0.9935604929924011\n",
      "Iteration 24454 Loss: 1.0938901901245117\n",
      "Iteration 24455 Loss: 0.7701901197433472\n",
      "Iteration 24456 Loss: 0.9400368928909302\n",
      "Iteration 24457 Loss: 1.1127251386642456\n",
      "Iteration 24458 Loss: 0.8475685715675354\n",
      "Iteration 24459 Loss: 0.6577695608139038\n",
      "Iteration 24459 Loss: 0.9854610562324524\n",
      "Iteration 24460 Loss: 1.015629529953003\n",
      "Iteration 24461 Loss: 1.1413350105285645\n",
      "Iteration 24462 Loss: 1.135831356048584\n",
      "Iteration 24463 Loss: 1.1192665100097656\n",
      "Iteration 24464 Loss: 1.269245982170105\n",
      "Iteration 24465 Loss: 1.1094273328781128\n",
      "Iteration 24466 Loss: 0.9785268902778625\n",
      "Iteration 24467 Loss: 0.7855230569839478\n",
      "Iteration 24468 Loss: 1.0762585401535034\n",
      "Iteration 24469 Loss: 1.1520941257476807\n",
      "Iteration 24469 Loss: 1.0783138275146484\n",
      "Iteration 24470 Loss: 0.9846309423446655\n",
      "Iteration 24471 Loss: 1.3439216613769531\n",
      "Iteration 24472 Loss: 1.0520968437194824\n",
      "Iteration 24473 Loss: 0.7745134234428406\n",
      "Iteration 24474 Loss: 1.0854833126068115\n",
      "Iteration 24475 Loss: 1.0989104509353638\n",
      "Iteration 24476 Loss: 1.044454574584961\n",
      "Iteration 24477 Loss: 1.0659345388412476\n",
      "Iteration 24478 Loss: 0.8285682797431946\n",
      "Iteration 24479 Loss: 1.1711323261260986\n",
      "Iteration 24479 Loss: 1.0449645519256592\n",
      "Iteration 24480 Loss: 1.6150044202804565\n",
      "Iteration 24481 Loss: 1.0889770984649658\n",
      "Iteration 24482 Loss: 1.210052728652954\n",
      "Iteration 24483 Loss: 0.9362040758132935\n",
      "Iteration 24484 Loss: 1.2246766090393066\n",
      "Iteration 24485 Loss: 1.56609308719635\n",
      "Iteration 24486 Loss: 1.1034340858459473\n",
      "Iteration 24487 Loss: 0.6363896131515503\n",
      "Iteration 24488 Loss: 0.6784209609031677\n",
      "Iteration 24489 Loss: 1.0995821952819824\n",
      "Iteration 24489 Loss: 1.1158835887908936\n",
      "Iteration 24490 Loss: 0.8585690259933472\n",
      "Iteration 24491 Loss: 1.1530665159225464\n",
      "Iteration 24492 Loss: 0.9275112748146057\n",
      "Iteration 24493 Loss: 0.7121491432189941\n",
      "Iteration 24494 Loss: 1.1467914581298828\n",
      "Iteration 24495 Loss: 1.0148407220840454\n",
      "Iteration 24496 Loss: 1.2695285081863403\n",
      "Iteration 24497 Loss: 1.4401991367340088\n",
      "Iteration 24498 Loss: 1.073908805847168\n",
      "Iteration 24499 Loss: 1.1783653497695923\n",
      "Iteration 24499 Loss: 1.0774929523468018\n",
      "Iteration 24500 Loss: 1.1999614238739014\n",
      "Iteration 24501 Loss: 0.8510977625846863\n",
      "Iteration 24502 Loss: 0.7674835920333862\n",
      "Iteration 24503 Loss: 1.274031162261963\n",
      "Iteration 24504 Loss: 0.9257637858390808\n",
      "Iteration 24505 Loss: 1.1102317571640015\n",
      "Iteration 24506 Loss: 1.0666558742523193\n",
      "Iteration 24507 Loss: 0.6774880290031433\n",
      "Iteration 24508 Loss: 1.0645437240600586\n",
      "Iteration 24509 Loss: 0.9905515909194946\n",
      "Iteration 24509 Loss: 0.992780864238739\n",
      "Iteration 24510 Loss: 0.8461148738861084\n",
      "Iteration 24511 Loss: 1.0898023843765259\n",
      "Iteration 24512 Loss: 0.8779903054237366\n",
      "Iteration 24513 Loss: 1.021716833114624\n",
      "Iteration 24514 Loss: 1.1509143114089966\n",
      "Iteration 24515 Loss: 1.0839279890060425\n",
      "Iteration 24516 Loss: 1.1736462116241455\n",
      "Iteration 24517 Loss: 1.3122761249542236\n",
      "Iteration 24518 Loss: 1.3282108306884766\n",
      "Iteration 24519 Loss: 0.9916605353355408\n",
      "Iteration 24519 Loss: 1.0876259803771973\n",
      "Iteration 24520 Loss: 0.9094453454017639\n",
      "Iteration 24521 Loss: 1.203610897064209\n",
      "Iteration 24522 Loss: 1.0818328857421875\n",
      "Iteration 24523 Loss: 0.8110504746437073\n",
      "Iteration 24524 Loss: 0.8972501158714294\n",
      "Iteration 24525 Loss: 0.9537999629974365\n",
      "Iteration 24526 Loss: 1.0898393392562866\n",
      "Iteration 24527 Loss: 0.9437194466590881\n",
      "Iteration 24528 Loss: 1.1138511896133423\n",
      "Iteration 24529 Loss: 0.9446941018104553\n",
      "Iteration 24529 Loss: 0.994909405708313\n",
      "Iteration 24530 Loss: 1.0076557397842407\n",
      "Iteration 24531 Loss: 0.7279150485992432\n",
      "Iteration 24532 Loss: 1.3930585384368896\n",
      "Iteration 24533 Loss: 1.29302179813385\n",
      "Iteration 24534 Loss: 1.0204161405563354\n",
      "Iteration 24535 Loss: 1.0590615272521973\n",
      "Iteration 24536 Loss: 1.039973497390747\n",
      "Iteration 24537 Loss: 1.1989223957061768\n",
      "Iteration 24538 Loss: 0.7266926169395447\n",
      "Iteration 24539 Loss: 1.1651909351348877\n",
      "Iteration 24539 Loss: 1.0631908178329468\n",
      "Iteration 24540 Loss: 1.2672628164291382\n",
      "Iteration 24541 Loss: 0.9914611577987671\n",
      "Iteration 24542 Loss: 0.8501974940299988\n",
      "Iteration 24543 Loss: 1.2835578918457031\n",
      "Iteration 24544 Loss: 0.9305292367935181\n",
      "Iteration 24545 Loss: 0.8317211270332336\n",
      "Iteration 24546 Loss: 0.9221458435058594\n",
      "Iteration 24547 Loss: 1.2700997591018677\n",
      "Iteration 24548 Loss: 1.071613073348999\n",
      "Iteration 24549 Loss: 1.3662679195404053\n",
      "Iteration 24549 Loss: 1.0784856081008911\n",
      "Iteration 24550 Loss: 0.8024957180023193\n",
      "Iteration 24551 Loss: 1.1646252870559692\n",
      "Iteration 24552 Loss: 0.9469881057739258\n",
      "Iteration 24553 Loss: 1.008773684501648\n",
      "Iteration 24554 Loss: 0.813256561756134\n",
      "Iteration 24555 Loss: 0.8291285634040833\n",
      "Iteration 24556 Loss: 1.0014032125473022\n",
      "Iteration 24557 Loss: 1.1611273288726807\n",
      "Iteration 24558 Loss: 1.3164923191070557\n",
      "Iteration 24559 Loss: 0.6957041025161743\n",
      "Iteration 24559 Loss: 0.9739995002746582\n",
      "Iteration 24560 Loss: 1.0154845714569092\n",
      "Iteration 24561 Loss: 1.1011481285095215\n",
      "Iteration 24562 Loss: 1.0237605571746826\n",
      "Iteration 24563 Loss: 1.201090693473816\n",
      "Iteration 24564 Loss: 0.9050658345222473\n",
      "Iteration 24565 Loss: 0.9274285435676575\n",
      "Iteration 24566 Loss: 1.233528733253479\n",
      "Iteration 24567 Loss: 1.0386672019958496\n",
      "Iteration 24568 Loss: 1.099678874015808\n",
      "Iteration 24569 Loss: 0.8815195560455322\n",
      "Iteration 24569 Loss: 1.042737364768982\n",
      "Iteration 24570 Loss: 1.462920904159546\n",
      "Iteration 24571 Loss: 0.973626971244812\n",
      "Iteration 24572 Loss: 0.9931109547615051\n",
      "Iteration 24573 Loss: 1.4952120780944824\n",
      "Iteration 24574 Loss: 1.0813874006271362\n",
      "Iteration 24575 Loss: 0.9143874049186707\n",
      "Iteration 24576 Loss: 1.0706136226654053\n",
      "Iteration 24577 Loss: 0.8549069166183472\n",
      "Iteration 24578 Loss: 1.2511789798736572\n",
      "Iteration 24579 Loss: 1.2028189897537231\n",
      "Iteration 24579 Loss: 1.130016565322876\n",
      "Iteration 24580 Loss: 0.7357577681541443\n",
      "Iteration 24581 Loss: 0.7991825342178345\n",
      "Iteration 24582 Loss: 0.9673953056335449\n",
      "Iteration 24583 Loss: 0.9795663356781006\n",
      "Iteration 24584 Loss: 0.9735012054443359\n",
      "Iteration 24585 Loss: 1.2560826539993286\n",
      "Iteration 24586 Loss: 1.2807337045669556\n",
      "Iteration 24587 Loss: 0.934949517250061\n",
      "Iteration 24588 Loss: 0.7590910792350769\n",
      "Iteration 24589 Loss: 1.1002678871154785\n",
      "Iteration 24589 Loss: 0.978652834892273\n",
      "Iteration 24590 Loss: 0.9209943413734436\n",
      "Iteration 24591 Loss: 1.4682217836380005\n",
      "Iteration 24592 Loss: 1.3464581966400146\n",
      "Iteration 24593 Loss: 1.3554184436798096\n",
      "Iteration 24594 Loss: 1.423598051071167\n",
      "Iteration 24595 Loss: 0.9412261247634888\n",
      "Iteration 24596 Loss: 0.6606346368789673\n",
      "Iteration 24597 Loss: 1.3343111276626587\n",
      "Iteration 24598 Loss: 0.9654490351676941\n",
      "Iteration 24599 Loss: 1.100785255432129\n",
      "Iteration 24599 Loss: 1.151709794998169\n",
      "Iteration 24600 Loss: 0.6904967427253723\n",
      "Iteration 24601 Loss: 1.121355652809143\n",
      "Iteration 24602 Loss: 0.9140483736991882\n",
      "Iteration 24603 Loss: 0.9169654846191406\n",
      "Iteration 24604 Loss: 0.9019557237625122\n",
      "Iteration 24605 Loss: 1.1077326536178589\n",
      "Iteration 24606 Loss: 0.7998408675193787\n",
      "Iteration 24607 Loss: 0.9965531826019287\n",
      "Iteration 24608 Loss: 1.120887041091919\n",
      "Iteration 24609 Loss: 1.124526023864746\n",
      "Iteration 24609 Loss: 0.9694361686706543\n",
      "Iteration 24610 Loss: 1.0707274675369263\n",
      "Iteration 24611 Loss: 1.2434487342834473\n",
      "Iteration 24612 Loss: 1.2129578590393066\n",
      "Iteration 24613 Loss: 1.0032079219818115\n",
      "Iteration 24614 Loss: 1.060489296913147\n",
      "Iteration 24615 Loss: 1.0255025625228882\n",
      "Iteration 24616 Loss: 0.9652099013328552\n",
      "Iteration 24617 Loss: 1.1306551694869995\n",
      "Iteration 24618 Loss: 1.1001198291778564\n",
      "Iteration 24619 Loss: 0.5184361338615417\n",
      "Iteration 24619 Loss: 1.0330755710601807\n",
      "Iteration 24620 Loss: 1.0207351446151733\n",
      "Iteration 24621 Loss: 1.108497977256775\n",
      "Iteration 24622 Loss: 0.9513453841209412\n",
      "Iteration 24623 Loss: 0.9790791273117065\n",
      "Iteration 24624 Loss: 0.889703631401062\n",
      "Iteration 24625 Loss: 0.9931159615516663\n",
      "Iteration 24626 Loss: 0.7158548831939697\n",
      "Iteration 24627 Loss: 1.081997036933899\n",
      "Iteration 24628 Loss: 1.2871160507202148\n",
      "Iteration 24629 Loss: 1.0778393745422363\n",
      "Iteration 24629 Loss: 1.0105284452438354\n",
      "Iteration 24630 Loss: 1.0572147369384766\n",
      "Iteration 24631 Loss: 1.023990273475647\n",
      "Iteration 24632 Loss: 1.232042908668518\n",
      "Iteration 24633 Loss: 1.1017001867294312\n",
      "Iteration 24634 Loss: 1.5263046026229858\n",
      "Iteration 24635 Loss: 1.3680875301361084\n",
      "Iteration 24636 Loss: 1.0462507009506226\n",
      "Iteration 24637 Loss: 0.8307351469993591\n",
      "Iteration 24638 Loss: 0.7664089202880859\n",
      "Iteration 24639 Loss: 1.0037188529968262\n",
      "Iteration 24639 Loss: 1.0956453084945679\n",
      "Iteration 24640 Loss: 1.1992918252944946\n",
      "Iteration 24641 Loss: 1.3789141178131104\n",
      "Iteration 24642 Loss: 0.8481921553611755\n",
      "Iteration 24643 Loss: 1.012731909751892\n",
      "Iteration 24644 Loss: 0.8013858795166016\n",
      "Iteration 24645 Loss: 0.9966711401939392\n",
      "Iteration 24646 Loss: 1.0742157697677612\n",
      "Iteration 24647 Loss: 0.9031738638877869\n",
      "Iteration 24648 Loss: 1.39704430103302\n",
      "Iteration 24649 Loss: 0.5195603370666504\n",
      "Iteration 24649 Loss: 1.013118028640747\n",
      "Iteration 24650 Loss: 0.9937657117843628\n",
      "Iteration 24651 Loss: 1.0974349975585938\n",
      "Iteration 24652 Loss: 0.9437531232833862\n",
      "Iteration 24653 Loss: 0.7946093082427979\n",
      "Iteration 24654 Loss: 0.8043051958084106\n",
      "Iteration 24655 Loss: 1.2321442365646362\n",
      "Iteration 24656 Loss: 1.087764024734497\n",
      "Iteration 24657 Loss: 0.9594573974609375\n",
      "Iteration 24658 Loss: 1.0828068256378174\n",
      "Iteration 24659 Loss: 1.0145790576934814\n",
      "Iteration 24659 Loss: 1.001062035560608\n",
      "Iteration 24660 Loss: 0.8572134375572205\n",
      "Iteration 24661 Loss: 1.0110876560211182\n",
      "Iteration 24662 Loss: 0.9872381687164307\n",
      "Iteration 24663 Loss: 0.8979656100273132\n",
      "Iteration 24664 Loss: 0.9846181273460388\n",
      "Iteration 24665 Loss: 1.2162405252456665\n",
      "Iteration 24666 Loss: 0.8150368332862854\n",
      "Iteration 24667 Loss: 1.150526762008667\n",
      "Iteration 24668 Loss: 1.189637303352356\n",
      "Iteration 24669 Loss: 1.0014772415161133\n",
      "Iteration 24669 Loss: 1.0111042261123657\n",
      "Iteration 24670 Loss: 0.5572717189788818\n",
      "Iteration 24671 Loss: 1.2460498809814453\n",
      "Iteration 24672 Loss: 1.0070122480392456\n",
      "Iteration 24673 Loss: 0.6852982044219971\n",
      "Iteration 24674 Loss: 1.0667074918746948\n",
      "Iteration 24675 Loss: 1.0485366582870483\n",
      "Iteration 24676 Loss: 1.2545838356018066\n",
      "Iteration 24677 Loss: 1.0910712480545044\n",
      "Iteration 24678 Loss: 1.2772412300109863\n",
      "Iteration 24679 Loss: 1.2251291275024414\n",
      "Iteration 24679 Loss: 1.0458900928497314\n",
      "Iteration 24680 Loss: 1.7405511140823364\n",
      "Iteration 24681 Loss: 1.0156958103179932\n",
      "Iteration 24682 Loss: 0.9223034381866455\n",
      "Iteration 24683 Loss: 0.6315069794654846\n",
      "Iteration 24684 Loss: 1.1506197452545166\n",
      "Iteration 24685 Loss: 1.1449613571166992\n",
      "Iteration 24686 Loss: 1.247090220451355\n",
      "Iteration 24687 Loss: 0.9871485829353333\n",
      "Iteration 24688 Loss: 1.4076179265975952\n",
      "Iteration 24689 Loss: 1.1229625940322876\n",
      "Iteration 24689 Loss: 1.1370456218719482\n",
      "Iteration 24690 Loss: 0.7614493370056152\n",
      "Iteration 24691 Loss: 1.2555904388427734\n",
      "Iteration 24692 Loss: 1.2190759181976318\n",
      "Iteration 24693 Loss: 1.1291474103927612\n",
      "Iteration 24694 Loss: 0.8587073087692261\n",
      "Iteration 24695 Loss: 1.2403115034103394\n",
      "Iteration 24696 Loss: 1.3924436569213867\n",
      "Iteration 24697 Loss: 0.7966711521148682\n",
      "Iteration 24698 Loss: 1.3206559419631958\n",
      "Iteration 24699 Loss: 0.8181580901145935\n",
      "Iteration 24699 Loss: 1.0792210102081299\n",
      "Iteration 24700 Loss: 1.1672788858413696\n",
      "Iteration 24701 Loss: 1.3412593603134155\n",
      "Iteration 24702 Loss: 1.2409805059432983\n",
      "Iteration 24703 Loss: 1.4585256576538086\n",
      "Iteration 24704 Loss: 0.882525622844696\n",
      "Iteration 24705 Loss: 1.0295860767364502\n",
      "Iteration 24706 Loss: 1.1368176937103271\n",
      "Iteration 24707 Loss: 1.0676637887954712\n",
      "Iteration 24708 Loss: 1.100913643836975\n",
      "Iteration 24709 Loss: 1.113006591796875\n",
      "Iteration 24709 Loss: 1.153855800628662\n",
      "Iteration 24710 Loss: 1.3138498067855835\n",
      "Iteration 24711 Loss: 1.057413101196289\n",
      "Iteration 24712 Loss: 0.8550293445587158\n",
      "Iteration 24713 Loss: 0.9603282809257507\n",
      "Iteration 24714 Loss: 1.3289389610290527\n",
      "Iteration 24715 Loss: 0.9700825810432434\n",
      "Iteration 24716 Loss: 1.148723840713501\n",
      "Iteration 24717 Loss: 0.6061810851097107\n",
      "Iteration 24718 Loss: 1.3534901142120361\n",
      "Iteration 24719 Loss: 1.3487319946289062\n",
      "Iteration 24719 Loss: 1.094276785850525\n",
      "Iteration 24720 Loss: 0.7311304211616516\n",
      "Iteration 24721 Loss: 0.9284551739692688\n",
      "Iteration 24722 Loss: 0.8899326324462891\n",
      "Iteration 24723 Loss: 1.0231291055679321\n",
      "Iteration 24724 Loss: 1.0812129974365234\n",
      "Iteration 24725 Loss: 0.9984256029129028\n",
      "Iteration 24726 Loss: 1.168989896774292\n",
      "Iteration 24727 Loss: 0.8839239478111267\n",
      "Iteration 24728 Loss: 0.8748369812965393\n",
      "Iteration 24729 Loss: 1.3854061365127563\n",
      "Iteration 24729 Loss: 0.9965441823005676\n",
      "Iteration 24730 Loss: 1.0182931423187256\n",
      "Iteration 24731 Loss: 1.2471762895584106\n",
      "Iteration 24732 Loss: 0.7516679167747498\n",
      "Iteration 24733 Loss: 1.0241061449050903\n",
      "Iteration 24734 Loss: 1.061711072921753\n",
      "Iteration 24735 Loss: 1.2654284238815308\n",
      "Iteration 24736 Loss: 1.072521448135376\n",
      "Iteration 24737 Loss: 0.9921967387199402\n",
      "Iteration 24738 Loss: 0.9464719295501709\n",
      "Iteration 24739 Loss: 0.84837806224823\n",
      "Iteration 24739 Loss: 1.0227950811386108\n",
      "Iteration 24740 Loss: 1.4515143632888794\n",
      "Iteration 24741 Loss: 1.216463327407837\n",
      "Iteration 24742 Loss: 0.9871708750724792\n",
      "Iteration 24743 Loss: 1.3164862394332886\n",
      "Iteration 24744 Loss: 1.4220669269561768\n",
      "Iteration 24745 Loss: 1.0527812242507935\n",
      "Iteration 24746 Loss: 1.6109302043914795\n",
      "Iteration 24747 Loss: 0.9278104305267334\n",
      "Iteration 24748 Loss: 1.4253216981887817\n",
      "Iteration 24749 Loss: 0.8893423676490784\n",
      "Iteration 24749 Loss: 1.2299888134002686\n",
      "Iteration 24750 Loss: 1.12061607837677\n",
      "Iteration 24751 Loss: 0.8088840842247009\n",
      "Iteration 24752 Loss: 0.8475093841552734\n",
      "Iteration 24753 Loss: 1.1361291408538818\n",
      "Iteration 24754 Loss: 1.3024859428405762\n",
      "Iteration 24755 Loss: 1.1718422174453735\n",
      "Iteration 24756 Loss: 0.9244669675827026\n",
      "Iteration 24757 Loss: 0.9516818523406982\n",
      "Iteration 24758 Loss: 1.0976686477661133\n",
      "Iteration 24759 Loss: 1.3800736665725708\n",
      "Iteration 24759 Loss: 1.0741357803344727\n",
      "Iteration 24760 Loss: 0.7078079581260681\n",
      "Iteration 24761 Loss: 1.291171908378601\n",
      "Iteration 24762 Loss: 1.2685706615447998\n",
      "Iteration 24763 Loss: 1.0611789226531982\n",
      "Iteration 24764 Loss: 0.8791853189468384\n",
      "Iteration 24765 Loss: 1.062618374824524\n",
      "Iteration 24766 Loss: 0.9276044964790344\n",
      "Iteration 24767 Loss: 1.2101901769638062\n",
      "Iteration 24768 Loss: 1.2301154136657715\n",
      "Iteration 24769 Loss: 0.9579256176948547\n",
      "Iteration 24769 Loss: 1.0596368312835693\n",
      "Iteration 24770 Loss: 0.9904707670211792\n",
      "Iteration 24771 Loss: 0.9873188138008118\n",
      "Iteration 24772 Loss: 1.1632925271987915\n",
      "Iteration 24773 Loss: 0.9064698815345764\n",
      "Iteration 24774 Loss: 0.697719931602478\n",
      "Iteration 24775 Loss: 0.6998522281646729\n",
      "Iteration 24776 Loss: 1.106151819229126\n",
      "Iteration 24777 Loss: 0.5130322575569153\n",
      "Iteration 24778 Loss: 1.2961608171463013\n",
      "Iteration 24779 Loss: 1.0493991374969482\n",
      "Iteration 24779 Loss: 0.9409868121147156\n",
      "Iteration 24780 Loss: 1.217334270477295\n",
      "Iteration 24781 Loss: 0.9637137651443481\n",
      "Iteration 24782 Loss: 0.8728761672973633\n",
      "Iteration 24783 Loss: 1.0422636270523071\n",
      "Iteration 24784 Loss: 0.9154433012008667\n",
      "Iteration 24785 Loss: 0.8474088907241821\n",
      "Iteration 24786 Loss: 1.0550282001495361\n",
      "Iteration 24787 Loss: 0.8487719297409058\n",
      "Iteration 24788 Loss: 1.24199378490448\n",
      "Iteration 24789 Loss: 1.0446196794509888\n",
      "Iteration 24789 Loss: 1.0049453973770142\n",
      "Iteration 24790 Loss: 1.043906569480896\n",
      "Iteration 24791 Loss: 0.8283087611198425\n",
      "Iteration 24792 Loss: 0.7870448231697083\n",
      "Iteration 24793 Loss: 1.1129945516586304\n",
      "Iteration 24794 Loss: 0.7990117073059082\n",
      "Iteration 24795 Loss: 0.9285878539085388\n",
      "Iteration 24796 Loss: 1.1595531702041626\n",
      "Iteration 24797 Loss: 0.8616428375244141\n",
      "Iteration 24798 Loss: 1.0900424718856812\n",
      "Iteration 24799 Loss: 1.168279767036438\n",
      "Iteration 24799 Loss: 0.9779373407363892\n",
      "Iteration 24800 Loss: 1.3258086442947388\n",
      "Iteration 24801 Loss: 1.067260980606079\n",
      "Iteration 24802 Loss: 1.214635968208313\n",
      "Iteration 24803 Loss: 1.2337685823440552\n",
      "Iteration 24804 Loss: 1.0300112962722778\n",
      "Iteration 24805 Loss: 1.2274696826934814\n",
      "Iteration 24806 Loss: 1.045297622680664\n",
      "Iteration 24807 Loss: 1.1930832862854004\n",
      "Iteration 24808 Loss: 1.0104044675827026\n",
      "Iteration 24809 Loss: 1.284785270690918\n",
      "Iteration 24809 Loss: 1.1632524728775024\n",
      "Iteration 24810 Loss: 1.1472461223602295\n",
      "Iteration 24811 Loss: 1.0303359031677246\n",
      "Iteration 24812 Loss: 1.4684358835220337\n",
      "Iteration 24813 Loss: 1.306333303451538\n",
      "Iteration 24814 Loss: 1.269033432006836\n",
      "Iteration 24815 Loss: 1.0868529081344604\n",
      "Iteration 24816 Loss: 0.9512064456939697\n",
      "Iteration 24817 Loss: 1.1407603025436401\n",
      "Iteration 24818 Loss: 0.8665300607681274\n",
      "Iteration 24819 Loss: 1.3015228509902954\n",
      "Iteration 24819 Loss: 1.1568257808685303\n",
      "Iteration 24820 Loss: 1.1305971145629883\n",
      "Iteration 24821 Loss: 1.2509993314743042\n",
      "Iteration 24822 Loss: 1.5421545505523682\n",
      "Iteration 24823 Loss: 0.7514181733131409\n",
      "Iteration 24824 Loss: 1.0039869546890259\n",
      "Iteration 24825 Loss: 1.3636119365692139\n",
      "Iteration 24826 Loss: 1.1026866436004639\n",
      "Iteration 24827 Loss: 1.3592312335968018\n",
      "Iteration 24828 Loss: 1.0391948223114014\n",
      "Iteration 24829 Loss: 1.0985921621322632\n",
      "Iteration 24829 Loss: 1.1642472743988037\n",
      "Iteration 24830 Loss: 0.9073902368545532\n",
      "Iteration 24831 Loss: 1.1801726818084717\n",
      "Iteration 24832 Loss: 0.8885968923568726\n",
      "Iteration 24833 Loss: 0.9428220391273499\n",
      "Iteration 24834 Loss: 0.9763572216033936\n",
      "Iteration 24835 Loss: 0.826796293258667\n",
      "Iteration 24836 Loss: 1.0992125272750854\n",
      "Iteration 24837 Loss: 0.8461806774139404\n",
      "Iteration 24838 Loss: 0.8839261531829834\n",
      "Iteration 24839 Loss: 1.184140920639038\n",
      "Iteration 24839 Loss: 0.9735596776008606\n",
      "Iteration 24840 Loss: 1.2607566118240356\n",
      "Iteration 24841 Loss: 1.124068021774292\n",
      "Iteration 24842 Loss: 1.3543729782104492\n",
      "Iteration 24843 Loss: 1.3644819259643555\n",
      "Iteration 24844 Loss: 1.2809017896652222\n",
      "Iteration 24845 Loss: 0.923688530921936\n",
      "Iteration 24846 Loss: 0.9636088013648987\n",
      "Iteration 24847 Loss: 1.0127410888671875\n",
      "Iteration 24848 Loss: 1.1011462211608887\n",
      "Iteration 24849 Loss: 0.7695943117141724\n",
      "Iteration 24849 Loss: 1.1155359745025635\n",
      "Iteration 24850 Loss: 0.8632926344871521\n",
      "Iteration 24851 Loss: 1.0524747371673584\n",
      "Iteration 24852 Loss: 0.8075444102287292\n",
      "Iteration 24853 Loss: 1.0182411670684814\n",
      "Iteration 24854 Loss: 0.8237743377685547\n",
      "Iteration 24855 Loss: 1.078974962234497\n",
      "Iteration 24856 Loss: 1.1437053680419922\n",
      "Iteration 24857 Loss: 0.7852075695991516\n",
      "Iteration 24858 Loss: 1.012164831161499\n",
      "Iteration 24859 Loss: 0.6053036451339722\n",
      "Iteration 24859 Loss: 0.9190683364868164\n",
      "Iteration 24860 Loss: 1.3424396514892578\n",
      "Iteration 24861 Loss: 0.8809236884117126\n",
      "Iteration 24862 Loss: 1.0420403480529785\n",
      "Iteration 24863 Loss: 1.0499087572097778\n",
      "Iteration 24864 Loss: 0.8882327675819397\n",
      "Iteration 24865 Loss: 1.2781665325164795\n",
      "Iteration 24866 Loss: 1.0973328351974487\n",
      "Iteration 24867 Loss: 1.1963683366775513\n",
      "Iteration 24868 Loss: 0.9286839962005615\n",
      "Iteration 24869 Loss: 1.184637427330017\n",
      "Iteration 24869 Loss: 1.0888733863830566\n",
      "Iteration 24870 Loss: 0.6348322629928589\n",
      "Iteration 24871 Loss: 1.1168571710586548\n",
      "Iteration 24872 Loss: 0.7590208053588867\n",
      "Iteration 24873 Loss: 0.7498364448547363\n",
      "Iteration 24874 Loss: 1.238827109336853\n",
      "Iteration 24875 Loss: 0.8759075403213501\n",
      "Iteration 24876 Loss: 0.8922338485717773\n",
      "Iteration 24877 Loss: 0.7827211618423462\n",
      "Iteration 24878 Loss: 1.2638962268829346\n",
      "Iteration 24879 Loss: 1.162711501121521\n",
      "Iteration 24879 Loss: 0.9476844072341919\n",
      "Iteration 24880 Loss: 1.2043476104736328\n",
      "Iteration 24881 Loss: 0.911642849445343\n",
      "Iteration 24882 Loss: 1.314908504486084\n",
      "Iteration 24883 Loss: 1.0451217889785767\n",
      "Iteration 24884 Loss: 0.9303616285324097\n",
      "Iteration 24885 Loss: 0.9627214670181274\n",
      "Iteration 24886 Loss: 0.8653414249420166\n",
      "Iteration 24887 Loss: 1.0861001014709473\n",
      "Iteration 24888 Loss: 1.2497167587280273\n",
      "Iteration 24889 Loss: 0.9689191579818726\n",
      "Iteration 24889 Loss: 1.0539181232452393\n",
      "Iteration 24890 Loss: 1.4485461711883545\n",
      "Iteration 24891 Loss: 0.9271331429481506\n",
      "Iteration 24892 Loss: 0.9098647832870483\n",
      "Iteration 24893 Loss: 1.2468993663787842\n",
      "Iteration 24894 Loss: 1.1858993768692017\n",
      "Iteration 24895 Loss: 0.9664947390556335\n",
      "Iteration 24896 Loss: 1.0913350582122803\n",
      "Iteration 24897 Loss: 1.0509246587753296\n",
      "Iteration 24898 Loss: 1.2103297710418701\n",
      "Iteration 24899 Loss: 1.197577714920044\n",
      "Iteration 24899 Loss: 1.1235005855560303\n",
      "Iteration 24900 Loss: 1.0693402290344238\n",
      "Iteration 24901 Loss: 1.1635924577713013\n",
      "Iteration 24902 Loss: 1.1108489036560059\n",
      "Iteration 24903 Loss: 1.1159929037094116\n",
      "Iteration 24904 Loss: 1.0842500925064087\n",
      "Iteration 24905 Loss: 1.1224101781845093\n",
      "Iteration 24906 Loss: 0.9122002124786377\n",
      "Iteration 24907 Loss: 1.3489853143692017\n",
      "Iteration 24908 Loss: 1.0743895769119263\n",
      "Iteration 24909 Loss: 1.2946149110794067\n",
      "Iteration 24909 Loss: 1.1296625137329102\n",
      "Iteration 24910 Loss: 0.9653207659721375\n",
      "Iteration 24911 Loss: 1.0238784551620483\n",
      "Iteration 24912 Loss: 1.2332426309585571\n",
      "Iteration 24913 Loss: 0.9936323165893555\n",
      "Iteration 24914 Loss: 1.110654354095459\n",
      "Iteration 24915 Loss: 0.9157233834266663\n",
      "Iteration 24916 Loss: 1.21909761428833\n",
      "Iteration 24917 Loss: 1.2130179405212402\n",
      "Iteration 24918 Loss: 1.511176586151123\n",
      "Iteration 24919 Loss: 1.495275616645813\n",
      "Iteration 24919 Loss: 1.1681020259857178\n",
      "Iteration 24920 Loss: 1.0991828441619873\n",
      "Iteration 24921 Loss: 1.0449696779251099\n",
      "Iteration 24922 Loss: 1.2632911205291748\n",
      "Iteration 24923 Loss: 0.9520153999328613\n",
      "Iteration 24924 Loss: 1.3649733066558838\n",
      "Iteration 24925 Loss: 1.096029281616211\n",
      "Iteration 24926 Loss: 1.029039740562439\n",
      "Iteration 24927 Loss: 1.1766446828842163\n",
      "Iteration 24928 Loss: 0.9080308675765991\n",
      "Iteration 24929 Loss: 1.2538368701934814\n",
      "Iteration 24929 Loss: 1.1188013553619385\n",
      "Iteration 24930 Loss: 1.1941001415252686\n",
      "Iteration 24931 Loss: 0.9260109066963196\n",
      "Iteration 24932 Loss: 0.943798840045929\n",
      "Iteration 24933 Loss: 1.1792376041412354\n",
      "Iteration 24934 Loss: 0.9456702470779419\n",
      "Iteration 24935 Loss: 1.1419626474380493\n",
      "Iteration 24936 Loss: 1.2251803874969482\n",
      "Iteration 24937 Loss: 1.3791478872299194\n",
      "Iteration 24938 Loss: 0.9550390839576721\n",
      "Iteration 24939 Loss: 1.1814367771148682\n",
      "Iteration 24939 Loss: 1.1071584224700928\n",
      "Iteration 24940 Loss: 1.0061068534851074\n",
      "Iteration 24941 Loss: 0.7360097169876099\n",
      "Iteration 24942 Loss: 1.2690223455429077\n",
      "Iteration 24943 Loss: 1.4135081768035889\n",
      "Iteration 24944 Loss: 1.434224247932434\n",
      "Iteration 24945 Loss: 0.9278043508529663\n",
      "Iteration 24946 Loss: 1.2350566387176514\n",
      "Iteration 24947 Loss: 1.5531809329986572\n",
      "Iteration 24948 Loss: 1.1378962993621826\n",
      "Iteration 24949 Loss: 1.204699993133545\n",
      "Iteration 24949 Loss: 1.1917508840560913\n",
      "Iteration 24950 Loss: 1.0716748237609863\n",
      "Iteration 24951 Loss: 0.8334838151931763\n",
      "Iteration 24952 Loss: 0.8227940201759338\n",
      "Iteration 24953 Loss: 1.1011083126068115\n",
      "Iteration 24954 Loss: 0.91492760181427\n",
      "Iteration 24955 Loss: 0.81903475522995\n",
      "Iteration 24956 Loss: 1.0903956890106201\n",
      "Iteration 24957 Loss: 1.403141975402832\n",
      "Iteration 24958 Loss: 1.092318058013916\n",
      "Iteration 24959 Loss: 1.0743767023086548\n",
      "Iteration 24959 Loss: 1.0223255157470703\n",
      "Iteration 24960 Loss: 1.347130298614502\n",
      "Iteration 24961 Loss: 0.9639701843261719\n",
      "Iteration 24962 Loss: 1.1255279779434204\n",
      "Iteration 24963 Loss: 0.964845597743988\n",
      "Iteration 24964 Loss: 1.1096594333648682\n",
      "Iteration 24965 Loss: 1.2387877702713013\n",
      "Iteration 24966 Loss: 1.0332903861999512\n",
      "Iteration 24967 Loss: 1.0803254842758179\n",
      "Iteration 24968 Loss: 1.2075257301330566\n",
      "Iteration 24969 Loss: 1.0996122360229492\n",
      "Iteration 24969 Loss: 1.1170674562454224\n",
      "Iteration 24970 Loss: 0.6937780976295471\n",
      "Iteration 24971 Loss: 1.3366849422454834\n",
      "Iteration 24972 Loss: 1.3164170980453491\n",
      "Iteration 24973 Loss: 1.35934579372406\n",
      "Iteration 24974 Loss: 1.0781325101852417\n",
      "Iteration 24975 Loss: 0.5706550478935242\n",
      "Iteration 24976 Loss: 0.9582759141921997\n",
      "Iteration 24977 Loss: 0.8581088781356812\n",
      "Iteration 24978 Loss: 0.8272057771682739\n",
      "Iteration 24979 Loss: 1.049547553062439\n",
      "Iteration 24979 Loss: 1.0048151016235352\n",
      "Iteration 24980 Loss: 1.3720941543579102\n",
      "Iteration 24981 Loss: 0.8175099492073059\n",
      "Iteration 24982 Loss: 1.4416773319244385\n",
      "Iteration 24983 Loss: 1.2844020128250122\n",
      "Iteration 24984 Loss: 0.8633806109428406\n",
      "Iteration 24985 Loss: 0.9688321948051453\n",
      "Iteration 24986 Loss: 1.0090411901474\n",
      "Iteration 24987 Loss: 1.1786167621612549\n",
      "Iteration 24988 Loss: 0.9666497707366943\n",
      "Iteration 24989 Loss: 1.219757080078125\n",
      "Iteration 24989 Loss: 1.1121959686279297\n",
      "Iteration 24990 Loss: 1.1778751611709595\n",
      "Iteration 24991 Loss: 1.129372000694275\n",
      "Iteration 24992 Loss: 0.9424411058425903\n",
      "Iteration 24993 Loss: 0.7578610777854919\n",
      "Iteration 24994 Loss: 1.0873736143112183\n",
      "Iteration 24995 Loss: 1.0795869827270508\n",
      "Iteration 24996 Loss: 1.211850881576538\n",
      "Iteration 24997 Loss: 0.5998692512512207\n",
      "Iteration 24998 Loss: 0.9221042394638062\n",
      "Iteration 24999 Loss: 1.161271095275879\n",
      "Iteration 24999 Loss: 1.0069605112075806\n",
      "Iteration 25000 Loss: 1.0623130798339844\n",
      "Iteration 25001 Loss: 1.2318354845046997\n",
      "Iteration 25002 Loss: 1.0099908113479614\n",
      "Iteration 25003 Loss: 0.9800547957420349\n",
      "Iteration 25004 Loss: 1.3180989027023315\n",
      "Iteration 25005 Loss: 0.848815381526947\n",
      "Iteration 25006 Loss: 1.2879047393798828\n",
      "Iteration 25007 Loss: 1.0919090509414673\n",
      "Iteration 25008 Loss: 1.0094858407974243\n",
      "Iteration 25009 Loss: 1.1142438650131226\n",
      "Iteration 25009 Loss: 1.0954651832580566\n",
      "Iteration 25010 Loss: 1.0720702409744263\n",
      "Iteration 25011 Loss: 1.0725336074829102\n",
      "Iteration 25012 Loss: 1.1618690490722656\n",
      "Iteration 25013 Loss: 0.7323226928710938\n",
      "Iteration 25014 Loss: 1.0489274263381958\n",
      "Iteration 25015 Loss: 0.8783771991729736\n",
      "Iteration 25016 Loss: 0.9713830351829529\n",
      "Iteration 25017 Loss: 1.1636979579925537\n",
      "Iteration 25018 Loss: 0.8904752135276794\n",
      "Iteration 25019 Loss: 1.110081434249878\n",
      "Iteration 25019 Loss: 1.0101737976074219\n",
      "Iteration 25020 Loss: 0.6975489854812622\n",
      "Iteration 25021 Loss: 1.0428920984268188\n",
      "Iteration 25022 Loss: 1.443377137184143\n",
      "Iteration 25023 Loss: 1.0556235313415527\n",
      "Iteration 25024 Loss: 1.2285650968551636\n",
      "Iteration 25025 Loss: 1.1300418376922607\n",
      "Iteration 25026 Loss: 0.6238724589347839\n",
      "Iteration 25027 Loss: 0.9849867820739746\n",
      "Iteration 25028 Loss: 1.214848279953003\n",
      "Iteration 25029 Loss: 0.9309976696968079\n",
      "Iteration 25029 Loss: 1.0352754592895508\n",
      "Iteration 25030 Loss: 1.192360520362854\n",
      "Iteration 25031 Loss: 1.010182499885559\n",
      "Iteration 25032 Loss: 0.8479219675064087\n",
      "Iteration 25033 Loss: 1.272800326347351\n",
      "Iteration 25034 Loss: 1.3394994735717773\n",
      "Iteration 25035 Loss: 1.6658557653427124\n",
      "Iteration 25036 Loss: 1.1772335767745972\n",
      "Iteration 25037 Loss: 1.2039557695388794\n",
      "Iteration 25038 Loss: 1.1938284635543823\n",
      "Iteration 25039 Loss: 1.2314720153808594\n",
      "Iteration 25039 Loss: 1.2135109901428223\n",
      "Iteration 25040 Loss: 0.7949205636978149\n",
      "Iteration 25041 Loss: 1.0085417032241821\n",
      "Iteration 25042 Loss: 1.2146481275558472\n",
      "Iteration 25043 Loss: 1.2936460971832275\n",
      "Iteration 25044 Loss: 1.240155577659607\n",
      "Iteration 25045 Loss: 1.4004778861999512\n",
      "Iteration 25046 Loss: 1.1847349405288696\n",
      "Iteration 25047 Loss: 0.9756645560264587\n",
      "Iteration 25048 Loss: 1.2111985683441162\n",
      "Iteration 25049 Loss: 1.0679807662963867\n",
      "Iteration 25049 Loss: 1.1391968727111816\n",
      "Iteration 25050 Loss: 1.2822905778884888\n",
      "Iteration 25051 Loss: 0.9825857281684875\n",
      "Iteration 25052 Loss: 1.0403251647949219\n",
      "Iteration 25053 Loss: 1.0684412717819214\n",
      "Iteration 25054 Loss: 0.9192757606506348\n",
      "Iteration 25055 Loss: 0.9329290986061096\n",
      "Iteration 25056 Loss: 0.9709550142288208\n",
      "Iteration 25057 Loss: 0.7153674960136414\n",
      "Iteration 25058 Loss: 1.1827725172042847\n",
      "Iteration 25059 Loss: 0.725609540939331\n",
      "Iteration 25059 Loss: 0.9820551872253418\n",
      "Iteration 25060 Loss: 0.7502003312110901\n",
      "Iteration 25061 Loss: 0.8335890173912048\n",
      "Iteration 25062 Loss: 1.0042721033096313\n",
      "Iteration 25063 Loss: 1.2629077434539795\n",
      "Iteration 25064 Loss: 0.8360410928726196\n",
      "Iteration 25065 Loss: 1.260190486907959\n",
      "Iteration 25066 Loss: 1.0836920738220215\n",
      "Iteration 25067 Loss: 1.1669232845306396\n",
      "Iteration 25068 Loss: 1.345660924911499\n",
      "Iteration 25069 Loss: 0.9493760466575623\n",
      "Iteration 25069 Loss: 1.0492852926254272\n",
      "Iteration 25070 Loss: 0.9259725213050842\n",
      "Iteration 25071 Loss: 1.0460349321365356\n",
      "Iteration 25072 Loss: 1.1844797134399414\n",
      "Iteration 25073 Loss: 1.412327766418457\n",
      "Iteration 25074 Loss: 1.0504868030548096\n",
      "Iteration 25075 Loss: 1.3345577716827393\n",
      "Iteration 25076 Loss: 1.381532073020935\n",
      "Iteration 25077 Loss: 0.7243465781211853\n",
      "Iteration 25078 Loss: 0.9023477435112\n",
      "Iteration 25079 Loss: 0.9840096235275269\n",
      "Iteration 25079 Loss: 1.0946094989776611\n",
      "Iteration 25080 Loss: 1.0476919412612915\n",
      "Iteration 25081 Loss: 0.9161428213119507\n",
      "Iteration 25082 Loss: 0.9757785797119141\n",
      "Iteration 25083 Loss: 1.1466747522354126\n",
      "Iteration 25084 Loss: 0.7878066301345825\n",
      "Iteration 25085 Loss: 1.2674466371536255\n",
      "Iteration 25086 Loss: 0.9529979825019836\n",
      "Iteration 25087 Loss: 0.6152295470237732\n",
      "Iteration 25088 Loss: 1.2156882286071777\n",
      "Iteration 25089 Loss: 0.9014686942100525\n",
      "Iteration 25089 Loss: 0.9826925992965698\n",
      "Iteration 25090 Loss: 1.1004709005355835\n",
      "Iteration 25091 Loss: 0.9284070134162903\n",
      "Iteration 25092 Loss: 0.9227888584136963\n",
      "Iteration 25093 Loss: 0.9017653465270996\n",
      "Iteration 25094 Loss: 0.9480754137039185\n",
      "Iteration 25095 Loss: 1.327492356300354\n",
      "Iteration 25096 Loss: 0.9091090559959412\n",
      "Iteration 25097 Loss: 0.6891738772392273\n",
      "Iteration 25098 Loss: 1.1364754438400269\n",
      "Iteration 25099 Loss: 1.1202574968338013\n",
      "Iteration 25099 Loss: 0.9984015226364136\n",
      "Iteration 25100 Loss: 1.2480758428573608\n",
      "Iteration 25101 Loss: 0.8653201460838318\n",
      "Iteration 25102 Loss: 1.0609444379806519\n",
      "Iteration 25103 Loss: 0.9098740220069885\n",
      "Iteration 25104 Loss: 1.3385897874832153\n",
      "Iteration 25105 Loss: 0.9982216358184814\n",
      "Iteration 25106 Loss: 0.917938768863678\n",
      "Iteration 25107 Loss: 1.1739150285720825\n",
      "Iteration 25108 Loss: 0.9565878510475159\n",
      "Iteration 25109 Loss: 0.8888963460922241\n",
      "Iteration 25109 Loss: 1.0358364582061768\n",
      "Iteration 25110 Loss: 1.1505597829818726\n",
      "Iteration 25111 Loss: 1.0313045978546143\n",
      "Iteration 25112 Loss: 0.995135486125946\n",
      "Iteration 25113 Loss: 0.7785581946372986\n",
      "Iteration 25114 Loss: 1.1892489194869995\n",
      "Iteration 25115 Loss: 1.5956194400787354\n",
      "Iteration 25116 Loss: 1.053952693939209\n",
      "Iteration 25117 Loss: 0.9468502402305603\n",
      "Iteration 25118 Loss: 1.2183846235275269\n",
      "Iteration 25119 Loss: 1.2001261711120605\n",
      "Iteration 25119 Loss: 1.1159740686416626\n",
      "Iteration 25120 Loss: 0.7679435014724731\n",
      "Iteration 25121 Loss: 1.18231999874115\n",
      "Iteration 25122 Loss: 1.2253806591033936\n",
      "Iteration 25123 Loss: 1.082215428352356\n",
      "Iteration 25124 Loss: 0.6631066799163818\n",
      "Iteration 25125 Loss: 1.3019721508026123\n",
      "Iteration 25126 Loss: 0.9221598505973816\n",
      "Iteration 25127 Loss: 0.9451901912689209\n",
      "Iteration 25128 Loss: 0.9686124324798584\n",
      "Iteration 25129 Loss: 0.8481463193893433\n",
      "Iteration 25129 Loss: 0.9907048344612122\n",
      "Iteration 25130 Loss: 1.333747148513794\n",
      "Iteration 25131 Loss: 1.010873794555664\n",
      "Iteration 25132 Loss: 1.2064194679260254\n",
      "Iteration 25133 Loss: 0.9331579804420471\n",
      "Iteration 25134 Loss: 1.008561611175537\n",
      "Iteration 25135 Loss: 0.8455175757408142\n",
      "Iteration 25136 Loss: 1.408889651298523\n",
      "Iteration 25137 Loss: 0.8430024981498718\n",
      "Iteration 25138 Loss: 1.1898001432418823\n",
      "Iteration 25139 Loss: 1.1792387962341309\n",
      "Iteration 25139 Loss: 1.0959208011627197\n",
      "Iteration 25140 Loss: 0.948250949382782\n",
      "Iteration 25141 Loss: 0.8919677138328552\n",
      "Iteration 25142 Loss: 0.5738623738288879\n",
      "Iteration 25143 Loss: 1.2054575681686401\n",
      "Iteration 25144 Loss: 1.3339223861694336\n",
      "Iteration 25145 Loss: 0.7767186760902405\n",
      "Iteration 25146 Loss: 0.5789557695388794\n",
      "Iteration 25147 Loss: 1.1476140022277832\n",
      "Iteration 25148 Loss: 0.8958863019943237\n",
      "Iteration 25149 Loss: 0.9024348258972168\n",
      "Iteration 25149 Loss: 0.9255071878433228\n",
      "Iteration 25150 Loss: 1.5581696033477783\n",
      "Iteration 25151 Loss: 1.0514096021652222\n",
      "Iteration 25152 Loss: 1.5094590187072754\n",
      "Iteration 25153 Loss: 0.9282158613204956\n",
      "Iteration 25154 Loss: 0.8395020961761475\n",
      "Iteration 25155 Loss: 1.1537420749664307\n",
      "Iteration 25156 Loss: 0.8312907218933105\n",
      "Iteration 25157 Loss: 0.8542606234550476\n",
      "Iteration 25158 Loss: 1.1071257591247559\n",
      "Iteration 25159 Loss: 0.9298321008682251\n",
      "Iteration 25159 Loss: 1.0763007402420044\n",
      "Iteration 25160 Loss: 1.2363967895507812\n",
      "Iteration 25161 Loss: 1.2183243036270142\n",
      "Iteration 25162 Loss: 0.9685052633285522\n",
      "Iteration 25163 Loss: 0.9644713997840881\n",
      "Iteration 25164 Loss: 1.5130869150161743\n",
      "Iteration 25165 Loss: 0.8836674094200134\n",
      "Iteration 25166 Loss: 1.1593040227890015\n",
      "Iteration 25167 Loss: 1.266648769378662\n",
      "Iteration 25168 Loss: 1.1388778686523438\n",
      "Iteration 25169 Loss: 0.9185349345207214\n",
      "Iteration 25169 Loss: 1.126781702041626\n",
      "Iteration 25170 Loss: 1.4208966493606567\n",
      "Iteration 25171 Loss: 1.2956558465957642\n",
      "Iteration 25172 Loss: 1.1576576232910156\n",
      "Iteration 25173 Loss: 0.9565280675888062\n",
      "Iteration 25174 Loss: 1.2725446224212646\n",
      "Iteration 25175 Loss: 0.9598808884620667\n",
      "Iteration 25176 Loss: 0.6388253569602966\n",
      "Iteration 25177 Loss: 1.2519915103912354\n",
      "Iteration 25178 Loss: 0.9736082553863525\n",
      "Iteration 25179 Loss: 1.1103061437606812\n",
      "Iteration 25179 Loss: 1.1037894487380981\n",
      "Iteration 25180 Loss: 1.0307663679122925\n",
      "Iteration 25181 Loss: 1.1670011281967163\n",
      "Iteration 25182 Loss: 1.0000501871109009\n",
      "Iteration 25183 Loss: 0.8553565740585327\n",
      "Iteration 25184 Loss: 0.9815133213996887\n",
      "Iteration 25185 Loss: 1.5921579599380493\n",
      "Iteration 25186 Loss: 1.0189707279205322\n",
      "Iteration 25187 Loss: 1.0780748128890991\n",
      "Iteration 25188 Loss: 1.0728002786636353\n",
      "Iteration 25189 Loss: 1.1290874481201172\n",
      "Iteration 25189 Loss: 1.0925778150558472\n",
      "Iteration 25190 Loss: 1.2557514905929565\n",
      "Iteration 25191 Loss: 0.8312108516693115\n",
      "Iteration 25192 Loss: 1.0854921340942383\n",
      "Iteration 25193 Loss: 0.7374565005302429\n",
      "Iteration 25194 Loss: 1.0009528398513794\n",
      "Iteration 25195 Loss: 0.9314436912536621\n",
      "Iteration 25196 Loss: 1.1209086179733276\n",
      "Iteration 25197 Loss: 1.0332345962524414\n",
      "Iteration 25198 Loss: 0.8912336826324463\n",
      "Iteration 25199 Loss: 1.0637352466583252\n",
      "Iteration 25199 Loss: 0.995141863822937\n",
      "Iteration 25200 Loss: 1.3730533123016357\n",
      "Iteration 25201 Loss: 1.015599250793457\n",
      "Iteration 25202 Loss: 1.359227180480957\n",
      "Iteration 25203 Loss: 0.7259268760681152\n",
      "Iteration 25204 Loss: 0.6881604790687561\n",
      "Iteration 25205 Loss: 1.1136754751205444\n",
      "Iteration 25206 Loss: 0.8491378426551819\n",
      "Iteration 25207 Loss: 1.3000568151474\n",
      "Iteration 25208 Loss: 1.1193279027938843\n",
      "Iteration 25209 Loss: 0.66177898645401\n",
      "Iteration 25209 Loss: 1.0205943584442139\n",
      "Iteration 25210 Loss: 0.8801543712615967\n",
      "Iteration 25211 Loss: 1.2630462646484375\n",
      "Iteration 25212 Loss: 0.6720383763313293\n",
      "Iteration 25213 Loss: 1.1525404453277588\n",
      "Iteration 25214 Loss: 1.2040047645568848\n",
      "Iteration 25215 Loss: 1.4853276014328003\n",
      "Iteration 25216 Loss: 0.9755420088768005\n",
      "Iteration 25217 Loss: 0.9987383484840393\n",
      "Iteration 25218 Loss: 1.164461612701416\n",
      "Iteration 25219 Loss: 1.0089726448059082\n",
      "Iteration 25219 Loss: 1.0804827213287354\n",
      "Iteration 25220 Loss: 0.6255791187286377\n",
      "Iteration 25221 Loss: 1.0443503856658936\n",
      "Iteration 25222 Loss: 1.2465646266937256\n",
      "Iteration 25223 Loss: 1.247894525527954\n",
      "Iteration 25224 Loss: 0.8172576427459717\n",
      "Iteration 25225 Loss: 1.1656053066253662\n",
      "Iteration 25226 Loss: 1.3329589366912842\n",
      "Iteration 25227 Loss: 1.01692795753479\n",
      "Iteration 25228 Loss: 1.1624640226364136\n",
      "Iteration 25229 Loss: 1.1749802827835083\n",
      "Iteration 25229 Loss: 1.0834583044052124\n",
      "Iteration 25230 Loss: 0.8043662309646606\n",
      "Iteration 25231 Loss: 1.2307071685791016\n",
      "Iteration 25232 Loss: 1.3380446434020996\n",
      "Iteration 25233 Loss: 0.9894014596939087\n",
      "Iteration 25234 Loss: 1.496150255203247\n",
      "Iteration 25235 Loss: 1.4824110269546509\n",
      "Iteration 25236 Loss: 1.3788015842437744\n",
      "Iteration 25237 Loss: 1.3765778541564941\n",
      "Iteration 25238 Loss: 1.030139684677124\n",
      "Iteration 25239 Loss: 1.2899765968322754\n",
      "Iteration 25239 Loss: 1.2416576147079468\n",
      "Iteration 25240 Loss: 1.0616767406463623\n",
      "Iteration 25241 Loss: 1.0322957038879395\n",
      "Iteration 25242 Loss: 0.9927805662155151\n",
      "Iteration 25243 Loss: 1.392931342124939\n",
      "Iteration 25244 Loss: 0.9153138995170593\n",
      "Iteration 25245 Loss: 1.229129672050476\n",
      "Iteration 25246 Loss: 1.033713459968567\n",
      "Iteration 25247 Loss: 1.2179688215255737\n",
      "Iteration 25248 Loss: 0.6846804618835449\n",
      "Iteration 25249 Loss: 1.3959587812423706\n",
      "Iteration 25249 Loss: 1.0956449508666992\n",
      "Iteration 25250 Loss: 1.40829336643219\n",
      "Iteration 25251 Loss: 1.4025160074234009\n",
      "Iteration 25252 Loss: 1.1745864152908325\n",
      "Iteration 25253 Loss: 1.0395066738128662\n",
      "Iteration 25254 Loss: 1.0862820148468018\n",
      "Iteration 25255 Loss: 0.9504640698432922\n",
      "Iteration 25256 Loss: 0.7319795489311218\n",
      "Iteration 25257 Loss: 1.0357065200805664\n",
      "Iteration 25258 Loss: 0.9758010506629944\n",
      "Iteration 25259 Loss: 0.7431308031082153\n",
      "Iteration 25259 Loss: 1.0548266172409058\n",
      "Iteration 25260 Loss: 0.9492978453636169\n",
      "Iteration 25261 Loss: 0.9646590352058411\n",
      "Iteration 25262 Loss: 1.1122931241989136\n",
      "Iteration 25263 Loss: 0.7116222381591797\n",
      "Iteration 25264 Loss: 1.4089592695236206\n",
      "Iteration 25265 Loss: 0.9199763536453247\n",
      "Iteration 25266 Loss: 1.1238594055175781\n",
      "Iteration 25267 Loss: 1.2033532857894897\n",
      "Iteration 25268 Loss: 0.7498160004615784\n",
      "Iteration 25269 Loss: 0.8761122822761536\n",
      "Iteration 25269 Loss: 1.0019948482513428\n",
      "Iteration 25270 Loss: 0.9668774008750916\n",
      "Iteration 25271 Loss: 0.9086576700210571\n",
      "Iteration 25272 Loss: 1.1970577239990234\n",
      "Iteration 25273 Loss: 1.2168179750442505\n",
      "Iteration 25274 Loss: 0.885915994644165\n",
      "Iteration 25275 Loss: 1.0852727890014648\n",
      "Iteration 25276 Loss: 1.1569831371307373\n",
      "Iteration 25277 Loss: 1.0339734554290771\n",
      "Iteration 25278 Loss: 1.141136884689331\n",
      "Iteration 25279 Loss: 1.42875337600708\n",
      "Iteration 25279 Loss: 1.102144718170166\n",
      "Iteration 25280 Loss: 0.7225309014320374\n",
      "Iteration 25281 Loss: 0.7953287363052368\n",
      "Iteration 25282 Loss: 0.6082382202148438\n",
      "Iteration 25283 Loss: 1.0539724826812744\n",
      "Iteration 25284 Loss: 0.6889601349830627\n",
      "Iteration 25285 Loss: 1.2481231689453125\n",
      "Iteration 25286 Loss: 1.1827497482299805\n",
      "Iteration 25287 Loss: 1.19514799118042\n",
      "Iteration 25288 Loss: 0.9153653979301453\n",
      "Iteration 25289 Loss: 1.0600693225860596\n",
      "Iteration 25289 Loss: 0.947048544883728\n",
      "Iteration 25290 Loss: 1.2019200325012207\n",
      "Iteration 25291 Loss: 1.102803111076355\n",
      "Iteration 25292 Loss: 1.1659783124923706\n",
      "Iteration 25293 Loss: 0.7312831282615662\n",
      "Iteration 25294 Loss: 0.8407382965087891\n",
      "Iteration 25295 Loss: 1.1706950664520264\n",
      "Iteration 25296 Loss: 0.9901968240737915\n",
      "Iteration 25297 Loss: 1.3753533363342285\n",
      "Iteration 25298 Loss: 1.1376434564590454\n",
      "Iteration 25299 Loss: 0.9238276481628418\n",
      "Iteration 25299 Loss: 1.0640439987182617\n",
      "Iteration 25300 Loss: 0.92176353931427\n",
      "Iteration 25301 Loss: 0.9209798574447632\n",
      "Iteration 25302 Loss: 0.9227287769317627\n",
      "Iteration 25303 Loss: 0.9032043218612671\n",
      "Iteration 25304 Loss: 1.1035734415054321\n",
      "Iteration 25305 Loss: 0.9814831018447876\n",
      "Iteration 25306 Loss: 0.9147294163703918\n",
      "Iteration 25307 Loss: 1.3511145114898682\n",
      "Iteration 25308 Loss: 1.2655267715454102\n",
      "Iteration 25309 Loss: 0.8751899600028992\n",
      "Iteration 25309 Loss: 1.0160293579101562\n",
      "Iteration 25310 Loss: 1.423680305480957\n",
      "Iteration 25311 Loss: 1.0009666681289673\n",
      "Iteration 25312 Loss: 1.024659276008606\n",
      "Iteration 25313 Loss: 1.1243785619735718\n",
      "Iteration 25314 Loss: 1.1246658563613892\n",
      "Iteration 25315 Loss: 1.0520076751708984\n",
      "Iteration 25316 Loss: 0.8913784623146057\n",
      "Iteration 25317 Loss: 0.9969598054885864\n",
      "Iteration 25318 Loss: 1.3327935934066772\n",
      "Iteration 25319 Loss: 1.1275136470794678\n",
      "Iteration 25319 Loss: 1.1099003553390503\n",
      "Iteration 25320 Loss: 1.2773951292037964\n",
      "Iteration 25321 Loss: 1.2684394121170044\n",
      "Iteration 25322 Loss: 1.1083654165267944\n",
      "Iteration 25323 Loss: 0.8350571990013123\n",
      "Iteration 25324 Loss: 1.1218599081039429\n",
      "Iteration 25325 Loss: 1.3108302354812622\n",
      "Iteration 25326 Loss: 1.0581797361373901\n",
      "Iteration 25327 Loss: 0.6917643547058105\n",
      "Iteration 25328 Loss: 0.9873262047767639\n",
      "Iteration 25329 Loss: 0.9249256253242493\n",
      "Iteration 25329 Loss: 1.0584144592285156\n",
      "Iteration 25330 Loss: 0.856042742729187\n",
      "Iteration 25331 Loss: 1.1998580694198608\n",
      "Iteration 25332 Loss: 0.9286648035049438\n",
      "Iteration 25333 Loss: 1.178173542022705\n",
      "Iteration 25334 Loss: 1.0761044025421143\n",
      "Iteration 25335 Loss: 0.8102685809135437\n",
      "Iteration 25336 Loss: 1.3226027488708496\n",
      "Iteration 25337 Loss: 1.4368942975997925\n",
      "Iteration 25338 Loss: 0.9156509637832642\n",
      "Iteration 25339 Loss: 0.8506960868835449\n",
      "Iteration 25339 Loss: 1.0574955940246582\n",
      "Iteration 25340 Loss: 1.0520684719085693\n",
      "Iteration 25341 Loss: 1.1001818180084229\n",
      "Iteration 25342 Loss: 1.0931771993637085\n",
      "Iteration 25343 Loss: 0.7535253167152405\n",
      "Iteration 25344 Loss: 0.8225511312484741\n",
      "Iteration 25345 Loss: 0.9433691501617432\n",
      "Iteration 25346 Loss: 1.2964104413986206\n",
      "Iteration 25347 Loss: 0.9280157089233398\n",
      "Iteration 25348 Loss: 0.8164584040641785\n",
      "Iteration 25349 Loss: 0.9742190837860107\n",
      "Iteration 25349 Loss: 0.9779976606369019\n",
      "Iteration 25350 Loss: 0.7754132747650146\n",
      "Iteration 25351 Loss: 1.152588129043579\n",
      "Iteration 25352 Loss: 0.9097834825515747\n",
      "Iteration 25353 Loss: 0.9981958270072937\n",
      "Iteration 25354 Loss: 1.4267618656158447\n",
      "Iteration 25355 Loss: 1.0832549333572388\n",
      "Iteration 25356 Loss: 1.4968760013580322\n",
      "Iteration 25357 Loss: 0.9946097731590271\n",
      "Iteration 25358 Loss: 0.9065938591957092\n",
      "Iteration 25359 Loss: 1.1441938877105713\n",
      "Iteration 25359 Loss: 1.0888270139694214\n",
      "Iteration 25360 Loss: 0.819787859916687\n",
      "Iteration 25361 Loss: 1.186572551727295\n",
      "Iteration 25362 Loss: 0.8358184099197388\n",
      "Iteration 25363 Loss: 1.1651214361190796\n",
      "Iteration 25364 Loss: 0.9592413306236267\n",
      "Iteration 25365 Loss: 0.8536490201950073\n",
      "Iteration 25366 Loss: 1.24032461643219\n",
      "Iteration 25367 Loss: 0.9917984008789062\n",
      "Iteration 25368 Loss: 1.4557958841323853\n",
      "Iteration 25369 Loss: 0.6107229590415955\n",
      "Iteration 25369 Loss: 1.01188325881958\n",
      "Iteration 25370 Loss: 0.8764809966087341\n",
      "Iteration 25371 Loss: 1.045430302619934\n",
      "Iteration 25372 Loss: 0.6176705360412598\n",
      "Iteration 25373 Loss: 1.178107738494873\n",
      "Iteration 25374 Loss: 0.6498077511787415\n",
      "Iteration 25375 Loss: 0.9867291450500488\n",
      "Iteration 25376 Loss: 0.9838716387748718\n",
      "Iteration 25377 Loss: 1.1973004341125488\n",
      "Iteration 25378 Loss: 1.1451534032821655\n",
      "Iteration 25379 Loss: 0.8858213424682617\n",
      "Iteration 25379 Loss: 0.9566373825073242\n",
      "Iteration 25380 Loss: 1.1900917291641235\n",
      "Iteration 25381 Loss: 1.0069506168365479\n",
      "Iteration 25382 Loss: 1.0333565473556519\n",
      "Iteration 25383 Loss: 1.3077102899551392\n",
      "Iteration 25384 Loss: 1.508009910583496\n",
      "Iteration 25385 Loss: 0.8287539482116699\n",
      "Iteration 25386 Loss: 0.9698390364646912\n",
      "Iteration 25387 Loss: 1.3602259159088135\n",
      "Iteration 25388 Loss: 1.2427712678909302\n",
      "Iteration 25389 Loss: 1.2524125576019287\n",
      "Iteration 25389 Loss: 1.1700122356414795\n",
      "Iteration 25390 Loss: 0.7910372614860535\n",
      "Iteration 25391 Loss: 1.1088389158248901\n",
      "Iteration 25392 Loss: 0.8537411689758301\n",
      "Iteration 25393 Loss: 1.1853196620941162\n",
      "Iteration 25394 Loss: 1.1019749641418457\n",
      "Iteration 25395 Loss: 1.1861376762390137\n",
      "Iteration 25396 Loss: 1.41172194480896\n",
      "Iteration 25397 Loss: 1.1699227094650269\n",
      "Iteration 25398 Loss: 0.9230415225028992\n",
      "Iteration 25399 Loss: 1.1150155067443848\n",
      "Iteration 25399 Loss: 1.0846751928329468\n",
      "Iteration 25400 Loss: 1.2579593658447266\n",
      "Iteration 25401 Loss: 1.3884414434432983\n",
      "Iteration 25402 Loss: 0.8993072509765625\n",
      "Iteration 25403 Loss: 1.2350714206695557\n",
      "Iteration 25404 Loss: 1.2764450311660767\n",
      "Iteration 25405 Loss: 1.141445279121399\n",
      "Iteration 25406 Loss: 1.1691633462905884\n",
      "Iteration 25407 Loss: 1.0830127000808716\n",
      "Iteration 25408 Loss: 1.1610960960388184\n",
      "Iteration 25409 Loss: 0.7630493640899658\n",
      "Iteration 25409 Loss: 1.1374990940093994\n",
      "Iteration 25410 Loss: 0.7466323375701904\n",
      "Iteration 25411 Loss: 0.9253777861595154\n",
      "Iteration 25412 Loss: 0.6432638168334961\n",
      "Iteration 25413 Loss: 1.1245274543762207\n",
      "Iteration 25414 Loss: 1.0094343423843384\n",
      "Iteration 25415 Loss: 1.1526137590408325\n",
      "Iteration 25416 Loss: 1.0080968141555786\n",
      "Iteration 25417 Loss: 0.9865601658821106\n",
      "Iteration 25418 Loss: 0.8168608546257019\n",
      "Iteration 25419 Loss: 1.1350184679031372\n",
      "Iteration 25419 Loss: 0.9548385739326477\n",
      "Iteration 25420 Loss: 1.151287317276001\n",
      "Iteration 25421 Loss: 0.9886582493782043\n",
      "Iteration 25422 Loss: 1.0419431924819946\n",
      "Iteration 25423 Loss: 0.8480672836303711\n",
      "Iteration 25424 Loss: 0.9466156363487244\n",
      "Iteration 25425 Loss: 0.8953049182891846\n",
      "Iteration 25426 Loss: 1.0447125434875488\n",
      "Iteration 25427 Loss: 0.6987137198448181\n",
      "Iteration 25428 Loss: 1.1239488124847412\n",
      "Iteration 25429 Loss: 0.9420853853225708\n",
      "Iteration 25429 Loss: 0.9681337475776672\n",
      "Iteration 25430 Loss: 1.0954669713974\n",
      "Iteration 25431 Loss: 0.679892897605896\n",
      "Iteration 25432 Loss: 1.100474238395691\n",
      "Iteration 25433 Loss: 1.129178524017334\n",
      "Iteration 25434 Loss: 0.7129653692245483\n",
      "Iteration 25435 Loss: 1.414892554283142\n",
      "Iteration 25436 Loss: 1.1438465118408203\n",
      "Iteration 25437 Loss: 0.7269080877304077\n",
      "Iteration 25438 Loss: 0.9678673148155212\n",
      "Iteration 25439 Loss: 1.0851671695709229\n",
      "Iteration 25439 Loss: 1.0056660175323486\n",
      "Iteration 25440 Loss: 0.9889865517616272\n",
      "Iteration 25441 Loss: 1.1882413625717163\n",
      "Iteration 25442 Loss: 1.1814048290252686\n",
      "Iteration 25443 Loss: 0.8534971475601196\n",
      "Iteration 25444 Loss: 0.8160504698753357\n",
      "Iteration 25445 Loss: 1.0998241901397705\n",
      "Iteration 25446 Loss: 1.20794677734375\n",
      "Iteration 25447 Loss: 0.9758514761924744\n",
      "Iteration 25448 Loss: 0.9287567138671875\n",
      "Iteration 25449 Loss: 1.038807988166809\n",
      "Iteration 25449 Loss: 1.0279366970062256\n",
      "Iteration 25450 Loss: 0.9773390293121338\n",
      "Iteration 25451 Loss: 0.9033950567245483\n",
      "Iteration 25452 Loss: 1.276673436164856\n",
      "Iteration 25453 Loss: 0.8056342005729675\n",
      "Iteration 25454 Loss: 1.1162742376327515\n",
      "Iteration 25455 Loss: 1.2708399295806885\n",
      "Iteration 25456 Loss: 0.7662819027900696\n",
      "Iteration 25457 Loss: 1.1500091552734375\n",
      "Iteration 25458 Loss: 1.2571790218353271\n",
      "Iteration 25459 Loss: 0.8839079141616821\n",
      "Iteration 25459 Loss: 1.0407533645629883\n",
      "Iteration 25460 Loss: 1.0562150478363037\n",
      "Iteration 25461 Loss: 0.7633482813835144\n",
      "Iteration 25462 Loss: 0.9416194558143616\n",
      "Iteration 25463 Loss: 1.3896301984786987\n",
      "Iteration 25464 Loss: 1.2183177471160889\n",
      "Iteration 25465 Loss: 1.2243043184280396\n",
      "Iteration 25466 Loss: 0.9787541031837463\n",
      "Iteration 25467 Loss: 1.2450226545333862\n",
      "Iteration 25468 Loss: 0.8997406363487244\n",
      "Iteration 25469 Loss: 1.3834251165390015\n",
      "Iteration 25469 Loss: 1.1100376844406128\n",
      "Iteration 25470 Loss: 1.0083667039871216\n",
      "Iteration 25471 Loss: 0.7106244564056396\n",
      "Iteration 25472 Loss: 1.187147855758667\n",
      "Iteration 25473 Loss: 1.1052204370498657\n",
      "Iteration 25474 Loss: 0.994944155216217\n",
      "Iteration 25475 Loss: 1.3258049488067627\n",
      "Iteration 25476 Loss: 1.3156827688217163\n",
      "Iteration 25477 Loss: 1.253358006477356\n",
      "Iteration 25478 Loss: 1.0119417905807495\n",
      "Iteration 25479 Loss: 1.0701333284378052\n",
      "Iteration 25479 Loss: 1.0983223915100098\n",
      "Iteration 25480 Loss: 1.2552921772003174\n",
      "Iteration 25481 Loss: 0.8619347214698792\n",
      "Iteration 25482 Loss: 0.9810281991958618\n",
      "Iteration 25483 Loss: 0.8973342776298523\n",
      "Iteration 25484 Loss: 1.1219751834869385\n",
      "Iteration 25485 Loss: 1.2052483558654785\n",
      "Iteration 25486 Loss: 0.91059809923172\n",
      "Iteration 25487 Loss: 0.9988611936569214\n",
      "Iteration 25488 Loss: 1.0011671781539917\n",
      "Iteration 25489 Loss: 1.307973027229309\n",
      "Iteration 25489 Loss: 1.0541411638259888\n",
      "Iteration 25490 Loss: 0.8927939534187317\n",
      "Iteration 25491 Loss: 1.30355966091156\n",
      "Iteration 25492 Loss: 1.0330004692077637\n",
      "Iteration 25493 Loss: 0.8671454191207886\n",
      "Iteration 25494 Loss: 1.4705591201782227\n",
      "Iteration 25495 Loss: 0.8855295777320862\n",
      "Iteration 25496 Loss: 1.1823034286499023\n",
      "Iteration 25497 Loss: 1.0020099878311157\n",
      "Iteration 25498 Loss: 0.9262182712554932\n",
      "Iteration 25499 Loss: 1.3479901552200317\n",
      "Iteration 25499 Loss: 1.0911110639572144\n",
      "Iteration 25500 Loss: 1.1782876253128052\n",
      "Iteration 25501 Loss: 1.3568676710128784\n",
      "Iteration 25502 Loss: 1.4329736232757568\n",
      "Iteration 25503 Loss: 1.1839088201522827\n",
      "Iteration 25504 Loss: 1.4190922975540161\n",
      "Iteration 25505 Loss: 0.9796568155288696\n",
      "Iteration 25506 Loss: 1.313601016998291\n",
      "Iteration 25507 Loss: 0.9961731433868408\n",
      "Iteration 25508 Loss: 1.4624074697494507\n",
      "Iteration 25509 Loss: 1.317057490348816\n",
      "Iteration 25509 Loss: 1.2640026807785034\n",
      "Iteration 25510 Loss: 1.0790690183639526\n",
      "Iteration 25511 Loss: 1.1085011959075928\n",
      "Iteration 25512 Loss: 0.9202975034713745\n",
      "Iteration 25513 Loss: 1.002423882484436\n",
      "Iteration 25514 Loss: 0.9429226517677307\n",
      "Iteration 25515 Loss: 1.2986100912094116\n",
      "Iteration 25516 Loss: 0.9215907454490662\n",
      "Iteration 25517 Loss: 0.898165762424469\n",
      "Iteration 25518 Loss: 1.1147381067276\n",
      "Iteration 25519 Loss: 1.100187063217163\n",
      "Iteration 25519 Loss: 1.038650631904602\n",
      "Iteration 25520 Loss: 1.1656231880187988\n",
      "Iteration 25521 Loss: 0.8961310982704163\n",
      "Iteration 25522 Loss: 1.0381693840026855\n",
      "Iteration 25523 Loss: 1.3320086002349854\n",
      "Iteration 25524 Loss: 1.213549256324768\n",
      "Iteration 25525 Loss: 0.7897586822509766\n",
      "Iteration 25526 Loss: 0.8854537606239319\n",
      "Iteration 25527 Loss: 0.7359061241149902\n",
      "Iteration 25528 Loss: 1.1126781702041626\n",
      "Iteration 25529 Loss: 1.197394609451294\n",
      "Iteration 25529 Loss: 1.0366674661636353\n",
      "Iteration 25530 Loss: 0.73988276720047\n",
      "Iteration 25531 Loss: 0.8718268275260925\n",
      "Iteration 25532 Loss: 1.245240569114685\n",
      "Iteration 25533 Loss: 1.502264142036438\n",
      "Iteration 25534 Loss: 1.0829755067825317\n",
      "Iteration 25535 Loss: 0.9806658029556274\n",
      "Iteration 25536 Loss: 0.9049967527389526\n",
      "Iteration 25537 Loss: 1.0902016162872314\n",
      "Iteration 25538 Loss: 0.9281457662582397\n",
      "Iteration 25539 Loss: 1.1463642120361328\n",
      "Iteration 25539 Loss: 1.049256443977356\n",
      "Iteration 25540 Loss: 0.8139897584915161\n",
      "Iteration 25541 Loss: 1.017788052558899\n",
      "Iteration 25542 Loss: 1.0865354537963867\n",
      "Iteration 25543 Loss: 0.9603103995323181\n",
      "Iteration 25544 Loss: 0.9394652843475342\n",
      "Iteration 25545 Loss: 0.92764812707901\n",
      "Iteration 25546 Loss: 1.218692660331726\n",
      "Iteration 25547 Loss: 1.1167173385620117\n",
      "Iteration 25548 Loss: 1.1994941234588623\n",
      "Iteration 25549 Loss: 0.8037984371185303\n",
      "Iteration 25549 Loss: 1.0084439516067505\n",
      "Iteration 25550 Loss: 1.2799155712127686\n",
      "Iteration 25551 Loss: 1.5179606676101685\n",
      "Iteration 25552 Loss: 1.2922135591506958\n",
      "Iteration 25553 Loss: 1.2832915782928467\n",
      "Iteration 25554 Loss: 1.074816107749939\n",
      "Iteration 25555 Loss: 1.1496434211730957\n",
      "Iteration 25556 Loss: 1.2392706871032715\n",
      "Iteration 25557 Loss: 1.114076018333435\n",
      "Iteration 25558 Loss: 1.2879328727722168\n",
      "Iteration 25559 Loss: 0.9171348214149475\n",
      "Iteration 25559 Loss: 1.215625524520874\n",
      "Iteration 25560 Loss: 0.896365225315094\n",
      "Iteration 25561 Loss: 1.4865214824676514\n",
      "Iteration 25562 Loss: 1.013137936592102\n",
      "Iteration 25563 Loss: 1.225415825843811\n",
      "Iteration 25564 Loss: 0.8743189573287964\n",
      "Iteration 25565 Loss: 1.0444334745407104\n",
      "Iteration 25566 Loss: 1.2254858016967773\n",
      "Iteration 25567 Loss: 0.9621648788452148\n",
      "Iteration 25568 Loss: 0.645494282245636\n",
      "Iteration 25569 Loss: 1.0935487747192383\n",
      "Iteration 25569 Loss: 1.0466886758804321\n",
      "Iteration 25570 Loss: 0.8928713798522949\n",
      "Iteration 25571 Loss: 0.6770387887954712\n",
      "Iteration 25572 Loss: 1.208614706993103\n",
      "Iteration 25573 Loss: 0.9872680902481079\n",
      "Iteration 25574 Loss: 1.1038659811019897\n",
      "Iteration 25575 Loss: 0.9950920939445496\n",
      "Iteration 25576 Loss: 0.9944347739219666\n",
      "Iteration 25577 Loss: 0.8958168029785156\n",
      "Iteration 25578 Loss: 0.7563978433609009\n",
      "Iteration 25579 Loss: 1.171447515487671\n",
      "Iteration 25579 Loss: 0.9682847857475281\n",
      "Iteration 25580 Loss: 1.196283221244812\n",
      "Iteration 25581 Loss: 0.94525146484375\n",
      "Iteration 25582 Loss: 1.505907416343689\n",
      "Iteration 25583 Loss: 0.8735213279724121\n",
      "Iteration 25584 Loss: 1.224086046218872\n",
      "Iteration 25585 Loss: 1.0657862424850464\n",
      "Iteration 25586 Loss: 1.1729438304901123\n",
      "Iteration 25587 Loss: 1.0815712213516235\n",
      "Iteration 25588 Loss: 1.1724727153778076\n",
      "Iteration 25589 Loss: 1.1009316444396973\n",
      "Iteration 25589 Loss: 1.1338756084442139\n",
      "Iteration 25590 Loss: 1.0525778532028198\n",
      "Iteration 25591 Loss: 0.8962196707725525\n",
      "Iteration 25592 Loss: 1.4087294340133667\n",
      "Iteration 25593 Loss: 1.1979902982711792\n",
      "Iteration 25594 Loss: 0.9322911500930786\n",
      "Iteration 25595 Loss: 1.0391361713409424\n",
      "Iteration 25596 Loss: 1.5057761669158936\n",
      "Iteration 25597 Loss: 1.0216169357299805\n",
      "Iteration 25598 Loss: 1.335218071937561\n",
      "Iteration 25599 Loss: 1.1603803634643555\n",
      "Iteration 25599 Loss: 1.1549936532974243\n",
      "Iteration 25600 Loss: 1.1197421550750732\n",
      "Iteration 25601 Loss: 0.9017612338066101\n",
      "Iteration 25602 Loss: 1.2028924226760864\n",
      "Iteration 25603 Loss: 1.0213873386383057\n",
      "Iteration 25604 Loss: 1.117052674293518\n",
      "Iteration 25605 Loss: 1.0260316133499146\n",
      "Iteration 25606 Loss: 0.996331512928009\n",
      "Iteration 25607 Loss: 1.0225250720977783\n",
      "Iteration 25608 Loss: 1.1121591329574585\n",
      "Iteration 25609 Loss: 1.104360818862915\n",
      "Iteration 25609 Loss: 1.0624244213104248\n",
      "Iteration 25610 Loss: 0.7083977460861206\n",
      "Iteration 25611 Loss: 1.1429085731506348\n",
      "Iteration 25612 Loss: 1.0664507150650024\n",
      "Iteration 25613 Loss: 1.1465058326721191\n",
      "Iteration 25614 Loss: 0.7996468544006348\n",
      "Iteration 25615 Loss: 1.326164722442627\n",
      "Iteration 25616 Loss: 0.9925627112388611\n",
      "Iteration 25617 Loss: 1.1594938039779663\n",
      "Iteration 25618 Loss: 1.2302273511886597\n",
      "Iteration 25619 Loss: 0.8861517310142517\n",
      "Iteration 25619 Loss: 1.0458509922027588\n",
      "Iteration 25620 Loss: 0.8699328899383545\n",
      "Iteration 25621 Loss: 1.1498079299926758\n",
      "Iteration 25622 Loss: 1.0910193920135498\n",
      "Iteration 25623 Loss: 1.3008975982666016\n",
      "Iteration 25624 Loss: 1.071703553199768\n",
      "Iteration 25625 Loss: 0.9797969460487366\n",
      "Iteration 25626 Loss: 0.8317601680755615\n",
      "Iteration 25627 Loss: 0.7628215551376343\n",
      "Iteration 25628 Loss: 1.1719568967819214\n",
      "Iteration 25629 Loss: 1.0985480546951294\n",
      "Iteration 25629 Loss: 1.0328245162963867\n",
      "Iteration 25630 Loss: 1.0171431303024292\n",
      "Iteration 25631 Loss: 1.1290068626403809\n",
      "Iteration 25632 Loss: 1.0543500185012817\n",
      "Iteration 25633 Loss: 1.0846887826919556\n",
      "Iteration 25634 Loss: 1.360058069229126\n",
      "Iteration 25635 Loss: 0.7653809189796448\n",
      "Iteration 25636 Loss: 1.0208081007003784\n",
      "Iteration 25637 Loss: 1.0001426935195923\n",
      "Iteration 25638 Loss: 0.9879975318908691\n",
      "Iteration 25639 Loss: 0.9113825559616089\n",
      "Iteration 25639 Loss: 1.0330959558486938\n",
      "Iteration 25640 Loss: 0.8350014686584473\n",
      "Iteration 25641 Loss: 0.9971970319747925\n",
      "Iteration 25642 Loss: 0.9305801391601562\n",
      "Iteration 25643 Loss: 1.143820881843567\n",
      "Iteration 25644 Loss: 1.2908787727355957\n",
      "Iteration 25645 Loss: 1.2346044778823853\n",
      "Iteration 25646 Loss: 1.0016556978225708\n",
      "Iteration 25647 Loss: 0.8997872471809387\n",
      "Iteration 25648 Loss: 1.0809963941574097\n",
      "Iteration 25649 Loss: 1.0985527038574219\n",
      "Iteration 25649 Loss: 1.0513074398040771\n",
      "Iteration 25650 Loss: 0.9585409760475159\n",
      "Iteration 25651 Loss: 1.2890576124191284\n",
      "Iteration 25652 Loss: 1.4947924613952637\n",
      "Iteration 25653 Loss: 1.2237330675125122\n",
      "Iteration 25654 Loss: 1.1256929636001587\n",
      "Iteration 25655 Loss: 1.1334762573242188\n",
      "Iteration 25656 Loss: 1.1678404808044434\n",
      "Iteration 25657 Loss: 1.0173168182373047\n",
      "Iteration 25658 Loss: 1.1711347103118896\n",
      "Iteration 25659 Loss: 1.205994963645935\n",
      "Iteration 25659 Loss: 1.1787580251693726\n",
      "Iteration 25660 Loss: 0.9345086216926575\n",
      "Iteration 25661 Loss: 1.0963776111602783\n",
      "Iteration 25662 Loss: 0.8014529943466187\n",
      "Iteration 25663 Loss: 1.266079068183899\n",
      "Iteration 25664 Loss: 1.0883769989013672\n",
      "Iteration 25665 Loss: 1.0657305717468262\n",
      "Iteration 25666 Loss: 1.252761721611023\n",
      "Iteration 25667 Loss: 0.8863720297813416\n",
      "Iteration 25668 Loss: 0.9801684617996216\n",
      "Iteration 25669 Loss: 1.0809661149978638\n",
      "Iteration 25669 Loss: 1.0452793836593628\n",
      "Iteration 25670 Loss: 1.0502912998199463\n",
      "Iteration 25671 Loss: 1.039482593536377\n",
      "Iteration 25672 Loss: 1.2538299560546875\n",
      "Iteration 25673 Loss: 0.9781603813171387\n",
      "Iteration 25674 Loss: 1.3089014291763306\n",
      "Iteration 25675 Loss: 1.214323878288269\n",
      "Iteration 25676 Loss: 0.888462483882904\n",
      "Iteration 25677 Loss: 0.9315280318260193\n",
      "Iteration 25678 Loss: 1.7293599843978882\n",
      "Iteration 25679 Loss: 1.0928254127502441\n",
      "Iteration 25679 Loss: 1.1487165689468384\n",
      "Iteration 25680 Loss: 0.7782325744628906\n",
      "Iteration 25681 Loss: 0.697172224521637\n",
      "Iteration 25682 Loss: 0.885824978351593\n",
      "Iteration 25683 Loss: 1.0470407009124756\n",
      "Iteration 25684 Loss: 1.379445195198059\n",
      "Iteration 25685 Loss: 1.4278231859207153\n",
      "Iteration 25686 Loss: 1.053417682647705\n",
      "Iteration 25687 Loss: 1.0977858304977417\n",
      "Iteration 25688 Loss: 0.941981852054596\n",
      "Iteration 25689 Loss: 0.8775336742401123\n",
      "Iteration 25689 Loss: 1.0186258554458618\n",
      "Iteration 25690 Loss: 0.6447535753250122\n",
      "Iteration 25691 Loss: 1.1212412118911743\n",
      "Iteration 25692 Loss: 0.9042026996612549\n",
      "Iteration 25693 Loss: 1.1773971319198608\n",
      "Iteration 25694 Loss: 1.3962721824645996\n",
      "Iteration 25695 Loss: 1.3770519495010376\n",
      "Iteration 25696 Loss: 1.1482219696044922\n",
      "Iteration 25697 Loss: 1.146867036819458\n",
      "Iteration 25698 Loss: 1.078626036643982\n",
      "Iteration 25699 Loss: 0.9054732918739319\n",
      "Iteration 25699 Loss: 1.0900107622146606\n",
      "Iteration 25700 Loss: 1.1788549423217773\n",
      "Iteration 25701 Loss: 1.4119676351547241\n",
      "Iteration 25702 Loss: 1.1310460567474365\n",
      "Iteration 25703 Loss: 1.3429062366485596\n",
      "Iteration 25704 Loss: 0.9312235116958618\n",
      "Iteration 25705 Loss: 1.2568799257278442\n",
      "Iteration 25706 Loss: 1.1101478338241577\n",
      "Iteration 25707 Loss: 1.2429372072219849\n",
      "Iteration 25708 Loss: 1.0137903690338135\n",
      "Iteration 25709 Loss: 1.0617564916610718\n",
      "Iteration 25709 Loss: 1.1681510210037231\n",
      "Iteration 25710 Loss: 1.2303038835525513\n",
      "Iteration 25711 Loss: 1.0777292251586914\n",
      "Iteration 25712 Loss: 0.9668659567832947\n",
      "Iteration 25713 Loss: 1.2306069135665894\n",
      "Iteration 25714 Loss: 0.9430902600288391\n",
      "Iteration 25715 Loss: 0.6429169178009033\n",
      "Iteration 25716 Loss: 1.0737245082855225\n",
      "Iteration 25717 Loss: 0.8938655257225037\n",
      "Iteration 25718 Loss: 1.170941948890686\n",
      "Iteration 25719 Loss: 0.792005717754364\n",
      "Iteration 25719 Loss: 1.0022051334381104\n",
      "Iteration 25720 Loss: 1.316224217414856\n",
      "Iteration 25721 Loss: 1.1371787786483765\n",
      "Iteration 25722 Loss: 0.9796142578125\n",
      "Iteration 25723 Loss: 1.083709955215454\n",
      "Iteration 25724 Loss: 1.0954755544662476\n",
      "Iteration 25725 Loss: 1.1981778144836426\n",
      "Iteration 25726 Loss: 0.9410626888275146\n",
      "Iteration 25727 Loss: 1.2670761346817017\n",
      "Iteration 25728 Loss: 0.9251103401184082\n",
      "Iteration 25729 Loss: 1.1032010316848755\n",
      "Iteration 25729 Loss: 1.1046831607818604\n",
      "Iteration 25730 Loss: 1.2078135013580322\n",
      "Iteration 25731 Loss: 0.7876376509666443\n",
      "Iteration 25732 Loss: 0.8997342586517334\n",
      "Iteration 25733 Loss: 1.3364046812057495\n",
      "Iteration 25734 Loss: 1.152317762374878\n",
      "Iteration 25735 Loss: 0.9705554246902466\n",
      "Iteration 25736 Loss: 1.423520803451538\n",
      "Iteration 25737 Loss: 0.9434211254119873\n",
      "Iteration 25738 Loss: 1.394715428352356\n",
      "Iteration 25739 Loss: 1.0421229600906372\n",
      "Iteration 25739 Loss: 1.1158244609832764\n",
      "Iteration 25740 Loss: 0.9931960701942444\n",
      "Iteration 25741 Loss: 1.041960597038269\n",
      "Iteration 25742 Loss: 0.7945507764816284\n",
      "Iteration 25743 Loss: 1.0800355672836304\n",
      "Iteration 25744 Loss: 1.023837924003601\n",
      "Iteration 25745 Loss: 0.5910881161689758\n",
      "Iteration 25746 Loss: 1.2650208473205566\n",
      "Iteration 25747 Loss: 0.8198944926261902\n",
      "Iteration 25748 Loss: 0.7469993233680725\n",
      "Iteration 25749 Loss: 0.9801971912384033\n",
      "Iteration 25749 Loss: 0.933678150177002\n",
      "Iteration 25750 Loss: 1.1007882356643677\n",
      "Iteration 25751 Loss: 1.4548012018203735\n",
      "Iteration 25752 Loss: 0.7889783382415771\n",
      "Iteration 25753 Loss: 1.1323915719985962\n",
      "Iteration 25754 Loss: 0.6416869759559631\n",
      "Iteration 25755 Loss: 0.8097445964813232\n",
      "Iteration 25756 Loss: 1.1092116832733154\n",
      "Iteration 25757 Loss: 1.04314124584198\n",
      "Iteration 25758 Loss: 0.8809505701065063\n",
      "Iteration 25759 Loss: 0.9930459856987\n",
      "Iteration 25759 Loss: 0.9954740405082703\n",
      "Iteration 25760 Loss: 1.1783608198165894\n",
      "Iteration 25761 Loss: 1.0550287961959839\n",
      "Iteration 25762 Loss: 1.2331013679504395\n",
      "Iteration 25763 Loss: 1.1698330640792847\n",
      "Iteration 25764 Loss: 0.7477527856826782\n",
      "Iteration 25765 Loss: 0.9811089634895325\n",
      "Iteration 25766 Loss: 0.8863431215286255\n",
      "Iteration 25767 Loss: 1.0448055267333984\n",
      "Iteration 25768 Loss: 1.2175676822662354\n",
      "Iteration 25769 Loss: 1.1788029670715332\n",
      "Iteration 25769 Loss: 1.0692704916000366\n",
      "Iteration 25770 Loss: 1.5602822303771973\n",
      "Iteration 25771 Loss: 0.9128602147102356\n",
      "Iteration 25772 Loss: 1.0645707845687866\n",
      "Iteration 25773 Loss: 1.1167019605636597\n",
      "Iteration 25774 Loss: 1.2854832410812378\n",
      "Iteration 25775 Loss: 0.9108756184577942\n",
      "Iteration 25776 Loss: 1.1794320344924927\n",
      "Iteration 25777 Loss: 1.3010224103927612\n",
      "Iteration 25778 Loss: 1.032795786857605\n",
      "Iteration 25779 Loss: 0.9405852556228638\n",
      "Iteration 25779 Loss: 1.1304609775543213\n",
      "Iteration 25780 Loss: 1.1227679252624512\n",
      "Iteration 25781 Loss: 0.7987755537033081\n",
      "Iteration 25782 Loss: 1.15855073928833\n",
      "Iteration 25783 Loss: 0.571186363697052\n",
      "Iteration 25784 Loss: 1.2595295906066895\n",
      "Iteration 25785 Loss: 1.042037844657898\n",
      "Iteration 25786 Loss: 1.5327485799789429\n",
      "Iteration 25787 Loss: 0.8542366027832031\n",
      "Iteration 25788 Loss: 1.2491685152053833\n",
      "Iteration 25789 Loss: 1.3996541500091553\n",
      "Iteration 25789 Loss: 1.0988656282424927\n",
      "Iteration 25790 Loss: 0.6525384783744812\n",
      "Iteration 25791 Loss: 1.191193699836731\n",
      "Iteration 25792 Loss: 0.8608331084251404\n",
      "Iteration 25793 Loss: 1.0303363800048828\n",
      "Iteration 25794 Loss: 1.005712628364563\n",
      "Iteration 25795 Loss: 0.8144829869270325\n",
      "Iteration 25796 Loss: 0.9888672232627869\n",
      "Iteration 25797 Loss: 0.9090145230293274\n",
      "Iteration 25798 Loss: 1.3583474159240723\n",
      "Iteration 25799 Loss: 0.9745137095451355\n",
      "Iteration 25799 Loss: 0.9785839915275574\n",
      "Iteration 25800 Loss: 0.8568145036697388\n",
      "Iteration 25801 Loss: 0.7779693603515625\n",
      "Iteration 25802 Loss: 1.1605271100997925\n",
      "Iteration 25803 Loss: 1.2898588180541992\n",
      "Iteration 25804 Loss: 0.9424839019775391\n",
      "Iteration 25805 Loss: 1.2276803255081177\n",
      "Iteration 25806 Loss: 1.0288032293319702\n",
      "Iteration 25807 Loss: 1.2413201332092285\n",
      "Iteration 25808 Loss: 1.2265633344650269\n",
      "Iteration 25809 Loss: 0.7835439443588257\n",
      "Iteration 25809 Loss: 1.0535564422607422\n",
      "Iteration 25810 Loss: 1.0497500896453857\n",
      "Iteration 25811 Loss: 1.1375592947006226\n",
      "Iteration 25812 Loss: 1.217811107635498\n",
      "Iteration 25813 Loss: 0.9939406514167786\n",
      "Iteration 25814 Loss: 1.1126779317855835\n",
      "Iteration 25815 Loss: 0.5501875281333923\n",
      "Iteration 25816 Loss: 1.0688961744308472\n",
      "Iteration 25817 Loss: 1.1696691513061523\n",
      "Iteration 25818 Loss: 0.9237419366836548\n",
      "Iteration 25819 Loss: 1.0349026918411255\n",
      "Iteration 25819 Loss: 1.0259137153625488\n",
      "Iteration 25820 Loss: 1.4032756090164185\n",
      "Iteration 25821 Loss: 1.3286149501800537\n",
      "Iteration 25822 Loss: 0.9363467693328857\n",
      "Iteration 25823 Loss: 0.9858968257904053\n",
      "Iteration 25824 Loss: 1.2493563890457153\n",
      "Iteration 25825 Loss: 0.7191486358642578\n",
      "Iteration 25826 Loss: 1.0357407331466675\n",
      "Iteration 25827 Loss: 1.2561147212982178\n",
      "Iteration 25828 Loss: 0.9636407494544983\n",
      "Iteration 25829 Loss: 0.9549309015274048\n",
      "Iteration 25829 Loss: 1.0833066701889038\n",
      "Iteration 25830 Loss: 1.4018906354904175\n",
      "Iteration 25831 Loss: 1.2022026777267456\n",
      "Iteration 25832 Loss: 1.0827405452728271\n",
      "Iteration 25833 Loss: 1.1416172981262207\n",
      "Iteration 25834 Loss: 1.4123823642730713\n",
      "Iteration 25835 Loss: 0.9437086582183838\n",
      "Iteration 25836 Loss: 0.9539856910705566\n",
      "Iteration 25837 Loss: 1.0366358757019043\n",
      "Iteration 25838 Loss: 1.0362849235534668\n",
      "Iteration 25839 Loss: 1.0695661306381226\n",
      "Iteration 25839 Loss: 1.1281015872955322\n",
      "Iteration 25840 Loss: 1.3502709865570068\n",
      "Iteration 25841 Loss: 1.1339114904403687\n",
      "Iteration 25842 Loss: 1.1562100648880005\n",
      "Iteration 25843 Loss: 1.3283393383026123\n",
      "Iteration 25844 Loss: 0.8353223204612732\n",
      "Iteration 25845 Loss: 1.2123998403549194\n",
      "Iteration 25846 Loss: 1.1649396419525146\n",
      "Iteration 25847 Loss: 0.8742655515670776\n",
      "Iteration 25848 Loss: 0.8353842496871948\n",
      "Iteration 25849 Loss: 1.4266618490219116\n",
      "Iteration 25849 Loss: 1.1317704916000366\n",
      "Iteration 25850 Loss: 0.9090170860290527\n",
      "Iteration 25851 Loss: 0.7949999570846558\n",
      "Iteration 25852 Loss: 1.16057288646698\n",
      "Iteration 25853 Loss: 1.0653817653656006\n",
      "Iteration 25854 Loss: 0.9545135498046875\n",
      "Iteration 25855 Loss: 1.1751362085342407\n",
      "Iteration 25856 Loss: 0.9761142730712891\n",
      "Iteration 25857 Loss: 0.8146966099739075\n",
      "Iteration 25858 Loss: 0.7419418692588806\n",
      "Iteration 25859 Loss: 1.5222992897033691\n",
      "Iteration 25859 Loss: 1.0114673376083374\n",
      "Iteration 25860 Loss: 1.0350501537322998\n",
      "Iteration 25861 Loss: 1.1166796684265137\n",
      "Iteration 25862 Loss: 1.2165765762329102\n",
      "Iteration 25863 Loss: 1.3155006170272827\n",
      "Iteration 25864 Loss: 1.1228065490722656\n",
      "Iteration 25865 Loss: 1.1821447610855103\n",
      "Iteration 25866 Loss: 0.7156590223312378\n",
      "Iteration 25867 Loss: 1.2637907266616821\n",
      "Iteration 25868 Loss: 1.1026535034179688\n",
      "Iteration 25869 Loss: 0.9721279144287109\n",
      "Iteration 25869 Loss: 1.1042989492416382\n",
      "Iteration 25870 Loss: 0.9359052777290344\n",
      "Iteration 25871 Loss: 1.2574454545974731\n",
      "Iteration 25872 Loss: 1.0009227991104126\n",
      "Iteration 25873 Loss: 1.2717418670654297\n",
      "Iteration 25874 Loss: 0.7800456881523132\n",
      "Iteration 25875 Loss: 0.8655068278312683\n",
      "Iteration 25876 Loss: 1.007143497467041\n",
      "Iteration 25877 Loss: 1.0696524381637573\n",
      "Iteration 25878 Loss: 1.2878950834274292\n",
      "Iteration 25879 Loss: 1.0077452659606934\n",
      "Iteration 25879 Loss: 1.0484005212783813\n",
      "Iteration 25880 Loss: 1.0821373462677002\n",
      "Iteration 25881 Loss: 1.325793981552124\n",
      "Iteration 25882 Loss: 1.3910807371139526\n",
      "Iteration 25883 Loss: 1.6196742057800293\n",
      "Iteration 25884 Loss: 1.0476229190826416\n",
      "Iteration 25885 Loss: 1.1025207042694092\n",
      "Iteration 25886 Loss: 0.7169507741928101\n",
      "Iteration 25887 Loss: 1.1190205812454224\n",
      "Iteration 25888 Loss: 0.7503650188446045\n",
      "Iteration 25889 Loss: 1.138291597366333\n",
      "Iteration 25889 Loss: 1.1293457746505737\n",
      "Iteration 25890 Loss: 1.297016978263855\n",
      "Iteration 25891 Loss: 1.1552460193634033\n",
      "Iteration 25892 Loss: 1.0664136409759521\n",
      "Iteration 25893 Loss: 1.2036924362182617\n",
      "Iteration 25894 Loss: 1.8791687488555908\n",
      "Iteration 25895 Loss: 1.0331982374191284\n",
      "Iteration 25896 Loss: 1.0396497249603271\n",
      "Iteration 25897 Loss: 0.74733966588974\n",
      "Iteration 25898 Loss: 0.8402748107910156\n",
      "Iteration 25899 Loss: 1.3314003944396973\n",
      "Iteration 25899 Loss: 1.1593401432037354\n",
      "Iteration 25900 Loss: 0.7484577894210815\n",
      "Iteration 25901 Loss: 0.9494611620903015\n",
      "Iteration 25902 Loss: 0.7594311237335205\n",
      "Iteration 25903 Loss: 1.2095016241073608\n",
      "Iteration 25904 Loss: 1.2236027717590332\n",
      "Iteration 25905 Loss: 0.9966027736663818\n",
      "Iteration 25906 Loss: 1.2088568210601807\n",
      "Iteration 25907 Loss: 1.1423527002334595\n",
      "Iteration 25908 Loss: 1.0105417966842651\n",
      "Iteration 25909 Loss: 1.3033926486968994\n",
      "Iteration 25909 Loss: 1.055220127105713\n",
      "Iteration 25910 Loss: 0.9304524064064026\n",
      "Iteration 25911 Loss: 1.2125979661941528\n",
      "Iteration 25912 Loss: 1.3973323106765747\n",
      "Iteration 25913 Loss: 0.8259173035621643\n",
      "Iteration 25914 Loss: 1.1720380783081055\n",
      "Iteration 25915 Loss: 0.8284560441970825\n",
      "Iteration 25916 Loss: 1.1589951515197754\n",
      "Iteration 25917 Loss: 0.9368265271186829\n",
      "Iteration 25918 Loss: 1.1732361316680908\n",
      "Iteration 25919 Loss: 0.7716725468635559\n",
      "Iteration 25919 Loss: 1.0407524108886719\n",
      "Iteration 25920 Loss: 0.6895345449447632\n",
      "Iteration 25921 Loss: 0.739523708820343\n",
      "Iteration 25922 Loss: 1.2218133211135864\n",
      "Iteration 25923 Loss: 0.8212404251098633\n",
      "Iteration 25924 Loss: 0.8116011619567871\n",
      "Iteration 25925 Loss: 1.1305930614471436\n",
      "Iteration 25926 Loss: 0.7524810433387756\n",
      "Iteration 25927 Loss: 0.769524872303009\n",
      "Iteration 25928 Loss: 1.3613345623016357\n",
      "Iteration 25929 Loss: 1.4285998344421387\n",
      "Iteration 25929 Loss: 0.972624659538269\n",
      "Iteration 25930 Loss: 0.9586781859397888\n",
      "Iteration 25931 Loss: 1.0910061597824097\n",
      "Iteration 25932 Loss: 0.8972676992416382\n",
      "Iteration 25933 Loss: 1.1200979948043823\n",
      "Iteration 25934 Loss: 0.7112448811531067\n",
      "Iteration 25935 Loss: 1.6186394691467285\n",
      "Iteration 25936 Loss: 1.0901517868041992\n",
      "Iteration 25937 Loss: 1.2719905376434326\n",
      "Iteration 25938 Loss: 1.023484468460083\n",
      "Iteration 25939 Loss: 1.3661646842956543\n",
      "Iteration 25939 Loss: 1.114872694015503\n",
      "Iteration 25940 Loss: 1.0205620527267456\n",
      "Iteration 25941 Loss: 1.1178953647613525\n",
      "Iteration 25942 Loss: 1.148159384727478\n",
      "Iteration 25943 Loss: 1.8074570894241333\n",
      "Iteration 25944 Loss: 1.03180730342865\n",
      "Iteration 25945 Loss: 1.3530831336975098\n",
      "Iteration 25946 Loss: 0.8833556771278381\n",
      "Iteration 25947 Loss: 1.452924370765686\n",
      "Iteration 25948 Loss: 1.0246669054031372\n",
      "Iteration 25949 Loss: 1.1721000671386719\n",
      "Iteration 25949 Loss: 1.2012012004852295\n",
      "Iteration 25950 Loss: 1.2963918447494507\n",
      "Iteration 25951 Loss: 1.224765658378601\n",
      "Iteration 25952 Loss: 1.2482103109359741\n",
      "Iteration 25953 Loss: 1.2056691646575928\n",
      "Iteration 25954 Loss: 0.7800815105438232\n",
      "Iteration 25955 Loss: 1.128147840499878\n",
      "Iteration 25956 Loss: 0.9703850150108337\n",
      "Iteration 25957 Loss: 1.2966302633285522\n",
      "Iteration 25958 Loss: 1.1889142990112305\n",
      "Iteration 25959 Loss: 1.3272517919540405\n",
      "Iteration 25959 Loss: 1.166644811630249\n",
      "Iteration 25960 Loss: 1.2700406312942505\n",
      "Iteration 25961 Loss: 1.0213675498962402\n",
      "Iteration 25962 Loss: 1.0495193004608154\n",
      "Iteration 25963 Loss: 1.0339775085449219\n",
      "Iteration 25964 Loss: 0.6383049488067627\n",
      "Iteration 25965 Loss: 0.9441459774971008\n",
      "Iteration 25966 Loss: 1.148578405380249\n",
      "Iteration 25967 Loss: 1.3484824895858765\n",
      "Iteration 25968 Loss: 1.085360050201416\n",
      "Iteration 25969 Loss: 0.6928734183311462\n",
      "Iteration 25969 Loss: 1.02326500415802\n",
      "Iteration 25970 Loss: 1.1918774843215942\n",
      "Iteration 25971 Loss: 1.0069098472595215\n",
      "Iteration 25972 Loss: 1.1528024673461914\n",
      "Iteration 25973 Loss: 1.377770185470581\n",
      "Iteration 25974 Loss: 1.1811705827713013\n",
      "Iteration 25975 Loss: 1.318587303161621\n",
      "Iteration 25976 Loss: 1.1756330728530884\n",
      "Iteration 25977 Loss: 0.8375348448753357\n",
      "Iteration 25978 Loss: 1.0765413045883179\n",
      "Iteration 25979 Loss: 1.119164228439331\n",
      "Iteration 25979 Loss: 1.143799066543579\n",
      "Iteration 25980 Loss: 1.269681692123413\n",
      "Iteration 25981 Loss: 1.1756588220596313\n",
      "Iteration 25982 Loss: 0.9638112187385559\n",
      "Iteration 25983 Loss: 0.9421802163124084\n",
      "Iteration 25984 Loss: 1.0979894399642944\n",
      "Iteration 25985 Loss: 0.738260805606842\n",
      "Iteration 25986 Loss: 1.0261898040771484\n",
      "Iteration 25987 Loss: 1.109403133392334\n",
      "Iteration 25988 Loss: 1.2691266536712646\n",
      "Iteration 25989 Loss: 0.885377824306488\n",
      "Iteration 25989 Loss: 1.047767996788025\n",
      "Iteration 25990 Loss: 1.3697178363800049\n",
      "Iteration 25991 Loss: 0.8184574246406555\n",
      "Iteration 25992 Loss: 0.6528298258781433\n",
      "Iteration 25993 Loss: 1.223416805267334\n",
      "Iteration 25994 Loss: 0.9122338891029358\n",
      "Iteration 25995 Loss: 0.9807921051979065\n",
      "Iteration 25996 Loss: 0.7308367490768433\n",
      "Iteration 25997 Loss: 0.97572922706604\n",
      "Iteration 25998 Loss: 1.0719590187072754\n",
      "Iteration 25999 Loss: 0.5533896684646606\n",
      "Iteration 25999 Loss: 0.9289361834526062\n",
      "Iteration 26000 Loss: 0.9619056582450867\n",
      "Iteration 26001 Loss: 1.1647974252700806\n",
      "Iteration 26002 Loss: 1.1732048988342285\n",
      "Iteration 26003 Loss: 1.2206884622573853\n",
      "Iteration 26004 Loss: 0.8156002759933472\n",
      "Iteration 26005 Loss: 0.9666344523429871\n",
      "Iteration 26006 Loss: 1.2040729522705078\n",
      "Iteration 26007 Loss: 1.2738507986068726\n",
      "Iteration 26008 Loss: 1.0984150171279907\n",
      "Iteration 26009 Loss: 0.9663467407226562\n",
      "Iteration 26009 Loss: 1.0845515727996826\n",
      "Iteration 26010 Loss: 0.9743387699127197\n",
      "Iteration 26011 Loss: 1.8401116132736206\n",
      "Iteration 26012 Loss: 0.8902532458305359\n",
      "Iteration 26013 Loss: 0.686598539352417\n",
      "Iteration 26014 Loss: 0.9087692499160767\n",
      "Iteration 26015 Loss: 0.8803905844688416\n",
      "Iteration 26016 Loss: 1.2974847555160522\n",
      "Iteration 26017 Loss: 0.911294162273407\n",
      "Iteration 26018 Loss: 0.9865708947181702\n",
      "Iteration 26019 Loss: 1.5046284198760986\n",
      "Iteration 26019 Loss: 1.0880439281463623\n",
      "Iteration 26020 Loss: 0.7632396221160889\n",
      "Iteration 26021 Loss: 0.8594620823860168\n",
      "Iteration 26022 Loss: 1.4321750402450562\n",
      "Iteration 26023 Loss: 1.0238409042358398\n",
      "Iteration 26024 Loss: 1.1036690473556519\n",
      "Iteration 26025 Loss: 0.776886522769928\n",
      "Iteration 26026 Loss: 1.0317230224609375\n",
      "Iteration 26027 Loss: 0.8358538150787354\n",
      "Iteration 26028 Loss: 1.0563623905181885\n",
      "Iteration 26029 Loss: 0.8511496782302856\n",
      "Iteration 26029 Loss: 0.973436176776886\n",
      "Iteration 26030 Loss: 0.8796380162239075\n",
      "Iteration 26031 Loss: 1.322074055671692\n",
      "Iteration 26032 Loss: 0.9722025394439697\n",
      "Iteration 26033 Loss: 0.9894658923149109\n",
      "Iteration 26034 Loss: 1.443476676940918\n",
      "Iteration 26035 Loss: 0.8191660642623901\n",
      "Iteration 26036 Loss: 1.097633719444275\n",
      "Iteration 26037 Loss: 0.738953709602356\n",
      "Iteration 26038 Loss: 0.829136312007904\n",
      "Iteration 26039 Loss: 1.1532785892486572\n",
      "Iteration 26039 Loss: 1.0245025157928467\n",
      "Iteration 26040 Loss: 1.025916576385498\n",
      "Iteration 26041 Loss: 0.9390750527381897\n",
      "Iteration 26042 Loss: 1.1452089548110962\n",
      "Iteration 26043 Loss: 0.9093103408813477\n",
      "Iteration 26044 Loss: 0.967221200466156\n",
      "Iteration 26045 Loss: 1.268918514251709\n",
      "Iteration 26046 Loss: 1.1964296102523804\n",
      "Iteration 26047 Loss: 1.3164459466934204\n",
      "Iteration 26048 Loss: 1.2135065793991089\n",
      "Iteration 26049 Loss: 1.0814052820205688\n",
      "Iteration 26049 Loss: 1.1063437461853027\n",
      "Iteration 26050 Loss: 1.2028491497039795\n",
      "Iteration 26051 Loss: 1.1868689060211182\n",
      "Iteration 26052 Loss: 1.3424270153045654\n",
      "Iteration 26053 Loss: 1.1407040357589722\n",
      "Iteration 26054 Loss: 0.8967561721801758\n",
      "Iteration 26055 Loss: 1.1105295419692993\n",
      "Iteration 26056 Loss: 1.20448637008667\n",
      "Iteration 26057 Loss: 0.9888327121734619\n",
      "Iteration 26058 Loss: 1.1952506303787231\n",
      "Iteration 26059 Loss: 0.8778994083404541\n",
      "Iteration 26059 Loss: 1.1146605014801025\n",
      "Iteration 26060 Loss: 0.7282754182815552\n",
      "Iteration 26061 Loss: 0.9759141206741333\n",
      "Iteration 26062 Loss: 0.8835998177528381\n",
      "Iteration 26063 Loss: 0.718573272228241\n",
      "Iteration 26064 Loss: 0.7986217737197876\n",
      "Iteration 26065 Loss: 1.143481731414795\n",
      "Iteration 26066 Loss: 0.8261845707893372\n",
      "Iteration 26067 Loss: 0.9150756597518921\n",
      "Iteration 26068 Loss: 0.9273821115493774\n",
      "Iteration 26069 Loss: 0.882378339767456\n",
      "Iteration 26069 Loss: 0.879948616027832\n",
      "Iteration 26070 Loss: 1.3160128593444824\n",
      "Iteration 26071 Loss: 0.9033275842666626\n",
      "Iteration 26072 Loss: 1.285702109336853\n",
      "Iteration 26073 Loss: 1.1430844068527222\n",
      "Iteration 26074 Loss: 1.2108819484710693\n",
      "Iteration 26075 Loss: 1.0405676364898682\n",
      "Iteration 26076 Loss: 1.0053049325942993\n",
      "Iteration 26077 Loss: 1.1870142221450806\n",
      "Iteration 26078 Loss: 1.0655373334884644\n",
      "Iteration 26079 Loss: 1.103216290473938\n",
      "Iteration 26079 Loss: 1.1260650157928467\n",
      "Iteration 26080 Loss: 1.1293593645095825\n",
      "Iteration 26081 Loss: 0.6914038062095642\n",
      "Iteration 26082 Loss: 1.0671653747558594\n",
      "Iteration 26083 Loss: 1.1746143102645874\n",
      "Iteration 26084 Loss: 1.0632179975509644\n",
      "Iteration 26085 Loss: 1.188661813735962\n",
      "Iteration 26086 Loss: 1.1979044675827026\n",
      "Iteration 26087 Loss: 0.9628661274909973\n",
      "Iteration 26088 Loss: 0.9654859304428101\n",
      "Iteration 26089 Loss: 1.1510467529296875\n",
      "Iteration 26089 Loss: 1.0591726303100586\n",
      "Iteration 26090 Loss: 0.6959340572357178\n",
      "Iteration 26091 Loss: 1.5303491353988647\n",
      "Iteration 26092 Loss: 1.3020273447036743\n",
      "Iteration 26093 Loss: 0.8215603828430176\n",
      "Iteration 26094 Loss: 1.1384354829788208\n",
      "Iteration 26095 Loss: 1.1360260248184204\n",
      "Iteration 26096 Loss: 0.7659125924110413\n",
      "Iteration 26097 Loss: 0.8889428973197937\n",
      "Iteration 26098 Loss: 1.1012459993362427\n",
      "Iteration 26099 Loss: 0.8667038083076477\n",
      "Iteration 26099 Loss: 1.0247137546539307\n",
      "Iteration 26100 Loss: 1.1138465404510498\n",
      "Iteration 26101 Loss: 1.1464179754257202\n",
      "Iteration 26102 Loss: 0.7683631181716919\n",
      "Iteration 26103 Loss: 1.2696021795272827\n",
      "Iteration 26104 Loss: 0.8072839379310608\n",
      "Iteration 26105 Loss: 0.9319491386413574\n",
      "Iteration 26106 Loss: 1.0340211391448975\n",
      "Iteration 26107 Loss: 0.9066787958145142\n",
      "Iteration 26108 Loss: 1.1005735397338867\n",
      "Iteration 26109 Loss: 1.2203072309494019\n",
      "Iteration 26109 Loss: 1.0299043655395508\n",
      "Iteration 26110 Loss: 0.7098406553268433\n",
      "Iteration 26111 Loss: 0.973956286907196\n",
      "Iteration 26112 Loss: 0.9089668393135071\n",
      "Iteration 26113 Loss: 0.8191481232643127\n",
      "Iteration 26114 Loss: 1.1001981496810913\n",
      "Iteration 26115 Loss: 1.290258526802063\n",
      "Iteration 26116 Loss: 1.0397090911865234\n",
      "Iteration 26117 Loss: 1.4278335571289062\n",
      "Iteration 26118 Loss: 1.5550620555877686\n",
      "Iteration 26119 Loss: 1.369517207145691\n",
      "Iteration 26119 Loss: 1.1194490194320679\n",
      "Iteration 26120 Loss: 1.2621660232543945\n",
      "Iteration 26121 Loss: 0.8526108860969543\n",
      "Iteration 26122 Loss: 1.0213651657104492\n",
      "Iteration 26123 Loss: 1.1912078857421875\n",
      "Iteration 26124 Loss: 1.2122690677642822\n",
      "Iteration 26125 Loss: 1.1239124536514282\n",
      "Iteration 26126 Loss: 1.1374754905700684\n",
      "Iteration 26127 Loss: 0.9729008078575134\n",
      "Iteration 26128 Loss: 1.3355029821395874\n",
      "Iteration 26129 Loss: 0.8973785042762756\n",
      "Iteration 26129 Loss: 1.1006790399551392\n",
      "Iteration 26130 Loss: 0.9895033240318298\n",
      "Iteration 26131 Loss: 1.1736794710159302\n",
      "Iteration 26132 Loss: 0.9950107336044312\n",
      "Iteration 26133 Loss: 1.4729342460632324\n",
      "Iteration 26134 Loss: 1.184415578842163\n",
      "Iteration 26135 Loss: 0.9779789447784424\n",
      "Iteration 26136 Loss: 1.2956366539001465\n",
      "Iteration 26137 Loss: 0.818655788898468\n",
      "Iteration 26138 Loss: 1.0653749704360962\n",
      "Iteration 26139 Loss: 1.1187361478805542\n",
      "Iteration 26139 Loss: 1.1091926097869873\n",
      "Iteration 26140 Loss: 1.1929538249969482\n",
      "Iteration 26141 Loss: 1.1339929103851318\n",
      "Iteration 26142 Loss: 1.0092459917068481\n",
      "Iteration 26143 Loss: 0.8370033502578735\n",
      "Iteration 26144 Loss: 1.0180047750473022\n",
      "Iteration 26145 Loss: 1.356541633605957\n",
      "Iteration 26146 Loss: 0.9922870993614197\n",
      "Iteration 26147 Loss: 0.8480656147003174\n",
      "Iteration 26148 Loss: 1.244899034500122\n",
      "Iteration 26149 Loss: 1.0872492790222168\n",
      "Iteration 26149 Loss: 1.0720242261886597\n",
      "Iteration 26150 Loss: 1.0991017818450928\n",
      "Iteration 26151 Loss: 0.9304051399230957\n",
      "Iteration 26152 Loss: 1.0780386924743652\n",
      "Iteration 26153 Loss: 1.145139455795288\n",
      "Iteration 26154 Loss: 1.2612566947937012\n",
      "Iteration 26155 Loss: 1.318188190460205\n",
      "Iteration 26156 Loss: 1.0020543336868286\n",
      "Iteration 26157 Loss: 0.6071595549583435\n",
      "Iteration 26158 Loss: 0.9969987273216248\n",
      "Iteration 26159 Loss: 1.3581773042678833\n",
      "Iteration 26159 Loss: 1.079651951789856\n",
      "Iteration 26160 Loss: 1.1290407180786133\n",
      "Iteration 26161 Loss: 1.0466033220291138\n",
      "Iteration 26162 Loss: 0.9626403450965881\n",
      "Iteration 26163 Loss: 0.4952121078968048\n",
      "Iteration 26164 Loss: 1.1490689516067505\n",
      "Iteration 26165 Loss: 1.3013767004013062\n",
      "Iteration 26166 Loss: 1.1987391710281372\n",
      "Iteration 26167 Loss: 1.071938395500183\n",
      "Iteration 26168 Loss: 0.8382224440574646\n",
      "Iteration 26169 Loss: 0.8388979434967041\n",
      "Iteration 26169 Loss: 1.003174066543579\n",
      "Iteration 26170 Loss: 0.769631028175354\n",
      "Iteration 26171 Loss: 1.3601638078689575\n",
      "Iteration 26172 Loss: 1.0180824995040894\n",
      "Iteration 26173 Loss: 1.2456523180007935\n",
      "Iteration 26174 Loss: 1.0121320486068726\n",
      "Iteration 26175 Loss: 0.9957409501075745\n",
      "Iteration 26176 Loss: 1.0469752550125122\n",
      "Iteration 26177 Loss: 0.8863641023635864\n",
      "Iteration 26178 Loss: 1.04572331905365\n",
      "Iteration 26179 Loss: 1.2238236665725708\n",
      "Iteration 26179 Loss: 1.0604288578033447\n",
      "Iteration 26180 Loss: 0.7137224674224854\n",
      "Iteration 26181 Loss: 1.4546623229980469\n",
      "Iteration 26182 Loss: 1.029770016670227\n",
      "Iteration 26183 Loss: 1.081039309501648\n",
      "Iteration 26184 Loss: 1.07828950881958\n",
      "Iteration 26185 Loss: 0.7286940813064575\n",
      "Iteration 26186 Loss: 1.2738116979599\n",
      "Iteration 26187 Loss: 0.9292721152305603\n",
      "Iteration 26188 Loss: 1.3897674083709717\n",
      "Iteration 26189 Loss: 0.7464499473571777\n",
      "Iteration 26189 Loss: 1.0425478219985962\n",
      "Iteration 26190 Loss: 1.1371278762817383\n",
      "Iteration 26191 Loss: 0.8044669032096863\n",
      "Iteration 26192 Loss: 1.0075807571411133\n",
      "Iteration 26193 Loss: 1.2887511253356934\n",
      "Iteration 26194 Loss: 1.2227158546447754\n",
      "Iteration 26195 Loss: 0.7548515796661377\n",
      "Iteration 26196 Loss: 0.9000312089920044\n",
      "Iteration 26197 Loss: 1.1580510139465332\n",
      "Iteration 26198 Loss: 1.3355388641357422\n",
      "Iteration 26199 Loss: 1.0954005718231201\n",
      "Iteration 26199 Loss: 1.0704514980316162\n",
      "Iteration 26200 Loss: 1.0108708143234253\n",
      "Iteration 26201 Loss: 1.187118649482727\n",
      "Iteration 26202 Loss: 1.2537100315093994\n",
      "Iteration 26203 Loss: 1.3678309917449951\n",
      "Iteration 26204 Loss: 1.228308081626892\n",
      "Iteration 26205 Loss: 0.9876844882965088\n",
      "Iteration 26206 Loss: 1.1906590461730957\n",
      "Iteration 26207 Loss: 0.9436257481575012\n",
      "Iteration 26208 Loss: 1.2268091440200806\n",
      "Iteration 26209 Loss: 0.9220418334007263\n",
      "Iteration 26209 Loss: 1.1318657398223877\n",
      "Iteration 26210 Loss: 0.875109076499939\n",
      "Iteration 26211 Loss: 1.291427731513977\n",
      "Iteration 26212 Loss: 1.0233802795410156\n",
      "Iteration 26213 Loss: 1.0234546661376953\n",
      "Iteration 26214 Loss: 1.2477867603302002\n",
      "Iteration 26215 Loss: 0.639598548412323\n",
      "Iteration 26216 Loss: 1.1884263753890991\n",
      "Iteration 26217 Loss: 0.886093020439148\n",
      "Iteration 26218 Loss: 0.917650580406189\n",
      "Iteration 26219 Loss: 0.9601128697395325\n",
      "Iteration 26219 Loss: 1.005303978919983\n",
      "Iteration 26220 Loss: 1.1099694967269897\n",
      "Iteration 26221 Loss: 1.1017237901687622\n",
      "Iteration 26222 Loss: 1.1858844757080078\n",
      "Iteration 26223 Loss: 1.3391766548156738\n",
      "Iteration 26224 Loss: 1.0850574970245361\n",
      "Iteration 26225 Loss: 0.9134703874588013\n",
      "Iteration 26226 Loss: 1.042122483253479\n",
      "Iteration 26227 Loss: 1.1121699810028076\n",
      "Iteration 26228 Loss: 0.910723865032196\n",
      "Iteration 26229 Loss: 1.2589080333709717\n",
      "Iteration 26229 Loss: 1.105920672416687\n",
      "Iteration 26230 Loss: 0.8063228726387024\n",
      "Iteration 26231 Loss: 0.9958004355430603\n",
      "Iteration 26232 Loss: 0.9486274123191833\n",
      "Iteration 26233 Loss: 1.0473089218139648\n",
      "Iteration 26234 Loss: 1.0108956098556519\n",
      "Iteration 26235 Loss: 1.163909673690796\n",
      "Iteration 26236 Loss: 0.863434374332428\n",
      "Iteration 26237 Loss: 1.143259048461914\n",
      "Iteration 26238 Loss: 0.9450619220733643\n",
      "Iteration 26239 Loss: 1.113282561302185\n",
      "Iteration 26239 Loss: 1.003790259361267\n",
      "Iteration 26240 Loss: 1.1478428840637207\n",
      "Iteration 26241 Loss: 1.3554683923721313\n",
      "Iteration 26242 Loss: 1.0083463191986084\n",
      "Iteration 26243 Loss: 1.1904945373535156\n",
      "Iteration 26244 Loss: 0.7454886436462402\n",
      "Iteration 26245 Loss: 1.3400431871414185\n",
      "Iteration 26246 Loss: 0.6729152202606201\n",
      "Iteration 26247 Loss: 1.168129324913025\n",
      "Iteration 26248 Loss: 1.1721696853637695\n",
      "Iteration 26249 Loss: 0.6325613260269165\n",
      "Iteration 26249 Loss: 1.0433459281921387\n",
      "Iteration 26250 Loss: 0.8830513954162598\n",
      "Iteration 26251 Loss: 0.7230691909790039\n",
      "Iteration 26252 Loss: 1.1066358089447021\n",
      "Iteration 26253 Loss: 0.8957799673080444\n",
      "Iteration 26254 Loss: 1.0504273176193237\n",
      "Iteration 26255 Loss: 0.6691682934761047\n",
      "Iteration 26256 Loss: 1.0891754627227783\n",
      "Iteration 26257 Loss: 0.9122076630592346\n",
      "Iteration 26258 Loss: 0.8118456602096558\n",
      "Iteration 26259 Loss: 0.8944317698478699\n",
      "Iteration 26259 Loss: 0.9035793542861938\n",
      "Iteration 26260 Loss: 0.9598625302314758\n",
      "Iteration 26261 Loss: 1.185054063796997\n",
      "Iteration 26262 Loss: 1.0190660953521729\n",
      "Iteration 26263 Loss: 0.7746132016181946\n",
      "Iteration 26264 Loss: 1.2865350246429443\n",
      "Iteration 26265 Loss: 0.9272711277008057\n",
      "Iteration 26266 Loss: 1.259520411491394\n",
      "Iteration 26267 Loss: 1.1940877437591553\n",
      "Iteration 26268 Loss: 1.0803930759429932\n",
      "Iteration 26269 Loss: 0.6089571714401245\n",
      "Iteration 26269 Loss: 1.0295360088348389\n",
      "Iteration 26270 Loss: 0.6856110692024231\n",
      "Iteration 26271 Loss: 1.079865574836731\n",
      "Iteration 26272 Loss: 1.045304775238037\n",
      "Iteration 26273 Loss: 1.0248478651046753\n",
      "Iteration 26274 Loss: 1.1442078351974487\n",
      "Iteration 26275 Loss: 0.961726188659668\n",
      "Iteration 26276 Loss: 1.3812617063522339\n",
      "Iteration 26277 Loss: 1.0253115892410278\n",
      "Iteration 26278 Loss: 1.0430283546447754\n",
      "Iteration 26279 Loss: 1.1484829187393188\n",
      "Iteration 26279 Loss: 1.0539648532867432\n",
      "Iteration 26280 Loss: 1.5400466918945312\n",
      "Iteration 26281 Loss: 1.0379104614257812\n",
      "Iteration 26282 Loss: 1.1061089038848877\n",
      "Iteration 26283 Loss: 1.1256860494613647\n",
      "Iteration 26284 Loss: 0.8453568816184998\n",
      "Iteration 26285 Loss: 1.0155786275863647\n",
      "Iteration 26286 Loss: 1.0675550699234009\n",
      "Iteration 26287 Loss: 0.699836254119873\n",
      "Iteration 26288 Loss: 0.9275469779968262\n",
      "Iteration 26289 Loss: 1.038400411605835\n",
      "Iteration 26289 Loss: 1.0404026508331299\n",
      "Iteration 26290 Loss: 1.2996695041656494\n",
      "Iteration 26291 Loss: 1.4033066034317017\n",
      "Iteration 26292 Loss: 0.7834442853927612\n",
      "Iteration 26293 Loss: 1.2202732563018799\n",
      "Iteration 26294 Loss: 1.166776418685913\n",
      "Iteration 26295 Loss: 1.1213760375976562\n",
      "Iteration 26296 Loss: 1.1780236959457397\n",
      "Iteration 26297 Loss: 0.9215640425682068\n",
      "Iteration 26298 Loss: 0.9283201694488525\n",
      "Iteration 26299 Loss: 0.7906213998794556\n",
      "Iteration 26299 Loss: 1.081337571144104\n",
      "Iteration 26300 Loss: 0.9553771018981934\n",
      "Iteration 26301 Loss: 0.8290254473686218\n",
      "Iteration 26302 Loss: 0.8938403725624084\n",
      "Iteration 26303 Loss: 1.3966724872589111\n",
      "Iteration 26304 Loss: 1.2765856981277466\n",
      "Iteration 26305 Loss: 0.8786015510559082\n",
      "Iteration 26306 Loss: 1.3354698419570923\n",
      "Iteration 26307 Loss: 0.7814642190933228\n",
      "Iteration 26308 Loss: 1.2834116220474243\n",
      "Iteration 26309 Loss: 0.8292417526245117\n",
      "Iteration 26309 Loss: 1.0459691286087036\n",
      "Iteration 26310 Loss: 1.1596152782440186\n",
      "Iteration 26311 Loss: 1.106249451637268\n",
      "Iteration 26312 Loss: 1.089646816253662\n",
      "Iteration 26313 Loss: 0.9143767952919006\n",
      "Iteration 26314 Loss: 1.054497241973877\n",
      "Iteration 26315 Loss: 1.0290664434432983\n",
      "Iteration 26316 Loss: 1.0649691820144653\n",
      "Iteration 26317 Loss: 0.9068992137908936\n",
      "Iteration 26318 Loss: 0.6783352494239807\n",
      "Iteration 26319 Loss: 1.044814944267273\n",
      "Iteration 26319 Loss: 1.0048470497131348\n",
      "Iteration 26320 Loss: 0.9396785497665405\n",
      "Iteration 26321 Loss: 0.9666174650192261\n",
      "Iteration 26322 Loss: 1.419788122177124\n",
      "Iteration 26323 Loss: 0.6510158777236938\n",
      "Iteration 26324 Loss: 0.8705375790596008\n",
      "Iteration 26325 Loss: 0.921563446521759\n",
      "Iteration 26326 Loss: 1.2973252534866333\n",
      "Iteration 26327 Loss: 1.0638830661773682\n",
      "Iteration 26328 Loss: 0.8132412433624268\n",
      "Iteration 26329 Loss: 1.0901991128921509\n",
      "Iteration 26329 Loss: 1.0033849477767944\n",
      "Iteration 26330 Loss: 0.95060133934021\n",
      "Iteration 26331 Loss: 1.0156331062316895\n",
      "Iteration 26332 Loss: 1.1387038230895996\n",
      "Iteration 26333 Loss: 0.7308397889137268\n",
      "Iteration 26334 Loss: 1.0817862749099731\n",
      "Iteration 26335 Loss: 1.1780954599380493\n",
      "Iteration 26336 Loss: 1.143875002861023\n",
      "Iteration 26337 Loss: 1.1906993389129639\n",
      "Iteration 26338 Loss: 1.1801420450210571\n",
      "Iteration 26339 Loss: 0.8860839009284973\n",
      "Iteration 26339 Loss: 1.049646019935608\n",
      "Iteration 26340 Loss: 0.9941372871398926\n",
      "Iteration 26341 Loss: 0.8056913018226624\n",
      "Iteration 26342 Loss: 1.3500113487243652\n",
      "Iteration 26343 Loss: 1.0631005764007568\n",
      "Iteration 26344 Loss: 0.9841382503509521\n",
      "Iteration 26345 Loss: 1.0030027627944946\n",
      "Iteration 26346 Loss: 0.9918333292007446\n",
      "Iteration 26347 Loss: 0.9386446475982666\n",
      "Iteration 26348 Loss: 1.1013139486312866\n",
      "Iteration 26349 Loss: 1.006780982017517\n",
      "Iteration 26349 Loss: 1.0238654613494873\n",
      "Iteration 26350 Loss: 1.00788152217865\n",
      "Iteration 26351 Loss: 1.1707390546798706\n",
      "Iteration 26352 Loss: 0.9037201404571533\n",
      "Iteration 26353 Loss: 0.9509689807891846\n",
      "Iteration 26354 Loss: 1.1577374935150146\n",
      "Iteration 26355 Loss: 1.0548455715179443\n",
      "Iteration 26356 Loss: 1.2380774021148682\n",
      "Iteration 26357 Loss: 1.1177281141281128\n",
      "Iteration 26358 Loss: 0.7806139588356018\n",
      "Iteration 26359 Loss: 1.1369380950927734\n",
      "Iteration 26359 Loss: 1.0519250631332397\n",
      "Iteration 26360 Loss: 0.7723510265350342\n",
      "Iteration 26361 Loss: 1.1201540231704712\n",
      "Iteration 26362 Loss: 0.828694224357605\n",
      "Iteration 26363 Loss: 1.4709744453430176\n",
      "Iteration 26364 Loss: 1.7714353799819946\n",
      "Iteration 26365 Loss: 0.9606146812438965\n",
      "Iteration 26366 Loss: 0.8928340673446655\n",
      "Iteration 26367 Loss: 1.0208728313446045\n",
      "Iteration 26368 Loss: 1.1531648635864258\n",
      "Iteration 26369 Loss: 1.2335212230682373\n",
      "Iteration 26369 Loss: 1.1224615573883057\n",
      "Iteration 26370 Loss: 1.4640312194824219\n",
      "Iteration 26371 Loss: 1.2392204999923706\n",
      "Iteration 26372 Loss: 1.077043890953064\n",
      "Iteration 26373 Loss: 1.074300765991211\n",
      "Iteration 26374 Loss: 1.3109506368637085\n",
      "Iteration 26375 Loss: 1.1430879831314087\n",
      "Iteration 26376 Loss: 1.1841859817504883\n",
      "Iteration 26377 Loss: 1.0417594909667969\n",
      "Iteration 26378 Loss: 1.0758013725280762\n",
      "Iteration 26379 Loss: 0.47584080696105957\n",
      "Iteration 26379 Loss: 1.1086223125457764\n",
      "Iteration 26380 Loss: 0.8856827616691589\n",
      "Iteration 26381 Loss: 0.9520155787467957\n",
      "Iteration 26382 Loss: 1.2552106380462646\n",
      "Iteration 26383 Loss: 0.8151902556419373\n",
      "Iteration 26384 Loss: 0.9672753214836121\n",
      "Iteration 26385 Loss: 1.4915860891342163\n",
      "Iteration 26386 Loss: 0.8359792828559875\n",
      "Iteration 26387 Loss: 0.9390401244163513\n",
      "Iteration 26388 Loss: 0.974446177482605\n",
      "Iteration 26389 Loss: 1.0698108673095703\n",
      "Iteration 26389 Loss: 1.01862370967865\n",
      "Iteration 26390 Loss: 1.0483568906784058\n",
      "Iteration 26391 Loss: 0.8899725675582886\n",
      "Iteration 26392 Loss: 0.9673323035240173\n",
      "Iteration 26393 Loss: 0.7514867186546326\n",
      "Iteration 26394 Loss: 1.153993844985962\n",
      "Iteration 26395 Loss: 0.9734799265861511\n",
      "Iteration 26396 Loss: 1.0251909494400024\n",
      "Iteration 26397 Loss: 1.00371253490448\n",
      "Iteration 26398 Loss: 1.043054461479187\n",
      "Iteration 26399 Loss: 1.13279390335083\n",
      "Iteration 26399 Loss: 0.9989374279975891\n",
      "Iteration 26400 Loss: 1.1534816026687622\n",
      "Iteration 26401 Loss: 1.1942639350891113\n",
      "Iteration 26402 Loss: 1.3059024810791016\n",
      "Iteration 26403 Loss: 0.9797465205192566\n",
      "Iteration 26404 Loss: 1.0383899211883545\n",
      "Iteration 26405 Loss: 0.6848085522651672\n",
      "Iteration 26406 Loss: 1.372035264968872\n",
      "Iteration 26407 Loss: 1.4263615608215332\n",
      "Iteration 26408 Loss: 0.8986170291900635\n",
      "Iteration 26409 Loss: 0.9423987865447998\n",
      "Iteration 26409 Loss: 1.0996005535125732\n",
      "Iteration 26410 Loss: 1.0560023784637451\n",
      "Iteration 26411 Loss: 0.8118991851806641\n",
      "Iteration 26412 Loss: 0.9360175132751465\n",
      "Iteration 26413 Loss: 1.0578137636184692\n",
      "Iteration 26414 Loss: 0.8177109360694885\n",
      "Iteration 26415 Loss: 1.2076698541641235\n",
      "Iteration 26416 Loss: 0.7996120452880859\n",
      "Iteration 26417 Loss: 1.0501387119293213\n",
      "Iteration 26418 Loss: 1.206279993057251\n",
      "Iteration 26419 Loss: 0.9136638045310974\n",
      "Iteration 26419 Loss: 0.9856807589530945\n",
      "Iteration 26420 Loss: 0.7868019938468933\n",
      "Iteration 26421 Loss: 1.2725907564163208\n",
      "Iteration 26422 Loss: 1.0143669843673706\n",
      "Iteration 26423 Loss: 1.1420122385025024\n",
      "Iteration 26424 Loss: 1.3085142374038696\n",
      "Iteration 26425 Loss: 1.03091299533844\n",
      "Iteration 26426 Loss: 1.0681710243225098\n",
      "Iteration 26427 Loss: 0.9853496551513672\n",
      "Iteration 26428 Loss: 0.9481052160263062\n",
      "Iteration 26429 Loss: 0.8644438982009888\n",
      "Iteration 26429 Loss: 1.0421268939971924\n",
      "Iteration 26430 Loss: 0.8194112777709961\n",
      "Iteration 26431 Loss: 1.1483769416809082\n",
      "Iteration 26432 Loss: 0.9247492551803589\n",
      "Iteration 26433 Loss: 1.1384211778640747\n",
      "Iteration 26434 Loss: 1.0260965824127197\n",
      "Iteration 26435 Loss: 1.2264893054962158\n",
      "Iteration 26436 Loss: 0.9047695398330688\n",
      "Iteration 26437 Loss: 1.2557580471038818\n",
      "Iteration 26438 Loss: 1.5851324796676636\n",
      "Iteration 26439 Loss: 1.0387980937957764\n",
      "Iteration 26439 Loss: 1.1068003177642822\n",
      "Iteration 26440 Loss: 1.035534381866455\n",
      "Iteration 26441 Loss: 1.3660192489624023\n",
      "Iteration 26442 Loss: 1.1080918312072754\n",
      "Iteration 26443 Loss: 0.9992763996124268\n",
      "Iteration 26444 Loss: 0.8305979371070862\n",
      "Iteration 26445 Loss: 0.693815290927887\n",
      "Iteration 26446 Loss: 1.176452398300171\n",
      "Iteration 26447 Loss: 1.0275559425354004\n",
      "Iteration 26448 Loss: 0.6807854771614075\n",
      "Iteration 26449 Loss: 0.9525836706161499\n",
      "Iteration 26449 Loss: 0.9870712161064148\n",
      "Iteration 26450 Loss: 1.0722585916519165\n",
      "Iteration 26451 Loss: 0.7988484501838684\n",
      "Iteration 26452 Loss: 1.044498324394226\n",
      "Iteration 26453 Loss: 0.9489134550094604\n",
      "Iteration 26454 Loss: 1.4136658906936646\n",
      "Iteration 26455 Loss: 1.0972380638122559\n",
      "Iteration 26456 Loss: 1.3043392896652222\n",
      "Iteration 26457 Loss: 0.8644558787345886\n",
      "Iteration 26458 Loss: 0.8723970651626587\n",
      "Iteration 26459 Loss: 0.8616190552711487\n",
      "Iteration 26459 Loss: 1.0278234481811523\n",
      "Iteration 26460 Loss: 1.1233631372451782\n",
      "Iteration 26461 Loss: 1.3354204893112183\n",
      "Iteration 26462 Loss: 0.976289689540863\n",
      "Iteration 26463 Loss: 0.810933530330658\n",
      "Iteration 26464 Loss: 0.7438180446624756\n",
      "Iteration 26465 Loss: 1.0124696493148804\n",
      "Iteration 26466 Loss: 1.1653921604156494\n",
      "Iteration 26467 Loss: 1.1498898267745972\n",
      "Iteration 26468 Loss: 0.7899922132492065\n",
      "Iteration 26469 Loss: 0.9779500961303711\n",
      "Iteration 26469 Loss: 1.0085519552230835\n",
      "Iteration 26470 Loss: 1.12508225440979\n",
      "Iteration 26471 Loss: 0.8540788292884827\n",
      "Iteration 26472 Loss: 1.058374047279358\n",
      "Iteration 26473 Loss: 0.7121396660804749\n",
      "Iteration 26474 Loss: 0.8723671436309814\n",
      "Iteration 26475 Loss: 1.2603354454040527\n",
      "Iteration 26476 Loss: 0.9201211333274841\n",
      "Iteration 26477 Loss: 0.9260923862457275\n",
      "Iteration 26478 Loss: 1.0669232606887817\n",
      "Iteration 26479 Loss: 1.199767827987671\n",
      "Iteration 26479 Loss: 0.9995282292366028\n",
      "Iteration 26480 Loss: 1.0578850507736206\n",
      "Iteration 26481 Loss: 1.1561181545257568\n",
      "Iteration 26482 Loss: 1.1512752771377563\n",
      "Iteration 26483 Loss: 0.9926989078521729\n",
      "Iteration 26484 Loss: 1.3245314359664917\n",
      "Iteration 26485 Loss: 1.0978928804397583\n",
      "Iteration 26486 Loss: 1.2240341901779175\n",
      "Iteration 26487 Loss: 1.0773500204086304\n",
      "Iteration 26488 Loss: 1.237412691116333\n",
      "Iteration 26489 Loss: 0.8084478378295898\n",
      "Iteration 26489 Loss: 1.112764596939087\n",
      "Iteration 26490 Loss: 0.48660582304000854\n",
      "Iteration 26491 Loss: 0.9031909704208374\n",
      "Iteration 26492 Loss: 0.880039393901825\n",
      "Iteration 26493 Loss: 1.4122263193130493\n",
      "Iteration 26494 Loss: 1.2752739191055298\n",
      "Iteration 26495 Loss: 1.3885602951049805\n",
      "Iteration 26496 Loss: 1.1363427639007568\n",
      "Iteration 26497 Loss: 0.9935242533683777\n",
      "Iteration 26498 Loss: 0.8071383237838745\n",
      "Iteration 26499 Loss: 1.156607747077942\n",
      "Iteration 26499 Loss: 1.0439510345458984\n",
      "Iteration 26500 Loss: 0.6524347066879272\n",
      "Iteration 26501 Loss: 0.9582764506340027\n",
      "Iteration 26502 Loss: 1.337549090385437\n",
      "Iteration 26503 Loss: 1.0485613346099854\n",
      "Iteration 26504 Loss: 0.9642944931983948\n",
      "Iteration 26505 Loss: 0.9733272790908813\n",
      "Iteration 26506 Loss: 1.033052921295166\n",
      "Iteration 26507 Loss: 1.1038548946380615\n",
      "Iteration 26508 Loss: 1.0326992273330688\n",
      "Iteration 26509 Loss: 1.1649956703186035\n",
      "Iteration 26509 Loss: 1.0269047021865845\n",
      "Iteration 26510 Loss: 1.038062334060669\n",
      "Iteration 26511 Loss: 0.9053606986999512\n",
      "Iteration 26512 Loss: 1.0066349506378174\n",
      "Iteration 26513 Loss: 1.0932507514953613\n",
      "Iteration 26514 Loss: 1.1675705909729004\n",
      "Iteration 26515 Loss: 0.994495689868927\n",
      "Iteration 26516 Loss: 1.207324743270874\n",
      "Iteration 26517 Loss: 0.7978458404541016\n",
      "Iteration 26518 Loss: 0.8661115765571594\n",
      "Iteration 26519 Loss: 1.5502545833587646\n",
      "Iteration 26519 Loss: 1.06269109249115\n",
      "Iteration 26520 Loss: 1.3381057977676392\n",
      "Iteration 26521 Loss: 1.035914421081543\n",
      "Iteration 26522 Loss: 0.9629534482955933\n",
      "Iteration 26523 Loss: 1.0138928890228271\n",
      "Iteration 26524 Loss: 1.0212275981903076\n",
      "Iteration 26525 Loss: 0.8556657433509827\n",
      "Iteration 26526 Loss: 1.0166079998016357\n",
      "Iteration 26527 Loss: 0.8887420296669006\n",
      "Iteration 26528 Loss: 1.1862071752548218\n",
      "Iteration 26529 Loss: 1.149266004562378\n",
      "Iteration 26529 Loss: 1.0468584299087524\n",
      "Iteration 26530 Loss: 0.9254088997840881\n",
      "Iteration 26531 Loss: 1.2410567998886108\n",
      "Iteration 26532 Loss: 1.0532357692718506\n",
      "Iteration 26533 Loss: 1.2144925594329834\n",
      "Iteration 26534 Loss: 0.769073486328125\n",
      "Iteration 26535 Loss: 1.0603371858596802\n",
      "Iteration 26536 Loss: 0.7755864858627319\n",
      "Iteration 26537 Loss: 0.7228273749351501\n",
      "Iteration 26538 Loss: 0.8038344383239746\n",
      "Iteration 26539 Loss: 1.0695796012878418\n",
      "Iteration 26539 Loss: 0.9635432362556458\n",
      "Iteration 26540 Loss: 1.113274335861206\n",
      "Iteration 26541 Loss: 0.9621478319168091\n",
      "Iteration 26542 Loss: 1.0407953262329102\n",
      "Iteration 26543 Loss: 1.1575344800949097\n",
      "Iteration 26544 Loss: 1.0368186235427856\n",
      "Iteration 26545 Loss: 1.1780670881271362\n",
      "Iteration 26546 Loss: 1.1256159543991089\n",
      "Iteration 26547 Loss: 0.8777583241462708\n",
      "Iteration 26548 Loss: 1.3645573854446411\n",
      "Iteration 26549 Loss: 1.0030016899108887\n",
      "Iteration 26549 Loss: 1.0859570503234863\n",
      "Iteration 26550 Loss: 1.128138780593872\n",
      "Iteration 26551 Loss: 1.0743402242660522\n",
      "Iteration 26552 Loss: 0.8867225646972656\n",
      "Iteration 26553 Loss: 0.7849356532096863\n",
      "Iteration 26554 Loss: 1.3230133056640625\n",
      "Iteration 26555 Loss: 0.8782752156257629\n",
      "Iteration 26556 Loss: 1.0206857919692993\n",
      "Iteration 26557 Loss: 0.8981891870498657\n",
      "Iteration 26558 Loss: 0.8461592197418213\n",
      "Iteration 26559 Loss: 1.410017728805542\n",
      "Iteration 26559 Loss: 1.025047779083252\n",
      "Iteration 26560 Loss: 1.089253306388855\n",
      "Iteration 26561 Loss: 1.2046937942504883\n",
      "Iteration 26562 Loss: 1.3803788423538208\n",
      "Iteration 26563 Loss: 1.2539671659469604\n",
      "Iteration 26564 Loss: 1.4098877906799316\n",
      "Iteration 26565 Loss: 0.9630854725837708\n",
      "Iteration 26566 Loss: 1.170384168624878\n",
      "Iteration 26567 Loss: 0.90651935338974\n",
      "Iteration 26568 Loss: 1.0838408470153809\n",
      "Iteration 26569 Loss: 1.1629687547683716\n",
      "Iteration 26569 Loss: 1.162497878074646\n",
      "Iteration 26570 Loss: 1.0132933855056763\n",
      "Iteration 26571 Loss: 1.0143094062805176\n",
      "Iteration 26572 Loss: 0.8924432396888733\n",
      "Iteration 26573 Loss: 1.2385135889053345\n",
      "Iteration 26574 Loss: 0.7594484090805054\n",
      "Iteration 26575 Loss: 0.9213946461677551\n",
      "Iteration 26576 Loss: 0.7578797340393066\n",
      "Iteration 26577 Loss: 1.1067345142364502\n",
      "Iteration 26578 Loss: 1.0665228366851807\n",
      "Iteration 26579 Loss: 0.7985429167747498\n",
      "Iteration 26579 Loss: 0.9569083452224731\n",
      "Iteration 26580 Loss: 1.3652663230895996\n",
      "Iteration 26581 Loss: 1.1147232055664062\n",
      "Iteration 26582 Loss: 0.7506654858589172\n",
      "Iteration 26583 Loss: 0.8943449854850769\n",
      "Iteration 26584 Loss: 1.1042561531066895\n",
      "Iteration 26585 Loss: 0.9896875619888306\n",
      "Iteration 26586 Loss: 1.0159108638763428\n",
      "Iteration 26587 Loss: 1.2873330116271973\n",
      "Iteration 26588 Loss: 1.0044469833374023\n",
      "Iteration 26589 Loss: 0.9346200227737427\n",
      "Iteration 26589 Loss: 1.0461254119873047\n",
      "Iteration 26590 Loss: 0.9676994681358337\n",
      "Iteration 26591 Loss: 0.7607055306434631\n",
      "Iteration 26592 Loss: 1.1893322467803955\n",
      "Iteration 26593 Loss: 0.9633131623268127\n",
      "Iteration 26594 Loss: 1.1384755373001099\n",
      "Iteration 26595 Loss: 1.1037641763687134\n",
      "Iteration 26596 Loss: 1.1534645557403564\n",
      "Iteration 26597 Loss: 0.7082783579826355\n",
      "Iteration 26598 Loss: 1.1961171627044678\n",
      "Iteration 26599 Loss: 1.343467116355896\n",
      "Iteration 26599 Loss: 1.0524617433547974\n",
      "Iteration 26600 Loss: 0.7302277088165283\n",
      "Iteration 26601 Loss: 1.1828274726867676\n",
      "Iteration 26602 Loss: 1.009721040725708\n",
      "Iteration 26603 Loss: 0.9415464401245117\n",
      "Iteration 26604 Loss: 1.0496925115585327\n",
      "Iteration 26605 Loss: 0.6550895571708679\n",
      "Iteration 26606 Loss: 1.1605093479156494\n",
      "Iteration 26607 Loss: 1.1878926753997803\n",
      "Iteration 26608 Loss: 0.9339511394500732\n",
      "Iteration 26609 Loss: 1.2251217365264893\n",
      "Iteration 26609 Loss: 1.0076580047607422\n",
      "Iteration 26610 Loss: 1.2764241695404053\n",
      "Iteration 26611 Loss: 1.0328505039215088\n",
      "Iteration 26612 Loss: 1.0041159391403198\n",
      "Iteration 26613 Loss: 0.7130103707313538\n",
      "Iteration 26614 Loss: 1.2134644985198975\n",
      "Iteration 26615 Loss: 0.9712349772453308\n",
      "Iteration 26616 Loss: 0.9887294769287109\n",
      "Iteration 26617 Loss: 1.1298680305480957\n",
      "Iteration 26618 Loss: 1.0365632772445679\n",
      "Iteration 26619 Loss: 0.8592560291290283\n",
      "Iteration 26619 Loss: 1.0225517749786377\n",
      "Iteration 26620 Loss: 1.0026293992996216\n",
      "Iteration 26621 Loss: 1.1521763801574707\n",
      "Iteration 26622 Loss: 1.0990239381790161\n",
      "Iteration 26623 Loss: 0.9649428725242615\n",
      "Iteration 26624 Loss: 1.1423064470291138\n",
      "Iteration 26625 Loss: 1.1966888904571533\n",
      "Iteration 26626 Loss: 0.9149121046066284\n",
      "Iteration 26627 Loss: 1.1688027381896973\n",
      "Iteration 26628 Loss: 1.1323291063308716\n",
      "Iteration 26629 Loss: 1.0007957220077515\n",
      "Iteration 26629 Loss: 1.0774606466293335\n",
      "Iteration 26630 Loss: 1.0171390771865845\n",
      "Iteration 26631 Loss: 0.9784136414527893\n",
      "Iteration 26632 Loss: 0.921425998210907\n",
      "Iteration 26633 Loss: 1.254958152770996\n",
      "Iteration 26634 Loss: 0.7477542161941528\n",
      "Iteration 26635 Loss: 1.2481266260147095\n",
      "Iteration 26636 Loss: 0.7951952815055847\n",
      "Iteration 26637 Loss: 1.3197667598724365\n",
      "Iteration 26638 Loss: 1.022956371307373\n",
      "Iteration 26639 Loss: 1.206122636795044\n",
      "Iteration 26639 Loss: 1.0511858463287354\n",
      "Iteration 26640 Loss: 0.9823318719863892\n",
      "Iteration 26641 Loss: 1.1184941530227661\n",
      "Iteration 26642 Loss: 1.0525860786437988\n",
      "Iteration 26643 Loss: 1.2213325500488281\n",
      "Iteration 26644 Loss: 1.2498092651367188\n",
      "Iteration 26645 Loss: 0.7274230718612671\n",
      "Iteration 26646 Loss: 0.9574163556098938\n",
      "Iteration 26647 Loss: 1.1104742288589478\n",
      "Iteration 26648 Loss: 1.047897219657898\n",
      "Iteration 26649 Loss: 1.2700467109680176\n",
      "Iteration 26649 Loss: 1.0737812519073486\n",
      "Iteration 26650 Loss: 0.9306625723838806\n",
      "Iteration 26651 Loss: 0.777141273021698\n",
      "Iteration 26652 Loss: 0.8705347180366516\n",
      "Iteration 26653 Loss: 1.0712480545043945\n",
      "Iteration 26654 Loss: 1.210282564163208\n",
      "Iteration 26655 Loss: 1.1841402053833008\n",
      "Iteration 26656 Loss: 0.7774831652641296\n",
      "Iteration 26657 Loss: 0.8079478740692139\n",
      "Iteration 26658 Loss: 1.303229808807373\n",
      "Iteration 26659 Loss: 1.0379825830459595\n",
      "Iteration 26659 Loss: 0.9970652461051941\n",
      "Iteration 26660 Loss: 0.8533710241317749\n",
      "Iteration 26661 Loss: 1.0704227685928345\n",
      "Iteration 26662 Loss: 0.7630314230918884\n",
      "Iteration 26663 Loss: 0.9074040651321411\n",
      "Iteration 26664 Loss: 0.9669836759567261\n",
      "Iteration 26665 Loss: 1.0974395275115967\n",
      "Iteration 26666 Loss: 0.78651362657547\n",
      "Iteration 26667 Loss: 1.1701290607452393\n",
      "Iteration 26668 Loss: 0.706252932548523\n",
      "Iteration 26669 Loss: 0.7780948281288147\n",
      "Iteration 26669 Loss: 0.909964382648468\n",
      "Iteration 26670 Loss: 1.0732264518737793\n",
      "Iteration 26671 Loss: 1.1109073162078857\n",
      "Iteration 26672 Loss: 1.2587080001831055\n",
      "Iteration 26673 Loss: 0.962382972240448\n",
      "Iteration 26674 Loss: 0.8831551671028137\n",
      "Iteration 26675 Loss: 0.6134591698646545\n",
      "Iteration 26676 Loss: 1.0401504039764404\n",
      "Iteration 26677 Loss: 1.1415677070617676\n",
      "Iteration 26678 Loss: 0.8478357195854187\n",
      "Iteration 26679 Loss: 1.2706128358840942\n",
      "Iteration 26679 Loss: 1.020200490951538\n",
      "Iteration 26680 Loss: 0.9551188349723816\n",
      "Iteration 26681 Loss: 0.9733360409736633\n",
      "Iteration 26682 Loss: 1.0029956102371216\n",
      "Iteration 26683 Loss: 0.8422671556472778\n",
      "Iteration 26684 Loss: 1.6113028526306152\n",
      "Iteration 26685 Loss: 0.8836725354194641\n",
      "Iteration 26686 Loss: 1.1251890659332275\n",
      "Iteration 26687 Loss: 0.8214300870895386\n",
      "Iteration 26688 Loss: 0.8552455902099609\n",
      "Iteration 26689 Loss: 0.7165312767028809\n",
      "Iteration 26689 Loss: 0.9787089228630066\n",
      "Iteration 26690 Loss: 1.2559350728988647\n",
      "Iteration 26691 Loss: 1.0590753555297852\n",
      "Iteration 26692 Loss: 1.0108295679092407\n",
      "Iteration 26693 Loss: 0.828609049320221\n",
      "Iteration 26694 Loss: 1.3017932176589966\n",
      "Iteration 26695 Loss: 0.9199486970901489\n",
      "Iteration 26696 Loss: 1.1386257410049438\n",
      "Iteration 26697 Loss: 1.191800832748413\n",
      "Iteration 26698 Loss: 1.0486136674880981\n",
      "Iteration 26699 Loss: 1.7152907848358154\n",
      "Iteration 26699 Loss: 1.14705228805542\n",
      "Iteration 26700 Loss: 0.9642287492752075\n",
      "Iteration 26701 Loss: 1.0460762977600098\n",
      "Iteration 26702 Loss: 1.2097516059875488\n",
      "Iteration 26703 Loss: 0.9192833304405212\n",
      "Iteration 26704 Loss: 1.12337064743042\n",
      "Iteration 26705 Loss: 1.3900387287139893\n",
      "Iteration 26706 Loss: 0.9872206449508667\n",
      "Iteration 26707 Loss: 1.0785250663757324\n",
      "Iteration 26708 Loss: 0.7712716460227966\n",
      "Iteration 26709 Loss: 0.812772274017334\n",
      "Iteration 26709 Loss: 1.0302540063858032\n",
      "Iteration 26710 Loss: 1.056633710861206\n",
      "Iteration 26711 Loss: 1.0583120584487915\n",
      "Iteration 26712 Loss: 1.603723406791687\n",
      "Iteration 26713 Loss: 0.8628157377243042\n",
      "Iteration 26714 Loss: 1.2963305711746216\n",
      "Iteration 26715 Loss: 1.3134068250656128\n",
      "Iteration 26716 Loss: 0.9984561204910278\n",
      "Iteration 26717 Loss: 0.7883185744285583\n",
      "Iteration 26718 Loss: 0.8182699084281921\n",
      "Iteration 26719 Loss: 1.1272810697555542\n",
      "Iteration 26719 Loss: 1.0923547744750977\n",
      "Iteration 26720 Loss: 1.0896953344345093\n",
      "Iteration 26721 Loss: 1.0183380842208862\n",
      "Iteration 26722 Loss: 0.9116289615631104\n",
      "Iteration 26723 Loss: 1.148939609527588\n",
      "Iteration 26724 Loss: 1.0495679378509521\n",
      "Iteration 26725 Loss: 0.8924729824066162\n",
      "Iteration 26726 Loss: 1.0694981813430786\n",
      "Iteration 26727 Loss: 1.0117563009262085\n",
      "Iteration 26728 Loss: 1.2345178127288818\n",
      "Iteration 26729 Loss: 1.3250341415405273\n",
      "Iteration 26729 Loss: 1.07514488697052\n",
      "Iteration 26730 Loss: 0.866478443145752\n",
      "Iteration 26731 Loss: 1.2017900943756104\n",
      "Iteration 26732 Loss: 0.9914703965187073\n",
      "Iteration 26733 Loss: 1.131150484085083\n",
      "Iteration 26734 Loss: 1.2102195024490356\n",
      "Iteration 26735 Loss: 1.1269419193267822\n",
      "Iteration 26736 Loss: 1.2038675546646118\n",
      "Iteration 26737 Loss: 1.0064688920974731\n",
      "Iteration 26738 Loss: 0.8096587061882019\n",
      "Iteration 26739 Loss: 0.8699904680252075\n",
      "Iteration 26739 Loss: 1.0418035984039307\n",
      "Iteration 26740 Loss: 0.8705140352249146\n",
      "Iteration 26741 Loss: 0.8270115256309509\n",
      "Iteration 26742 Loss: 0.9512365460395813\n",
      "Iteration 26743 Loss: 1.2005524635314941\n",
      "Iteration 26744 Loss: 1.04768967628479\n",
      "Iteration 26745 Loss: 0.7825860977172852\n",
      "Iteration 26746 Loss: 1.1609320640563965\n",
      "Iteration 26747 Loss: 1.383178949356079\n",
      "Iteration 26748 Loss: 1.1485525369644165\n",
      "Iteration 26749 Loss: 1.2876275777816772\n",
      "Iteration 26749 Loss: 1.0659881830215454\n",
      "Iteration 26750 Loss: 1.0509183406829834\n",
      "Iteration 26751 Loss: 1.2341983318328857\n",
      "Iteration 26752 Loss: 1.112726092338562\n",
      "Iteration 26753 Loss: 1.0768674612045288\n",
      "Iteration 26754 Loss: 1.1347814798355103\n",
      "Iteration 26755 Loss: 0.9649043679237366\n",
      "Iteration 26756 Loss: 1.0693261623382568\n",
      "Iteration 26757 Loss: 1.2317436933517456\n",
      "Iteration 26758 Loss: 0.7478036880493164\n",
      "Iteration 26759 Loss: 1.249528408050537\n",
      "Iteration 26759 Loss: 1.0872799158096313\n",
      "Iteration 26760 Loss: 1.067368745803833\n",
      "Iteration 26761 Loss: 1.000247597694397\n",
      "Iteration 26762 Loss: 1.127549171447754\n",
      "Iteration 26763 Loss: 1.053002953529358\n",
      "Iteration 26764 Loss: 1.48163640499115\n",
      "Iteration 26765 Loss: 1.2961335182189941\n",
      "Iteration 26766 Loss: 0.843928337097168\n",
      "Iteration 26767 Loss: 1.1698371171951294\n",
      "Iteration 26768 Loss: 1.2708698511123657\n",
      "Iteration 26769 Loss: 0.7448541522026062\n",
      "Iteration 26769 Loss: 1.105542778968811\n",
      "Iteration 26770 Loss: 0.7279115915298462\n",
      "Iteration 26771 Loss: 0.7856923937797546\n",
      "Iteration 26772 Loss: 1.0665037631988525\n",
      "Iteration 26773 Loss: 1.069375991821289\n",
      "Iteration 26774 Loss: 1.099899172782898\n",
      "Iteration 26775 Loss: 0.8753505349159241\n",
      "Iteration 26776 Loss: 1.2246519327163696\n",
      "Iteration 26777 Loss: 1.3326753377914429\n",
      "Iteration 26778 Loss: 1.015110969543457\n",
      "Iteration 26779 Loss: 1.148769736289978\n",
      "Iteration 26779 Loss: 1.034594178199768\n",
      "Iteration 26780 Loss: 0.6247597336769104\n",
      "Iteration 26781 Loss: 0.9665005207061768\n",
      "Iteration 26782 Loss: 0.9292351603507996\n",
      "Iteration 26783 Loss: 1.2791204452514648\n",
      "Iteration 26784 Loss: 0.9425931572914124\n",
      "Iteration 26785 Loss: 1.0033073425292969\n",
      "Iteration 26786 Loss: 0.8349462747573853\n",
      "Iteration 26787 Loss: 1.0998526811599731\n",
      "Iteration 26788 Loss: 0.6613404750823975\n",
      "Iteration 26789 Loss: 1.5745261907577515\n",
      "Iteration 26789 Loss: 0.9916181564331055\n",
      "Iteration 26790 Loss: 1.1966429948806763\n",
      "Iteration 26791 Loss: 0.9550262689590454\n",
      "Iteration 26792 Loss: 0.6277276277542114\n",
      "Iteration 26793 Loss: 0.8623654842376709\n",
      "Iteration 26794 Loss: 1.104438066482544\n",
      "Iteration 26795 Loss: 1.593399167060852\n",
      "Iteration 26796 Loss: 1.2332239151000977\n",
      "Iteration 26797 Loss: 1.0484360456466675\n",
      "Iteration 26798 Loss: 1.1083976030349731\n",
      "Iteration 26799 Loss: 1.042635440826416\n",
      "Iteration 26799 Loss: 1.0772292613983154\n",
      "Iteration 26800 Loss: 1.1835556030273438\n",
      "Iteration 26801 Loss: 0.8079391717910767\n",
      "Iteration 26802 Loss: 0.9515844583511353\n",
      "Iteration 26803 Loss: 1.229116439819336\n",
      "Iteration 26804 Loss: 0.7460981607437134\n",
      "Iteration 26805 Loss: 1.2368320226669312\n",
      "Iteration 26806 Loss: 1.3431999683380127\n",
      "Iteration 26807 Loss: 1.1092753410339355\n",
      "Iteration 26808 Loss: 0.9978457093238831\n",
      "Iteration 26809 Loss: 1.0186238288879395\n",
      "Iteration 26809 Loss: 1.0624068975448608\n",
      "Iteration 26810 Loss: 1.2012596130371094\n",
      "Iteration 26811 Loss: 0.5876038074493408\n",
      "Iteration 26812 Loss: 0.5175328850746155\n",
      "Iteration 26813 Loss: 0.7402889728546143\n",
      "Iteration 26814 Loss: 1.2712416648864746\n",
      "Iteration 26815 Loss: 0.6399770379066467\n",
      "Iteration 26816 Loss: 0.7476454973220825\n",
      "Iteration 26817 Loss: 1.2966880798339844\n",
      "Iteration 26818 Loss: 1.4083448648452759\n",
      "Iteration 26819 Loss: 1.2519598007202148\n",
      "Iteration 26819 Loss: 0.9662542343139648\n",
      "Iteration 26820 Loss: 0.9364603757858276\n",
      "Iteration 26821 Loss: 1.1304467916488647\n",
      "Iteration 26822 Loss: 1.1233760118484497\n",
      "Iteration 26823 Loss: 1.1097638607025146\n",
      "Iteration 26824 Loss: 0.9252546429634094\n",
      "Iteration 26825 Loss: 0.8260834217071533\n",
      "Iteration 26826 Loss: 0.7639304995536804\n",
      "Iteration 26827 Loss: 1.1612765789031982\n",
      "Iteration 26828 Loss: 0.9225661754608154\n",
      "Iteration 26829 Loss: 0.9880067110061646\n",
      "Iteration 26829 Loss: 0.9887164831161499\n",
      "Iteration 26830 Loss: 0.786410391330719\n",
      "Iteration 26831 Loss: 1.0413519144058228\n",
      "Iteration 26832 Loss: 0.9184795022010803\n",
      "Iteration 26833 Loss: 1.0238851308822632\n",
      "Iteration 26834 Loss: 0.9409804940223694\n",
      "Iteration 26835 Loss: 1.1863117218017578\n",
      "Iteration 26836 Loss: 1.2336783409118652\n",
      "Iteration 26837 Loss: 1.4562218189239502\n",
      "Iteration 26838 Loss: 0.7562254071235657\n",
      "Iteration 26839 Loss: 1.0777842998504639\n",
      "Iteration 26839 Loss: 1.04213285446167\n",
      "Iteration 26840 Loss: 1.3746000528335571\n",
      "Iteration 26841 Loss: 0.787807285785675\n",
      "Iteration 26842 Loss: 1.3125979900360107\n",
      "Iteration 26843 Loss: 1.2352803945541382\n",
      "Iteration 26844 Loss: 1.0076969861984253\n",
      "Iteration 26845 Loss: 1.2537258863449097\n",
      "Iteration 26846 Loss: 0.9407960176467896\n",
      "Iteration 26847 Loss: 1.0326985120773315\n",
      "Iteration 26848 Loss: 0.9134624600410461\n",
      "Iteration 26849 Loss: 0.6995099782943726\n",
      "Iteration 26849 Loss: 1.0558176040649414\n",
      "Iteration 26850 Loss: 1.0914829969406128\n",
      "Iteration 26851 Loss: 1.5373998880386353\n",
      "Iteration 26852 Loss: 1.1149510145187378\n",
      "Iteration 26853 Loss: 0.7078778147697449\n",
      "Iteration 26854 Loss: 1.1988728046417236\n",
      "Iteration 26855 Loss: 0.9312481880187988\n",
      "Iteration 26856 Loss: 1.1679019927978516\n",
      "Iteration 26857 Loss: 1.0041050910949707\n",
      "Iteration 26858 Loss: 1.1303602457046509\n",
      "Iteration 26859 Loss: 0.9456972479820251\n",
      "Iteration 26859 Loss: 1.0829896926879883\n",
      "Iteration 26860 Loss: 0.9355243444442749\n",
      "Iteration 26861 Loss: 1.2821290493011475\n",
      "Iteration 26862 Loss: 1.1987981796264648\n",
      "Iteration 26863 Loss: 1.2179207801818848\n",
      "Iteration 26864 Loss: 1.2870298624038696\n",
      "Iteration 26865 Loss: 1.0331851243972778\n",
      "Iteration 26866 Loss: 0.910223662853241\n",
      "Iteration 26867 Loss: 1.1255947351455688\n",
      "Iteration 26868 Loss: 1.3404234647750854\n",
      "Iteration 26869 Loss: 0.93871009349823\n",
      "Iteration 26869 Loss: 1.1269539594650269\n",
      "Iteration 26870 Loss: 0.784690260887146\n",
      "Iteration 26871 Loss: 0.885990560054779\n",
      "Iteration 26872 Loss: 1.2201849222183228\n",
      "Iteration 26873 Loss: 1.0470921993255615\n",
      "Iteration 26874 Loss: 0.8197425603866577\n",
      "Iteration 26875 Loss: 0.7958579659461975\n",
      "Iteration 26876 Loss: 0.7842795848846436\n",
      "Iteration 26877 Loss: 1.1482945680618286\n",
      "Iteration 26878 Loss: 1.0062671899795532\n",
      "Iteration 26879 Loss: 1.4727131128311157\n",
      "Iteration 26879 Loss: 0.9965112805366516\n",
      "Iteration 26880 Loss: 1.2155624628067017\n",
      "Iteration 26881 Loss: 0.8737416863441467\n",
      "Iteration 26882 Loss: 1.177823543548584\n",
      "Iteration 26883 Loss: 1.0403591394424438\n",
      "Iteration 26884 Loss: 1.0124255418777466\n",
      "Iteration 26885 Loss: 0.7263712286949158\n",
      "Iteration 26886 Loss: 0.8597940802574158\n",
      "Iteration 26887 Loss: 1.0778802633285522\n",
      "Iteration 26888 Loss: 0.8482369184494019\n",
      "Iteration 26889 Loss: 0.8356796503067017\n",
      "Iteration 26889 Loss: 0.9667873382568359\n",
      "Iteration 26890 Loss: 1.1378039121627808\n",
      "Iteration 26891 Loss: 1.1338086128234863\n",
      "Iteration 26892 Loss: 0.7595623135566711\n",
      "Iteration 26893 Loss: 0.5985041260719299\n",
      "Iteration 26894 Loss: 1.1981579065322876\n",
      "Iteration 26895 Loss: 1.1440194845199585\n",
      "Iteration 26896 Loss: 1.4534302949905396\n",
      "Iteration 26897 Loss: 1.0362623929977417\n",
      "Iteration 26898 Loss: 0.8776229023933411\n",
      "Iteration 26899 Loss: 1.1420562267303467\n",
      "Iteration 26899 Loss: 1.0481228828430176\n",
      "Iteration 26900 Loss: 0.8429619669914246\n",
      "Iteration 26901 Loss: 0.8042202591896057\n",
      "Iteration 26902 Loss: 0.7485600709915161\n",
      "Iteration 26903 Loss: 0.8641057014465332\n",
      "Iteration 26904 Loss: 0.8919628858566284\n",
      "Iteration 26905 Loss: 1.1136596202850342\n",
      "Iteration 26906 Loss: 0.9886739253997803\n",
      "Iteration 26907 Loss: 0.6505171656608582\n",
      "Iteration 26908 Loss: 1.1861822605133057\n",
      "Iteration 26909 Loss: 0.9362086057662964\n",
      "Iteration 26909 Loss: 0.9027053117752075\n",
      "Iteration 26910 Loss: 0.975257158279419\n",
      "Iteration 26911 Loss: 1.1323834657669067\n",
      "Iteration 26912 Loss: 0.9708250164985657\n",
      "Iteration 26913 Loss: 0.756499707698822\n",
      "Iteration 26914 Loss: 0.8257297873497009\n",
      "Iteration 26915 Loss: 1.1346145868301392\n",
      "Iteration 26916 Loss: 1.356207251548767\n",
      "Iteration 26917 Loss: 1.1299982070922852\n",
      "Iteration 26918 Loss: 0.8648167252540588\n",
      "Iteration 26919 Loss: 0.9264844655990601\n",
      "Iteration 26919 Loss: 1.0072816610336304\n",
      "Iteration 26920 Loss: 0.9148314595222473\n",
      "Iteration 26921 Loss: 1.0912870168685913\n",
      "Iteration 26922 Loss: 1.263146162033081\n",
      "Iteration 26923 Loss: 0.9707764387130737\n",
      "Iteration 26924 Loss: 1.4298990964889526\n",
      "Iteration 26925 Loss: 0.9091960191726685\n",
      "Iteration 26926 Loss: 0.9988070130348206\n",
      "Iteration 26927 Loss: 1.0952266454696655\n",
      "Iteration 26928 Loss: 1.234829068183899\n",
      "Iteration 26929 Loss: 1.0059105157852173\n",
      "Iteration 26929 Loss: 1.09139084815979\n",
      "Iteration 26930 Loss: 1.065657377243042\n",
      "Iteration 26931 Loss: 1.1848818063735962\n",
      "Iteration 26932 Loss: 1.0766500234603882\n",
      "Iteration 26933 Loss: 0.8063094019889832\n",
      "Iteration 26934 Loss: 1.1721738576889038\n",
      "Iteration 26935 Loss: 1.1278598308563232\n",
      "Iteration 26936 Loss: 0.9388316869735718\n",
      "Iteration 26937 Loss: 1.030643343925476\n",
      "Iteration 26938 Loss: 1.0142662525177002\n",
      "Iteration 26939 Loss: 0.9156646728515625\n",
      "Iteration 26939 Loss: 1.0332938432693481\n",
      "Iteration 26940 Loss: 0.9491872787475586\n",
      "Iteration 26941 Loss: 1.0533946752548218\n",
      "Iteration 26942 Loss: 1.0204426050186157\n",
      "Iteration 26943 Loss: 1.0261768102645874\n",
      "Iteration 26944 Loss: 1.1613444089889526\n",
      "Iteration 26945 Loss: 1.2121349573135376\n",
      "Iteration 26946 Loss: 0.8970087170600891\n",
      "Iteration 26947 Loss: 1.0387378931045532\n",
      "Iteration 26948 Loss: 1.0537729263305664\n",
      "Iteration 26949 Loss: 1.3071402311325073\n",
      "Iteration 26949 Loss: 1.0719341039657593\n",
      "Iteration 26950 Loss: 0.7908397912979126\n",
      "Iteration 26951 Loss: 0.9519681334495544\n",
      "Iteration 26952 Loss: 1.28583824634552\n",
      "Iteration 26953 Loss: 1.2581321001052856\n",
      "Iteration 26954 Loss: 0.9742940664291382\n",
      "Iteration 26955 Loss: 1.0304203033447266\n",
      "Iteration 26956 Loss: 1.506925106048584\n",
      "Iteration 26957 Loss: 1.0098036527633667\n",
      "Iteration 26958 Loss: 1.2374154329299927\n",
      "Iteration 26959 Loss: 1.142258882522583\n",
      "Iteration 26959 Loss: 1.1187896728515625\n",
      "Iteration 26960 Loss: 1.2155301570892334\n",
      "Iteration 26961 Loss: 1.0652000904083252\n",
      "Iteration 26962 Loss: 1.3882496356964111\n",
      "Iteration 26963 Loss: 1.017087459564209\n",
      "Iteration 26964 Loss: 0.8671415448188782\n",
      "Iteration 26965 Loss: 1.1514670848846436\n",
      "Iteration 26966 Loss: 1.2519975900650024\n",
      "Iteration 26967 Loss: 1.2435003519058228\n",
      "Iteration 26968 Loss: 0.7130814790725708\n",
      "Iteration 26969 Loss: 1.2272690534591675\n",
      "Iteration 26969 Loss: 1.1140525341033936\n",
      "Iteration 26970 Loss: 1.1016031503677368\n",
      "Iteration 26971 Loss: 1.065930962562561\n",
      "Iteration 26972 Loss: 1.0569838285446167\n",
      "Iteration 26973 Loss: 1.1127179861068726\n",
      "Iteration 26974 Loss: 1.0403380393981934\n",
      "Iteration 26975 Loss: 1.209558129310608\n",
      "Iteration 26976 Loss: 0.9423363208770752\n",
      "Iteration 26977 Loss: 1.0975847244262695\n",
      "Iteration 26978 Loss: 0.9581553936004639\n",
      "Iteration 26979 Loss: 0.775966227054596\n",
      "Iteration 26979 Loss: 1.036117434501648\n",
      "Iteration 26980 Loss: 1.0754151344299316\n",
      "Iteration 26981 Loss: 1.189561128616333\n",
      "Iteration 26982 Loss: 1.068589687347412\n",
      "Iteration 26983 Loss: 1.1081074476242065\n",
      "Iteration 26984 Loss: 0.6797250509262085\n",
      "Iteration 26985 Loss: 0.9620769023895264\n",
      "Iteration 26986 Loss: 0.9512103199958801\n",
      "Iteration 26987 Loss: 0.9109516739845276\n",
      "Iteration 26988 Loss: 1.020335078239441\n",
      "Iteration 26989 Loss: 1.0451706647872925\n",
      "Iteration 26989 Loss: 1.0011142492294312\n",
      "Iteration 26990 Loss: 1.0710605382919312\n",
      "Iteration 26991 Loss: 1.2564136981964111\n",
      "Iteration 26992 Loss: 1.1002613306045532\n",
      "Iteration 26993 Loss: 0.9223094582557678\n",
      "Iteration 26994 Loss: 0.723336398601532\n",
      "Iteration 26995 Loss: 1.430876612663269\n",
      "Iteration 26996 Loss: 1.081358790397644\n",
      "Iteration 26997 Loss: 1.0685299634933472\n",
      "Iteration 26998 Loss: 1.1302967071533203\n",
      "Iteration 26999 Loss: 1.077066421508789\n",
      "Iteration 26999 Loss: 1.0861510038375854\n",
      "Iteration 27000 Loss: 1.0623259544372559\n",
      "Iteration 27001 Loss: 0.862058699131012\n",
      "Iteration 27002 Loss: 0.9554846286773682\n",
      "Iteration 27003 Loss: 1.209484338760376\n",
      "Iteration 27004 Loss: 0.9937719702720642\n",
      "Iteration 27005 Loss: 0.901833176612854\n",
      "Iteration 27006 Loss: 1.0073758363723755\n",
      "Iteration 27007 Loss: 1.2875643968582153\n",
      "Iteration 27008 Loss: 1.2404252290725708\n",
      "Iteration 27009 Loss: 1.2252167463302612\n",
      "Iteration 27009 Loss: 1.0745540857315063\n",
      "Iteration 27010 Loss: 0.8504844307899475\n",
      "Iteration 27011 Loss: 1.2296565771102905\n",
      "Iteration 27012 Loss: 1.2642349004745483\n",
      "Iteration 27013 Loss: 0.7883135080337524\n",
      "Iteration 27014 Loss: 0.7154278755187988\n",
      "Iteration 27015 Loss: 1.1314254999160767\n",
      "Iteration 27016 Loss: 1.3987994194030762\n",
      "Iteration 27017 Loss: 0.8230723738670349\n",
      "Iteration 27018 Loss: 1.1715309619903564\n",
      "Iteration 27019 Loss: 0.7962868213653564\n",
      "Iteration 27019 Loss: 1.016923189163208\n",
      "Iteration 27020 Loss: 1.440840482711792\n",
      "Iteration 27021 Loss: 0.9381593465805054\n",
      "Iteration 27022 Loss: 1.3908679485321045\n",
      "Iteration 27023 Loss: 1.0824837684631348\n",
      "Iteration 27024 Loss: 0.8728655576705933\n",
      "Iteration 27025 Loss: 0.7671403884887695\n",
      "Iteration 27026 Loss: 1.4386942386627197\n",
      "Iteration 27027 Loss: 1.2950036525726318\n",
      "Iteration 27028 Loss: 0.8931196331977844\n",
      "Iteration 27029 Loss: 1.0291599035263062\n",
      "Iteration 27029 Loss: 1.1148334741592407\n",
      "Iteration 27030 Loss: 0.8161272406578064\n",
      "Iteration 27031 Loss: 1.2793453931808472\n",
      "Iteration 27032 Loss: 0.9602871537208557\n",
      "Iteration 27033 Loss: 0.8147883415222168\n",
      "Iteration 27034 Loss: 1.1641491651535034\n",
      "Iteration 27035 Loss: 1.27006995677948\n",
      "Iteration 27036 Loss: 0.9812040328979492\n",
      "Iteration 27037 Loss: 0.8966091871261597\n",
      "Iteration 27038 Loss: 0.9561179876327515\n",
      "Iteration 27039 Loss: 1.0502090454101562\n",
      "Iteration 27039 Loss: 1.0188907384872437\n",
      "Iteration 27040 Loss: 0.8338982462882996\n",
      "Iteration 27041 Loss: 0.9322429299354553\n",
      "Iteration 27042 Loss: 0.9820058941841125\n",
      "Iteration 27043 Loss: 1.2240116596221924\n",
      "Iteration 27044 Loss: 0.8928984999656677\n",
      "Iteration 27045 Loss: 1.081331491470337\n",
      "Iteration 27046 Loss: 0.9729361534118652\n",
      "Iteration 27047 Loss: 1.185502052307129\n",
      "Iteration 27048 Loss: 0.9624130129814148\n",
      "Iteration 27049 Loss: 0.9586740136146545\n",
      "Iteration 27049 Loss: 1.0025913715362549\n",
      "Iteration 27050 Loss: 1.2309927940368652\n",
      "Iteration 27051 Loss: 0.6935827136039734\n",
      "Iteration 27052 Loss: 0.8915218710899353\n",
      "Iteration 27053 Loss: 1.7954132556915283\n",
      "Iteration 27054 Loss: 1.1298025846481323\n",
      "Iteration 27055 Loss: 1.2138465642929077\n",
      "Iteration 27056 Loss: 0.6618191003799438\n",
      "Iteration 27057 Loss: 0.8687469959259033\n",
      "Iteration 27058 Loss: 0.9983124136924744\n",
      "Iteration 27059 Loss: 0.8738245368003845\n",
      "Iteration 27059 Loss: 1.0357862710952759\n",
      "Iteration 27060 Loss: 0.9045703411102295\n",
      "Iteration 27061 Loss: 1.4097622632980347\n",
      "Iteration 27062 Loss: 1.1273902654647827\n",
      "Iteration 27063 Loss: 1.090958595275879\n",
      "Iteration 27064 Loss: 1.0666865110397339\n",
      "Iteration 27065 Loss: 1.056464433670044\n",
      "Iteration 27066 Loss: 0.8726966977119446\n",
      "Iteration 27067 Loss: 1.136187195777893\n",
      "Iteration 27068 Loss: 1.0678350925445557\n",
      "Iteration 27069 Loss: 0.7562905550003052\n",
      "Iteration 27069 Loss: 1.0488842725753784\n",
      "Iteration 27070 Loss: 1.2208679914474487\n",
      "Iteration 27071 Loss: 0.9060041904449463\n",
      "Iteration 27072 Loss: 1.0645564794540405\n",
      "Iteration 27073 Loss: 1.1103676557540894\n",
      "Iteration 27074 Loss: 1.1212327480316162\n",
      "Iteration 27075 Loss: 0.7852360010147095\n",
      "Iteration 27076 Loss: 1.2664395570755005\n",
      "Iteration 27077 Loss: 1.051253318786621\n",
      "Iteration 27078 Loss: 1.4588733911514282\n",
      "Iteration 27079 Loss: 0.9821056723594666\n",
      "Iteration 27079 Loss: 1.096693754196167\n",
      "Iteration 27080 Loss: 1.271770715713501\n",
      "Iteration 27081 Loss: 1.1352014541625977\n",
      "Iteration 27082 Loss: 1.1629588603973389\n",
      "Iteration 27083 Loss: 1.359391450881958\n",
      "Iteration 27084 Loss: 0.796259343624115\n",
      "Iteration 27085 Loss: 0.7526756525039673\n",
      "Iteration 27086 Loss: 0.9327227473258972\n",
      "Iteration 27087 Loss: 0.998577892780304\n",
      "Iteration 27088 Loss: 0.6659561395645142\n",
      "Iteration 27089 Loss: 1.0151348114013672\n",
      "Iteration 27089 Loss: 1.0090649127960205\n",
      "Iteration 27090 Loss: 1.238194227218628\n",
      "Iteration 27091 Loss: 1.0838747024536133\n",
      "Iteration 27092 Loss: 1.3543269634246826\n",
      "Iteration 27093 Loss: 0.9495384097099304\n",
      "Iteration 27094 Loss: 0.8574632406234741\n",
      "Iteration 27095 Loss: 1.3536255359649658\n",
      "Iteration 27096 Loss: 1.3029968738555908\n",
      "Iteration 27097 Loss: 1.1575613021850586\n",
      "Iteration 27098 Loss: 1.1830534934997559\n",
      "Iteration 27099 Loss: 0.8703370690345764\n",
      "Iteration 27099 Loss: 1.1350972652435303\n",
      "Iteration 27100 Loss: 1.226096749305725\n",
      "Iteration 27101 Loss: 0.903192937374115\n",
      "Iteration 27102 Loss: 0.9365426898002625\n",
      "Iteration 27103 Loss: 0.9901385307312012\n",
      "Iteration 27104 Loss: 1.504044532775879\n",
      "Iteration 27105 Loss: 1.109859824180603\n",
      "Iteration 27106 Loss: 1.0994834899902344\n",
      "Iteration 27107 Loss: 0.8187722563743591\n",
      "Iteration 27108 Loss: 0.8970028162002563\n",
      "Iteration 27109 Loss: 0.7610939741134644\n",
      "Iteration 27109 Loss: 1.0246226787567139\n",
      "Iteration 27110 Loss: 1.0589724779129028\n",
      "Iteration 27111 Loss: 1.2670841217041016\n",
      "Iteration 27112 Loss: 1.1654880046844482\n",
      "Iteration 27113 Loss: 1.0610178709030151\n",
      "Iteration 27114 Loss: 1.0196589231491089\n",
      "Iteration 27115 Loss: 1.034397840499878\n",
      "Iteration 27116 Loss: 1.404304027557373\n",
      "Iteration 27117 Loss: 1.0256803035736084\n",
      "Iteration 27118 Loss: 1.0398435592651367\n",
      "Iteration 27119 Loss: 1.0243209600448608\n",
      "Iteration 27119 Loss: 1.110076904296875\n",
      "Iteration 27120 Loss: 1.2460170984268188\n",
      "Iteration 27121 Loss: 0.9600452780723572\n",
      "Iteration 27122 Loss: 0.7806586623191833\n",
      "Iteration 27123 Loss: 1.3395916223526\n",
      "Iteration 27124 Loss: 1.0692429542541504\n",
      "Iteration 27125 Loss: 1.1851699352264404\n",
      "Iteration 27126 Loss: 1.0064059495925903\n",
      "Iteration 27127 Loss: 1.1498796939849854\n",
      "Iteration 27128 Loss: 1.0212187767028809\n",
      "Iteration 27129 Loss: 0.9985741972923279\n",
      "Iteration 27129 Loss: 1.0756803750991821\n",
      "Iteration 27130 Loss: 0.9184175133705139\n",
      "Iteration 27131 Loss: 0.9431977272033691\n",
      "Iteration 27132 Loss: 1.2101725339889526\n",
      "Iteration 27133 Loss: 1.4012373685836792\n",
      "Iteration 27134 Loss: 0.9112526774406433\n",
      "Iteration 27135 Loss: 1.3866280317306519\n",
      "Iteration 27136 Loss: 1.1586933135986328\n",
      "Iteration 27137 Loss: 1.355292558670044\n",
      "Iteration 27138 Loss: 1.126220703125\n",
      "Iteration 27139 Loss: 0.8733258247375488\n",
      "Iteration 27139 Loss: 1.1284438371658325\n",
      "Iteration 27140 Loss: 0.8345407247543335\n",
      "Iteration 27141 Loss: 1.1220821142196655\n",
      "Iteration 27142 Loss: 1.0260123014450073\n",
      "Iteration 27143 Loss: 0.8684219121932983\n",
      "Iteration 27144 Loss: 0.8011462688446045\n",
      "Iteration 27145 Loss: 0.9529065489768982\n",
      "Iteration 27146 Loss: 1.2571886777877808\n",
      "Iteration 27147 Loss: 0.8709917664527893\n",
      "Iteration 27148 Loss: 1.164157509803772\n",
      "Iteration 27149 Loss: 0.8080289363861084\n",
      "Iteration 27149 Loss: 0.9705476760864258\n",
      "Iteration 27150 Loss: 0.9236377477645874\n",
      "Iteration 27151 Loss: 0.9945626854896545\n",
      "Iteration 27152 Loss: 1.3632090091705322\n",
      "Iteration 27153 Loss: 1.3928884267807007\n",
      "Iteration 27154 Loss: 0.9992083311080933\n",
      "Iteration 27155 Loss: 0.9199327826499939\n",
      "Iteration 27156 Loss: 0.9296826124191284\n",
      "Iteration 27157 Loss: 1.122153401374817\n",
      "Iteration 27158 Loss: 1.0907108783721924\n",
      "Iteration 27159 Loss: 0.8347331285476685\n",
      "Iteration 27159 Loss: 1.0570719242095947\n",
      "Iteration 27160 Loss: 1.0056971311569214\n",
      "Iteration 27161 Loss: 1.1205763816833496\n",
      "Iteration 27162 Loss: 1.0842320919036865\n",
      "Iteration 27163 Loss: 1.1038192510604858\n",
      "Iteration 27164 Loss: 1.2154808044433594\n",
      "Iteration 27165 Loss: 0.8958991765975952\n",
      "Iteration 27166 Loss: 1.0905758142471313\n",
      "Iteration 27167 Loss: 1.1589078903198242\n",
      "Iteration 27168 Loss: 1.0592551231384277\n",
      "Iteration 27169 Loss: 0.9813379645347595\n",
      "Iteration 27169 Loss: 1.0715782642364502\n",
      "Iteration 27170 Loss: 1.5784685611724854\n",
      "Iteration 27171 Loss: 1.2763848304748535\n",
      "Iteration 27172 Loss: 1.237028956413269\n",
      "Iteration 27173 Loss: 1.4576135873794556\n",
      "Iteration 27174 Loss: 1.193708896636963\n",
      "Iteration 27175 Loss: 1.0529484748840332\n",
      "Iteration 27176 Loss: 1.4056527614593506\n",
      "Iteration 27177 Loss: 1.127627968788147\n",
      "Iteration 27178 Loss: 1.0925066471099854\n",
      "Iteration 27179 Loss: 1.358168125152588\n",
      "Iteration 27179 Loss: 1.2780109643936157\n",
      "Iteration 27180 Loss: 1.325453519821167\n",
      "Iteration 27181 Loss: 0.9995265007019043\n",
      "Iteration 27182 Loss: 0.9042409658432007\n",
      "Iteration 27183 Loss: 1.2408479452133179\n",
      "Iteration 27184 Loss: 0.9869182705879211\n",
      "Iteration 27185 Loss: 1.0240821838378906\n",
      "Iteration 27186 Loss: 0.979085385799408\n",
      "Iteration 27187 Loss: 1.16470205783844\n",
      "Iteration 27188 Loss: 1.025983452796936\n",
      "Iteration 27189 Loss: 0.7787730097770691\n",
      "Iteration 27189 Loss: 1.0429613590240479\n",
      "Iteration 27190 Loss: 1.143314003944397\n",
      "Iteration 27191 Loss: 0.8436535596847534\n",
      "Iteration 27192 Loss: 1.2182635068893433\n",
      "Iteration 27193 Loss: 1.103513479232788\n",
      "Iteration 27194 Loss: 0.999542772769928\n",
      "Iteration 27195 Loss: 1.0701360702514648\n",
      "Iteration 27196 Loss: 0.9992234110832214\n",
      "Iteration 27197 Loss: 1.1926125288009644\n",
      "Iteration 27198 Loss: 1.2025182247161865\n",
      "Iteration 27199 Loss: 1.1674034595489502\n",
      "Iteration 27199 Loss: 1.0940181016921997\n",
      "Iteration 27200 Loss: 0.9955433011054993\n",
      "Iteration 27201 Loss: 1.1629287004470825\n",
      "Iteration 27202 Loss: 1.111733078956604\n",
      "Iteration 27203 Loss: 1.0870258808135986\n",
      "Iteration 27204 Loss: 0.9015395641326904\n",
      "Iteration 27205 Loss: 0.9494400024414062\n",
      "Iteration 27206 Loss: 0.9092811942100525\n",
      "Iteration 27207 Loss: 0.7342650890350342\n",
      "Iteration 27208 Loss: 1.060999870300293\n",
      "Iteration 27209 Loss: 1.0374892950057983\n",
      "Iteration 27209 Loss: 0.995024561882019\n",
      "Iteration 27210 Loss: 1.1905540227890015\n",
      "Iteration 27211 Loss: 1.4111658334732056\n",
      "Iteration 27212 Loss: 1.1837652921676636\n",
      "Iteration 27213 Loss: 0.8926635384559631\n",
      "Iteration 27214 Loss: 1.2453371286392212\n",
      "Iteration 27215 Loss: 0.8438194990158081\n",
      "Iteration 27216 Loss: 0.8925038576126099\n",
      "Iteration 27217 Loss: 0.9958903789520264\n",
      "Iteration 27218 Loss: 1.0249249935150146\n",
      "Iteration 27219 Loss: 1.1801456212997437\n",
      "Iteration 27219 Loss: 1.0860769748687744\n",
      "Iteration 27220 Loss: 0.9073027968406677\n",
      "Iteration 27221 Loss: 0.9937334656715393\n",
      "Iteration 27222 Loss: 1.1068100929260254\n",
      "Iteration 27223 Loss: 0.8150149583816528\n",
      "Iteration 27224 Loss: 0.9910822510719299\n",
      "Iteration 27225 Loss: 0.9443040490150452\n",
      "Iteration 27226 Loss: 1.1074141263961792\n",
      "Iteration 27227 Loss: 0.9517109394073486\n",
      "Iteration 27228 Loss: 1.0001283884048462\n",
      "Iteration 27229 Loss: 1.6799938678741455\n",
      "Iteration 27229 Loss: 1.049749493598938\n",
      "Iteration 27230 Loss: 0.7011029720306396\n",
      "Iteration 27231 Loss: 1.2900917530059814\n",
      "Iteration 27232 Loss: 0.849983811378479\n",
      "Iteration 27233 Loss: 1.1354886293411255\n",
      "Iteration 27234 Loss: 1.3028074502944946\n",
      "Iteration 27235 Loss: 0.8925555348396301\n",
      "Iteration 27236 Loss: 1.136897087097168\n",
      "Iteration 27237 Loss: 1.1253207921981812\n",
      "Iteration 27238 Loss: 1.1373459100723267\n",
      "Iteration 27239 Loss: 0.8331778645515442\n",
      "Iteration 27239 Loss: 1.0404770374298096\n",
      "Iteration 27240 Loss: 1.063075065612793\n",
      "Iteration 27241 Loss: 1.422359585762024\n",
      "Iteration 27242 Loss: 1.131709337234497\n",
      "Iteration 27243 Loss: 0.9657486081123352\n",
      "Iteration 27244 Loss: 0.7762995362281799\n",
      "Iteration 27245 Loss: 1.0114891529083252\n",
      "Iteration 27246 Loss: 0.9028856158256531\n",
      "Iteration 27247 Loss: 1.1140917539596558\n",
      "Iteration 27248 Loss: 0.9404265880584717\n",
      "Iteration 27249 Loss: 1.0149106979370117\n",
      "Iteration 27249 Loss: 1.034299612045288\n",
      "Iteration 27250 Loss: 1.3665181398391724\n",
      "Iteration 27251 Loss: 1.0216869115829468\n",
      "Iteration 27252 Loss: 1.1319690942764282\n",
      "Iteration 27253 Loss: 1.2841473817825317\n",
      "Iteration 27254 Loss: 1.0195457935333252\n",
      "Iteration 27255 Loss: 0.9045757055282593\n",
      "Iteration 27256 Loss: 1.3087353706359863\n",
      "Iteration 27257 Loss: 1.2204984426498413\n",
      "Iteration 27258 Loss: 0.6564747095108032\n",
      "Iteration 27259 Loss: 0.9691236615180969\n",
      "Iteration 27259 Loss: 1.0883275270462036\n",
      "Iteration 27260 Loss: 1.3017756938934326\n",
      "Iteration 27261 Loss: 1.1995187997817993\n",
      "Iteration 27262 Loss: 1.25508451461792\n",
      "Iteration 27263 Loss: 0.9251909255981445\n",
      "Iteration 27264 Loss: 1.0807878971099854\n",
      "Iteration 27265 Loss: 1.5017027854919434\n",
      "Iteration 27266 Loss: 1.0430200099945068\n",
      "Iteration 27267 Loss: 1.106878638267517\n",
      "Iteration 27268 Loss: 0.82076096534729\n",
      "Iteration 27269 Loss: 1.3789377212524414\n",
      "Iteration 27269 Loss: 1.1613657474517822\n",
      "Iteration 27270 Loss: 1.1917027235031128\n",
      "Iteration 27271 Loss: 1.190723180770874\n",
      "Iteration 27272 Loss: 1.1351549625396729\n",
      "Iteration 27273 Loss: 1.0612056255340576\n",
      "Iteration 27274 Loss: 0.8827757239341736\n",
      "Iteration 27275 Loss: 1.1096256971359253\n",
      "Iteration 27276 Loss: 1.047505259513855\n",
      "Iteration 27277 Loss: 1.0522598028182983\n",
      "Iteration 27278 Loss: 0.7331060767173767\n",
      "Iteration 27279 Loss: 1.0132488012313843\n",
      "Iteration 27279 Loss: 1.0417306423187256\n",
      "Iteration 27280 Loss: 0.860564112663269\n",
      "Iteration 27281 Loss: 1.1531567573547363\n",
      "Iteration 27282 Loss: 1.3089630603790283\n",
      "Iteration 27283 Loss: 1.3008520603179932\n",
      "Iteration 27284 Loss: 0.7961940169334412\n",
      "Iteration 27285 Loss: 0.8467060327529907\n",
      "Iteration 27286 Loss: 0.9834390878677368\n",
      "Iteration 27287 Loss: 1.5604262351989746\n",
      "Iteration 27288 Loss: 1.0187852382659912\n",
      "Iteration 27289 Loss: 1.3760031461715698\n",
      "Iteration 27289 Loss: 1.1205089092254639\n",
      "Iteration 27290 Loss: 1.145121693611145\n",
      "Iteration 27291 Loss: 0.7900292873382568\n",
      "Iteration 27292 Loss: 1.143062949180603\n",
      "Iteration 27293 Loss: 1.1395343542099\n",
      "Iteration 27294 Loss: 1.002469539642334\n",
      "Iteration 27295 Loss: 1.002505898475647\n",
      "Iteration 27296 Loss: 1.0518180131912231\n",
      "Iteration 27297 Loss: 0.9704616665840149\n",
      "Iteration 27298 Loss: 0.8000338077545166\n",
      "Iteration 27299 Loss: 1.2473539113998413\n",
      "Iteration 27299 Loss: 1.0292390584945679\n",
      "Iteration 27300 Loss: 1.1953961849212646\n",
      "Iteration 27301 Loss: 0.8310500383377075\n",
      "Iteration 27302 Loss: 0.81786048412323\n",
      "Iteration 27303 Loss: 1.156620740890503\n",
      "Iteration 27304 Loss: 1.2640166282653809\n",
      "Iteration 27305 Loss: 1.2431045770645142\n",
      "Iteration 27306 Loss: 1.199821949005127\n",
      "Iteration 27307 Loss: 0.7732940316200256\n",
      "Iteration 27308 Loss: 1.3497512340545654\n",
      "Iteration 27309 Loss: 0.8950030207633972\n",
      "Iteration 27309 Loss: 1.07259202003479\n",
      "Iteration 27310 Loss: 1.0879582166671753\n",
      "Iteration 27311 Loss: 0.7108409404754639\n",
      "Iteration 27312 Loss: 0.9517532587051392\n",
      "Iteration 27313 Loss: 1.2864447832107544\n",
      "Iteration 27314 Loss: 0.8632575869560242\n",
      "Iteration 27315 Loss: 0.9783645272254944\n",
      "Iteration 27316 Loss: 1.0761733055114746\n",
      "Iteration 27317 Loss: 1.2080937623977661\n",
      "Iteration 27318 Loss: 1.3186365365982056\n",
      "Iteration 27319 Loss: 1.07115638256073\n",
      "Iteration 27319 Loss: 1.0552679300308228\n",
      "Iteration 27320 Loss: 1.0169919729232788\n",
      "Iteration 27321 Loss: 0.8979895114898682\n",
      "Iteration 27322 Loss: 1.113333821296692\n",
      "Iteration 27323 Loss: 1.2331218719482422\n",
      "Iteration 27324 Loss: 0.9511222839355469\n",
      "Iteration 27325 Loss: 0.8149476647377014\n",
      "Iteration 27326 Loss: 0.6179835200309753\n",
      "Iteration 27327 Loss: 1.0610593557357788\n",
      "Iteration 27328 Loss: 1.1007615327835083\n",
      "Iteration 27329 Loss: 1.006422519683838\n",
      "Iteration 27329 Loss: 0.9813734292984009\n",
      "Iteration 27330 Loss: 1.064293384552002\n",
      "Iteration 27331 Loss: 0.8574551343917847\n",
      "Iteration 27332 Loss: 1.0702013969421387\n",
      "Iteration 27333 Loss: 0.9571038484573364\n",
      "Iteration 27334 Loss: 1.4175560474395752\n",
      "Iteration 27335 Loss: 0.7751009464263916\n",
      "Iteration 27336 Loss: 1.2135958671569824\n",
      "Iteration 27337 Loss: 0.8930681347846985\n",
      "Iteration 27338 Loss: 0.8853027820587158\n",
      "Iteration 27339 Loss: 1.3364858627319336\n",
      "Iteration 27339 Loss: 1.0470163822174072\n",
      "Iteration 27340 Loss: 1.176340103149414\n",
      "Iteration 27341 Loss: 0.8848410248756409\n",
      "Iteration 27342 Loss: 1.215751051902771\n",
      "Iteration 27343 Loss: 1.0352848768234253\n",
      "Iteration 27344 Loss: 1.366979956626892\n",
      "Iteration 27345 Loss: 0.9145232439041138\n",
      "Iteration 27346 Loss: 1.1261587142944336\n",
      "Iteration 27347 Loss: 0.7608016133308411\n",
      "Iteration 27348 Loss: 0.8708983659744263\n",
      "Iteration 27349 Loss: 1.179112195968628\n",
      "Iteration 27349 Loss: 1.0530691146850586\n",
      "Iteration 27350 Loss: 1.0865249633789062\n",
      "Iteration 27351 Loss: 0.8989989757537842\n",
      "Iteration 27352 Loss: 1.0686724185943604\n",
      "Iteration 27353 Loss: 0.8755384683609009\n",
      "Iteration 27354 Loss: 1.0263341665267944\n",
      "Iteration 27355 Loss: 1.1528292894363403\n",
      "Iteration 27356 Loss: 0.8909857869148254\n",
      "Iteration 27357 Loss: 0.9159964323043823\n",
      "Iteration 27358 Loss: 0.8889870643615723\n",
      "Iteration 27359 Loss: 0.9922362565994263\n",
      "Iteration 27359 Loss: 0.9797104001045227\n",
      "Iteration 27360 Loss: 1.1893421411514282\n",
      "Iteration 27361 Loss: 1.1047462224960327\n",
      "Iteration 27362 Loss: 0.9817562699317932\n",
      "Iteration 27363 Loss: 1.1238892078399658\n",
      "Iteration 27364 Loss: 1.1734578609466553\n",
      "Iteration 27365 Loss: 1.0540951490402222\n",
      "Iteration 27366 Loss: 1.0177006721496582\n",
      "Iteration 27367 Loss: 1.000019907951355\n",
      "Iteration 27368 Loss: 1.0553152561187744\n",
      "Iteration 27369 Loss: 0.8592635989189148\n",
      "Iteration 27369 Loss: 1.05595862865448\n",
      "Iteration 27370 Loss: 0.9952827095985413\n",
      "Iteration 27371 Loss: 0.8129960894584656\n",
      "Iteration 27372 Loss: 1.094095230102539\n",
      "Iteration 27373 Loss: 0.8349123001098633\n",
      "Iteration 27374 Loss: 0.6699612736701965\n",
      "Iteration 27375 Loss: 1.2991621494293213\n",
      "Iteration 27376 Loss: 1.1102559566497803\n",
      "Iteration 27377 Loss: 1.1740425825119019\n",
      "Iteration 27378 Loss: 0.9496911764144897\n",
      "Iteration 27379 Loss: 0.8878561854362488\n",
      "Iteration 27379 Loss: 0.9828256368637085\n",
      "Iteration 27380 Loss: 0.8178438544273376\n",
      "Iteration 27381 Loss: 1.3947334289550781\n",
      "Iteration 27382 Loss: 1.0457464456558228\n",
      "Iteration 27383 Loss: 1.0718878507614136\n",
      "Iteration 27384 Loss: 0.6310959458351135\n",
      "Iteration 27385 Loss: 0.7269863486289978\n",
      "Iteration 27386 Loss: 1.1601545810699463\n",
      "Iteration 27387 Loss: 1.199671983718872\n",
      "Iteration 27388 Loss: 0.971294641494751\n",
      "Iteration 27389 Loss: 1.2437654733657837\n",
      "Iteration 27389 Loss: 1.026318073272705\n",
      "Iteration 27390 Loss: 0.9255468249320984\n",
      "Iteration 27391 Loss: 0.8516918420791626\n",
      "Iteration 27392 Loss: 0.9363672137260437\n",
      "Iteration 27393 Loss: 0.8994152545928955\n",
      "Iteration 27394 Loss: 1.325655460357666\n",
      "Iteration 27395 Loss: 0.812313437461853\n",
      "Iteration 27396 Loss: 1.0025265216827393\n",
      "Iteration 27397 Loss: 1.0192513465881348\n",
      "Iteration 27398 Loss: 1.3445442914962769\n",
      "Iteration 27399 Loss: 0.8332297205924988\n",
      "Iteration 27399 Loss: 0.9950542449951172\n",
      "Iteration 27400 Loss: 1.0540212392807007\n",
      "Iteration 27401 Loss: 0.8768177032470703\n",
      "Iteration 27402 Loss: 1.1664693355560303\n",
      "Iteration 27403 Loss: 1.1609302759170532\n",
      "Iteration 27404 Loss: 0.824979305267334\n",
      "Iteration 27405 Loss: 1.1034826040267944\n",
      "Iteration 27406 Loss: 1.4739739894866943\n",
      "Iteration 27407 Loss: 0.8215559720993042\n",
      "Iteration 27408 Loss: 1.3595603704452515\n",
      "Iteration 27409 Loss: 1.0344101190567017\n",
      "Iteration 27409 Loss: 1.0876200199127197\n",
      "Iteration 27410 Loss: 1.324920415878296\n",
      "Iteration 27411 Loss: 0.9198914766311646\n",
      "Iteration 27412 Loss: 1.0842759609222412\n",
      "Iteration 27413 Loss: 1.110007405281067\n",
      "Iteration 27414 Loss: 0.775070071220398\n",
      "Iteration 27415 Loss: 0.8641431331634521\n",
      "Iteration 27416 Loss: 1.0797243118286133\n",
      "Iteration 27417 Loss: 0.7582418322563171\n",
      "Iteration 27418 Loss: 1.2785205841064453\n",
      "Iteration 27419 Loss: 0.9479265809059143\n",
      "Iteration 27419 Loss: 1.0142722129821777\n",
      "Iteration 27420 Loss: 0.5077430009841919\n",
      "Iteration 27421 Loss: 0.8394972681999207\n",
      "Iteration 27422 Loss: 1.2252694368362427\n",
      "Iteration 27423 Loss: 1.1630144119262695\n",
      "Iteration 27424 Loss: 0.9925119876861572\n",
      "Iteration 27425 Loss: 1.0450928211212158\n",
      "Iteration 27426 Loss: 1.3843194246292114\n",
      "Iteration 27427 Loss: 1.0714070796966553\n",
      "Iteration 27428 Loss: 1.2516371011734009\n",
      "Iteration 27429 Loss: 0.7433170676231384\n",
      "Iteration 27429 Loss: 1.0223809480667114\n",
      "Iteration 27430 Loss: 0.7758125066757202\n",
      "Iteration 27431 Loss: 0.8785796761512756\n",
      "Iteration 27432 Loss: 0.8136761784553528\n",
      "Iteration 27433 Loss: 1.1385619640350342\n",
      "Iteration 27434 Loss: 1.2595359086990356\n",
      "Iteration 27435 Loss: 1.1221692562103271\n",
      "Iteration 27436 Loss: 1.1964383125305176\n",
      "Iteration 27437 Loss: 1.078954815864563\n",
      "Iteration 27438 Loss: 1.142608642578125\n",
      "Iteration 27439 Loss: 1.335425853729248\n",
      "Iteration 27439 Loss: 1.0741764307022095\n",
      "Iteration 27440 Loss: 1.0584524869918823\n",
      "Iteration 27441 Loss: 0.7196381688117981\n",
      "Iteration 27442 Loss: 0.9621210694313049\n",
      "Iteration 27443 Loss: 1.1331490278244019\n",
      "Iteration 27444 Loss: 1.1344258785247803\n",
      "Iteration 27445 Loss: 0.9029064774513245\n",
      "Iteration 27446 Loss: 0.8492394089698792\n",
      "Iteration 27447 Loss: 0.9940376877784729\n",
      "Iteration 27448 Loss: 1.3704633712768555\n",
      "Iteration 27449 Loss: 1.0766654014587402\n",
      "Iteration 27449 Loss: 1.0201098918914795\n",
      "Iteration 27450 Loss: 0.9538379311561584\n",
      "Iteration 27451 Loss: 1.1665300130844116\n",
      "Iteration 27452 Loss: 1.0630073547363281\n",
      "Iteration 27453 Loss: 0.8189166784286499\n",
      "Iteration 27454 Loss: 0.732154130935669\n",
      "Iteration 27455 Loss: 1.13239324092865\n",
      "Iteration 27456 Loss: 1.5091407299041748\n",
      "Iteration 27457 Loss: 0.9924405217170715\n",
      "Iteration 27458 Loss: 1.1447011232376099\n",
      "Iteration 27459 Loss: 1.2855883836746216\n",
      "Iteration 27459 Loss: 1.0798709392547607\n",
      "Iteration 27460 Loss: 1.0341483354568481\n",
      "Iteration 27461 Loss: 0.8842920660972595\n",
      "Iteration 27462 Loss: 0.9100647568702698\n",
      "Iteration 27463 Loss: 0.9463891983032227\n",
      "Iteration 27464 Loss: 0.7148042917251587\n",
      "Iteration 27465 Loss: 0.858657717704773\n",
      "Iteration 27466 Loss: 1.0854121446609497\n",
      "Iteration 27467 Loss: 1.1319104433059692\n",
      "Iteration 27468 Loss: 1.0615843534469604\n",
      "Iteration 27469 Loss: 0.8572397828102112\n",
      "Iteration 27469 Loss: 0.9484502673149109\n",
      "Iteration 27470 Loss: 0.953755795955658\n",
      "Iteration 27471 Loss: 1.3412630558013916\n",
      "Iteration 27472 Loss: 1.2378582954406738\n",
      "Iteration 27473 Loss: 1.1971312761306763\n",
      "Iteration 27474 Loss: 1.0459073781967163\n",
      "Iteration 27475 Loss: 0.9797763228416443\n",
      "Iteration 27476 Loss: 0.9998498558998108\n",
      "Iteration 27477 Loss: 0.7337645292282104\n",
      "Iteration 27478 Loss: 1.011446237564087\n",
      "Iteration 27479 Loss: 1.0308914184570312\n",
      "Iteration 27479 Loss: 1.0531644821166992\n",
      "Iteration 27480 Loss: 0.8787807822227478\n",
      "Iteration 27481 Loss: 0.9183059334754944\n",
      "Iteration 27482 Loss: 0.8984324932098389\n",
      "Iteration 27483 Loss: 1.000479817390442\n",
      "Iteration 27484 Loss: 1.312370777130127\n",
      "Iteration 27485 Loss: 1.2305363416671753\n",
      "Iteration 27486 Loss: 0.8163115382194519\n",
      "Iteration 27487 Loss: 1.0160945653915405\n",
      "Iteration 27488 Loss: 0.9159809350967407\n",
      "Iteration 27489 Loss: 1.2608705759048462\n",
      "Iteration 27489 Loss: 1.0248162746429443\n",
      "Iteration 27490 Loss: 0.8885919451713562\n",
      "Iteration 27491 Loss: 1.1203370094299316\n",
      "Iteration 27492 Loss: 0.9962466359138489\n",
      "Iteration 27493 Loss: 1.157448410987854\n",
      "Iteration 27494 Loss: 1.0209121704101562\n",
      "Iteration 27495 Loss: 1.4359763860702515\n",
      "Iteration 27496 Loss: 0.9734633564949036\n",
      "Iteration 27497 Loss: 1.0956542491912842\n",
      "Iteration 27498 Loss: 0.8976508378982544\n",
      "Iteration 27499 Loss: 1.117773175239563\n",
      "Iteration 27499 Loss: 1.07040536403656\n",
      "Iteration 27500 Loss: 1.1171108484268188\n",
      "Iteration 27501 Loss: 0.9708991050720215\n",
      "Iteration 27502 Loss: 0.9365570545196533\n",
      "Iteration 27503 Loss: 0.8408197164535522\n",
      "Iteration 27504 Loss: 1.2208000421524048\n",
      "Iteration 27505 Loss: 1.1354479789733887\n",
      "Iteration 27506 Loss: 0.9389064311981201\n",
      "Iteration 27507 Loss: 0.9549734592437744\n",
      "Iteration 27508 Loss: 0.8915027976036072\n",
      "Iteration 27509 Loss: 1.1578643321990967\n",
      "Iteration 27509 Loss: 1.0164881944656372\n",
      "Iteration 27510 Loss: 1.0212002992630005\n",
      "Iteration 27511 Loss: 0.9900450706481934\n",
      "Iteration 27512 Loss: 0.9682121872901917\n",
      "Iteration 27513 Loss: 0.8030245304107666\n",
      "Iteration 27514 Loss: 1.284378170967102\n",
      "Iteration 27515 Loss: 0.7147420644760132\n",
      "Iteration 27516 Loss: 0.8731222748756409\n",
      "Iteration 27517 Loss: 0.9992689490318298\n",
      "Iteration 27518 Loss: 1.2605429887771606\n",
      "Iteration 27519 Loss: 1.1182276010513306\n",
      "Iteration 27519 Loss: 1.0032763481140137\n",
      "Iteration 27520 Loss: 1.0145894289016724\n",
      "Iteration 27521 Loss: 0.9310594201087952\n",
      "Iteration 27522 Loss: 0.9521146416664124\n",
      "Iteration 27523 Loss: 1.1481144428253174\n",
      "Iteration 27524 Loss: 0.6428990960121155\n",
      "Iteration 27525 Loss: 0.914268970489502\n",
      "Iteration 27526 Loss: 0.9263100028038025\n",
      "Iteration 27527 Loss: 0.9184466004371643\n",
      "Iteration 27528 Loss: 0.7159175276756287\n",
      "Iteration 27529 Loss: 1.128309726715088\n",
      "Iteration 27529 Loss: 0.9292029142379761\n",
      "Iteration 27530 Loss: 1.0618531703948975\n",
      "Iteration 27531 Loss: 1.3584262132644653\n",
      "Iteration 27532 Loss: 1.1441106796264648\n",
      "Iteration 27533 Loss: 0.9388397932052612\n",
      "Iteration 27534 Loss: 0.8207846879959106\n",
      "Iteration 27535 Loss: 1.1292791366577148\n",
      "Iteration 27536 Loss: 1.1614327430725098\n",
      "Iteration 27537 Loss: 1.1379735469818115\n",
      "Iteration 27538 Loss: 1.1067074537277222\n",
      "Iteration 27539 Loss: 1.0362955331802368\n",
      "Iteration 27539 Loss: 1.0895702838897705\n",
      "Iteration 27540 Loss: 1.287316918373108\n",
      "Iteration 27541 Loss: 0.8185782432556152\n",
      "Iteration 27542 Loss: 1.1603292226791382\n",
      "Iteration 27543 Loss: 0.9616069197654724\n",
      "Iteration 27544 Loss: 1.223356008529663\n",
      "Iteration 27545 Loss: 1.101643681526184\n",
      "Iteration 27546 Loss: 1.244290828704834\n",
      "Iteration 27547 Loss: 1.0599783658981323\n",
      "Iteration 27548 Loss: 1.15108060836792\n",
      "Iteration 27549 Loss: 0.992434561252594\n",
      "Iteration 27549 Loss: 1.1000616550445557\n",
      "Iteration 27550 Loss: 1.1082768440246582\n",
      "Iteration 27551 Loss: 0.9212440252304077\n",
      "Iteration 27552 Loss: 1.1116515398025513\n",
      "Iteration 27553 Loss: 0.8786139488220215\n",
      "Iteration 27554 Loss: 1.089699387550354\n",
      "Iteration 27555 Loss: 0.993496835231781\n",
      "Iteration 27556 Loss: 0.9199827909469604\n",
      "Iteration 27557 Loss: 0.9246806502342224\n",
      "Iteration 27558 Loss: 0.9051732420921326\n",
      "Iteration 27559 Loss: 1.297832727432251\n",
      "Iteration 27559 Loss: 1.0150651931762695\n",
      "Iteration 27560 Loss: 1.3383898735046387\n",
      "Iteration 27561 Loss: 0.6165081262588501\n",
      "Iteration 27562 Loss: 1.0249282121658325\n",
      "Iteration 27563 Loss: 1.1314380168914795\n",
      "Iteration 27564 Loss: 0.9453912377357483\n",
      "Iteration 27565 Loss: 1.1944935321807861\n",
      "Iteration 27566 Loss: 1.1844067573547363\n",
      "Iteration 27567 Loss: 1.2457832098007202\n",
      "Iteration 27568 Loss: 1.0106316804885864\n",
      "Iteration 27569 Loss: 1.3764137029647827\n",
      "Iteration 27569 Loss: 1.1068384647369385\n",
      "Iteration 27570 Loss: 0.8669297099113464\n",
      "Iteration 27571 Loss: 1.155873417854309\n",
      "Iteration 27572 Loss: 1.0267736911773682\n",
      "Iteration 27573 Loss: 0.8614352345466614\n",
      "Iteration 27574 Loss: 0.7029176950454712\n",
      "Iteration 27575 Loss: 1.2029829025268555\n",
      "Iteration 27576 Loss: 1.0606153011322021\n",
      "Iteration 27577 Loss: 1.1853795051574707\n",
      "Iteration 27578 Loss: 0.8822511434555054\n",
      "Iteration 27579 Loss: 0.8226106762886047\n",
      "Iteration 27579 Loss: 0.9767768979072571\n",
      "Iteration 27580 Loss: 0.8849025368690491\n",
      "Iteration 27581 Loss: 0.8614545464515686\n",
      "Iteration 27582 Loss: 1.0665675401687622\n",
      "Iteration 27583 Loss: 0.8933178186416626\n",
      "Iteration 27584 Loss: 1.446052074432373\n",
      "Iteration 27585 Loss: 1.1864402294158936\n",
      "Iteration 27586 Loss: 0.8004385232925415\n",
      "Iteration 27587 Loss: 0.6789724230766296\n",
      "Iteration 27588 Loss: 1.0485355854034424\n",
      "Iteration 27589 Loss: 1.0300909280776978\n",
      "Iteration 27589 Loss: 0.9896772503852844\n",
      "Iteration 27590 Loss: 1.0811563730239868\n",
      "Iteration 27591 Loss: 0.7619261145591736\n",
      "Iteration 27592 Loss: 1.1182751655578613\n",
      "Iteration 27593 Loss: 1.2864948511123657\n",
      "Iteration 27594 Loss: 1.0848127603530884\n",
      "Iteration 27595 Loss: 1.0436742305755615\n",
      "Iteration 27596 Loss: 1.0885385274887085\n",
      "Iteration 27597 Loss: 0.7954646944999695\n",
      "Iteration 27598 Loss: 1.0990105867385864\n",
      "Iteration 27599 Loss: 0.9154478907585144\n",
      "Iteration 27599 Loss: 1.0274800062179565\n",
      "Iteration 27600 Loss: 0.7571263313293457\n",
      "Iteration 27601 Loss: 0.9147554636001587\n",
      "Iteration 27602 Loss: 1.00081467628479\n",
      "Iteration 27603 Loss: 0.8829734921455383\n",
      "Iteration 27604 Loss: 0.7514610290527344\n",
      "Iteration 27605 Loss: 1.2537652254104614\n",
      "Iteration 27606 Loss: 0.9866558313369751\n",
      "Iteration 27607 Loss: 1.0518988370895386\n",
      "Iteration 27608 Loss: 0.9963493347167969\n",
      "Iteration 27609 Loss: 0.8214154243469238\n",
      "Iteration 27609 Loss: 0.9417216181755066\n",
      "Iteration 27610 Loss: 1.1174453496932983\n",
      "Iteration 27611 Loss: 0.9757709503173828\n",
      "Iteration 27612 Loss: 0.817126989364624\n",
      "Iteration 27613 Loss: 0.9142881631851196\n",
      "Iteration 27614 Loss: 0.8164814710617065\n",
      "Iteration 27615 Loss: 1.019402027130127\n",
      "Iteration 27616 Loss: 0.5904931426048279\n",
      "Iteration 27617 Loss: 0.8167581558227539\n",
      "Iteration 27618 Loss: 1.108108401298523\n",
      "Iteration 27619 Loss: 1.1604589223861694\n",
      "Iteration 27619 Loss: 0.9336333274841309\n",
      "Iteration 27620 Loss: 1.4710513353347778\n",
      "Iteration 27621 Loss: 0.8846884369850159\n",
      "Iteration 27622 Loss: 0.8811862468719482\n",
      "Iteration 27623 Loss: 1.473272442817688\n",
      "Iteration 27624 Loss: 1.0228910446166992\n",
      "Iteration 27625 Loss: 0.5738666653633118\n",
      "Iteration 27626 Loss: 0.9448254108428955\n",
      "Iteration 27627 Loss: 0.8956019878387451\n",
      "Iteration 27628 Loss: 1.1480093002319336\n",
      "Iteration 27629 Loss: 1.196314811706543\n",
      "Iteration 27629 Loss: 1.049170732498169\n",
      "Iteration 27630 Loss: 0.8956149816513062\n",
      "Iteration 27631 Loss: 1.6238957643508911\n",
      "Iteration 27632 Loss: 1.0851402282714844\n",
      "Iteration 27633 Loss: 0.8323009014129639\n",
      "Iteration 27634 Loss: 1.1474803686141968\n",
      "Iteration 27635 Loss: 0.9873484373092651\n",
      "Iteration 27636 Loss: 0.9390093684196472\n",
      "Iteration 27637 Loss: 1.1637663841247559\n",
      "Iteration 27638 Loss: 1.0305222272872925\n",
      "Iteration 27639 Loss: 1.1080951690673828\n",
      "Iteration 27639 Loss: 1.0813173055648804\n",
      "Iteration 27640 Loss: 1.0430662631988525\n",
      "Iteration 27641 Loss: 0.8354040384292603\n",
      "Iteration 27642 Loss: 1.077049970626831\n",
      "Iteration 27643 Loss: 1.2374173402786255\n",
      "Iteration 27644 Loss: 0.7043803334236145\n",
      "Iteration 27645 Loss: 1.0960570573806763\n",
      "Iteration 27646 Loss: 0.8353463411331177\n",
      "Iteration 27647 Loss: 0.8053765892982483\n",
      "Iteration 27648 Loss: 1.2654892206192017\n",
      "Iteration 27649 Loss: 0.7930492162704468\n",
      "Iteration 27649 Loss: 0.9692637324333191\n",
      "Iteration 27650 Loss: 0.8718788027763367\n",
      "Iteration 27651 Loss: 1.1760919094085693\n",
      "Iteration 27652 Loss: 0.9751532673835754\n",
      "Iteration 27653 Loss: 0.715640664100647\n",
      "Iteration 27654 Loss: 1.0761115550994873\n",
      "Iteration 27655 Loss: 0.7548338770866394\n",
      "Iteration 27656 Loss: 0.9852039813995361\n",
      "Iteration 27657 Loss: 1.1179357767105103\n",
      "Iteration 27658 Loss: 0.8874801397323608\n",
      "Iteration 27659 Loss: 1.2159814834594727\n",
      "Iteration 27659 Loss: 0.9776312112808228\n",
      "Iteration 27660 Loss: 1.4419306516647339\n",
      "Iteration 27661 Loss: 1.2284828424453735\n",
      "Iteration 27662 Loss: 1.2879985570907593\n",
      "Iteration 27663 Loss: 1.5011979341506958\n",
      "Iteration 27664 Loss: 1.031111478805542\n",
      "Iteration 27665 Loss: 1.0993753671646118\n",
      "Iteration 27666 Loss: 1.1207183599472046\n",
      "Iteration 27667 Loss: 1.0953946113586426\n",
      "Iteration 27668 Loss: 1.2177544832229614\n",
      "Iteration 27669 Loss: 1.275390386581421\n",
      "Iteration 27669 Loss: 1.2299354076385498\n",
      "Iteration 27670 Loss: 1.3631854057312012\n",
      "Iteration 27671 Loss: 1.3198351860046387\n",
      "Iteration 27672 Loss: 0.8679971098899841\n",
      "Iteration 27673 Loss: 1.2563244104385376\n",
      "Iteration 27674 Loss: 1.0084574222564697\n",
      "Iteration 27675 Loss: 0.9742390513420105\n",
      "Iteration 27676 Loss: 0.8627791404724121\n",
      "Iteration 27677 Loss: 0.7520917057991028\n",
      "Iteration 27678 Loss: 0.8630028367042542\n",
      "Iteration 27679 Loss: 1.051732063293457\n",
      "Iteration 27679 Loss: 1.0319644212722778\n",
      "Iteration 27680 Loss: 1.3893929719924927\n",
      "Iteration 27681 Loss: 1.2620190382003784\n",
      "Iteration 27682 Loss: 1.0253242254257202\n",
      "Iteration 27683 Loss: 0.782145619392395\n",
      "Iteration 27684 Loss: 0.9949781894683838\n",
      "Iteration 27685 Loss: 0.7828496098518372\n",
      "Iteration 27686 Loss: 1.366707682609558\n",
      "Iteration 27687 Loss: 0.7137865424156189\n",
      "Iteration 27688 Loss: 1.2258702516555786\n",
      "Iteration 27689 Loss: 0.8179660439491272\n",
      "Iteration 27689 Loss: 1.0361039638519287\n",
      "Iteration 27690 Loss: 1.1999322175979614\n",
      "Iteration 27691 Loss: 1.1707607507705688\n",
      "Iteration 27692 Loss: 0.882792592048645\n",
      "Iteration 27693 Loss: 0.9150790572166443\n",
      "Iteration 27694 Loss: 0.8993268609046936\n",
      "Iteration 27695 Loss: 1.1800527572631836\n",
      "Iteration 27696 Loss: 0.9318926334381104\n",
      "Iteration 27697 Loss: 0.7581959962844849\n",
      "Iteration 27698 Loss: 0.7009405493736267\n",
      "Iteration 27699 Loss: 1.1075507402420044\n",
      "Iteration 27699 Loss: 0.9746522903442383\n",
      "Iteration 27700 Loss: 1.0088140964508057\n",
      "Iteration 27701 Loss: 1.1167659759521484\n",
      "Iteration 27702 Loss: 0.660205066204071\n",
      "Iteration 27703 Loss: 0.98138028383255\n",
      "Iteration 27704 Loss: 1.085172176361084\n",
      "Iteration 27705 Loss: 1.2347086668014526\n",
      "Iteration 27706 Loss: 0.8170914053916931\n",
      "Iteration 27707 Loss: 1.1937625408172607\n",
      "Iteration 27708 Loss: 1.267830729484558\n",
      "Iteration 27709 Loss: 0.8174718618392944\n",
      "Iteration 27709 Loss: 1.0183203220367432\n",
      "Iteration 27710 Loss: 0.938561201095581\n",
      "Iteration 27711 Loss: 1.0463905334472656\n",
      "Iteration 27712 Loss: 1.1871310472488403\n",
      "Iteration 27713 Loss: 1.3910505771636963\n",
      "Iteration 27714 Loss: 0.5845099091529846\n",
      "Iteration 27715 Loss: 0.7889723181724548\n",
      "Iteration 27716 Loss: 1.2800437211990356\n",
      "Iteration 27717 Loss: 0.9010398387908936\n",
      "Iteration 27718 Loss: 1.1690776348114014\n",
      "Iteration 27719 Loss: 0.9004524350166321\n",
      "Iteration 27719 Loss: 1.0187227725982666\n",
      "Iteration 27720 Loss: 1.4043554067611694\n",
      "Iteration 27721 Loss: 0.8752304911613464\n",
      "Iteration 27722 Loss: 1.1575992107391357\n",
      "Iteration 27723 Loss: 1.199354648590088\n",
      "Iteration 27724 Loss: 1.0241293907165527\n",
      "Iteration 27725 Loss: 1.2905179262161255\n",
      "Iteration 27726 Loss: 0.6636456251144409\n",
      "Iteration 27727 Loss: 1.207708477973938\n",
      "Iteration 27728 Loss: 1.0341459512710571\n",
      "Iteration 27729 Loss: 1.1176220178604126\n",
      "Iteration 27729 Loss: 1.097430944442749\n",
      "Iteration 27730 Loss: 1.1189454793930054\n",
      "Iteration 27731 Loss: 0.9374556541442871\n",
      "Iteration 27732 Loss: 1.0950634479522705\n",
      "Iteration 27733 Loss: 0.9914495348930359\n",
      "Iteration 27734 Loss: 1.138480305671692\n",
      "Iteration 27735 Loss: 1.2594960927963257\n",
      "Iteration 27736 Loss: 1.0547574758529663\n",
      "Iteration 27737 Loss: 1.007004737854004\n",
      "Iteration 27738 Loss: 0.9517189264297485\n",
      "Iteration 27739 Loss: 1.2921596765518188\n",
      "Iteration 27739 Loss: 1.0846530199050903\n",
      "Iteration 27740 Loss: 1.0241644382476807\n",
      "Iteration 27741 Loss: 0.936060905456543\n",
      "Iteration 27742 Loss: 0.7944074869155884\n",
      "Iteration 27743 Loss: 1.0017584562301636\n",
      "Iteration 27744 Loss: 0.844725489616394\n",
      "Iteration 27745 Loss: 1.0812435150146484\n",
      "Iteration 27746 Loss: 0.9537872076034546\n",
      "Iteration 27747 Loss: 0.8299345374107361\n",
      "Iteration 27748 Loss: 1.2998764514923096\n",
      "Iteration 27749 Loss: 1.0738046169281006\n",
      "Iteration 27749 Loss: 0.9839762449264526\n",
      "Iteration 27750 Loss: 0.9956280589103699\n",
      "Iteration 27751 Loss: 0.9307809472084045\n",
      "Iteration 27752 Loss: 0.9310728907585144\n",
      "Iteration 27753 Loss: 1.2607306241989136\n",
      "Iteration 27754 Loss: 1.0536047220230103\n",
      "Iteration 27755 Loss: 0.9368764758110046\n",
      "Iteration 27756 Loss: 1.2179564237594604\n",
      "Iteration 27757 Loss: 1.1704105138778687\n",
      "Iteration 27758 Loss: 1.0767080783843994\n",
      "Iteration 27759 Loss: 1.0169850587844849\n",
      "Iteration 27759 Loss: 1.0590753555297852\n",
      "Iteration 27760 Loss: 0.982474148273468\n",
      "Iteration 27761 Loss: 1.2064030170440674\n",
      "Iteration 27762 Loss: 1.2946635484695435\n",
      "Iteration 27763 Loss: 1.117146372795105\n",
      "Iteration 27764 Loss: 0.630764365196228\n",
      "Iteration 27765 Loss: 0.8079190850257874\n",
      "Iteration 27766 Loss: 0.6001048684120178\n",
      "Iteration 27767 Loss: 1.3874297142028809\n",
      "Iteration 27768 Loss: 0.8640446662902832\n",
      "Iteration 27769 Loss: 0.78000807762146\n",
      "Iteration 27769 Loss: 0.9670957326889038\n",
      "Iteration 27770 Loss: 1.0397119522094727\n",
      "Iteration 27771 Loss: 0.9974151849746704\n",
      "Iteration 27772 Loss: 0.6077098250389099\n",
      "Iteration 27773 Loss: 0.8273328542709351\n",
      "Iteration 27774 Loss: 0.7863847613334656\n",
      "Iteration 27775 Loss: 1.0864322185516357\n",
      "Iteration 27776 Loss: 0.9057571887969971\n",
      "Iteration 27777 Loss: 0.7462030649185181\n",
      "Iteration 27778 Loss: 1.0522626638412476\n",
      "Iteration 27779 Loss: 1.033797025680542\n",
      "Iteration 27779 Loss: 0.9083006978034973\n",
      "Iteration 27780 Loss: 0.8061248064041138\n",
      "Iteration 27781 Loss: 1.423218846321106\n",
      "Iteration 27782 Loss: 1.2996128797531128\n",
      "Iteration 27783 Loss: 1.1012794971466064\n",
      "Iteration 27784 Loss: 0.9241952896118164\n",
      "Iteration 27785 Loss: 1.07113516330719\n",
      "Iteration 27786 Loss: 0.7094070911407471\n",
      "Iteration 27787 Loss: 0.782543957233429\n",
      "Iteration 27788 Loss: 0.7567451000213623\n",
      "Iteration 27789 Loss: 0.9096916913986206\n",
      "Iteration 27789 Loss: 0.9783954620361328\n",
      "Iteration 27790 Loss: 0.9325278997421265\n",
      "Iteration 27791 Loss: 0.8785620331764221\n",
      "Iteration 27792 Loss: 1.0911544561386108\n",
      "Iteration 27793 Loss: 0.912879228591919\n",
      "Iteration 27794 Loss: 1.5779495239257812\n",
      "Iteration 27795 Loss: 1.062933087348938\n",
      "Iteration 27796 Loss: 1.0624949932098389\n",
      "Iteration 27797 Loss: 0.7950143218040466\n",
      "Iteration 27798 Loss: 0.9673726558685303\n",
      "Iteration 27799 Loss: 1.0554397106170654\n",
      "Iteration 27799 Loss: 1.0336328744888306\n",
      "Iteration 27800 Loss: 0.8153008222579956\n",
      "Iteration 27801 Loss: 0.9589597582817078\n",
      "Iteration 27802 Loss: 0.5824325680732727\n",
      "Iteration 27803 Loss: 0.932682991027832\n",
      "Iteration 27804 Loss: 1.0205049514770508\n",
      "Iteration 27805 Loss: 0.7937093377113342\n",
      "Iteration 27806 Loss: 0.9419546127319336\n",
      "Iteration 27807 Loss: 1.2398353815078735\n",
      "Iteration 27808 Loss: 1.0890402793884277\n",
      "Iteration 27809 Loss: 0.8053318858146667\n",
      "Iteration 27809 Loss: 0.9179752469062805\n",
      "Iteration 27810 Loss: 1.0742368698120117\n",
      "Iteration 27811 Loss: 1.037229061126709\n",
      "Iteration 27812 Loss: 0.54799884557724\n",
      "Iteration 27813 Loss: 0.6574100255966187\n",
      "Iteration 27814 Loss: 1.3785231113433838\n",
      "Iteration 27815 Loss: 0.9050418138504028\n",
      "Iteration 27816 Loss: 1.0487580299377441\n",
      "Iteration 27817 Loss: 0.9691386818885803\n",
      "Iteration 27818 Loss: 1.186779499053955\n",
      "Iteration 27819 Loss: 1.1307809352874756\n",
      "Iteration 27819 Loss: 0.9935897588729858\n",
      "Iteration 27820 Loss: 1.0914009809494019\n",
      "Iteration 27821 Loss: 1.1878416538238525\n",
      "Iteration 27822 Loss: 0.7410580515861511\n",
      "Iteration 27823 Loss: 1.310981273651123\n",
      "Iteration 27824 Loss: 0.9867967367172241\n",
      "Iteration 27825 Loss: 0.7876586318016052\n",
      "Iteration 27826 Loss: 0.8955180048942566\n",
      "Iteration 27827 Loss: 0.9407609701156616\n",
      "Iteration 27828 Loss: 0.598732054233551\n",
      "Iteration 27829 Loss: 0.9046511054039001\n",
      "Iteration 27829 Loss: 0.9445399045944214\n",
      "Iteration 27830 Loss: 1.162517786026001\n",
      "Iteration 27831 Loss: 0.9950034022331238\n",
      "Iteration 27832 Loss: 0.9392062425613403\n",
      "Iteration 27833 Loss: 1.1604057550430298\n",
      "Iteration 27834 Loss: 1.0855578184127808\n",
      "Iteration 27835 Loss: 0.9378621578216553\n",
      "Iteration 27836 Loss: 1.245489239692688\n",
      "Iteration 27837 Loss: 0.945289671421051\n",
      "Iteration 27838 Loss: 0.9974275827407837\n",
      "Iteration 27839 Loss: 0.9291292428970337\n",
      "Iteration 27839 Loss: 1.039788842201233\n",
      "Iteration 27840 Loss: 0.9790682196617126\n",
      "Iteration 27841 Loss: 1.2619493007659912\n",
      "Iteration 27842 Loss: 0.9563791751861572\n",
      "Iteration 27843 Loss: 1.2814480066299438\n",
      "Iteration 27844 Loss: 0.8563187122344971\n",
      "Iteration 27845 Loss: 1.0592973232269287\n",
      "Iteration 27846 Loss: 0.8618022799491882\n",
      "Iteration 27847 Loss: 1.0328971147537231\n",
      "Iteration 27848 Loss: 1.317096471786499\n",
      "Iteration 27849 Loss: 0.9032517075538635\n",
      "Iteration 27849 Loss: 1.0509507656097412\n",
      "Iteration 27850 Loss: 1.1851047277450562\n",
      "Iteration 27851 Loss: 1.131122350692749\n",
      "Iteration 27852 Loss: 0.9879478812217712\n",
      "Iteration 27853 Loss: 0.9275276064872742\n",
      "Iteration 27854 Loss: 1.2552615404129028\n",
      "Iteration 27855 Loss: 1.1698967218399048\n",
      "Iteration 27856 Loss: 1.2831144332885742\n",
      "Iteration 27857 Loss: 1.520704984664917\n",
      "Iteration 27858 Loss: 0.985636293888092\n",
      "Iteration 27859 Loss: 0.8394003510475159\n",
      "Iteration 27859 Loss: 1.1285717487335205\n",
      "Iteration 27860 Loss: 0.944563627243042\n",
      "Iteration 27861 Loss: 1.1253432035446167\n",
      "Iteration 27862 Loss: 0.916343629360199\n",
      "Iteration 27863 Loss: 0.8307768702507019\n",
      "Iteration 27864 Loss: 1.2960758209228516\n",
      "Iteration 27865 Loss: 0.957687497138977\n",
      "Iteration 27866 Loss: 0.8800008893013\n",
      "Iteration 27867 Loss: 0.9868093132972717\n",
      "Iteration 27868 Loss: 1.015382170677185\n",
      "Iteration 27869 Loss: 1.0411688089370728\n",
      "Iteration 27869 Loss: 0.9994152188301086\n",
      "Iteration 27870 Loss: 1.2636053562164307\n",
      "Iteration 27871 Loss: 1.131471037864685\n",
      "Iteration 27872 Loss: 0.89607173204422\n",
      "Iteration 27873 Loss: 0.8174536824226379\n",
      "Iteration 27874 Loss: 0.979988694190979\n",
      "Iteration 27875 Loss: 0.5013734698295593\n",
      "Iteration 27876 Loss: 1.1380468606948853\n",
      "Iteration 27877 Loss: 1.0736249685287476\n",
      "Iteration 27878 Loss: 0.8756086826324463\n",
      "Iteration 27879 Loss: 1.1083788871765137\n",
      "Iteration 27879 Loss: 0.9785623550415039\n",
      "Iteration 27880 Loss: 0.928486704826355\n",
      "Iteration 27881 Loss: 1.0578687191009521\n",
      "Iteration 27882 Loss: 1.1779487133026123\n",
      "Iteration 27883 Loss: 0.885413646697998\n",
      "Iteration 27884 Loss: 1.3492059707641602\n",
      "Iteration 27885 Loss: 0.7069293856620789\n",
      "Iteration 27886 Loss: 0.8731656074523926\n",
      "Iteration 27887 Loss: 0.9699323177337646\n",
      "Iteration 27888 Loss: 1.3020665645599365\n",
      "Iteration 27889 Loss: 1.2781225442886353\n",
      "Iteration 27889 Loss: 1.052914023399353\n",
      "Iteration 27890 Loss: 0.9976633787155151\n",
      "Iteration 27891 Loss: 0.9689686298370361\n",
      "Iteration 27892 Loss: 1.148547649383545\n",
      "Iteration 27893 Loss: 1.0253385305404663\n",
      "Iteration 27894 Loss: 0.9045283198356628\n",
      "Iteration 27895 Loss: 1.345410943031311\n",
      "Iteration 27896 Loss: 1.1106147766113281\n",
      "Iteration 27897 Loss: 0.9756367206573486\n",
      "Iteration 27898 Loss: 0.9360501170158386\n",
      "Iteration 27899 Loss: 1.2498010396957397\n",
      "Iteration 27899 Loss: 1.0662559270858765\n",
      "Iteration 27900 Loss: 1.0936856269836426\n",
      "Iteration 27901 Loss: 0.7604840993881226\n",
      "Iteration 27902 Loss: 0.9350539445877075\n",
      "Iteration 27903 Loss: 0.8588463068008423\n",
      "Iteration 27904 Loss: 0.8476043343544006\n",
      "Iteration 27905 Loss: 0.6807024478912354\n",
      "Iteration 27906 Loss: 1.1809308528900146\n",
      "Iteration 27907 Loss: 0.9560526609420776\n",
      "Iteration 27908 Loss: 1.165897011756897\n",
      "Iteration 27909 Loss: 1.2601244449615479\n",
      "Iteration 27909 Loss: 0.9739381670951843\n",
      "Iteration 27910 Loss: 1.1971137523651123\n",
      "Iteration 27911 Loss: 1.174504041671753\n",
      "Iteration 27912 Loss: 0.803156852722168\n",
      "Iteration 27913 Loss: 1.0194189548492432\n",
      "Iteration 27914 Loss: 1.0041663646697998\n",
      "Iteration 27915 Loss: 1.0246496200561523\n",
      "Iteration 27916 Loss: 0.8566315770149231\n",
      "Iteration 27917 Loss: 1.2259702682495117\n",
      "Iteration 27918 Loss: 0.7832773923873901\n",
      "Iteration 27919 Loss: 1.1480787992477417\n",
      "Iteration 27919 Loss: 1.0236966609954834\n",
      "Iteration 27920 Loss: 1.3790632486343384\n",
      "Iteration 27921 Loss: 1.0096516609191895\n",
      "Iteration 27922 Loss: 0.9814267158508301\n",
      "Iteration 27923 Loss: 1.1574246883392334\n",
      "Iteration 27924 Loss: 1.2298402786254883\n",
      "Iteration 27925 Loss: 1.2710148096084595\n",
      "Iteration 27926 Loss: 1.3046481609344482\n",
      "Iteration 27927 Loss: 1.0896748304367065\n",
      "Iteration 27928 Loss: 1.0818973779678345\n",
      "Iteration 27929 Loss: 1.1438497304916382\n",
      "Iteration 27929 Loss: 1.1648491621017456\n",
      "Iteration 27930 Loss: 1.2534757852554321\n",
      "Iteration 27931 Loss: 0.718774139881134\n",
      "Iteration 27932 Loss: 0.85367351770401\n",
      "Iteration 27933 Loss: 0.753868579864502\n",
      "Iteration 27934 Loss: 0.5897492170333862\n",
      "Iteration 27935 Loss: 0.9812955260276794\n",
      "Iteration 27936 Loss: 1.170310378074646\n",
      "Iteration 27937 Loss: 0.8330424427986145\n",
      "Iteration 27938 Loss: 0.8270136117935181\n",
      "Iteration 27939 Loss: 1.101874828338623\n",
      "Iteration 27939 Loss: 0.9083077311515808\n",
      "Iteration 27940 Loss: 1.010343074798584\n",
      "Iteration 27941 Loss: 0.9744275808334351\n",
      "Iteration 27942 Loss: 0.9538561105728149\n",
      "Iteration 27943 Loss: 0.7258909940719604\n",
      "Iteration 27944 Loss: 1.1206847429275513\n",
      "Iteration 27945 Loss: 1.235630989074707\n",
      "Iteration 27946 Loss: 1.1362065076828003\n",
      "Iteration 27947 Loss: 1.2211544513702393\n",
      "Iteration 27948 Loss: 0.9909260869026184\n",
      "Iteration 27949 Loss: 1.4330500364303589\n",
      "Iteration 27949 Loss: 1.0802170038223267\n",
      "Iteration 27950 Loss: 0.9007061123847961\n",
      "Iteration 27951 Loss: 1.260007381439209\n",
      "Iteration 27952 Loss: 1.1483862400054932\n",
      "Iteration 27953 Loss: 0.7970916628837585\n",
      "Iteration 27954 Loss: 1.3365345001220703\n",
      "Iteration 27955 Loss: 0.8542906045913696\n",
      "Iteration 27956 Loss: 1.2223261594772339\n",
      "Iteration 27957 Loss: 1.1288559436798096\n",
      "Iteration 27958 Loss: 1.2784883975982666\n",
      "Iteration 27959 Loss: 0.8842883706092834\n",
      "Iteration 27959 Loss: 1.0810974836349487\n",
      "Iteration 27960 Loss: 1.5310381650924683\n",
      "Iteration 27961 Loss: 1.0676425695419312\n",
      "Iteration 27962 Loss: 1.2237919569015503\n",
      "Iteration 27963 Loss: 0.9442551136016846\n",
      "Iteration 27964 Loss: 0.9792459011077881\n",
      "Iteration 27965 Loss: 0.6477369666099548\n",
      "Iteration 27966 Loss: 1.0176308155059814\n",
      "Iteration 27967 Loss: 1.1332948207855225\n",
      "Iteration 27968 Loss: 1.4426658153533936\n",
      "Iteration 27969 Loss: 1.2276721000671387\n",
      "Iteration 27969 Loss: 1.121497392654419\n",
      "Iteration 27970 Loss: 0.9638316035270691\n",
      "Iteration 27971 Loss: 0.7524701356887817\n",
      "Iteration 27972 Loss: 1.178403615951538\n",
      "Iteration 27973 Loss: 0.9349942803382874\n",
      "Iteration 27974 Loss: 0.7272663712501526\n",
      "Iteration 27975 Loss: 1.0301438570022583\n",
      "Iteration 27976 Loss: 1.1375548839569092\n",
      "Iteration 27977 Loss: 0.8226627707481384\n",
      "Iteration 27978 Loss: 0.8665199279785156\n",
      "Iteration 27979 Loss: 1.3128085136413574\n",
      "Iteration 27979 Loss: 0.9726654887199402\n",
      "Iteration 27980 Loss: 1.2476636171340942\n",
      "Iteration 27981 Loss: 1.0705419778823853\n",
      "Iteration 27982 Loss: 0.9019725322723389\n",
      "Iteration 27983 Loss: 1.4468063116073608\n",
      "Iteration 27984 Loss: 1.3156863451004028\n",
      "Iteration 27985 Loss: 0.9692264199256897\n",
      "Iteration 27986 Loss: 1.0389484167099\n",
      "Iteration 27987 Loss: 0.8278725743293762\n",
      "Iteration 27988 Loss: 0.99686199426651\n",
      "Iteration 27989 Loss: 1.0411007404327393\n",
      "Iteration 27989 Loss: 1.0856680870056152\n",
      "Iteration 27990 Loss: 1.0388524532318115\n",
      "Iteration 27991 Loss: 1.0073565244674683\n",
      "Iteration 27992 Loss: 0.9014096856117249\n",
      "Iteration 27993 Loss: 1.329880952835083\n",
      "Iteration 27994 Loss: 1.056746482849121\n",
      "Iteration 27995 Loss: 0.8193485736846924\n",
      "Iteration 27996 Loss: 0.8302152156829834\n",
      "Iteration 27997 Loss: 1.044757604598999\n",
      "Iteration 27998 Loss: 0.8771795034408569\n",
      "Iteration 27999 Loss: 0.8939723372459412\n",
      "Iteration 27999 Loss: 0.9799720048904419\n",
      "Iteration 28000 Loss: 0.8808525800704956\n",
      "Iteration 28001 Loss: 1.3669322729110718\n",
      "Iteration 28002 Loss: 0.7924870252609253\n",
      "Iteration 28003 Loss: 0.8120758533477783\n",
      "Iteration 28004 Loss: 1.2654141187667847\n",
      "Iteration 28005 Loss: 1.0478591918945312\n",
      "Iteration 28006 Loss: 0.964557945728302\n",
      "Iteration 28007 Loss: 1.1660078763961792\n",
      "Iteration 28008 Loss: 0.9253053665161133\n",
      "Iteration 28009 Loss: 1.0630090236663818\n",
      "Iteration 28009 Loss: 1.0284501314163208\n",
      "Iteration 28010 Loss: 1.4982903003692627\n",
      "Iteration 28011 Loss: 1.1496576070785522\n",
      "Iteration 28012 Loss: 1.0233324766159058\n",
      "Iteration 28013 Loss: 0.9583614468574524\n",
      "Iteration 28014 Loss: 1.0782876014709473\n",
      "Iteration 28015 Loss: 0.9609730243682861\n",
      "Iteration 28016 Loss: 1.0844348669052124\n",
      "Iteration 28017 Loss: 1.2039051055908203\n",
      "Iteration 28018 Loss: 1.3348346948623657\n",
      "Iteration 28019 Loss: 1.0900019407272339\n",
      "Iteration 28019 Loss: 1.1382079124450684\n",
      "Iteration 28020 Loss: 1.1295139789581299\n",
      "Iteration 28021 Loss: 0.9261457324028015\n",
      "Iteration 28022 Loss: 0.8551119565963745\n",
      "Iteration 28023 Loss: 1.2941179275512695\n",
      "Iteration 28024 Loss: 1.1665133237838745\n",
      "Iteration 28025 Loss: 1.1388120651245117\n",
      "Iteration 28026 Loss: 1.0865460634231567\n",
      "Iteration 28027 Loss: 1.134952425956726\n",
      "Iteration 28028 Loss: 1.122861385345459\n",
      "Iteration 28029 Loss: 1.5706827640533447\n",
      "Iteration 28029 Loss: 1.1425257921218872\n",
      "Iteration 28030 Loss: 0.8602933883666992\n",
      "Iteration 28031 Loss: 1.1826220750808716\n",
      "Iteration 28032 Loss: 0.7414166331291199\n",
      "Iteration 28033 Loss: 0.9642848372459412\n",
      "Iteration 28034 Loss: 1.1670291423797607\n",
      "Iteration 28035 Loss: 0.8878160119056702\n",
      "Iteration 28036 Loss: 0.8991402387619019\n",
      "Iteration 28037 Loss: 0.9499479532241821\n",
      "Iteration 28038 Loss: 0.9522009491920471\n",
      "Iteration 28039 Loss: 0.9781594276428223\n",
      "Iteration 28039 Loss: 0.9582910537719727\n",
      "Iteration 28040 Loss: 1.0156158208847046\n",
      "Iteration 28041 Loss: 0.9947602152824402\n",
      "Iteration 28042 Loss: 0.9248639345169067\n",
      "Iteration 28043 Loss: 0.7818436622619629\n",
      "Iteration 28044 Loss: 1.2181826829910278\n",
      "Iteration 28045 Loss: 1.561309814453125\n",
      "Iteration 28046 Loss: 0.879984438419342\n",
      "Iteration 28047 Loss: 0.9622592926025391\n",
      "Iteration 28048 Loss: 1.3371421098709106\n",
      "Iteration 28049 Loss: 1.0678000450134277\n",
      "Iteration 28049 Loss: 1.0743762254714966\n",
      "Iteration 28050 Loss: 0.6876920461654663\n",
      "Iteration 28051 Loss: 0.9807789921760559\n",
      "Iteration 28052 Loss: 0.8656013607978821\n",
      "Iteration 28053 Loss: 1.131537914276123\n",
      "Iteration 28054 Loss: 0.818711519241333\n",
      "Iteration 28055 Loss: 0.8649702072143555\n",
      "Iteration 28056 Loss: 0.8667401075363159\n",
      "Iteration 28057 Loss: 1.0097063779830933\n",
      "Iteration 28058 Loss: 0.9778860211372375\n",
      "Iteration 28059 Loss: 1.2297708988189697\n",
      "Iteration 28059 Loss: 0.9433395266532898\n",
      "Iteration 28060 Loss: 1.234969973564148\n",
      "Iteration 28061 Loss: 0.9713661670684814\n",
      "Iteration 28062 Loss: 1.0936022996902466\n",
      "Iteration 28063 Loss: 1.1058368682861328\n",
      "Iteration 28064 Loss: 0.9164323806762695\n",
      "Iteration 28065 Loss: 1.1748573780059814\n",
      "Iteration 28066 Loss: 1.1727735996246338\n",
      "Iteration 28067 Loss: 0.8797208666801453\n",
      "Iteration 28068 Loss: 0.8415572047233582\n",
      "Iteration 28069 Loss: 0.5986392498016357\n",
      "Iteration 28069 Loss: 0.9989755749702454\n",
      "Iteration 28070 Loss: 1.166338324546814\n",
      "Iteration 28071 Loss: 1.1183255910873413\n",
      "Iteration 28072 Loss: 1.1871527433395386\n",
      "Iteration 28073 Loss: 1.0927194356918335\n",
      "Iteration 28074 Loss: 0.9072470664978027\n",
      "Iteration 28075 Loss: 1.3312007188796997\n",
      "Iteration 28076 Loss: 0.7243411540985107\n",
      "Iteration 28077 Loss: 0.9266961216926575\n",
      "Iteration 28078 Loss: 1.3051780462265015\n",
      "Iteration 28079 Loss: 0.7223599553108215\n",
      "Iteration 28079 Loss: 1.0481560230255127\n",
      "Iteration 28080 Loss: 0.8690035939216614\n",
      "Iteration 28081 Loss: 1.003615140914917\n",
      "Iteration 28082 Loss: 1.057906985282898\n",
      "Iteration 28083 Loss: 0.9066057801246643\n",
      "Iteration 28084 Loss: 1.0248734951019287\n",
      "Iteration 28085 Loss: 1.219224452972412\n",
      "Iteration 28086 Loss: 1.100252389907837\n",
      "Iteration 28087 Loss: 1.0957391262054443\n",
      "Iteration 28088 Loss: 0.9895535111427307\n",
      "Iteration 28089 Loss: 1.0423002243041992\n",
      "Iteration 28089 Loss: 1.030907392501831\n",
      "Iteration 28090 Loss: 1.098588466644287\n",
      "Iteration 28091 Loss: 1.075231909751892\n",
      "Iteration 28092 Loss: 1.0728366374969482\n",
      "Iteration 28093 Loss: 1.28282630443573\n",
      "Iteration 28094 Loss: 1.311915636062622\n",
      "Iteration 28095 Loss: 1.3287924528121948\n",
      "Iteration 28096 Loss: 0.7345304489135742\n",
      "Iteration 28097 Loss: 1.1588242053985596\n",
      "Iteration 28098 Loss: 1.0500969886779785\n",
      "Iteration 28099 Loss: 0.9561194181442261\n",
      "Iteration 28099 Loss: 1.1069762706756592\n",
      "Iteration 28100 Loss: 0.9912334680557251\n",
      "Iteration 28101 Loss: 0.8832312822341919\n",
      "Iteration 28102 Loss: 0.9223835468292236\n",
      "Iteration 28103 Loss: 0.9668880701065063\n",
      "Iteration 28104 Loss: 1.2159273624420166\n",
      "Iteration 28105 Loss: 0.9765968918800354\n",
      "Iteration 28106 Loss: 0.8601527214050293\n",
      "Iteration 28107 Loss: 0.9484356045722961\n",
      "Iteration 28108 Loss: 0.8234310150146484\n",
      "Iteration 28109 Loss: 1.349204182624817\n",
      "Iteration 28109 Loss: 0.9937483668327332\n",
      "Iteration 28110 Loss: 1.2713770866394043\n",
      "Iteration 28111 Loss: 0.8664290308952332\n",
      "Iteration 28112 Loss: 1.3332209587097168\n",
      "Iteration 28113 Loss: 0.9119424223899841\n",
      "Iteration 28114 Loss: 0.8226087689399719\n",
      "Iteration 28115 Loss: 1.0018672943115234\n",
      "Iteration 28116 Loss: 1.303890585899353\n",
      "Iteration 28117 Loss: 0.8313236236572266\n",
      "Iteration 28118 Loss: 1.2964588403701782\n",
      "Iteration 28119 Loss: 0.81456458568573\n",
      "Iteration 28119 Loss: 1.0453683137893677\n",
      "Iteration 28120 Loss: 0.929304838180542\n",
      "Iteration 28121 Loss: 0.8348953127861023\n",
      "Iteration 28122 Loss: 0.7588648796081543\n",
      "Iteration 28123 Loss: 1.2710210084915161\n",
      "Iteration 28124 Loss: 1.082564115524292\n",
      "Iteration 28125 Loss: 1.4811327457427979\n",
      "Iteration 28126 Loss: 0.9382233023643494\n",
      "Iteration 28127 Loss: 1.204163908958435\n",
      "Iteration 28128 Loss: 1.072619080543518\n",
      "Iteration 28129 Loss: 1.307608962059021\n",
      "Iteration 28129 Loss: 1.0880396366119385\n",
      "Iteration 28130 Loss: 1.4459595680236816\n",
      "Iteration 28131 Loss: 1.1567137241363525\n",
      "Iteration 28132 Loss: 0.8729762434959412\n",
      "Iteration 28133 Loss: 1.058233618736267\n",
      "Iteration 28134 Loss: 1.0108002424240112\n",
      "Iteration 28135 Loss: 1.2870734930038452\n",
      "Iteration 28136 Loss: 1.30849027633667\n",
      "Iteration 28137 Loss: 1.1516108512878418\n",
      "Iteration 28138 Loss: 1.0787769556045532\n",
      "Iteration 28139 Loss: 0.8276644945144653\n",
      "Iteration 28139 Loss: 1.1198298931121826\n",
      "Iteration 28140 Loss: 0.9146246314048767\n",
      "Iteration 28141 Loss: 0.9673547148704529\n",
      "Iteration 28142 Loss: 1.188575267791748\n",
      "Iteration 28143 Loss: 1.0545892715454102\n",
      "Iteration 28144 Loss: 1.270955204963684\n",
      "Iteration 28145 Loss: 1.2183529138565063\n",
      "Iteration 28146 Loss: 1.3303660154342651\n",
      "Iteration 28147 Loss: 1.0976542234420776\n",
      "Iteration 28148 Loss: 1.1900426149368286\n",
      "Iteration 28149 Loss: 1.1786242723464966\n",
      "Iteration 28149 Loss: 1.1411139965057373\n",
      "Iteration 28150 Loss: 0.9558713436126709\n",
      "Iteration 28151 Loss: 1.0985605716705322\n",
      "Iteration 28152 Loss: 0.9338366985321045\n",
      "Iteration 28153 Loss: 0.742378294467926\n",
      "Iteration 28154 Loss: 1.1686360836029053\n",
      "Iteration 28155 Loss: 1.215093970298767\n",
      "Iteration 28156 Loss: 0.8142454028129578\n",
      "Iteration 28157 Loss: 0.7481354475021362\n",
      "Iteration 28158 Loss: 0.7059286832809448\n",
      "Iteration 28159 Loss: 1.3629133701324463\n",
      "Iteration 28159 Loss: 0.9745599627494812\n",
      "Iteration 28160 Loss: 1.2272590398788452\n",
      "Iteration 28161 Loss: 1.0752216577529907\n",
      "Iteration 28162 Loss: 1.0843974351882935\n",
      "Iteration 28163 Loss: 0.9804442524909973\n",
      "Iteration 28164 Loss: 0.3582879304885864\n",
      "Iteration 28165 Loss: 0.7098435163497925\n",
      "Iteration 28166 Loss: 1.078054428100586\n",
      "Iteration 28167 Loss: 1.05469810962677\n",
      "Iteration 28168 Loss: 0.9928934574127197\n",
      "Iteration 28169 Loss: 1.0336968898773193\n",
      "Iteration 28169 Loss: 0.9594796299934387\n",
      "Iteration 28170 Loss: 1.1794095039367676\n",
      "Iteration 28171 Loss: 1.2031031847000122\n",
      "Iteration 28172 Loss: 1.0615133047103882\n",
      "Iteration 28173 Loss: 1.0513228178024292\n",
      "Iteration 28174 Loss: 1.207517147064209\n",
      "Iteration 28175 Loss: 1.2288987636566162\n",
      "Iteration 28176 Loss: 1.0557982921600342\n",
      "Iteration 28177 Loss: 1.0386666059494019\n",
      "Iteration 28178 Loss: 1.0598559379577637\n",
      "Iteration 28179 Loss: 0.9938877820968628\n",
      "Iteration 28179 Loss: 1.1079974174499512\n",
      "Iteration 28180 Loss: 0.7594226598739624\n",
      "Iteration 28181 Loss: 1.0064213275909424\n",
      "Iteration 28182 Loss: 1.0059261322021484\n",
      "Iteration 28183 Loss: 1.0577757358551025\n",
      "Iteration 28184 Loss: 1.0528424978256226\n",
      "Iteration 28185 Loss: 1.2364866733551025\n",
      "Iteration 28186 Loss: 1.0892668962478638\n",
      "Iteration 28187 Loss: 0.9586077928543091\n",
      "Iteration 28188 Loss: 0.947806715965271\n",
      "Iteration 28189 Loss: 0.8492213487625122\n",
      "Iteration 28189 Loss: 0.9963777661323547\n",
      "Iteration 28190 Loss: 1.0675851106643677\n",
      "Iteration 28191 Loss: 1.2073266506195068\n",
      "Iteration 28192 Loss: 1.144118309020996\n",
      "Iteration 28193 Loss: 0.9345349669456482\n",
      "Iteration 28194 Loss: 0.7991127967834473\n",
      "Iteration 28195 Loss: 1.2427988052368164\n",
      "Iteration 28196 Loss: 1.0359994173049927\n",
      "Iteration 28197 Loss: 0.8552252650260925\n",
      "Iteration 28198 Loss: 1.4591190814971924\n",
      "Iteration 28199 Loss: 0.9681042432785034\n",
      "Iteration 28199 Loss: 1.07139253616333\n",
      "Iteration 28200 Loss: 1.1947448253631592\n",
      "Iteration 28201 Loss: 0.6299971342086792\n",
      "Iteration 28202 Loss: 1.2204878330230713\n",
      "Iteration 28203 Loss: 0.8777689337730408\n",
      "Iteration 28204 Loss: 1.1329091787338257\n",
      "Iteration 28205 Loss: 1.1362797021865845\n",
      "Iteration 28206 Loss: 1.1747218370437622\n",
      "Iteration 28207 Loss: 0.9586710929870605\n",
      "Iteration 28208 Loss: 0.8720777034759521\n",
      "Iteration 28209 Loss: 0.9186728000640869\n",
      "Iteration 28209 Loss: 1.0116331577301025\n",
      "Iteration 28210 Loss: 1.170081377029419\n",
      "Iteration 28211 Loss: 1.1627917289733887\n",
      "Iteration 28212 Loss: 1.320866346359253\n",
      "Iteration 28213 Loss: 1.059936285018921\n",
      "Iteration 28214 Loss: 1.1184344291687012\n",
      "Iteration 28215 Loss: 1.0561494827270508\n",
      "Iteration 28216 Loss: 0.9948487281799316\n",
      "Iteration 28217 Loss: 0.9456351399421692\n",
      "Iteration 28218 Loss: 1.0697230100631714\n",
      "Iteration 28219 Loss: 1.4095160961151123\n",
      "Iteration 28219 Loss: 1.13079833984375\n",
      "Iteration 28220 Loss: 0.7715804576873779\n",
      "Iteration 28221 Loss: 0.7721615433692932\n",
      "Iteration 28222 Loss: 0.6891312599182129\n",
      "Iteration 28223 Loss: 1.5665382146835327\n",
      "Iteration 28224 Loss: 1.1752780675888062\n",
      "Iteration 28225 Loss: 1.2620271444320679\n",
      "Iteration 28226 Loss: 1.0422654151916504\n",
      "Iteration 28227 Loss: 1.2511931657791138\n",
      "Iteration 28228 Loss: 0.8920470476150513\n",
      "Iteration 28229 Loss: 1.1460685729980469\n",
      "Iteration 28229 Loss: 1.0568289756774902\n",
      "Iteration 28230 Loss: 1.3106321096420288\n",
      "Iteration 28231 Loss: 1.1229485273361206\n",
      "Iteration 28232 Loss: 0.9677484631538391\n",
      "Iteration 28233 Loss: 1.167960524559021\n",
      "Iteration 28234 Loss: 1.2387832403182983\n",
      "Iteration 28235 Loss: 1.126816987991333\n",
      "Iteration 28236 Loss: 1.0138195753097534\n",
      "Iteration 28237 Loss: 0.5081205368041992\n",
      "Iteration 28238 Loss: 1.1011282205581665\n",
      "Iteration 28239 Loss: 1.179700255393982\n",
      "Iteration 28239 Loss: 1.0737658739089966\n",
      "Iteration 28240 Loss: 1.328606367111206\n",
      "Iteration 28241 Loss: 1.1274996995925903\n",
      "Iteration 28242 Loss: 1.405923843383789\n",
      "Iteration 28243 Loss: 0.9028967618942261\n",
      "Iteration 28244 Loss: 1.077646017074585\n",
      "Iteration 28245 Loss: 0.9619764685630798\n",
      "Iteration 28246 Loss: 1.3488928079605103\n",
      "Iteration 28247 Loss: 0.983860969543457\n",
      "Iteration 28248 Loss: 1.1938912868499756\n",
      "Iteration 28249 Loss: 1.3869948387145996\n",
      "Iteration 28249 Loss: 1.1718189716339111\n",
      "Iteration 28250 Loss: 1.0008084774017334\n",
      "Iteration 28251 Loss: 1.0357344150543213\n",
      "Iteration 28252 Loss: 1.2499061822891235\n",
      "Iteration 28253 Loss: 0.9408601522445679\n",
      "Iteration 28254 Loss: 0.7702000141143799\n",
      "Iteration 28255 Loss: 1.2249064445495605\n",
      "Iteration 28256 Loss: 1.2336176633834839\n",
      "Iteration 28257 Loss: 0.834453284740448\n",
      "Iteration 28258 Loss: 0.943422257900238\n",
      "Iteration 28259 Loss: 0.9034931659698486\n",
      "Iteration 28259 Loss: 1.0137403011322021\n",
      "Iteration 28260 Loss: 1.0523865222930908\n",
      "Iteration 28261 Loss: 0.9789737462997437\n",
      "Iteration 28262 Loss: 1.185693383216858\n",
      "Iteration 28263 Loss: 0.7425580620765686\n",
      "Iteration 28264 Loss: 1.3040441274642944\n",
      "Iteration 28265 Loss: 0.8281037211418152\n",
      "Iteration 28266 Loss: 1.1966936588287354\n",
      "Iteration 28267 Loss: 1.114366888999939\n",
      "Iteration 28268 Loss: 0.8983610272407532\n",
      "Iteration 28269 Loss: 0.8506829142570496\n",
      "Iteration 28269 Loss: 1.0151863098144531\n",
      "Iteration 28270 Loss: 1.2660084962844849\n",
      "Iteration 28271 Loss: 0.997313916683197\n",
      "Iteration 28272 Loss: 0.6443463563919067\n",
      "Iteration 28273 Loss: 1.2441130876541138\n",
      "Iteration 28274 Loss: 1.2876766920089722\n",
      "Iteration 28275 Loss: 0.9663998484611511\n",
      "Iteration 28276 Loss: 0.9251323342323303\n",
      "Iteration 28277 Loss: 1.0266727209091187\n",
      "Iteration 28278 Loss: 1.1159251928329468\n",
      "Iteration 28279 Loss: 0.8309224247932434\n",
      "Iteration 28279 Loss: 1.0304510593414307\n",
      "Iteration 28280 Loss: 1.0747764110565186\n",
      "Iteration 28281 Loss: 0.8897412419319153\n",
      "Iteration 28282 Loss: 0.9456044435501099\n",
      "Iteration 28283 Loss: 1.0695078372955322\n",
      "Iteration 28284 Loss: 1.305687665939331\n",
      "Iteration 28285 Loss: 0.7863839268684387\n",
      "Iteration 28286 Loss: 0.9937481880187988\n",
      "Iteration 28287 Loss: 1.152178168296814\n",
      "Iteration 28288 Loss: 1.2645196914672852\n",
      "Iteration 28289 Loss: 0.9732208251953125\n",
      "Iteration 28289 Loss: 1.045536756515503\n",
      "Iteration 28290 Loss: 0.9427260756492615\n",
      "Iteration 28291 Loss: 1.025671124458313\n",
      "Iteration 28292 Loss: 0.5548969507217407\n",
      "Iteration 28293 Loss: 1.1595102548599243\n",
      "Iteration 28294 Loss: 0.6186140775680542\n",
      "Iteration 28295 Loss: 1.1991132497787476\n",
      "Iteration 28296 Loss: 1.2540981769561768\n",
      "Iteration 28297 Loss: 1.2504326105117798\n",
      "Iteration 28298 Loss: 0.8864632844924927\n",
      "Iteration 28299 Loss: 1.2609531879425049\n",
      "Iteration 28299 Loss: 1.0152479410171509\n",
      "Iteration 28300 Loss: 1.028372883796692\n",
      "Iteration 28301 Loss: 1.3923083543777466\n",
      "Iteration 28302 Loss: 1.0440024137496948\n",
      "Iteration 28303 Loss: 0.8840105533599854\n",
      "Iteration 28304 Loss: 0.771390438079834\n",
      "Iteration 28305 Loss: 0.9353658556938171\n",
      "Iteration 28306 Loss: 1.1044774055480957\n",
      "Iteration 28307 Loss: 0.9439764618873596\n",
      "Iteration 28308 Loss: 0.9085150957107544\n",
      "Iteration 28309 Loss: 0.8036844730377197\n",
      "Iteration 28309 Loss: 0.9816104173660278\n",
      "Iteration 28310 Loss: 1.1470048427581787\n",
      "Iteration 28311 Loss: 0.9362946152687073\n",
      "Iteration 28312 Loss: 0.9171968698501587\n",
      "Iteration 28313 Loss: 1.176876425743103\n",
      "Iteration 28314 Loss: 1.0582635402679443\n",
      "Iteration 28315 Loss: 1.099120855331421\n",
      "Iteration 28316 Loss: 1.09050714969635\n",
      "Iteration 28317 Loss: 0.9076564908027649\n",
      "Iteration 28318 Loss: 1.2300500869750977\n",
      "Iteration 28319 Loss: 0.5185909867286682\n",
      "Iteration 28319 Loss: 1.0081562995910645\n",
      "Iteration 28320 Loss: 1.1679606437683105\n",
      "Iteration 28321 Loss: 1.0834475755691528\n",
      "Iteration 28322 Loss: 1.145815372467041\n",
      "Iteration 28323 Loss: 1.0497963428497314\n",
      "Iteration 28324 Loss: 0.9820983409881592\n",
      "Iteration 28325 Loss: 0.873254120349884\n",
      "Iteration 28326 Loss: 1.1582746505737305\n",
      "Iteration 28327 Loss: 1.2771403789520264\n",
      "Iteration 28328 Loss: 1.3908485174179077\n",
      "Iteration 28329 Loss: 0.9327715039253235\n",
      "Iteration 28329 Loss: 1.1061408519744873\n",
      "Iteration 28330 Loss: 1.099625825881958\n",
      "Iteration 28331 Loss: 1.1991368532180786\n",
      "Iteration 28332 Loss: 1.0094822645187378\n",
      "Iteration 28333 Loss: 0.9690139293670654\n",
      "Iteration 28334 Loss: 1.0299134254455566\n",
      "Iteration 28335 Loss: 1.0433189868927002\n",
      "Iteration 28336 Loss: 1.0817145109176636\n",
      "Iteration 28337 Loss: 0.7317185997962952\n",
      "Iteration 28338 Loss: 0.9582474827766418\n",
      "Iteration 28339 Loss: 1.11260187625885\n",
      "Iteration 28339 Loss: 1.0234774351119995\n",
      "Iteration 28340 Loss: 0.8762744069099426\n",
      "Iteration 28341 Loss: 1.1254894733428955\n",
      "Iteration 28342 Loss: 1.1804338693618774\n",
      "Iteration 28343 Loss: 0.8753940463066101\n",
      "Iteration 28344 Loss: 0.9691575169563293\n",
      "Iteration 28345 Loss: 0.9852355718612671\n",
      "Iteration 28346 Loss: 1.0856719017028809\n",
      "Iteration 28347 Loss: 1.1910887956619263\n",
      "Iteration 28348 Loss: 1.006343126296997\n",
      "Iteration 28349 Loss: 1.0057122707366943\n",
      "Iteration 28349 Loss: 1.0300800800323486\n",
      "Iteration 28350 Loss: 0.7699720859527588\n",
      "Iteration 28351 Loss: 1.0424227714538574\n",
      "Iteration 28352 Loss: 1.076677918434143\n",
      "Iteration 28353 Loss: 1.1817570924758911\n",
      "Iteration 28354 Loss: 1.2367134094238281\n",
      "Iteration 28355 Loss: 1.3452423810958862\n",
      "Iteration 28356 Loss: 0.9657241702079773\n",
      "Iteration 28357 Loss: 0.9301835298538208\n",
      "Iteration 28358 Loss: 1.0480743646621704\n",
      "Iteration 28359 Loss: 1.276362657546997\n",
      "Iteration 28359 Loss: 1.087312936782837\n",
      "Iteration 28360 Loss: 1.1523401737213135\n",
      "Iteration 28361 Loss: 1.1809179782867432\n",
      "Iteration 28362 Loss: 1.4018434286117554\n",
      "Iteration 28363 Loss: 0.9425677061080933\n",
      "Iteration 28364 Loss: 1.0015076398849487\n",
      "Iteration 28365 Loss: 0.7871775031089783\n",
      "Iteration 28366 Loss: 1.0601507425308228\n",
      "Iteration 28367 Loss: 0.9665681719779968\n",
      "Iteration 28368 Loss: 1.0741263628005981\n",
      "Iteration 28369 Loss: 0.65985107421875\n",
      "Iteration 28369 Loss: 1.0227051973342896\n",
      "Iteration 28370 Loss: 1.2346898317337036\n",
      "Iteration 28371 Loss: 0.8422824144363403\n",
      "Iteration 28372 Loss: 1.1124892234802246\n",
      "Iteration 28373 Loss: 0.9503781199455261\n",
      "Iteration 28374 Loss: 1.1733269691467285\n",
      "Iteration 28375 Loss: 1.2599177360534668\n",
      "Iteration 28376 Loss: 1.1637585163116455\n",
      "Iteration 28377 Loss: 0.9612530469894409\n",
      "Iteration 28378 Loss: 1.1978636980056763\n",
      "Iteration 28379 Loss: 0.9148321151733398\n",
      "Iteration 28379 Loss: 1.081079125404358\n",
      "Iteration 28380 Loss: 1.0874375104904175\n",
      "Iteration 28381 Loss: 1.299926519393921\n",
      "Iteration 28382 Loss: 1.0349104404449463\n",
      "Iteration 28383 Loss: 0.89173823595047\n",
      "Iteration 28384 Loss: 0.8075644969940186\n",
      "Iteration 28385 Loss: 1.4771915674209595\n",
      "Iteration 28386 Loss: 0.8189098238945007\n",
      "Iteration 28387 Loss: 0.8727965354919434\n",
      "Iteration 28388 Loss: 1.3671435117721558\n",
      "Iteration 28389 Loss: 1.1826558113098145\n",
      "Iteration 28389 Loss: 1.0840275287628174\n",
      "Iteration 28390 Loss: 1.182870864868164\n",
      "Iteration 28391 Loss: 0.8915567994117737\n",
      "Iteration 28392 Loss: 1.4394664764404297\n",
      "Iteration 28393 Loss: 1.2344359159469604\n",
      "Iteration 28394 Loss: 0.7338775992393494\n",
      "Iteration 28395 Loss: 1.1489590406417847\n",
      "Iteration 28396 Loss: 1.350710391998291\n",
      "Iteration 28397 Loss: 1.0177806615829468\n",
      "Iteration 28398 Loss: 1.224456548690796\n",
      "Iteration 28399 Loss: 1.2108062505722046\n",
      "Iteration 28399 Loss: 1.1434919834136963\n",
      "Iteration 28400 Loss: 1.1719694137573242\n",
      "Iteration 28401 Loss: 0.8105512261390686\n",
      "Iteration 28402 Loss: 0.953167736530304\n",
      "Iteration 28403 Loss: 1.1762943267822266\n",
      "Iteration 28404 Loss: 0.9768180847167969\n",
      "Iteration 28405 Loss: 0.8916329741477966\n",
      "Iteration 28406 Loss: 1.1159957647323608\n",
      "Iteration 28407 Loss: 0.8244941234588623\n",
      "Iteration 28408 Loss: 1.0796202421188354\n",
      "Iteration 28409 Loss: 0.860117495059967\n",
      "Iteration 28409 Loss: 0.9860661625862122\n",
      "Iteration 28410 Loss: 0.9727631211280823\n",
      "Iteration 28411 Loss: 1.2079782485961914\n",
      "Iteration 28412 Loss: 0.8664060831069946\n",
      "Iteration 28413 Loss: 0.9042720198631287\n",
      "Iteration 28414 Loss: 1.2330354452133179\n",
      "Iteration 28415 Loss: 1.2710208892822266\n",
      "Iteration 28416 Loss: 0.9180390238761902\n",
      "Iteration 28417 Loss: 0.6744424104690552\n",
      "Iteration 28418 Loss: 0.8967552781105042\n",
      "Iteration 28419 Loss: 1.2788187265396118\n",
      "Iteration 28419 Loss: 1.022353172302246\n",
      "Iteration 28420 Loss: 0.8607243895530701\n",
      "Iteration 28421 Loss: 1.0797828435897827\n",
      "Iteration 28422 Loss: 1.3567701578140259\n",
      "Iteration 28423 Loss: 1.0207104682922363\n",
      "Iteration 28424 Loss: 0.6941786408424377\n",
      "Iteration 28425 Loss: 1.2135213613510132\n",
      "Iteration 28426 Loss: 0.8481345772743225\n",
      "Iteration 28427 Loss: 1.041931390762329\n",
      "Iteration 28428 Loss: 1.0397753715515137\n",
      "Iteration 28429 Loss: 1.1034659147262573\n",
      "Iteration 28429 Loss: 1.0258995294570923\n",
      "Iteration 28430 Loss: 0.9858085513114929\n",
      "Iteration 28431 Loss: 0.8889538645744324\n",
      "Iteration 28432 Loss: 0.766981840133667\n",
      "Iteration 28433 Loss: 0.7322098612785339\n",
      "Iteration 28434 Loss: 0.8843815326690674\n",
      "Iteration 28435 Loss: 1.4764416217803955\n",
      "Iteration 28436 Loss: 1.172773838043213\n",
      "Iteration 28437 Loss: 0.8250065445899963\n",
      "Iteration 28438 Loss: 0.734200119972229\n",
      "Iteration 28439 Loss: 0.8729291558265686\n",
      "Iteration 28439 Loss: 0.933968722820282\n",
      "Iteration 28440 Loss: 0.9126560091972351\n",
      "Iteration 28441 Loss: 0.8907994031906128\n",
      "Iteration 28442 Loss: 0.6714780926704407\n",
      "Iteration 28443 Loss: 0.9561172723770142\n",
      "Iteration 28444 Loss: 0.8273051977157593\n",
      "Iteration 28445 Loss: 1.1277111768722534\n",
      "Iteration 28446 Loss: 0.7620985507965088\n",
      "Iteration 28447 Loss: 1.147925615310669\n",
      "Iteration 28448 Loss: 0.9467559456825256\n",
      "Iteration 28449 Loss: 1.0197311639785767\n",
      "Iteration 28449 Loss: 0.9262579083442688\n",
      "Iteration 28450 Loss: 0.9149336218833923\n",
      "Iteration 28451 Loss: 1.0700063705444336\n",
      "Iteration 28452 Loss: 0.942170262336731\n",
      "Iteration 28453 Loss: 1.4631377458572388\n",
      "Iteration 28454 Loss: 0.8466745615005493\n",
      "Iteration 28455 Loss: 1.2156953811645508\n",
      "Iteration 28456 Loss: 0.8124013543128967\n",
      "Iteration 28457 Loss: 1.299397349357605\n",
      "Iteration 28458 Loss: 1.01445472240448\n",
      "Iteration 28459 Loss: 1.1295851469039917\n",
      "Iteration 28459 Loss: 1.070845603942871\n",
      "Iteration 28460 Loss: 1.0055553913116455\n",
      "Iteration 28461 Loss: 0.9271827340126038\n",
      "Iteration 28462 Loss: 0.941726803779602\n",
      "Iteration 28463 Loss: 0.677342414855957\n",
      "Iteration 28464 Loss: 1.2846434116363525\n",
      "Iteration 28465 Loss: 1.0681803226470947\n",
      "Iteration 28466 Loss: 0.9254953861236572\n",
      "Iteration 28467 Loss: 1.2359670400619507\n",
      "Iteration 28468 Loss: 1.3089911937713623\n",
      "Iteration 28469 Loss: 1.1813669204711914\n",
      "Iteration 28469 Loss: 1.0556451082229614\n",
      "Iteration 28470 Loss: 0.6981880068778992\n",
      "Iteration 28471 Loss: 1.0788955688476562\n",
      "Iteration 28472 Loss: 1.275258183479309\n",
      "Iteration 28473 Loss: 1.040610909461975\n",
      "Iteration 28474 Loss: 0.777192234992981\n",
      "Iteration 28475 Loss: 1.0088106393814087\n",
      "Iteration 28476 Loss: 1.379847526550293\n",
      "Iteration 28477 Loss: 1.1349891424179077\n",
      "Iteration 28478 Loss: 1.1769139766693115\n",
      "Iteration 28479 Loss: 1.1610621213912964\n",
      "Iteration 28479 Loss: 1.0731767416000366\n",
      "Iteration 28480 Loss: 1.0739781856536865\n",
      "Iteration 28481 Loss: 1.0401219129562378\n",
      "Iteration 28482 Loss: 0.7702100872993469\n",
      "Iteration 28483 Loss: 1.0484915971755981\n",
      "Iteration 28484 Loss: 0.9879044890403748\n",
      "Iteration 28485 Loss: 1.2588086128234863\n",
      "Iteration 28486 Loss: 1.0758211612701416\n",
      "Iteration 28487 Loss: 0.6760432720184326\n",
      "Iteration 28488 Loss: 0.7761632800102234\n",
      "Iteration 28489 Loss: 1.0638301372528076\n",
      "Iteration 28489 Loss: 0.9771372675895691\n",
      "Iteration 28490 Loss: 1.1573611497879028\n",
      "Iteration 28491 Loss: 1.296535611152649\n",
      "Iteration 28492 Loss: 0.8032287955284119\n",
      "Iteration 28493 Loss: 1.3436639308929443\n",
      "Iteration 28494 Loss: 0.7491054534912109\n",
      "Iteration 28495 Loss: 1.1241955757141113\n",
      "Iteration 28496 Loss: 1.135604739189148\n",
      "Iteration 28497 Loss: 0.998234748840332\n",
      "Iteration 28498 Loss: 1.4687563180923462\n",
      "Iteration 28499 Loss: 0.9820446968078613\n",
      "Iteration 28499 Loss: 1.1058729887008667\n",
      "Iteration 28500 Loss: 1.0197511911392212\n",
      "Iteration 28501 Loss: 1.1943080425262451\n",
      "Iteration 28502 Loss: 1.3324055671691895\n",
      "Iteration 28503 Loss: 0.8507739305496216\n",
      "Iteration 28504 Loss: 1.1037544012069702\n",
      "Iteration 28505 Loss: 1.5227428674697876\n",
      "Iteration 28506 Loss: 1.207771897315979\n",
      "Iteration 28507 Loss: 0.9898389577865601\n",
      "Iteration 28508 Loss: 1.0209298133850098\n",
      "Iteration 28509 Loss: 0.8039714694023132\n",
      "Iteration 28509 Loss: 1.10462486743927\n",
      "Iteration 28510 Loss: 0.9879071116447449\n",
      "Iteration 28511 Loss: 0.810615062713623\n",
      "Iteration 28512 Loss: 1.1761103868484497\n",
      "Iteration 28513 Loss: 1.0174592733383179\n",
      "Iteration 28514 Loss: 0.9501532912254333\n",
      "Iteration 28515 Loss: 0.9940232634544373\n",
      "Iteration 28516 Loss: 1.060666799545288\n",
      "Iteration 28517 Loss: 1.2393258810043335\n",
      "Iteration 28518 Loss: 0.6270830631256104\n",
      "Iteration 28519 Loss: 0.5912197828292847\n",
      "Iteration 28519 Loss: 0.9454563856124878\n",
      "Iteration 28520 Loss: 1.3619567155838013\n",
      "Iteration 28521 Loss: 1.40387761592865\n",
      "Iteration 28522 Loss: 1.4661656618118286\n",
      "Iteration 28523 Loss: 1.1303654909133911\n",
      "Iteration 28524 Loss: 0.9998428821563721\n",
      "Iteration 28525 Loss: 1.1924294233322144\n",
      "Iteration 28526 Loss: 0.8744581341743469\n",
      "Iteration 28527 Loss: 1.0460116863250732\n",
      "Iteration 28528 Loss: 1.3368558883666992\n",
      "Iteration 28529 Loss: 0.8370868563652039\n",
      "Iteration 28529 Loss: 1.164905071258545\n",
      "Iteration 28530 Loss: 0.7255390882492065\n",
      "Iteration 28531 Loss: 1.4954127073287964\n",
      "Iteration 28532 Loss: 0.7390541434288025\n",
      "Iteration 28533 Loss: 0.8796698451042175\n",
      "Iteration 28534 Loss: 1.0564384460449219\n",
      "Iteration 28535 Loss: 0.5784171223640442\n",
      "Iteration 28536 Loss: 1.2200998067855835\n",
      "Iteration 28537 Loss: 1.0163122415542603\n",
      "Iteration 28538 Loss: 1.1839184761047363\n",
      "Iteration 28539 Loss: 1.3106746673583984\n",
      "Iteration 28539 Loss: 1.020553708076477\n",
      "Iteration 28540 Loss: 1.0999619960784912\n",
      "Iteration 28541 Loss: 1.065942406654358\n",
      "Iteration 28542 Loss: 1.2665281295776367\n",
      "Iteration 28543 Loss: 0.9409025311470032\n",
      "Iteration 28544 Loss: 1.026139259338379\n",
      "Iteration 28545 Loss: 0.926518440246582\n",
      "Iteration 28546 Loss: 1.057773232460022\n",
      "Iteration 28547 Loss: 1.129277229309082\n",
      "Iteration 28548 Loss: 1.2542743682861328\n",
      "Iteration 28549 Loss: 0.8469855189323425\n",
      "Iteration 28549 Loss: 1.0614303350448608\n",
      "Iteration 28550 Loss: 1.317487359046936\n",
      "Iteration 28551 Loss: 0.9271124601364136\n",
      "Iteration 28552 Loss: 1.12980318069458\n",
      "Iteration 28553 Loss: 1.4660106897354126\n",
      "Iteration 28554 Loss: 0.9447593688964844\n",
      "Iteration 28555 Loss: 1.0792806148529053\n",
      "Iteration 28556 Loss: 0.8238666653633118\n",
      "Iteration 28557 Loss: 1.0241189002990723\n",
      "Iteration 28558 Loss: 0.9450333118438721\n",
      "Iteration 28559 Loss: 0.9590566754341125\n",
      "Iteration 28559 Loss: 1.0616528987884521\n",
      "Iteration 28560 Loss: 0.6703208088874817\n",
      "Iteration 28561 Loss: 0.7532325387001038\n",
      "Iteration 28562 Loss: 0.8272565007209778\n",
      "Iteration 28563 Loss: 1.0418028831481934\n",
      "Iteration 28564 Loss: 0.8786481618881226\n",
      "Iteration 28565 Loss: 1.1632341146469116\n",
      "Iteration 28566 Loss: 0.9125567674636841\n",
      "Iteration 28567 Loss: 0.9062688946723938\n",
      "Iteration 28568 Loss: 0.723519504070282\n",
      "Iteration 28569 Loss: 1.0794764757156372\n",
      "Iteration 28569 Loss: 0.8956316709518433\n",
      "Iteration 28570 Loss: 1.3425095081329346\n",
      "Iteration 28571 Loss: 0.7463081479072571\n",
      "Iteration 28572 Loss: 0.8330144286155701\n",
      "Iteration 28573 Loss: 0.9808920621871948\n",
      "Iteration 28574 Loss: 0.9153478741645813\n",
      "Iteration 28575 Loss: 0.8301498293876648\n",
      "Iteration 28576 Loss: 0.8645094633102417\n",
      "Iteration 28577 Loss: 0.9011238217353821\n",
      "Iteration 28578 Loss: 1.1385831832885742\n",
      "Iteration 28579 Loss: 1.0131335258483887\n",
      "Iteration 28579 Loss: 0.9565571546554565\n",
      "Iteration 28580 Loss: 0.7029468417167664\n",
      "Iteration 28581 Loss: 1.0810476541519165\n",
      "Iteration 28582 Loss: 0.8255879282951355\n",
      "Iteration 28583 Loss: 1.2059962749481201\n",
      "Iteration 28584 Loss: 0.9257967472076416\n",
      "Iteration 28585 Loss: 0.7209576964378357\n",
      "Iteration 28586 Loss: 1.0515638589859009\n",
      "Iteration 28587 Loss: 1.1245853900909424\n",
      "Iteration 28588 Loss: 0.8974255919456482\n",
      "Iteration 28589 Loss: 0.6103872656822205\n",
      "Iteration 28589 Loss: 0.9146294593811035\n",
      "Iteration 28590 Loss: 0.9705235362052917\n",
      "Iteration 28591 Loss: 0.8255142569541931\n",
      "Iteration 28592 Loss: 0.9661099314689636\n",
      "Iteration 28593 Loss: 1.1844456195831299\n",
      "Iteration 28594 Loss: 1.1500474214553833\n",
      "Iteration 28595 Loss: 0.9329749941825867\n",
      "Iteration 28596 Loss: 1.5378309488296509\n",
      "Iteration 28597 Loss: 1.147049903869629\n",
      "Iteration 28598 Loss: 0.8882635235786438\n",
      "Iteration 28599 Loss: 1.2444566488265991\n",
      "Iteration 28599 Loss: 1.0847216844558716\n",
      "Iteration 28600 Loss: 1.1897482872009277\n",
      "Iteration 28601 Loss: 1.3332549333572388\n",
      "Iteration 28602 Loss: 0.9078214764595032\n",
      "Iteration 28603 Loss: 0.9365829825401306\n",
      "Iteration 28604 Loss: 0.8124099969863892\n",
      "Iteration 28605 Loss: 1.3314694166183472\n",
      "Iteration 28606 Loss: 1.0346260070800781\n",
      "Iteration 28607 Loss: 0.8941928148269653\n",
      "Iteration 28608 Loss: 1.6489150524139404\n",
      "Iteration 28609 Loss: 0.8836101293563843\n",
      "Iteration 28609 Loss: 1.0972630977630615\n",
      "Iteration 28610 Loss: 0.941543698310852\n",
      "Iteration 28611 Loss: 0.9347715973854065\n",
      "Iteration 28612 Loss: 1.4022042751312256\n",
      "Iteration 28613 Loss: 1.0709948539733887\n",
      "Iteration 28614 Loss: 1.0758894681930542\n",
      "Iteration 28615 Loss: 1.3633135557174683\n",
      "Iteration 28616 Loss: 1.477486491203308\n",
      "Iteration 28617 Loss: 1.3532111644744873\n",
      "Iteration 28618 Loss: 1.2294672727584839\n",
      "Iteration 28619 Loss: 0.8033921122550964\n",
      "Iteration 28619 Loss: 1.1652274131774902\n",
      "Iteration 28620 Loss: 0.8412042856216431\n",
      "Iteration 28621 Loss: 1.027286171913147\n",
      "Iteration 28622 Loss: 1.219512939453125\n",
      "Iteration 28623 Loss: 0.8593860268592834\n",
      "Iteration 28624 Loss: 0.8877686858177185\n",
      "Iteration 28625 Loss: 0.862446665763855\n",
      "Iteration 28626 Loss: 1.3945215940475464\n",
      "Iteration 28627 Loss: 1.0500410795211792\n",
      "Iteration 28628 Loss: 1.1430013179779053\n",
      "Iteration 28629 Loss: 1.0015488862991333\n",
      "Iteration 28629 Loss: 1.0286717414855957\n",
      "Iteration 28630 Loss: 1.04973566532135\n",
      "Iteration 28631 Loss: 1.0166693925857544\n",
      "Iteration 28632 Loss: 1.7707586288452148\n",
      "Iteration 28633 Loss: 0.8954295516014099\n",
      "Iteration 28634 Loss: 1.3349264860153198\n",
      "Iteration 28635 Loss: 0.8850365877151489\n",
      "Iteration 28636 Loss: 1.0523712635040283\n",
      "Iteration 28637 Loss: 0.9863002896308899\n",
      "Iteration 28638 Loss: 1.3174381256103516\n",
      "Iteration 28639 Loss: 0.8827402591705322\n",
      "Iteration 28639 Loss: 1.119140625\n",
      "Iteration 28640 Loss: 1.053301453590393\n",
      "Iteration 28641 Loss: 0.9219972491264343\n",
      "Iteration 28642 Loss: 1.0260354280471802\n",
      "Iteration 28643 Loss: 1.2062360048294067\n",
      "Iteration 28644 Loss: 1.0451760292053223\n",
      "Iteration 28645 Loss: 0.8267483711242676\n",
      "Iteration 28646 Loss: 1.2833223342895508\n",
      "Iteration 28647 Loss: 1.1690454483032227\n",
      "Iteration 28648 Loss: 0.9900810718536377\n",
      "Iteration 28649 Loss: 0.8706400394439697\n",
      "Iteration 28649 Loss: 1.0392582416534424\n",
      "Iteration 28650 Loss: 1.1890490055084229\n",
      "Iteration 28651 Loss: 1.0543619394302368\n",
      "Iteration 28652 Loss: 1.003598928451538\n",
      "Iteration 28653 Loss: 0.8907389044761658\n",
      "Iteration 28654 Loss: 1.0547525882720947\n",
      "Iteration 28655 Loss: 1.0119255781173706\n",
      "Iteration 28656 Loss: 0.8339406251907349\n",
      "Iteration 28657 Loss: 0.8505015969276428\n",
      "Iteration 28658 Loss: 1.2231475114822388\n",
      "Iteration 28659 Loss: 1.0100367069244385\n",
      "Iteration 28659 Loss: 1.0122053623199463\n",
      "Iteration 28660 Loss: 1.168479561805725\n",
      "Iteration 28661 Loss: 1.1141479015350342\n",
      "Iteration 28662 Loss: 1.39967942237854\n",
      "Iteration 28663 Loss: 1.048919916152954\n",
      "Iteration 28664 Loss: 1.081225037574768\n",
      "Iteration 28665 Loss: 0.956885576248169\n",
      "Iteration 28666 Loss: 1.4349995851516724\n",
      "Iteration 28667 Loss: 1.3786356449127197\n",
      "Iteration 28668 Loss: 1.1763641834259033\n",
      "Iteration 28669 Loss: 1.0175044536590576\n",
      "Iteration 28669 Loss: 1.1776840686798096\n",
      "Iteration 28670 Loss: 1.406042218208313\n",
      "Iteration 28671 Loss: 1.430881381034851\n",
      "Iteration 28672 Loss: 0.633827269077301\n",
      "Iteration 28673 Loss: 1.3654977083206177\n",
      "Iteration 28674 Loss: 0.7927393913269043\n",
      "Iteration 28675 Loss: 1.044814944267273\n",
      "Iteration 28676 Loss: 1.25835120677948\n",
      "Iteration 28677 Loss: 0.8126190304756165\n",
      "Iteration 28678 Loss: 1.4630560874938965\n",
      "Iteration 28679 Loss: 0.9135680198669434\n",
      "Iteration 28679 Loss: 1.1121397018432617\n",
      "Iteration 28680 Loss: 1.0949146747589111\n",
      "Iteration 28681 Loss: 0.9988996982574463\n",
      "Iteration 28682 Loss: 1.3259828090667725\n",
      "Iteration 28683 Loss: 0.929420530796051\n",
      "Iteration 28684 Loss: 1.394501805305481\n",
      "Iteration 28685 Loss: 1.0703879594802856\n",
      "Iteration 28686 Loss: 1.0998644828796387\n",
      "Iteration 28687 Loss: 0.8224855065345764\n",
      "Iteration 28688 Loss: 1.184901237487793\n",
      "Iteration 28689 Loss: 0.8643342852592468\n",
      "Iteration 28689 Loss: 1.0785692930221558\n",
      "Iteration 28690 Loss: 0.6881253123283386\n",
      "Iteration 28691 Loss: 1.2251605987548828\n",
      "Iteration 28692 Loss: 1.1916320323944092\n",
      "Iteration 28693 Loss: 0.8236666917800903\n",
      "Iteration 28694 Loss: 0.8250989317893982\n",
      "Iteration 28695 Loss: 1.3095325231552124\n",
      "Iteration 28696 Loss: 1.0109977722167969\n",
      "Iteration 28697 Loss: 1.060153841972351\n",
      "Iteration 28698 Loss: 1.2154899835586548\n",
      "Iteration 28699 Loss: 1.0885438919067383\n",
      "Iteration 28699 Loss: 1.0438401699066162\n",
      "Iteration 28700 Loss: 1.036259651184082\n",
      "Iteration 28701 Loss: 0.7392697930335999\n",
      "Iteration 28702 Loss: 1.0981457233428955\n",
      "Iteration 28703 Loss: 0.8348420262336731\n",
      "Iteration 28704 Loss: 0.8681842088699341\n",
      "Iteration 28705 Loss: 1.1115609407424927\n",
      "Iteration 28706 Loss: 0.9872658252716064\n",
      "Iteration 28707 Loss: 1.0235939025878906\n",
      "Iteration 28708 Loss: 0.9270317554473877\n",
      "Iteration 28709 Loss: 0.9343594312667847\n",
      "Iteration 28709 Loss: 0.9560513496398926\n",
      "Iteration 28710 Loss: 1.1022424697875977\n",
      "Iteration 28711 Loss: 0.8802042007446289\n",
      "Iteration 28712 Loss: 1.1253576278686523\n",
      "Iteration 28713 Loss: 1.1270240545272827\n",
      "Iteration 28714 Loss: 1.081864356994629\n",
      "Iteration 28715 Loss: 0.9615280628204346\n",
      "Iteration 28716 Loss: 1.252511978149414\n",
      "Iteration 28717 Loss: 1.1544543504714966\n",
      "Iteration 28718 Loss: 1.296532154083252\n",
      "Iteration 28719 Loss: 1.0258324146270752\n",
      "Iteration 28719 Loss: 1.100755214691162\n",
      "Iteration 28720 Loss: 1.3014869689941406\n",
      "Iteration 28721 Loss: 1.0585731267929077\n",
      "Iteration 28722 Loss: 0.9448114037513733\n",
      "Iteration 28723 Loss: 1.0776475667953491\n",
      "Iteration 28724 Loss: 1.1593838930130005\n",
      "Iteration 28725 Loss: 1.2975833415985107\n",
      "Iteration 28726 Loss: 0.6445937156677246\n",
      "Iteration 28727 Loss: 0.9775650501251221\n",
      "Iteration 28728 Loss: 0.7418220639228821\n",
      "Iteration 28729 Loss: 0.7340980172157288\n",
      "Iteration 28729 Loss: 0.9937565922737122\n",
      "Iteration 28730 Loss: 0.8127759099006653\n",
      "Iteration 28731 Loss: 1.4237140417099\n",
      "Iteration 28732 Loss: 1.332194209098816\n",
      "Iteration 28733 Loss: 1.2030216455459595\n",
      "Iteration 28734 Loss: 0.9540773034095764\n",
      "Iteration 28735 Loss: 0.9900270700454712\n",
      "Iteration 28736 Loss: 1.1351215839385986\n",
      "Iteration 28737 Loss: 1.2158788442611694\n",
      "Iteration 28738 Loss: 1.1391109228134155\n",
      "Iteration 28739 Loss: 1.156785249710083\n",
      "Iteration 28739 Loss: 1.1362706422805786\n",
      "Iteration 28740 Loss: 1.2453781366348267\n",
      "Iteration 28741 Loss: 1.4049146175384521\n",
      "Iteration 28742 Loss: 0.9670738577842712\n",
      "Iteration 28743 Loss: 1.3501173257827759\n",
      "Iteration 28744 Loss: 1.1119898557662964\n",
      "Iteration 28745 Loss: 1.2583012580871582\n",
      "Iteration 28746 Loss: 0.9718254208564758\n",
      "Iteration 28747 Loss: 0.9413846135139465\n",
      "Iteration 28748 Loss: 0.9534615278244019\n",
      "Iteration 28749 Loss: 1.0367233753204346\n",
      "Iteration 28749 Loss: 1.1241168975830078\n",
      "Iteration 28750 Loss: 0.9832370281219482\n",
      "Iteration 28751 Loss: 1.2187118530273438\n",
      "Iteration 28752 Loss: 1.4293843507766724\n",
      "Iteration 28753 Loss: 0.8649036288261414\n",
      "Iteration 28754 Loss: 1.2971285581588745\n",
      "Iteration 28755 Loss: 0.8795232176780701\n",
      "Iteration 28756 Loss: 1.0511325597763062\n",
      "Iteration 28757 Loss: 1.0882927179336548\n",
      "Iteration 28758 Loss: 1.1474151611328125\n",
      "Iteration 28759 Loss: 0.9884482622146606\n",
      "Iteration 28759 Loss: 1.0948177576065063\n",
      "Iteration 28760 Loss: 1.0225725173950195\n",
      "Iteration 28761 Loss: 0.9052118062973022\n",
      "Iteration 28762 Loss: 0.9751565456390381\n",
      "Iteration 28763 Loss: 0.9596995115280151\n",
      "Iteration 28764 Loss: 1.145326852798462\n",
      "Iteration 28765 Loss: 1.3589584827423096\n",
      "Iteration 28766 Loss: 1.1508454084396362\n",
      "Iteration 28767 Loss: 1.2687914371490479\n",
      "Iteration 28768 Loss: 0.882727324962616\n",
      "Iteration 28769 Loss: 1.31559157371521\n",
      "Iteration 28769 Loss: 1.0984880924224854\n",
      "Iteration 28770 Loss: 0.9195436239242554\n",
      "Iteration 28771 Loss: 1.0859785079956055\n",
      "Iteration 28772 Loss: 0.5612054467201233\n",
      "Iteration 28773 Loss: 0.9674209952354431\n",
      "Iteration 28774 Loss: 0.9392043352127075\n",
      "Iteration 28775 Loss: 1.3404061794281006\n",
      "Iteration 28776 Loss: 1.1114501953125\n",
      "Iteration 28777 Loss: 0.9453038573265076\n",
      "Iteration 28778 Loss: 1.118401050567627\n",
      "Iteration 28779 Loss: 0.8235774636268616\n",
      "Iteration 28779 Loss: 0.9812491536140442\n",
      "Iteration 28780 Loss: 0.965156614780426\n",
      "Iteration 28781 Loss: 0.5572437047958374\n",
      "Iteration 28782 Loss: 0.8189113736152649\n",
      "Iteration 28783 Loss: 1.1156569719314575\n",
      "Iteration 28784 Loss: 0.877652108669281\n",
      "Iteration 28785 Loss: 1.0349315404891968\n",
      "Iteration 28786 Loss: 1.1596523523330688\n",
      "Iteration 28787 Loss: 1.1052768230438232\n",
      "Iteration 28788 Loss: 0.5762255787849426\n",
      "Iteration 28789 Loss: 1.0284990072250366\n",
      "Iteration 28789 Loss: 0.9239206314086914\n",
      "Iteration 28790 Loss: 0.7656582593917847\n",
      "Iteration 28791 Loss: 1.0969696044921875\n",
      "Iteration 28792 Loss: 0.9663135409355164\n",
      "Iteration 28793 Loss: 1.024121880531311\n",
      "Iteration 28794 Loss: 1.4195154905319214\n",
      "Iteration 28795 Loss: 1.1800827980041504\n",
      "Iteration 28796 Loss: 1.1947424411773682\n",
      "Iteration 28797 Loss: 0.9085412621498108\n",
      "Iteration 28798 Loss: 1.6384085416793823\n",
      "Iteration 28799 Loss: 1.2169523239135742\n",
      "Iteration 28799 Loss: 1.1411305665969849\n",
      "Iteration 28800 Loss: 0.9833423495292664\n",
      "Iteration 28801 Loss: 1.0622594356536865\n",
      "Iteration 28802 Loss: 1.0140653848648071\n",
      "Iteration 28803 Loss: 1.1639370918273926\n",
      "Iteration 28804 Loss: 0.7737833261489868\n",
      "Iteration 28805 Loss: 1.1269561052322388\n",
      "Iteration 28806 Loss: 0.8750382661819458\n",
      "Iteration 28807 Loss: 0.8005278706550598\n",
      "Iteration 28808 Loss: 0.6978626251220703\n",
      "Iteration 28809 Loss: 1.000873327255249\n",
      "Iteration 28809 Loss: 0.9498645663261414\n",
      "Iteration 28810 Loss: 1.0341827869415283\n",
      "Iteration 28811 Loss: 1.3506830930709839\n",
      "Iteration 28812 Loss: 1.12108314037323\n",
      "Iteration 28813 Loss: 1.2016273736953735\n",
      "Iteration 28814 Loss: 0.9000070095062256\n",
      "Iteration 28815 Loss: 0.8781196475028992\n",
      "Iteration 28816 Loss: 1.183316707611084\n",
      "Iteration 28817 Loss: 1.0103018283843994\n",
      "Iteration 28818 Loss: 1.1782855987548828\n",
      "Iteration 28819 Loss: 1.1164867877960205\n",
      "Iteration 28819 Loss: 1.0974094867706299\n",
      "Iteration 28820 Loss: 0.8238104581832886\n",
      "Iteration 28821 Loss: 0.8466648459434509\n",
      "Iteration 28822 Loss: 0.8889956474304199\n",
      "Iteration 28823 Loss: 1.0887595415115356\n",
      "Iteration 28824 Loss: 1.0575007200241089\n",
      "Iteration 28825 Loss: 1.3449931144714355\n",
      "Iteration 28826 Loss: 1.1260305643081665\n",
      "Iteration 28827 Loss: 1.4581122398376465\n",
      "Iteration 28828 Loss: 0.6330122351646423\n",
      "Iteration 28829 Loss: 1.0180606842041016\n",
      "Iteration 28829 Loss: 1.0285940170288086\n",
      "Iteration 28830 Loss: 1.2746670246124268\n",
      "Iteration 28831 Loss: 1.1673810482025146\n",
      "Iteration 28832 Loss: 0.9645520448684692\n",
      "Iteration 28833 Loss: 0.9333553314208984\n",
      "Iteration 28834 Loss: 1.0064094066619873\n",
      "Iteration 28835 Loss: 1.2088780403137207\n",
      "Iteration 28836 Loss: 0.8969969749450684\n",
      "Iteration 28837 Loss: 1.4176840782165527\n",
      "Iteration 28838 Loss: 0.887781023979187\n",
      "Iteration 28839 Loss: 1.000156044960022\n",
      "Iteration 28839 Loss: 1.0757862329483032\n",
      "Iteration 28840 Loss: 1.0717862844467163\n",
      "Iteration 28841 Loss: 1.2161024808883667\n",
      "Iteration 28842 Loss: 0.7637348175048828\n",
      "Iteration 28843 Loss: 1.0876286029815674\n",
      "Iteration 28844 Loss: 1.2362072467803955\n",
      "Iteration 28845 Loss: 1.0861296653747559\n",
      "Iteration 28846 Loss: 1.212652325630188\n",
      "Iteration 28847 Loss: 1.4905214309692383\n",
      "Iteration 28848 Loss: 1.1196668148040771\n",
      "Iteration 28849 Loss: 0.9541553854942322\n",
      "Iteration 28849 Loss: 1.1238585710525513\n",
      "Iteration 28850 Loss: 1.1094807386398315\n",
      "Iteration 28851 Loss: 0.7664883136749268\n",
      "Iteration 28852 Loss: 1.1362568140029907\n",
      "Iteration 28853 Loss: 0.9733774065971375\n",
      "Iteration 28854 Loss: 0.8426541090011597\n",
      "Iteration 28855 Loss: 1.1186280250549316\n",
      "Iteration 28856 Loss: 0.7081462144851685\n",
      "Iteration 28857 Loss: 1.2032136917114258\n",
      "Iteration 28858 Loss: 1.167923927307129\n",
      "Iteration 28859 Loss: 1.357471227645874\n",
      "Iteration 28859 Loss: 1.038364052772522\n",
      "Iteration 28860 Loss: 1.0032298564910889\n",
      "Iteration 28861 Loss: 1.1677931547164917\n",
      "Iteration 28862 Loss: 1.2543330192565918\n",
      "Iteration 28863 Loss: 1.056364893913269\n",
      "Iteration 28864 Loss: 1.055908203125\n",
      "Iteration 28865 Loss: 1.028456211090088\n",
      "Iteration 28866 Loss: 0.8215826749801636\n",
      "Iteration 28867 Loss: 1.1641342639923096\n",
      "Iteration 28868 Loss: 0.8987494111061096\n",
      "Iteration 28869 Loss: 1.1157951354980469\n",
      "Iteration 28869 Loss: 1.0566346645355225\n",
      "Iteration 28870 Loss: 0.9284077286720276\n",
      "Iteration 28871 Loss: 1.063995599746704\n",
      "Iteration 28872 Loss: 1.1891142129898071\n",
      "Iteration 28873 Loss: 1.4410406351089478\n",
      "Iteration 28874 Loss: 0.8244765400886536\n",
      "Iteration 28875 Loss: 0.8059549927711487\n",
      "Iteration 28876 Loss: 0.9394107460975647\n",
      "Iteration 28877 Loss: 1.26079523563385\n",
      "Iteration 28878 Loss: 1.0719976425170898\n",
      "Iteration 28879 Loss: 1.1354175806045532\n",
      "Iteration 28879 Loss: 1.06606125831604\n",
      "Iteration 28880 Loss: 1.1382826566696167\n",
      "Iteration 28881 Loss: 1.2200722694396973\n",
      "Iteration 28882 Loss: 1.0819085836410522\n",
      "Iteration 28883 Loss: 1.171302080154419\n",
      "Iteration 28884 Loss: 1.303100347518921\n",
      "Iteration 28885 Loss: 1.117706537246704\n",
      "Iteration 28886 Loss: 1.0666745901107788\n",
      "Iteration 28887 Loss: 1.3030474185943604\n",
      "Iteration 28888 Loss: 1.1917904615402222\n",
      "Iteration 28889 Loss: 1.0193334817886353\n",
      "Iteration 28889 Loss: 1.1613218784332275\n",
      "Iteration 28890 Loss: 1.181910753250122\n",
      "Iteration 28891 Loss: 0.9602685570716858\n",
      "Iteration 28892 Loss: 0.9614907503128052\n",
      "Iteration 28893 Loss: 1.0047696828842163\n",
      "Iteration 28894 Loss: 1.1779932975769043\n",
      "Iteration 28895 Loss: 0.9308288097381592\n",
      "Iteration 28896 Loss: 0.9634712934494019\n",
      "Iteration 28897 Loss: 1.0293041467666626\n",
      "Iteration 28898 Loss: 1.3809354305267334\n",
      "Iteration 28899 Loss: 1.1918280124664307\n",
      "Iteration 28899 Loss: 1.0782802104949951\n",
      "Iteration 28900 Loss: 1.1283154487609863\n",
      "Iteration 28901 Loss: 1.3198269605636597\n",
      "Iteration 28902 Loss: 1.340567708015442\n",
      "Iteration 28903 Loss: 1.0338733196258545\n",
      "Iteration 28904 Loss: 0.9911393523216248\n",
      "Iteration 28905 Loss: 0.8713054060935974\n",
      "Iteration 28906 Loss: 1.0444988012313843\n",
      "Iteration 28907 Loss: 1.2695379257202148\n",
      "Iteration 28908 Loss: 1.1327500343322754\n",
      "Iteration 28909 Loss: 0.8625556230545044\n",
      "Iteration 28909 Loss: 1.0994369983673096\n",
      "Iteration 28910 Loss: 1.0073647499084473\n",
      "Iteration 28911 Loss: 0.8232544660568237\n",
      "Iteration 28912 Loss: 1.0536532402038574\n",
      "Iteration 28913 Loss: 0.8084699511528015\n",
      "Iteration 28914 Loss: 1.0769238471984863\n",
      "Iteration 28915 Loss: 1.1155517101287842\n",
      "Iteration 28916 Loss: 1.0675630569458008\n",
      "Iteration 28917 Loss: 1.1293913125991821\n",
      "Iteration 28918 Loss: 0.8430233597755432\n",
      "Iteration 28919 Loss: 1.0372164249420166\n",
      "Iteration 28919 Loss: 0.9962412118911743\n",
      "Iteration 28920 Loss: 1.0965120792388916\n",
      "Iteration 28921 Loss: 1.15474271774292\n",
      "Iteration 28922 Loss: 0.8949117660522461\n",
      "Iteration 28923 Loss: 0.9371766448020935\n",
      "Iteration 28924 Loss: 1.1864324808120728\n",
      "Iteration 28925 Loss: 0.7620350122451782\n",
      "Iteration 28926 Loss: 1.0206477642059326\n",
      "Iteration 28927 Loss: 1.0625791549682617\n",
      "Iteration 28928 Loss: 1.1404160261154175\n",
      "Iteration 28929 Loss: 1.0181620121002197\n",
      "Iteration 28929 Loss: 1.0273616313934326\n",
      "Iteration 28930 Loss: 0.6375956535339355\n",
      "Iteration 28931 Loss: 1.1083776950836182\n",
      "Iteration 28932 Loss: 1.0009979009628296\n",
      "Iteration 28933 Loss: 0.9940280318260193\n",
      "Iteration 28934 Loss: 0.7968154549598694\n",
      "Iteration 28935 Loss: 0.9310342669487\n",
      "Iteration 28936 Loss: 1.0683753490447998\n",
      "Iteration 28937 Loss: 0.9260568022727966\n",
      "Iteration 28938 Loss: 1.068469762802124\n",
      "Iteration 28939 Loss: 0.9171568751335144\n",
      "Iteration 28939 Loss: 0.9448907971382141\n",
      "Iteration 28940 Loss: 0.9372351169586182\n",
      "Iteration 28941 Loss: 0.7959354519844055\n",
      "Iteration 28942 Loss: 0.69575035572052\n",
      "Iteration 28943 Loss: 0.6855868101119995\n",
      "Iteration 28944 Loss: 1.2998809814453125\n",
      "Iteration 28945 Loss: 1.1816507577896118\n",
      "Iteration 28946 Loss: 1.3537994623184204\n",
      "Iteration 28947 Loss: 0.8901175856590271\n",
      "Iteration 28948 Loss: 1.0239832401275635\n",
      "Iteration 28949 Loss: 1.1450597047805786\n",
      "Iteration 28949 Loss: 1.0009000301361084\n",
      "Iteration 28950 Loss: 0.9392333626747131\n",
      "Iteration 28951 Loss: 0.9061790704727173\n",
      "Iteration 28952 Loss: 1.0913249254226685\n",
      "Iteration 28953 Loss: 0.8666418790817261\n",
      "Iteration 28954 Loss: 0.9938089847564697\n",
      "Iteration 28955 Loss: 1.2844451665878296\n",
      "Iteration 28956 Loss: 1.0174115896224976\n",
      "Iteration 28957 Loss: 1.1122664213180542\n",
      "Iteration 28958 Loss: 1.173142671585083\n",
      "Iteration 28959 Loss: 1.0928763151168823\n",
      "Iteration 28959 Loss: 1.047732949256897\n",
      "Iteration 28960 Loss: 1.0232189893722534\n",
      "Iteration 28961 Loss: 0.6869481801986694\n",
      "Iteration 28962 Loss: 1.0886765718460083\n",
      "Iteration 28963 Loss: 1.239707350730896\n",
      "Iteration 28964 Loss: 0.8109778165817261\n",
      "Iteration 28965 Loss: 1.115339756011963\n",
      "Iteration 28966 Loss: 0.9925119280815125\n",
      "Iteration 28967 Loss: 1.1058276891708374\n",
      "Iteration 28968 Loss: 1.2240365743637085\n",
      "Iteration 28969 Loss: 1.2436796426773071\n",
      "Iteration 28969 Loss: 1.053092360496521\n",
      "Iteration 28970 Loss: 0.8784817457199097\n",
      "Iteration 28971 Loss: 0.9625118374824524\n",
      "Iteration 28972 Loss: 1.050782322883606\n",
      "Iteration 28973 Loss: 1.2406078577041626\n",
      "Iteration 28974 Loss: 0.9652951955795288\n",
      "Iteration 28975 Loss: 1.0665721893310547\n",
      "Iteration 28976 Loss: 1.209049940109253\n",
      "Iteration 28977 Loss: 0.9450347423553467\n",
      "Iteration 28978 Loss: 0.8047011494636536\n",
      "Iteration 28979 Loss: 1.2802772521972656\n",
      "Iteration 28979 Loss: 1.040331482887268\n",
      "Iteration 28980 Loss: 0.7402371764183044\n",
      "Iteration 28981 Loss: 0.7484145760536194\n",
      "Iteration 28982 Loss: 1.2522088289260864\n",
      "Iteration 28983 Loss: 0.9485389590263367\n",
      "Iteration 28984 Loss: 1.08189857006073\n",
      "Iteration 28985 Loss: 1.1779747009277344\n",
      "Iteration 28986 Loss: 1.0626451969146729\n",
      "Iteration 28987 Loss: 0.969240128993988\n",
      "Iteration 28988 Loss: 1.0115102529525757\n",
      "Iteration 28989 Loss: 0.9559633731842041\n",
      "Iteration 28989 Loss: 0.9948631525039673\n",
      "Iteration 28990 Loss: 0.6650688052177429\n",
      "Iteration 28991 Loss: 1.1512898206710815\n",
      "Iteration 28992 Loss: 0.894870936870575\n",
      "Iteration 28993 Loss: 0.8753345608711243\n",
      "Iteration 28994 Loss: 1.1808325052261353\n",
      "Iteration 28995 Loss: 1.2006704807281494\n",
      "Iteration 28996 Loss: 1.0802638530731201\n",
      "Iteration 28997 Loss: 0.6678937077522278\n",
      "Iteration 28998 Loss: 1.2125273942947388\n",
      "Iteration 28999 Loss: 1.202821135520935\n",
      "Iteration 28999 Loss: 1.0131572484970093\n",
      "Iteration 29000 Loss: 0.8819141387939453\n",
      "Iteration 29001 Loss: 1.030233383178711\n",
      "Iteration 29002 Loss: 1.056340217590332\n",
      "Iteration 29003 Loss: 0.9785940647125244\n",
      "Iteration 29004 Loss: 1.2993013858795166\n",
      "Iteration 29005 Loss: 1.003428339958191\n",
      "Iteration 29006 Loss: 0.9763347506523132\n",
      "Iteration 29007 Loss: 1.0027401447296143\n",
      "Iteration 29008 Loss: 0.8384817838668823\n",
      "Iteration 29009 Loss: 0.8600163459777832\n",
      "Iteration 29009 Loss: 0.9927384257316589\n",
      "Iteration 29010 Loss: 1.0240713357925415\n",
      "Iteration 29011 Loss: 1.1425803899765015\n",
      "Iteration 29012 Loss: 0.7414872646331787\n",
      "Iteration 29013 Loss: 1.219577670097351\n",
      "Iteration 29014 Loss: 0.9997460842132568\n",
      "Iteration 29015 Loss: 1.0803006887435913\n",
      "Iteration 29016 Loss: 0.9738919734954834\n",
      "Iteration 29017 Loss: 1.3324819803237915\n",
      "Iteration 29018 Loss: 1.107600212097168\n",
      "Iteration 29019 Loss: 1.168591856956482\n",
      "Iteration 29019 Loss: 1.0790330171585083\n",
      "Iteration 29020 Loss: 1.2972965240478516\n",
      "Iteration 29021 Loss: 1.2621363401412964\n",
      "Iteration 29022 Loss: 0.7876749038696289\n",
      "Iteration 29023 Loss: 0.6594775915145874\n",
      "Iteration 29024 Loss: 0.7446317672729492\n",
      "Iteration 29025 Loss: 1.2010482549667358\n",
      "Iteration 29026 Loss: 1.014158844947815\n",
      "Iteration 29027 Loss: 1.0368841886520386\n",
      "Iteration 29028 Loss: 0.7870423793792725\n",
      "Iteration 29029 Loss: 0.8677995204925537\n",
      "Iteration 29029 Loss: 0.9658150672912598\n",
      "Iteration 29030 Loss: 1.3582684993743896\n",
      "Iteration 29031 Loss: 0.860893189907074\n",
      "Iteration 29032 Loss: 1.384635090827942\n",
      "Iteration 29033 Loss: 0.8603079319000244\n",
      "Iteration 29034 Loss: 0.7747812867164612\n",
      "Iteration 29035 Loss: 1.1309415102005005\n",
      "Iteration 29036 Loss: 1.2093884944915771\n",
      "Iteration 29037 Loss: 0.818778932094574\n",
      "Iteration 29038 Loss: 0.7023245692253113\n",
      "Iteration 29039 Loss: 0.8141728639602661\n",
      "Iteration 29039 Loss: 0.991449236869812\n",
      "Iteration 29040 Loss: 0.8317660093307495\n",
      "Iteration 29041 Loss: 0.924906849861145\n",
      "Iteration 29042 Loss: 0.8596489429473877\n",
      "Iteration 29043 Loss: 0.8523966670036316\n",
      "Iteration 29044 Loss: 0.9075673222541809\n",
      "Iteration 29045 Loss: 1.300716519355774\n",
      "Iteration 29046 Loss: 0.9876335859298706\n",
      "Iteration 29047 Loss: 1.1050024032592773\n",
      "Iteration 29048 Loss: 1.2209141254425049\n",
      "Iteration 29049 Loss: 1.1822723150253296\n",
      "Iteration 29049 Loss: 1.017282485961914\n",
      "Iteration 29050 Loss: 1.244179129600525\n",
      "Iteration 29051 Loss: 0.9732643961906433\n",
      "Iteration 29052 Loss: 1.1490057706832886\n",
      "Iteration 29053 Loss: 1.1290918588638306\n",
      "Iteration 29054 Loss: 1.200182557106018\n",
      "Iteration 29055 Loss: 1.385345220565796\n",
      "Iteration 29056 Loss: 0.9789547324180603\n",
      "Iteration 29057 Loss: 1.2003297805786133\n",
      "Iteration 29058 Loss: 0.9098924398422241\n",
      "Iteration 29059 Loss: 1.154543399810791\n",
      "Iteration 29059 Loss: 1.132478952407837\n",
      "Iteration 29060 Loss: 1.1787809133529663\n",
      "Iteration 29061 Loss: 1.2991584539413452\n",
      "Iteration 29062 Loss: 1.048190951347351\n",
      "Iteration 29063 Loss: 0.6852558255195618\n",
      "Iteration 29064 Loss: 0.7453644275665283\n",
      "Iteration 29065 Loss: 0.9130573272705078\n",
      "Iteration 29066 Loss: 1.1686058044433594\n",
      "Iteration 29067 Loss: 0.9590054750442505\n",
      "Iteration 29068 Loss: 0.9981270432472229\n",
      "Iteration 29069 Loss: 1.137908935546875\n",
      "Iteration 29069 Loss: 1.01334547996521\n",
      "Iteration 29070 Loss: 0.8615561723709106\n",
      "Iteration 29071 Loss: 0.9285698533058167\n",
      "Iteration 29072 Loss: 0.8771077990531921\n",
      "Iteration 29073 Loss: 1.1392136812210083\n",
      "Iteration 29074 Loss: 1.0074931383132935\n",
      "Iteration 29075 Loss: 0.9008013606071472\n",
      "Iteration 29076 Loss: 0.9017893671989441\n",
      "Iteration 29077 Loss: 0.8129286766052246\n",
      "Iteration 29078 Loss: 1.3391047716140747\n",
      "Iteration 29079 Loss: 0.950576663017273\n",
      "Iteration 29079 Loss: 0.9719141125679016\n",
      "Iteration 29080 Loss: 1.145861029624939\n",
      "Iteration 29081 Loss: 0.9612625241279602\n",
      "Iteration 29082 Loss: 1.2966090440750122\n",
      "Iteration 29083 Loss: 1.038676142692566\n",
      "Iteration 29084 Loss: 0.9758602976799011\n",
      "Iteration 29085 Loss: 0.7479040026664734\n",
      "Iteration 29086 Loss: 0.9895449876785278\n",
      "Iteration 29087 Loss: 1.0726286172866821\n",
      "Iteration 29088 Loss: 1.2148292064666748\n",
      "Iteration 29089 Loss: 1.1500463485717773\n",
      "Iteration 29089 Loss: 1.0593222379684448\n",
      "Iteration 29090 Loss: 0.9226137399673462\n",
      "Iteration 29091 Loss: 1.099441647529602\n",
      "Iteration 29092 Loss: 1.3347699642181396\n",
      "Iteration 29093 Loss: 0.9535074234008789\n",
      "Iteration 29094 Loss: 1.2315360307693481\n",
      "Iteration 29095 Loss: 0.9166090488433838\n",
      "Iteration 29096 Loss: 1.1375890970230103\n",
      "Iteration 29097 Loss: 0.9925047755241394\n",
      "Iteration 29098 Loss: 1.0819354057312012\n",
      "Iteration 29099 Loss: 1.0625776052474976\n",
      "Iteration 29099 Loss: 1.0733084678649902\n",
      "Iteration 29100 Loss: 1.1291677951812744\n",
      "Iteration 29101 Loss: 1.2515945434570312\n",
      "Iteration 29102 Loss: 1.0759024620056152\n",
      "Iteration 29103 Loss: 0.8643642663955688\n",
      "Iteration 29104 Loss: 0.9623598456382751\n",
      "Iteration 29105 Loss: 1.3003875017166138\n",
      "Iteration 29106 Loss: 0.8827162981033325\n",
      "Iteration 29107 Loss: 0.6068005561828613\n",
      "Iteration 29108 Loss: 0.8762754797935486\n",
      "Iteration 29109 Loss: 1.265324592590332\n",
      "Iteration 29109 Loss: 1.0214893817901611\n",
      "Iteration 29110 Loss: 1.102231740951538\n",
      "Iteration 29111 Loss: 1.1026943922042847\n",
      "Iteration 29112 Loss: 0.9467401504516602\n",
      "Iteration 29113 Loss: 1.0493007898330688\n",
      "Iteration 29114 Loss: 1.1831743717193604\n",
      "Iteration 29115 Loss: 0.7105144262313843\n",
      "Iteration 29116 Loss: 0.9672938585281372\n",
      "Iteration 29117 Loss: 0.8146236538887024\n",
      "Iteration 29118 Loss: 1.340254783630371\n",
      "Iteration 29119 Loss: 0.8298314809799194\n",
      "Iteration 29119 Loss: 1.0046659708023071\n",
      "Iteration 29120 Loss: 1.322104573249817\n",
      "Iteration 29121 Loss: 1.6764544248580933\n",
      "Iteration 29122 Loss: 1.0703637599945068\n",
      "Iteration 29123 Loss: 1.100264310836792\n",
      "Iteration 29124 Loss: 0.9780042767524719\n",
      "Iteration 29125 Loss: 0.8645654320716858\n",
      "Iteration 29126 Loss: 1.1769567728042603\n",
      "Iteration 29127 Loss: 1.0978010892868042\n",
      "Iteration 29128 Loss: 0.7883199453353882\n",
      "Iteration 29129 Loss: 1.107951283454895\n",
      "Iteration 29129 Loss: 1.1182787418365479\n",
      "Iteration 29130 Loss: 0.7835585474967957\n",
      "Iteration 29131 Loss: 0.9369795918464661\n",
      "Iteration 29132 Loss: 1.3701738119125366\n",
      "Iteration 29133 Loss: 1.1954647302627563\n",
      "Iteration 29134 Loss: 1.03676176071167\n",
      "Iteration 29135 Loss: 1.2268860340118408\n",
      "Iteration 29136 Loss: 0.8655228018760681\n",
      "Iteration 29137 Loss: 0.6303085088729858\n",
      "Iteration 29138 Loss: 1.0016587972640991\n",
      "Iteration 29139 Loss: 0.7467047572135925\n",
      "Iteration 29139 Loss: 0.9794018864631653\n",
      "Iteration 29140 Loss: 0.8656731247901917\n",
      "Iteration 29141 Loss: 1.2663366794586182\n",
      "Iteration 29142 Loss: 1.1768701076507568\n",
      "Iteration 29143 Loss: 0.8561156392097473\n",
      "Iteration 29144 Loss: 0.8223240971565247\n",
      "Iteration 29145 Loss: 0.7142447829246521\n",
      "Iteration 29146 Loss: 0.9359553456306458\n",
      "Iteration 29147 Loss: 1.2271511554718018\n",
      "Iteration 29148 Loss: 0.9969643950462341\n",
      "Iteration 29149 Loss: 0.8741090893745422\n",
      "Iteration 29149 Loss: 0.9735744595527649\n",
      "Iteration 29150 Loss: 0.9609143137931824\n",
      "Iteration 29151 Loss: 1.116745948791504\n",
      "Iteration 29152 Loss: 1.4357173442840576\n",
      "Iteration 29153 Loss: 1.0776046514511108\n",
      "Iteration 29154 Loss: 1.189220666885376\n",
      "Iteration 29155 Loss: 1.195704698562622\n",
      "Iteration 29156 Loss: 0.8324068188667297\n",
      "Iteration 29157 Loss: 0.7574678063392639\n",
      "Iteration 29158 Loss: 1.0017304420471191\n",
      "Iteration 29159 Loss: 0.6579044461250305\n",
      "Iteration 29159 Loss: 1.0225417613983154\n",
      "Iteration 29160 Loss: 1.229392409324646\n",
      "Iteration 29161 Loss: 1.0416655540466309\n",
      "Iteration 29162 Loss: 0.8603231906890869\n",
      "Iteration 29163 Loss: 0.9810804128646851\n",
      "Iteration 29164 Loss: 0.8938503861427307\n",
      "Iteration 29165 Loss: 1.208292841911316\n",
      "Iteration 29166 Loss: 1.2792224884033203\n",
      "Iteration 29167 Loss: 0.9461168646812439\n",
      "Iteration 29168 Loss: 0.6470170617103577\n",
      "Iteration 29169 Loss: 1.0100152492523193\n",
      "Iteration 29169 Loss: 1.009697675704956\n",
      "Iteration 29170 Loss: 1.1339479684829712\n",
      "Iteration 29171 Loss: 0.9694923162460327\n",
      "Iteration 29172 Loss: 1.14408540725708\n",
      "Iteration 29173 Loss: 0.9362616539001465\n",
      "Iteration 29174 Loss: 1.0489652156829834\n",
      "Iteration 29175 Loss: 1.0582102537155151\n",
      "Iteration 29176 Loss: 0.9258931279182434\n",
      "Iteration 29177 Loss: 0.9723522663116455\n",
      "Iteration 29178 Loss: 1.4354788064956665\n",
      "Iteration 29179 Loss: 1.2307087182998657\n",
      "Iteration 29179 Loss: 1.0855395793914795\n",
      "Iteration 29180 Loss: 0.8239886164665222\n",
      "Iteration 29181 Loss: 0.8412457704544067\n",
      "Iteration 29182 Loss: 1.309173583984375\n",
      "Iteration 29183 Loss: 0.557811439037323\n",
      "Iteration 29184 Loss: 1.0400160551071167\n",
      "Iteration 29185 Loss: 1.04043710231781\n",
      "Iteration 29186 Loss: 1.2450077533721924\n",
      "Iteration 29187 Loss: 1.295172095298767\n",
      "Iteration 29188 Loss: 0.8326151967048645\n",
      "Iteration 29189 Loss: 1.1874006986618042\n",
      "Iteration 29189 Loss: 1.017286777496338\n",
      "Iteration 29190 Loss: 0.9755072593688965\n",
      "Iteration 29191 Loss: 1.437619686126709\n",
      "Iteration 29192 Loss: 1.182201623916626\n",
      "Iteration 29193 Loss: 1.3235664367675781\n",
      "Iteration 29194 Loss: 1.1601802110671997\n",
      "Iteration 29195 Loss: 1.1076483726501465\n",
      "Iteration 29196 Loss: 1.1185256242752075\n",
      "Iteration 29197 Loss: 0.9612184166908264\n",
      "Iteration 29198 Loss: 0.8469263911247253\n",
      "Iteration 29199 Loss: 1.1007399559020996\n",
      "Iteration 29199 Loss: 1.1214134693145752\n",
      "Iteration 29200 Loss: 0.7490400075912476\n",
      "Iteration 29201 Loss: 1.0493172407150269\n",
      "Iteration 29202 Loss: 1.1291453838348389\n",
      "Iteration 29203 Loss: 0.9066260457038879\n",
      "Iteration 29204 Loss: 0.820039689540863\n",
      "Iteration 29205 Loss: 0.942348062992096\n",
      "Iteration 29206 Loss: 1.0166341066360474\n",
      "Iteration 29207 Loss: 1.1293338537216187\n",
      "Iteration 29208 Loss: 1.0482115745544434\n",
      "Iteration 29209 Loss: 1.3009108304977417\n",
      "Iteration 29209 Loss: 1.0091607570648193\n",
      "Iteration 29210 Loss: 0.8561708331108093\n",
      "Iteration 29211 Loss: 0.9589056372642517\n",
      "Iteration 29212 Loss: 0.985588014125824\n",
      "Iteration 29213 Loss: 1.21024751663208\n",
      "Iteration 29214 Loss: 1.199290156364441\n",
      "Iteration 29215 Loss: 0.8994748592376709\n",
      "Iteration 29216 Loss: 1.0737320184707642\n",
      "Iteration 29217 Loss: 0.8818767070770264\n",
      "Iteration 29218 Loss: 0.7224306464195251\n",
      "Iteration 29219 Loss: 0.8472683429718018\n",
      "Iteration 29219 Loss: 0.963498592376709\n",
      "Iteration 29220 Loss: 1.0751488208770752\n",
      "Iteration 29221 Loss: 1.1295584440231323\n",
      "Iteration 29222 Loss: 1.003725528717041\n",
      "Iteration 29223 Loss: 0.7258780002593994\n",
      "Iteration 29224 Loss: 1.2571167945861816\n",
      "Iteration 29225 Loss: 1.2419270277023315\n",
      "Iteration 29226 Loss: 1.377581238746643\n",
      "Iteration 29227 Loss: 1.1194820404052734\n",
      "Iteration 29228 Loss: 0.8742755651473999\n",
      "Iteration 29229 Loss: 1.2083221673965454\n",
      "Iteration 29229 Loss: 1.1013015508651733\n",
      "Iteration 29230 Loss: 1.2646392583847046\n",
      "Iteration 29231 Loss: 0.9052621126174927\n",
      "Iteration 29232 Loss: 1.1741491556167603\n",
      "Iteration 29233 Loss: 1.0445771217346191\n",
      "Iteration 29234 Loss: 0.9289220571517944\n",
      "Iteration 29235 Loss: 1.0483133792877197\n",
      "Iteration 29236 Loss: 0.9002455472946167\n",
      "Iteration 29237 Loss: 1.145876407623291\n",
      "Iteration 29238 Loss: 0.9416487216949463\n",
      "Iteration 29239 Loss: 1.0130987167358398\n",
      "Iteration 29239 Loss: 1.0366731882095337\n",
      "Iteration 29240 Loss: 1.0039721727371216\n",
      "Iteration 29241 Loss: 1.0271981954574585\n",
      "Iteration 29242 Loss: 0.9755470752716064\n",
      "Iteration 29243 Loss: 1.1201965808868408\n",
      "Iteration 29244 Loss: 0.8127485513687134\n",
      "Iteration 29245 Loss: 1.018825888633728\n",
      "Iteration 29246 Loss: 1.4265741109848022\n",
      "Iteration 29247 Loss: 1.4670531749725342\n",
      "Iteration 29248 Loss: 1.118226170539856\n",
      "Iteration 29249 Loss: 1.0034879446029663\n",
      "Iteration 29249 Loss: 1.09738290309906\n",
      "Iteration 29250 Loss: 1.1924800872802734\n",
      "Iteration 29251 Loss: 0.9360385537147522\n",
      "Iteration 29252 Loss: 1.5780117511749268\n",
      "Iteration 29253 Loss: 1.039747714996338\n",
      "Iteration 29254 Loss: 0.536472737789154\n",
      "Iteration 29255 Loss: 0.6880989074707031\n",
      "Iteration 29256 Loss: 1.0410056114196777\n",
      "Iteration 29257 Loss: 0.8937515616416931\n",
      "Iteration 29258 Loss: 0.8148418664932251\n",
      "Iteration 29259 Loss: 0.9864627718925476\n",
      "Iteration 29259 Loss: 0.9706910848617554\n",
      "Iteration 29260 Loss: 1.4359209537506104\n",
      "Iteration 29261 Loss: 1.1868189573287964\n",
      "Iteration 29262 Loss: 0.8816600441932678\n",
      "Iteration 29263 Loss: 0.9161664843559265\n",
      "Iteration 29264 Loss: 1.1227792501449585\n",
      "Iteration 29265 Loss: 1.044169306755066\n",
      "Iteration 29266 Loss: 0.6894177198410034\n",
      "Iteration 29267 Loss: 1.216336727142334\n",
      "Iteration 29268 Loss: 1.3350379467010498\n",
      "Iteration 29269 Loss: 0.9889040589332581\n",
      "Iteration 29269 Loss: 1.0817210674285889\n",
      "Iteration 29270 Loss: 0.9647188782691956\n",
      "Iteration 29271 Loss: 1.137289047241211\n",
      "Iteration 29272 Loss: 0.7567294836044312\n",
      "Iteration 29273 Loss: 1.01549232006073\n",
      "Iteration 29274 Loss: 1.0732040405273438\n",
      "Iteration 29275 Loss: 1.2712595462799072\n",
      "Iteration 29276 Loss: 0.8728006482124329\n",
      "Iteration 29277 Loss: 0.7230323553085327\n",
      "Iteration 29278 Loss: 1.1342867612838745\n",
      "Iteration 29279 Loss: 0.8165299892425537\n",
      "Iteration 29279 Loss: 0.976534366607666\n",
      "Iteration 29280 Loss: 1.0818654298782349\n",
      "Iteration 29281 Loss: 0.8378891348838806\n",
      "Iteration 29282 Loss: 1.1559489965438843\n",
      "Iteration 29283 Loss: 1.0963213443756104\n",
      "Iteration 29284 Loss: 1.1188052892684937\n",
      "Iteration 29285 Loss: 0.9782198071479797\n",
      "Iteration 29286 Loss: 0.970380961894989\n",
      "Iteration 29287 Loss: 0.8719807267189026\n",
      "Iteration 29288 Loss: 0.9037933349609375\n",
      "Iteration 29289 Loss: 1.0015616416931152\n",
      "Iteration 29289 Loss: 1.0016766786575317\n",
      "Iteration 29290 Loss: 0.8261045813560486\n",
      "Iteration 29291 Loss: 0.8146925568580627\n",
      "Iteration 29292 Loss: 1.0442532300949097\n",
      "Iteration 29293 Loss: 0.8664100766181946\n",
      "Iteration 29294 Loss: 0.8899883031845093\n",
      "Iteration 29295 Loss: 1.123796820640564\n",
      "Iteration 29296 Loss: 0.9687632918357849\n",
      "Iteration 29297 Loss: 1.0269330739974976\n",
      "Iteration 29298 Loss: 1.0668007135391235\n",
      "Iteration 29299 Loss: 0.954946756362915\n",
      "Iteration 29299 Loss: 0.958268940448761\n",
      "Iteration 29300 Loss: 0.8161872029304504\n",
      "Iteration 29301 Loss: 0.919744074344635\n",
      "Iteration 29302 Loss: 0.8674560785293579\n",
      "Iteration 29303 Loss: 1.2024577856063843\n",
      "Iteration 29304 Loss: 1.0909260511398315\n",
      "Iteration 29305 Loss: 1.3351960182189941\n",
      "Iteration 29306 Loss: 1.1801985502243042\n",
      "Iteration 29307 Loss: 1.0563452243804932\n",
      "Iteration 29308 Loss: 1.20737624168396\n",
      "Iteration 29309 Loss: 0.7946587800979614\n",
      "Iteration 29309 Loss: 1.047054648399353\n",
      "Iteration 29310 Loss: 1.244137167930603\n",
      "Iteration 29311 Loss: 1.1471517086029053\n",
      "Iteration 29312 Loss: 1.0565106868743896\n",
      "Iteration 29313 Loss: 0.8657485246658325\n",
      "Iteration 29314 Loss: 1.1665714979171753\n",
      "Iteration 29315 Loss: 0.9909155368804932\n",
      "Iteration 29316 Loss: 0.8157407641410828\n",
      "Iteration 29317 Loss: 1.050344705581665\n",
      "Iteration 29318 Loss: 1.0433117151260376\n",
      "Iteration 29319 Loss: 0.8597373962402344\n",
      "Iteration 29319 Loss: 1.0240169763565063\n",
      "Iteration 29320 Loss: 0.6810194849967957\n",
      "Iteration 29321 Loss: 0.8996267914772034\n",
      "Iteration 29322 Loss: 1.012626051902771\n",
      "Iteration 29323 Loss: 1.081763505935669\n",
      "Iteration 29324 Loss: 1.008745551109314\n",
      "Iteration 29325 Loss: 1.0009440183639526\n",
      "Iteration 29326 Loss: 1.0199224948883057\n",
      "Iteration 29327 Loss: 1.1288504600524902\n",
      "Iteration 29328 Loss: 1.0512577295303345\n",
      "Iteration 29329 Loss: 1.0231539011001587\n",
      "Iteration 29329 Loss: 0.9907911419868469\n",
      "Iteration 29330 Loss: 1.1719175577163696\n",
      "Iteration 29331 Loss: 0.9762406945228577\n",
      "Iteration 29332 Loss: 1.019227385520935\n",
      "Iteration 29333 Loss: 1.0197347402572632\n",
      "Iteration 29334 Loss: 0.8076527714729309\n",
      "Iteration 29335 Loss: 1.0398242473602295\n",
      "Iteration 29336 Loss: 1.271319031715393\n",
      "Iteration 29337 Loss: 1.09751296043396\n",
      "Iteration 29338 Loss: 0.5329515933990479\n",
      "Iteration 29339 Loss: 1.0584716796875\n",
      "Iteration 29339 Loss: 0.9994853734970093\n",
      "Iteration 29340 Loss: 0.9277178645133972\n",
      "Iteration 29341 Loss: 1.1556360721588135\n",
      "Iteration 29342 Loss: 0.7850289940834045\n",
      "Iteration 29343 Loss: 0.886846661567688\n",
      "Iteration 29344 Loss: 1.0009526014328003\n",
      "Iteration 29345 Loss: 1.2107285261154175\n",
      "Iteration 29346 Loss: 1.0276800394058228\n",
      "Iteration 29347 Loss: 0.5721267461776733\n",
      "Iteration 29348 Loss: 1.0061888694763184\n",
      "Iteration 29349 Loss: 0.9537708759307861\n",
      "Iteration 29349 Loss: 0.9526677131652832\n",
      "Iteration 29350 Loss: 0.931175947189331\n",
      "Iteration 29351 Loss: 0.8470831513404846\n",
      "Iteration 29352 Loss: 1.0620073080062866\n",
      "Iteration 29353 Loss: 0.582004964351654\n",
      "Iteration 29354 Loss: 0.8723796606063843\n",
      "Iteration 29355 Loss: 0.8962679505348206\n",
      "Iteration 29356 Loss: 1.0563170909881592\n",
      "Iteration 29357 Loss: 0.8885864019393921\n",
      "Iteration 29358 Loss: 1.120756983757019\n",
      "Iteration 29359 Loss: 1.1839841604232788\n",
      "Iteration 29359 Loss: 0.9440563321113586\n",
      "Iteration 29360 Loss: 0.5575419664382935\n",
      "Iteration 29361 Loss: 1.3865776062011719\n",
      "Iteration 29362 Loss: 1.0085880756378174\n",
      "Iteration 29363 Loss: 1.1671785116195679\n",
      "Iteration 29364 Loss: 0.8016480803489685\n",
      "Iteration 29365 Loss: 0.8963642120361328\n",
      "Iteration 29366 Loss: 0.7047716379165649\n",
      "Iteration 29367 Loss: 1.0772751569747925\n",
      "Iteration 29368 Loss: 1.4309090375900269\n",
      "Iteration 29369 Loss: 1.0198005437850952\n",
      "Iteration 29369 Loss: 1.0050655603408813\n",
      "Iteration 29370 Loss: 0.7200617790222168\n",
      "Iteration 29371 Loss: 0.986332893371582\n",
      "Iteration 29372 Loss: 1.1642783880233765\n",
      "Iteration 29373 Loss: 0.900222897529602\n",
      "Iteration 29374 Loss: 1.0007308721542358\n",
      "Iteration 29375 Loss: 1.139057993888855\n",
      "Iteration 29376 Loss: 0.8372021913528442\n",
      "Iteration 29377 Loss: 1.004689335823059\n",
      "Iteration 29378 Loss: 1.0911110639572144\n",
      "Iteration 29379 Loss: 1.027849555015564\n",
      "Iteration 29379 Loss: 0.9871536493301392\n",
      "Iteration 29380 Loss: 0.9835488796234131\n",
      "Iteration 29381 Loss: 1.0814881324768066\n",
      "Iteration 29382 Loss: 1.040177345275879\n",
      "Iteration 29383 Loss: 1.2037290334701538\n",
      "Iteration 29384 Loss: 1.2797502279281616\n",
      "Iteration 29385 Loss: 1.394288182258606\n",
      "Iteration 29386 Loss: 0.998806357383728\n",
      "Iteration 29387 Loss: 1.0846920013427734\n",
      "Iteration 29388 Loss: 0.9234026074409485\n",
      "Iteration 29389 Loss: 0.8499611020088196\n",
      "Iteration 29389 Loss: 1.083984375\n",
      "Iteration 29390 Loss: 0.9509840607643127\n",
      "Iteration 29391 Loss: 1.0486449003219604\n",
      "Iteration 29392 Loss: 1.1323070526123047\n",
      "Iteration 29393 Loss: 1.0061651468276978\n",
      "Iteration 29394 Loss: 0.758118212223053\n",
      "Iteration 29395 Loss: 0.7341821193695068\n",
      "Iteration 29396 Loss: 1.0856624841690063\n",
      "Iteration 29397 Loss: 1.211532711982727\n",
      "Iteration 29398 Loss: 1.5231844186782837\n",
      "Iteration 29399 Loss: 1.3109147548675537\n",
      "Iteration 29399 Loss: 1.0761696100234985\n",
      "Iteration 29400 Loss: 1.0569087266921997\n",
      "Iteration 29401 Loss: 0.7899600267410278\n",
      "Iteration 29402 Loss: 1.2817033529281616\n",
      "Iteration 29403 Loss: 1.0270088911056519\n",
      "Iteration 29404 Loss: 1.089454174041748\n",
      "Iteration 29405 Loss: 1.0138059854507446\n",
      "Iteration 29406 Loss: 0.7876195907592773\n",
      "Iteration 29407 Loss: 1.0042539834976196\n",
      "Iteration 29408 Loss: 1.042737603187561\n",
      "Iteration 29409 Loss: 1.088603138923645\n",
      "Iteration 29409 Loss: 1.0182056427001953\n",
      "Iteration 29410 Loss: 1.015974760055542\n",
      "Iteration 29411 Loss: 1.296610951423645\n",
      "Iteration 29412 Loss: 1.3704153299331665\n",
      "Iteration 29413 Loss: 0.9392784833908081\n",
      "Iteration 29414 Loss: 1.0577824115753174\n",
      "Iteration 29415 Loss: 1.202052116394043\n",
      "Iteration 29416 Loss: 1.1690735816955566\n",
      "Iteration 29417 Loss: 1.0797981023788452\n",
      "Iteration 29418 Loss: 0.8912779092788696\n",
      "Iteration 29419 Loss: 0.889032781124115\n",
      "Iteration 29419 Loss: 1.0911296606063843\n",
      "Iteration 29420 Loss: 0.8783243894577026\n",
      "Iteration 29421 Loss: 1.0020787715911865\n",
      "Iteration 29422 Loss: 1.026738166809082\n",
      "Iteration 29423 Loss: 1.147600531578064\n",
      "Iteration 29424 Loss: 1.1619726419448853\n",
      "Iteration 29425 Loss: 1.0582678318023682\n",
      "Iteration 29426 Loss: 1.2711429595947266\n",
      "Iteration 29427 Loss: 1.0282104015350342\n",
      "Iteration 29428 Loss: 0.9316509366035461\n",
      "Iteration 29429 Loss: 0.9725633263587952\n",
      "Iteration 29429 Loss: 1.047855019569397\n",
      "Iteration 29430 Loss: 1.18893301486969\n",
      "Iteration 29431 Loss: 1.1385774612426758\n",
      "Iteration 29432 Loss: 0.8307456374168396\n",
      "Iteration 29433 Loss: 1.006836175918579\n",
      "Iteration 29434 Loss: 1.088005781173706\n",
      "Iteration 29435 Loss: 1.2211471796035767\n",
      "Iteration 29436 Loss: 1.362027645111084\n",
      "Iteration 29437 Loss: 1.0071579217910767\n",
      "Iteration 29438 Loss: 0.9787611961364746\n",
      "Iteration 29439 Loss: 1.138957142829895\n",
      "Iteration 29439 Loss: 1.0961148738861084\n",
      "Iteration 29440 Loss: 0.9353342652320862\n",
      "Iteration 29441 Loss: 0.9780232906341553\n",
      "Iteration 29442 Loss: 0.9761215448379517\n",
      "Iteration 29443 Loss: 1.2443664073944092\n",
      "Iteration 29444 Loss: 0.9809237122535706\n",
      "Iteration 29445 Loss: 1.191325306892395\n",
      "Iteration 29446 Loss: 1.3695704936981201\n",
      "Iteration 29447 Loss: 1.125672698020935\n",
      "Iteration 29448 Loss: 1.0577328205108643\n",
      "Iteration 29449 Loss: 1.1670180559158325\n",
      "Iteration 29449 Loss: 1.1026087999343872\n",
      "Iteration 29450 Loss: 1.1686060428619385\n",
      "Iteration 29451 Loss: 0.9593223333358765\n",
      "Iteration 29452 Loss: 1.1557815074920654\n",
      "Iteration 29453 Loss: 1.2351983785629272\n",
      "Iteration 29454 Loss: 1.4963730573654175\n",
      "Iteration 29455 Loss: 1.1657413244247437\n",
      "Iteration 29456 Loss: 1.0496453046798706\n",
      "Iteration 29457 Loss: 1.337438941001892\n",
      "Iteration 29458 Loss: 0.8723846673965454\n",
      "Iteration 29459 Loss: 0.9770916700363159\n",
      "Iteration 29459 Loss: 1.1417583227157593\n",
      "Iteration 29460 Loss: 0.7272617220878601\n",
      "Iteration 29461 Loss: 0.9452894926071167\n",
      "Iteration 29462 Loss: 0.690085232257843\n",
      "Iteration 29463 Loss: 1.0532289743423462\n",
      "Iteration 29464 Loss: 1.0976037979125977\n",
      "Iteration 29465 Loss: 1.051608681678772\n",
      "Iteration 29466 Loss: 0.9877355694770813\n",
      "Iteration 29467 Loss: 0.8159440159797668\n",
      "Iteration 29468 Loss: 0.9630792140960693\n",
      "Iteration 29469 Loss: 0.957771897315979\n",
      "Iteration 29469 Loss: 0.9289608001708984\n",
      "Iteration 29470 Loss: 0.9651724696159363\n",
      "Iteration 29471 Loss: 0.8923836350440979\n",
      "Iteration 29472 Loss: 1.14211106300354\n",
      "Iteration 29473 Loss: 0.9380686283111572\n",
      "Iteration 29474 Loss: 0.9073522090911865\n",
      "Iteration 29475 Loss: 0.9995350241661072\n",
      "Iteration 29476 Loss: 0.8661864995956421\n",
      "Iteration 29477 Loss: 0.8211347460746765\n",
      "Iteration 29478 Loss: 1.2084527015686035\n",
      "Iteration 29479 Loss: 1.2004584074020386\n",
      "Iteration 29479 Loss: 0.9940854907035828\n",
      "Iteration 29480 Loss: 1.0375993251800537\n",
      "Iteration 29481 Loss: 0.8491337895393372\n",
      "Iteration 29482 Loss: 1.041409969329834\n",
      "Iteration 29483 Loss: 0.8384619355201721\n",
      "Iteration 29484 Loss: 1.1034467220306396\n",
      "Iteration 29485 Loss: 1.3349689245224\n",
      "Iteration 29486 Loss: 0.8457752466201782\n",
      "Iteration 29487 Loss: 1.095935583114624\n",
      "Iteration 29488 Loss: 1.0288456678390503\n",
      "Iteration 29489 Loss: 0.8677905797958374\n",
      "Iteration 29489 Loss: 1.0043368339538574\n",
      "Iteration 29490 Loss: 1.0367357730865479\n",
      "Iteration 29491 Loss: 1.2997779846191406\n",
      "Iteration 29492 Loss: 1.3814761638641357\n",
      "Iteration 29493 Loss: 1.0038100481033325\n",
      "Iteration 29494 Loss: 0.9670312404632568\n",
      "Iteration 29495 Loss: 1.0911030769348145\n",
      "Iteration 29496 Loss: 0.577113926410675\n",
      "Iteration 29497 Loss: 1.1286665201187134\n",
      "Iteration 29498 Loss: 1.3294116258621216\n",
      "Iteration 29499 Loss: 0.7816397547721863\n",
      "Iteration 29499 Loss: 1.0596766471862793\n",
      "Iteration 29500 Loss: 1.1137973070144653\n",
      "Iteration 29501 Loss: 0.7451307773590088\n",
      "Iteration 29502 Loss: 0.7145595550537109\n",
      "Iteration 29503 Loss: 1.1011383533477783\n",
      "Iteration 29504 Loss: 0.8978464007377625\n",
      "Iteration 29505 Loss: 0.9651855230331421\n",
      "Iteration 29506 Loss: 1.037840723991394\n",
      "Iteration 29507 Loss: 0.7141336798667908\n",
      "Iteration 29508 Loss: 0.8213555812835693\n",
      "Iteration 29509 Loss: 0.8303304314613342\n",
      "Iteration 29509 Loss: 0.8941317796707153\n",
      "Iteration 29510 Loss: 0.8678596615791321\n",
      "Iteration 29511 Loss: 0.7806202173233032\n",
      "Iteration 29512 Loss: 1.1660727262496948\n",
      "Iteration 29513 Loss: 1.2562506198883057\n",
      "Iteration 29514 Loss: 1.1045888662338257\n",
      "Iteration 29515 Loss: 1.0541601181030273\n",
      "Iteration 29516 Loss: 0.9897929430007935\n",
      "Iteration 29517 Loss: 0.9467275738716125\n",
      "Iteration 29518 Loss: 1.2731199264526367\n",
      "Iteration 29519 Loss: 1.090766429901123\n",
      "Iteration 29519 Loss: 1.0529959201812744\n",
      "Iteration 29520 Loss: 1.4692025184631348\n",
      "Iteration 29521 Loss: 0.6270626187324524\n",
      "Iteration 29522 Loss: 1.007273554801941\n",
      "Iteration 29523 Loss: 1.1288893222808838\n",
      "Iteration 29524 Loss: 0.9741615653038025\n",
      "Iteration 29525 Loss: 1.0895205736160278\n",
      "Iteration 29526 Loss: 1.1055223941802979\n",
      "Iteration 29527 Loss: 1.0563528537750244\n",
      "Iteration 29528 Loss: 0.9194530248641968\n",
      "Iteration 29529 Loss: 1.1221972703933716\n",
      "Iteration 29529 Loss: 1.0499635934829712\n",
      "Iteration 29530 Loss: 1.197060465812683\n",
      "Iteration 29531 Loss: 0.9558469653129578\n",
      "Iteration 29532 Loss: 0.7738819718360901\n",
      "Iteration 29533 Loss: 0.9987525939941406\n",
      "Iteration 29534 Loss: 1.192838430404663\n",
      "Iteration 29535 Loss: 0.9060742259025574\n",
      "Iteration 29536 Loss: 1.01315176486969\n",
      "Iteration 29537 Loss: 1.2145485877990723\n",
      "Iteration 29538 Loss: 1.1966832876205444\n",
      "Iteration 29539 Loss: 1.016646146774292\n",
      "Iteration 29539 Loss: 1.0465484857559204\n",
      "Iteration 29540 Loss: 1.1209534406661987\n",
      "Iteration 29541 Loss: 1.404461145401001\n",
      "Iteration 29542 Loss: 1.0792251825332642\n",
      "Iteration 29543 Loss: 0.8437824249267578\n",
      "Iteration 29544 Loss: 1.1315470933914185\n",
      "Iteration 29545 Loss: 1.2520440816879272\n",
      "Iteration 29546 Loss: 0.9986129999160767\n",
      "Iteration 29547 Loss: 1.0587527751922607\n",
      "Iteration 29548 Loss: 0.9771894216537476\n",
      "Iteration 29549 Loss: 1.2614061832427979\n",
      "Iteration 29549 Loss: 1.112797498703003\n",
      "Iteration 29550 Loss: 0.801179051399231\n",
      "Iteration 29551 Loss: 1.2674059867858887\n",
      "Iteration 29552 Loss: 0.8266226649284363\n",
      "Iteration 29553 Loss: 0.9056084156036377\n",
      "Iteration 29554 Loss: 0.9479310512542725\n",
      "Iteration 29555 Loss: 1.02336847782135\n",
      "Iteration 29556 Loss: 0.8452670574188232\n",
      "Iteration 29557 Loss: 1.0770817995071411\n",
      "Iteration 29558 Loss: 1.2987909317016602\n",
      "Iteration 29559 Loss: 1.2593398094177246\n",
      "Iteration 29559 Loss: 1.0252596139907837\n",
      "Iteration 29560 Loss: 0.7971877455711365\n",
      "Iteration 29561 Loss: 0.8048974871635437\n",
      "Iteration 29562 Loss: 1.3149933815002441\n",
      "Iteration 29563 Loss: 0.6907398700714111\n",
      "Iteration 29564 Loss: 1.021340012550354\n",
      "Iteration 29565 Loss: 0.8338564038276672\n",
      "Iteration 29566 Loss: 1.0071659088134766\n",
      "Iteration 29567 Loss: 1.291780710220337\n",
      "Iteration 29568 Loss: 0.7031145691871643\n",
      "Iteration 29569 Loss: 0.989647388458252\n",
      "Iteration 29569 Loss: 0.9454723596572876\n",
      "Iteration 29570 Loss: 1.1260327100753784\n",
      "Iteration 29571 Loss: 1.2688202857971191\n",
      "Iteration 29572 Loss: 0.8475443720817566\n",
      "Iteration 29573 Loss: 0.8580648303031921\n",
      "Iteration 29574 Loss: 1.1757010221481323\n",
      "Iteration 29575 Loss: 1.120451807975769\n",
      "Iteration 29576 Loss: 1.1267919540405273\n",
      "Iteration 29577 Loss: 0.9346189498901367\n",
      "Iteration 29578 Loss: 1.108398675918579\n",
      "Iteration 29579 Loss: 1.1384660005569458\n",
      "Iteration 29579 Loss: 1.0704890489578247\n",
      "Iteration 29580 Loss: 1.514625072479248\n",
      "Iteration 29581 Loss: 0.9442486763000488\n",
      "Iteration 29582 Loss: 0.8349543213844299\n",
      "Iteration 29583 Loss: 0.8963715434074402\n",
      "Iteration 29584 Loss: 0.8891420364379883\n",
      "Iteration 29585 Loss: 1.062737226486206\n",
      "Iteration 29586 Loss: 0.7799272537231445\n",
      "Iteration 29587 Loss: 1.0831892490386963\n",
      "Iteration 29588 Loss: 1.0492700338363647\n",
      "Iteration 29589 Loss: 0.9348974227905273\n",
      "Iteration 29589 Loss: 0.9989362955093384\n",
      "Iteration 29590 Loss: 1.3920152187347412\n",
      "Iteration 29591 Loss: 1.1456034183502197\n",
      "Iteration 29592 Loss: 0.9580477476119995\n",
      "Iteration 29593 Loss: 0.9627318978309631\n",
      "Iteration 29594 Loss: 1.2434184551239014\n",
      "Iteration 29595 Loss: 1.0693762302398682\n",
      "Iteration 29596 Loss: 1.1461116075515747\n",
      "Iteration 29597 Loss: 1.153752326965332\n",
      "Iteration 29598 Loss: 0.6908121705055237\n",
      "Iteration 29599 Loss: 0.7742022275924683\n",
      "Iteration 29599 Loss: 1.0536071062088013\n",
      "Iteration 29600 Loss: 0.8663283586502075\n",
      "Iteration 29601 Loss: 1.065029501914978\n",
      "Iteration 29602 Loss: 1.0297688245773315\n",
      "Iteration 29603 Loss: 0.9211140871047974\n",
      "Iteration 29604 Loss: 0.9993559122085571\n",
      "Iteration 29605 Loss: 0.6564949750900269\n",
      "Iteration 29606 Loss: 1.0196131467819214\n",
      "Iteration 29607 Loss: 1.3938685655593872\n",
      "Iteration 29608 Loss: 0.8540491461753845\n",
      "Iteration 29609 Loss: 0.9223830103874207\n",
      "Iteration 29609 Loss: 0.9728005528450012\n",
      "Iteration 29610 Loss: 0.8673433065414429\n",
      "Iteration 29611 Loss: 1.5352753400802612\n",
      "Iteration 29612 Loss: 0.9090604782104492\n",
      "Iteration 29613 Loss: 1.2214791774749756\n",
      "Iteration 29614 Loss: 1.0834285020828247\n",
      "Iteration 29615 Loss: 0.9884901642799377\n",
      "Iteration 29616 Loss: 1.1756458282470703\n",
      "Iteration 29617 Loss: 1.1073814630508423\n",
      "Iteration 29618 Loss: 0.6197919845581055\n",
      "Iteration 29619 Loss: 0.931544303894043\n",
      "Iteration 29619 Loss: 1.0439441204071045\n",
      "Iteration 29620 Loss: 0.8806037306785583\n",
      "Iteration 29621 Loss: 0.8538305759429932\n",
      "Iteration 29622 Loss: 1.0105928182601929\n",
      "Iteration 29623 Loss: 1.1163606643676758\n",
      "Iteration 29624 Loss: 1.1118401288986206\n",
      "Iteration 29625 Loss: 1.1327685117721558\n",
      "Iteration 29626 Loss: 1.3716505765914917\n",
      "Iteration 29627 Loss: 0.9015464782714844\n",
      "Iteration 29628 Loss: 1.1408228874206543\n",
      "Iteration 29629 Loss: 1.0667513608932495\n",
      "Iteration 29629 Loss: 1.058676838874817\n",
      "Iteration 29630 Loss: 1.0471982955932617\n",
      "Iteration 29631 Loss: 0.893511176109314\n",
      "Iteration 29632 Loss: 1.0481928586959839\n",
      "Iteration 29633 Loss: 1.0357619524002075\n",
      "Iteration 29634 Loss: 0.6344446539878845\n",
      "Iteration 29635 Loss: 1.1732999086380005\n",
      "Iteration 29636 Loss: 1.0294052362442017\n",
      "Iteration 29637 Loss: 0.8647108674049377\n",
      "Iteration 29638 Loss: 0.7382313013076782\n",
      "Iteration 29639 Loss: 1.3748371601104736\n",
      "Iteration 29639 Loss: 0.9839593768119812\n",
      "Iteration 29640 Loss: 1.0877913236618042\n",
      "Iteration 29641 Loss: 0.7778741717338562\n",
      "Iteration 29642 Loss: 0.9882768988609314\n",
      "Iteration 29643 Loss: 1.0894367694854736\n",
      "Iteration 29644 Loss: 1.0063130855560303\n",
      "Iteration 29645 Loss: 0.9530324935913086\n",
      "Iteration 29646 Loss: 0.7335398197174072\n",
      "Iteration 29647 Loss: 0.8620471954345703\n",
      "Iteration 29648 Loss: 0.9566176533699036\n",
      "Iteration 29649 Loss: 1.298592448234558\n",
      "Iteration 29649 Loss: 0.9753521680831909\n",
      "Iteration 29650 Loss: 0.7965071797370911\n",
      "Iteration 29651 Loss: 1.1380157470703125\n",
      "Iteration 29652 Loss: 1.2522215843200684\n",
      "Iteration 29653 Loss: 0.8255342245101929\n",
      "Iteration 29654 Loss: 1.4195711612701416\n",
      "Iteration 29655 Loss: 1.4989031553268433\n",
      "Iteration 29656 Loss: 1.0746161937713623\n",
      "Iteration 29657 Loss: 0.9436514973640442\n",
      "Iteration 29658 Loss: 1.3132908344268799\n",
      "Iteration 29659 Loss: 0.8400141000747681\n",
      "Iteration 29659 Loss: 1.1102325916290283\n",
      "Iteration 29660 Loss: 1.0964083671569824\n",
      "Iteration 29661 Loss: 1.143452763557434\n",
      "Iteration 29662 Loss: 1.218872308731079\n",
      "Iteration 29663 Loss: 0.9641546607017517\n",
      "Iteration 29664 Loss: 1.2138689756393433\n",
      "Iteration 29665 Loss: 1.0873712301254272\n",
      "Iteration 29666 Loss: 0.9079151153564453\n",
      "Iteration 29667 Loss: 0.7476034760475159\n",
      "Iteration 29668 Loss: 0.905021607875824\n",
      "Iteration 29669 Loss: 0.9331905841827393\n",
      "Iteration 29669 Loss: 1.0217859745025635\n",
      "Iteration 29670 Loss: 0.863561749458313\n",
      "Iteration 29671 Loss: 1.0139609575271606\n",
      "Iteration 29672 Loss: 1.0463027954101562\n",
      "Iteration 29673 Loss: 1.0354864597320557\n",
      "Iteration 29674 Loss: 1.2239398956298828\n",
      "Iteration 29675 Loss: 1.2144603729248047\n",
      "Iteration 29676 Loss: 1.0907622575759888\n",
      "Iteration 29677 Loss: 0.8128713965415955\n",
      "Iteration 29678 Loss: 1.0704419612884521\n",
      "Iteration 29679 Loss: 0.8993692398071289\n",
      "Iteration 29679 Loss: 1.0271155834197998\n",
      "Iteration 29680 Loss: 0.6264286637306213\n",
      "Iteration 29681 Loss: 0.7631537318229675\n",
      "Iteration 29682 Loss: 1.0813332796096802\n",
      "Iteration 29683 Loss: 0.9775015115737915\n",
      "Iteration 29684 Loss: 1.0650036334991455\n",
      "Iteration 29685 Loss: 0.751579225063324\n",
      "Iteration 29686 Loss: 0.9943545460700989\n",
      "Iteration 29687 Loss: 0.979266345500946\n",
      "Iteration 29688 Loss: 0.7859386205673218\n",
      "Iteration 29689 Loss: 1.0318472385406494\n",
      "Iteration 29689 Loss: 0.9056406021118164\n",
      "Iteration 29690 Loss: 0.9771237969398499\n",
      "Iteration 29691 Loss: 1.0550588369369507\n",
      "Iteration 29692 Loss: 1.2341609001159668\n",
      "Iteration 29693 Loss: 1.164759635925293\n",
      "Iteration 29694 Loss: 0.9083434343338013\n",
      "Iteration 29695 Loss: 1.0971943140029907\n",
      "Iteration 29696 Loss: 1.0464767217636108\n",
      "Iteration 29697 Loss: 0.852032482624054\n",
      "Iteration 29698 Loss: 1.246584177017212\n",
      "Iteration 29699 Loss: 0.8685874938964844\n",
      "Iteration 29699 Loss: 1.0450321435928345\n",
      "Iteration 29700 Loss: 1.0164014101028442\n",
      "Iteration 29701 Loss: 1.23761785030365\n",
      "Iteration 29702 Loss: 1.1581389904022217\n",
      "Iteration 29703 Loss: 1.3671417236328125\n",
      "Iteration 29704 Loss: 1.100864291191101\n",
      "Iteration 29705 Loss: 1.0122040510177612\n",
      "Iteration 29706 Loss: 1.176133394241333\n",
      "Iteration 29707 Loss: 1.5122134685516357\n",
      "Iteration 29708 Loss: 0.7599061727523804\n",
      "Iteration 29709 Loss: 0.799786388874054\n",
      "Iteration 29709 Loss: 1.1140408515930176\n",
      "Iteration 29710 Loss: 0.8680857419967651\n",
      "Iteration 29711 Loss: 0.9896724820137024\n",
      "Iteration 29712 Loss: 0.8263860940933228\n",
      "Iteration 29713 Loss: 1.078559160232544\n",
      "Iteration 29714 Loss: 1.073848843574524\n",
      "Iteration 29715 Loss: 1.0974417924880981\n",
      "Iteration 29716 Loss: 0.8552517890930176\n",
      "Iteration 29717 Loss: 0.9531077146530151\n",
      "Iteration 29718 Loss: 1.2979226112365723\n",
      "Iteration 29719 Loss: 1.1451135873794556\n",
      "Iteration 29719 Loss: 1.0185390710830688\n",
      "Iteration 29720 Loss: 1.129181146621704\n",
      "Iteration 29721 Loss: 1.4106409549713135\n",
      "Iteration 29722 Loss: 0.9749511480331421\n",
      "Iteration 29723 Loss: 0.7758162617683411\n",
      "Iteration 29724 Loss: 0.8857944011688232\n",
      "Iteration 29725 Loss: 0.779093861579895\n",
      "Iteration 29726 Loss: 0.903967022895813\n",
      "Iteration 29727 Loss: 1.0419228076934814\n",
      "Iteration 29728 Loss: 0.7700263261795044\n",
      "Iteration 29729 Loss: 1.0404174327850342\n",
      "Iteration 29729 Loss: 0.9711810946464539\n",
      "Iteration 29730 Loss: 0.9882361888885498\n",
      "Iteration 29731 Loss: 1.1584758758544922\n",
      "Iteration 29732 Loss: 1.2829136848449707\n",
      "Iteration 29733 Loss: 0.9916108846664429\n",
      "Iteration 29734 Loss: 1.018808364868164\n",
      "Iteration 29735 Loss: 0.8449397087097168\n",
      "Iteration 29736 Loss: 1.217181921005249\n",
      "Iteration 29737 Loss: 0.7437636852264404\n",
      "Iteration 29738 Loss: 1.1049926280975342\n",
      "Iteration 29739 Loss: 0.9179238080978394\n",
      "Iteration 29739 Loss: 1.0268847942352295\n",
      "Iteration 29740 Loss: 0.7447327375411987\n",
      "Iteration 29741 Loss: 0.9220885038375854\n",
      "Iteration 29742 Loss: 1.235946774482727\n",
      "Iteration 29743 Loss: 1.2209221124649048\n",
      "Iteration 29744 Loss: 1.156566858291626\n",
      "Iteration 29745 Loss: 0.714665412902832\n",
      "Iteration 29746 Loss: 0.9940534830093384\n",
      "Iteration 29747 Loss: 1.1402387619018555\n",
      "Iteration 29748 Loss: 0.8718950748443604\n",
      "Iteration 29749 Loss: 0.8627796173095703\n",
      "Iteration 29749 Loss: 0.9863889813423157\n",
      "Iteration 29750 Loss: 1.1816205978393555\n",
      "Iteration 29751 Loss: 0.8441617488861084\n",
      "Iteration 29752 Loss: 0.8236862421035767\n",
      "Iteration 29753 Loss: 0.883230447769165\n",
      "Iteration 29754 Loss: 1.1364861726760864\n",
      "Iteration 29755 Loss: 0.8209804892539978\n",
      "Iteration 29756 Loss: 0.6054739952087402\n",
      "Iteration 29757 Loss: 0.9355602860450745\n",
      "Iteration 29758 Loss: 1.2926123142242432\n",
      "Iteration 29759 Loss: 0.9775551557540894\n",
      "Iteration 29759 Loss: 0.9501367807388306\n",
      "Iteration 29760 Loss: 0.7508822679519653\n",
      "Iteration 29761 Loss: 0.8046404123306274\n",
      "Iteration 29762 Loss: 0.9820144176483154\n",
      "Iteration 29763 Loss: 0.6895937323570251\n",
      "Iteration 29764 Loss: 0.8797860741615295\n",
      "Iteration 29765 Loss: 0.8057618737220764\n",
      "Iteration 29766 Loss: 1.1554683446884155\n",
      "Iteration 29767 Loss: 1.0376962423324585\n",
      "Iteration 29768 Loss: 0.8423187732696533\n",
      "Iteration 29769 Loss: 0.9596400260925293\n",
      "Iteration 29769 Loss: 0.8907801508903503\n",
      "Iteration 29770 Loss: 1.1016535758972168\n",
      "Iteration 29771 Loss: 1.0495374202728271\n",
      "Iteration 29772 Loss: 1.0478143692016602\n",
      "Iteration 29773 Loss: 0.9289716482162476\n",
      "Iteration 29774 Loss: 1.1197031736373901\n",
      "Iteration 29775 Loss: 0.9899060726165771\n",
      "Iteration 29776 Loss: 1.305134892463684\n",
      "Iteration 29777 Loss: 1.3887673616409302\n",
      "Iteration 29778 Loss: 1.4912506341934204\n",
      "Iteration 29779 Loss: 0.9244585633277893\n",
      "Iteration 29779 Loss: 1.134719729423523\n",
      "Iteration 29780 Loss: 1.0230374336242676\n",
      "Iteration 29781 Loss: 0.6334661841392517\n",
      "Iteration 29782 Loss: 0.9640865921974182\n",
      "Iteration 29783 Loss: 0.9647268652915955\n",
      "Iteration 29784 Loss: 0.8513851165771484\n",
      "Iteration 29785 Loss: 0.9223432540893555\n",
      "Iteration 29786 Loss: 0.9458082318305969\n",
      "Iteration 29787 Loss: 1.374194622039795\n",
      "Iteration 29788 Loss: 1.1497904062271118\n",
      "Iteration 29789 Loss: 1.0840067863464355\n",
      "Iteration 29789 Loss: 0.9912845492362976\n",
      "Iteration 29790 Loss: 0.9044957160949707\n",
      "Iteration 29791 Loss: 1.0606036186218262\n",
      "Iteration 29792 Loss: 0.8056302070617676\n",
      "Iteration 29793 Loss: 1.2297239303588867\n",
      "Iteration 29794 Loss: 0.975610613822937\n",
      "Iteration 29795 Loss: 0.947106659412384\n",
      "Iteration 29796 Loss: 0.9672067761421204\n",
      "Iteration 29797 Loss: 1.438165307044983\n",
      "Iteration 29798 Loss: 1.3621826171875\n",
      "Iteration 29799 Loss: 1.095339298248291\n",
      "Iteration 29799 Loss: 1.0786064863204956\n",
      "Iteration 29800 Loss: 0.8340045213699341\n",
      "Iteration 29801 Loss: 1.044608473777771\n",
      "Iteration 29802 Loss: 0.9216778874397278\n",
      "Iteration 29803 Loss: 0.9030730724334717\n",
      "Iteration 29804 Loss: 1.305782437324524\n",
      "Iteration 29805 Loss: 0.8819804787635803\n",
      "Iteration 29806 Loss: 1.2475460767745972\n",
      "Iteration 29807 Loss: 0.9636034965515137\n",
      "Iteration 29808 Loss: 1.3842532634735107\n",
      "Iteration 29809 Loss: 1.212000846862793\n",
      "Iteration 29809 Loss: 1.0698531866073608\n",
      "Iteration 29810 Loss: 0.9672216773033142\n",
      "Iteration 29811 Loss: 1.0226123332977295\n",
      "Iteration 29812 Loss: 0.9340142011642456\n",
      "Iteration 29813 Loss: 1.425564169883728\n",
      "Iteration 29814 Loss: 1.0710419416427612\n",
      "Iteration 29815 Loss: 1.2670844793319702\n",
      "Iteration 29816 Loss: 0.8541357517242432\n",
      "Iteration 29817 Loss: 0.841957151889801\n",
      "Iteration 29818 Loss: 0.8043780326843262\n",
      "Iteration 29819 Loss: 1.0477380752563477\n",
      "Iteration 29819 Loss: 1.023574709892273\n",
      "Iteration 29820 Loss: 1.2183114290237427\n",
      "Iteration 29821 Loss: 1.1057754755020142\n",
      "Iteration 29822 Loss: 1.049465537071228\n",
      "Iteration 29823 Loss: 1.1836223602294922\n",
      "Iteration 29824 Loss: 0.7602115869522095\n",
      "Iteration 29825 Loss: 0.8660339117050171\n",
      "Iteration 29826 Loss: 1.1207501888275146\n",
      "Iteration 29827 Loss: 0.947401225566864\n",
      "Iteration 29828 Loss: 0.7804616093635559\n",
      "Iteration 29829 Loss: 0.9165550470352173\n",
      "Iteration 29829 Loss: 0.9948588609695435\n",
      "Iteration 29830 Loss: 0.9491401314735413\n",
      "Iteration 29831 Loss: 1.2045577764511108\n",
      "Iteration 29832 Loss: 0.8351234793663025\n",
      "Iteration 29833 Loss: 0.9291491508483887\n",
      "Iteration 29834 Loss: 0.923176646232605\n",
      "Iteration 29835 Loss: 1.048249363899231\n",
      "Iteration 29836 Loss: 1.1927937269210815\n",
      "Iteration 29837 Loss: 0.7071341872215271\n",
      "Iteration 29838 Loss: 1.0496779680252075\n",
      "Iteration 29839 Loss: 0.8152326941490173\n",
      "Iteration 29839 Loss: 0.9654234647750854\n",
      "Iteration 29840 Loss: 1.088626503944397\n",
      "Iteration 29841 Loss: 1.1381384134292603\n",
      "Iteration 29842 Loss: 0.9099524617195129\n",
      "Iteration 29843 Loss: 0.9363475441932678\n",
      "Iteration 29844 Loss: 0.9739876985549927\n",
      "Iteration 29845 Loss: 1.0525760650634766\n",
      "Iteration 29846 Loss: 1.064387321472168\n",
      "Iteration 29847 Loss: 0.9587727189064026\n",
      "Iteration 29848 Loss: 1.1763886213302612\n",
      "Iteration 29849 Loss: 1.3613046407699585\n",
      "Iteration 29849 Loss: 1.0660481452941895\n",
      "Iteration 29850 Loss: 1.1411677598953247\n",
      "Iteration 29851 Loss: 1.213917851448059\n",
      "Iteration 29852 Loss: 1.0750435590744019\n",
      "Iteration 29853 Loss: 1.1229429244995117\n",
      "Iteration 29854 Loss: 1.0684499740600586\n",
      "Iteration 29855 Loss: 1.1318769454956055\n",
      "Iteration 29856 Loss: 1.033312201499939\n",
      "Iteration 29857 Loss: 1.3912817239761353\n",
      "Iteration 29858 Loss: 1.259950041770935\n",
      "Iteration 29859 Loss: 1.190912127494812\n",
      "Iteration 29859 Loss: 1.1628854274749756\n",
      "Iteration 29860 Loss: 1.2340421676635742\n",
      "Iteration 29861 Loss: 0.8007878661155701\n",
      "Iteration 29862 Loss: 1.1464157104492188\n",
      "Iteration 29863 Loss: 1.0347323417663574\n",
      "Iteration 29864 Loss: 1.2072426080703735\n",
      "Iteration 29865 Loss: 0.9804857969284058\n",
      "Iteration 29866 Loss: 1.4019817113876343\n",
      "Iteration 29867 Loss: 0.8922768831253052\n",
      "Iteration 29868 Loss: 0.9894009828567505\n",
      "Iteration 29869 Loss: 1.0770288705825806\n",
      "Iteration 29869 Loss: 1.076439380645752\n",
      "Iteration 29870 Loss: 1.0114402770996094\n",
      "Iteration 29871 Loss: 1.0858705043792725\n",
      "Iteration 29872 Loss: 1.114784598350525\n",
      "Iteration 29873 Loss: 1.0079560279846191\n",
      "Iteration 29874 Loss: 1.289568305015564\n",
      "Iteration 29875 Loss: 1.1126017570495605\n",
      "Iteration 29876 Loss: 1.077284574508667\n",
      "Iteration 29877 Loss: 0.8861321806907654\n",
      "Iteration 29878 Loss: 1.243021845817566\n",
      "Iteration 29879 Loss: 0.9241414070129395\n",
      "Iteration 29879 Loss: 1.0752801895141602\n",
      "Iteration 29880 Loss: 0.8132756352424622\n",
      "Iteration 29881 Loss: 0.6961964964866638\n",
      "Iteration 29882 Loss: 0.8477327823638916\n",
      "Iteration 29883 Loss: 0.8660539388656616\n",
      "Iteration 29884 Loss: 0.9778162240982056\n",
      "Iteration 29885 Loss: 0.76214200258255\n",
      "Iteration 29886 Loss: 1.0070672035217285\n",
      "Iteration 29887 Loss: 1.0812541246414185\n",
      "Iteration 29888 Loss: 0.9871616363525391\n",
      "Iteration 29889 Loss: 0.9701951742172241\n",
      "Iteration 29889 Loss: 0.9008895754814148\n",
      "Iteration 29890 Loss: 1.240769863128662\n",
      "Iteration 29891 Loss: 0.6978543400764465\n",
      "Iteration 29892 Loss: 0.9669294357299805\n",
      "Iteration 29893 Loss: 0.9865254163742065\n",
      "Iteration 29894 Loss: 1.1229815483093262\n",
      "Iteration 29895 Loss: 1.1597572565078735\n",
      "Iteration 29896 Loss: 1.0586944818496704\n",
      "Iteration 29897 Loss: 0.9389511346817017\n",
      "Iteration 29898 Loss: 1.1530046463012695\n",
      "Iteration 29899 Loss: 0.944610595703125\n",
      "Iteration 29899 Loss: 1.0270079374313354\n",
      "Iteration 29900 Loss: 0.6767253875732422\n",
      "Iteration 29901 Loss: 1.0097432136535645\n",
      "Iteration 29902 Loss: 0.6799272894859314\n",
      "Iteration 29903 Loss: 0.9154446125030518\n",
      "Iteration 29904 Loss: 1.2973130941390991\n",
      "Iteration 29905 Loss: 1.0245405435562134\n",
      "Iteration 29906 Loss: 0.7758409380912781\n",
      "Iteration 29907 Loss: 1.3535577058792114\n",
      "Iteration 29908 Loss: 1.099562644958496\n",
      "Iteration 29909 Loss: 0.816741943359375\n",
      "Iteration 29909 Loss: 0.9649397134780884\n",
      "Iteration 29910 Loss: 0.8661828637123108\n",
      "Iteration 29911 Loss: 1.157318115234375\n",
      "Iteration 29912 Loss: 0.7780096530914307\n",
      "Iteration 29913 Loss: 0.5288019776344299\n",
      "Iteration 29914 Loss: 1.1712294816970825\n",
      "Iteration 29915 Loss: 0.8369826674461365\n",
      "Iteration 29916 Loss: 1.1100353002548218\n",
      "Iteration 29917 Loss: 1.2684093713760376\n",
      "Iteration 29918 Loss: 1.0268129110336304\n",
      "Iteration 29919 Loss: 0.9808040857315063\n",
      "Iteration 29919 Loss: 0.9724586606025696\n",
      "Iteration 29920 Loss: 0.6819335222244263\n",
      "Iteration 29921 Loss: 1.1790697574615479\n",
      "Iteration 29922 Loss: 0.9677508473396301\n",
      "Iteration 29923 Loss: 1.5471440553665161\n",
      "Iteration 29924 Loss: 0.9392115473747253\n",
      "Iteration 29925 Loss: 0.9442918300628662\n",
      "Iteration 29926 Loss: 1.0087270736694336\n",
      "Iteration 29927 Loss: 1.2117887735366821\n",
      "Iteration 29928 Loss: 0.9133978486061096\n",
      "Iteration 29929 Loss: 1.1397322416305542\n",
      "Iteration 29929 Loss: 1.0533047914505005\n",
      "Iteration 29930 Loss: 0.9705346822738647\n",
      "Iteration 29931 Loss: 1.400331735610962\n",
      "Iteration 29932 Loss: 1.0718201398849487\n",
      "Iteration 29933 Loss: 0.7891099452972412\n",
      "Iteration 29934 Loss: 1.1087238788604736\n",
      "Iteration 29935 Loss: 0.891662061214447\n",
      "Iteration 29936 Loss: 1.0392475128173828\n",
      "Iteration 29937 Loss: 0.9091746807098389\n",
      "Iteration 29938 Loss: 0.9621127843856812\n",
      "Iteration 29939 Loss: 1.1891283988952637\n",
      "Iteration 29939 Loss: 1.0331846475601196\n",
      "Iteration 29940 Loss: 0.8506184220314026\n",
      "Iteration 29941 Loss: 0.6651156544685364\n",
      "Iteration 29942 Loss: 0.7048354744911194\n",
      "Iteration 29943 Loss: 0.7518576979637146\n",
      "Iteration 29944 Loss: 1.1975326538085938\n",
      "Iteration 29945 Loss: 1.095954179763794\n",
      "Iteration 29946 Loss: 0.5552445650100708\n",
      "Iteration 29947 Loss: 1.4197124242782593\n",
      "Iteration 29948 Loss: 0.9369465112686157\n",
      "Iteration 29949 Loss: 0.8227726817131042\n",
      "Iteration 29949 Loss: 0.900058925151825\n",
      "Iteration 29950 Loss: 1.0960803031921387\n",
      "Iteration 29951 Loss: 0.9233772158622742\n",
      "Iteration 29952 Loss: 0.7533742785453796\n",
      "Iteration 29953 Loss: 0.9166257977485657\n",
      "Iteration 29954 Loss: 1.0127716064453125\n",
      "Iteration 29955 Loss: 0.8622767329216003\n",
      "Iteration 29956 Loss: 1.2549233436584473\n",
      "Iteration 29957 Loss: 1.1520333290100098\n",
      "Iteration 29958 Loss: 1.2021855115890503\n",
      "Iteration 29959 Loss: 0.7440375089645386\n",
      "Iteration 29959 Loss: 0.991768479347229\n",
      "Iteration 29960 Loss: 0.9490689039230347\n",
      "Iteration 29961 Loss: 1.0940170288085938\n",
      "Iteration 29962 Loss: 1.078395962715149\n",
      "Iteration 29963 Loss: 1.464416742324829\n",
      "Iteration 29964 Loss: 0.7337527871131897\n",
      "Iteration 29965 Loss: 0.9037079215049744\n",
      "Iteration 29966 Loss: 1.0486339330673218\n",
      "Iteration 29967 Loss: 1.1208291053771973\n",
      "Iteration 29968 Loss: 1.3793015480041504\n",
      "Iteration 29969 Loss: 1.2774440050125122\n",
      "Iteration 29969 Loss: 1.104956865310669\n",
      "Iteration 29970 Loss: 1.0649747848510742\n",
      "Iteration 29971 Loss: 0.9223740100860596\n",
      "Iteration 29972 Loss: 1.0199222564697266\n",
      "Iteration 29973 Loss: 1.1263837814331055\n",
      "Iteration 29974 Loss: 1.1761645078659058\n",
      "Iteration 29975 Loss: 1.1751823425292969\n",
      "Iteration 29976 Loss: 1.2793810367584229\n",
      "Iteration 29977 Loss: 0.8233187198638916\n",
      "Iteration 29978 Loss: 1.0903760194778442\n",
      "Iteration 29979 Loss: 0.9861577749252319\n",
      "Iteration 29979 Loss: 1.0664235353469849\n",
      "Iteration 29980 Loss: 1.2223808765411377\n",
      "Iteration 29981 Loss: 1.091443419456482\n",
      "Iteration 29982 Loss: 1.1168804168701172\n",
      "Iteration 29983 Loss: 1.0567419528961182\n",
      "Iteration 29984 Loss: 0.9121943116188049\n",
      "Iteration 29985 Loss: 1.1448214054107666\n",
      "Iteration 29986 Loss: 0.9893243908882141\n",
      "Iteration 29987 Loss: 1.2090157270431519\n",
      "Iteration 29988 Loss: 0.9781064391136169\n",
      "Iteration 29989 Loss: 1.009272813796997\n",
      "Iteration 29989 Loss: 1.073018193244934\n",
      "Iteration 29990 Loss: 0.834831714630127\n",
      "Iteration 29991 Loss: 1.1370782852172852\n",
      "Iteration 29992 Loss: 0.8224837183952332\n",
      "Iteration 29993 Loss: 1.137549638748169\n",
      "Iteration 29994 Loss: 0.8549143671989441\n",
      "Iteration 29995 Loss: 1.292743444442749\n",
      "Iteration 29996 Loss: 1.2486522197723389\n",
      "Iteration 29997 Loss: 1.087917447090149\n",
      "Iteration 29998 Loss: 0.747795820236206\n",
      "Iteration 29999 Loss: 1.0155292749404907\n",
      "Iteration 29999 Loss: 1.0179495811462402\n",
      "Iteration 30000 Loss: 1.0973222255706787\n",
      "Iteration 30001 Loss: 1.3093786239624023\n",
      "Iteration 30002 Loss: 0.9452639222145081\n",
      "Iteration 30003 Loss: 0.667880654335022\n",
      "Iteration 30004 Loss: 1.1205627918243408\n",
      "Iteration 30005 Loss: 1.344310998916626\n",
      "Iteration 30006 Loss: 1.0416100025177002\n",
      "Iteration 30007 Loss: 0.7800804972648621\n",
      "Iteration 30008 Loss: 0.8093271255493164\n",
      "Iteration 30009 Loss: 1.1022589206695557\n",
      "Iteration 30009 Loss: 1.0217995643615723\n",
      "Iteration 30010 Loss: 0.8892852663993835\n",
      "Iteration 30011 Loss: 0.8815908432006836\n",
      "Iteration 30012 Loss: 0.7958707213401794\n",
      "Iteration 30013 Loss: 1.1099879741668701\n",
      "Iteration 30014 Loss: 1.2386105060577393\n",
      "Iteration 30015 Loss: 1.2908066511154175\n",
      "Iteration 30016 Loss: 0.9586638808250427\n",
      "Iteration 30017 Loss: 0.9493897557258606\n",
      "Iteration 30018 Loss: 0.7687288522720337\n",
      "Iteration 30019 Loss: 0.8740742802619934\n",
      "Iteration 30019 Loss: 0.975700855255127\n",
      "Iteration 30020 Loss: 0.8504189848899841\n",
      "Iteration 30021 Loss: 1.02211332321167\n",
      "Iteration 30022 Loss: 1.1105223894119263\n",
      "Iteration 30023 Loss: 0.6626240611076355\n",
      "Iteration 30024 Loss: 1.2383389472961426\n",
      "Iteration 30025 Loss: 0.9735957384109497\n",
      "Iteration 30026 Loss: 1.214812159538269\n",
      "Iteration 30027 Loss: 0.8355109691619873\n",
      "Iteration 30028 Loss: 0.9566015005111694\n",
      "Iteration 30029 Loss: 0.8944172263145447\n",
      "Iteration 30029 Loss: 0.9758955240249634\n",
      "Iteration 30030 Loss: 1.3564112186431885\n",
      "Iteration 30031 Loss: 0.8713629245758057\n",
      "Iteration 30032 Loss: 1.0238299369812012\n",
      "Iteration 30033 Loss: 0.9822981953620911\n",
      "Iteration 30034 Loss: 0.9728422164916992\n",
      "Iteration 30035 Loss: 1.3348896503448486\n",
      "Iteration 30036 Loss: 1.034090280532837\n",
      "Iteration 30037 Loss: 0.9421069622039795\n",
      "Iteration 30038 Loss: 1.0693351030349731\n",
      "Iteration 30039 Loss: 0.746728241443634\n",
      "Iteration 30039 Loss: 1.0333894491195679\n",
      "Iteration 30040 Loss: 1.2041560411453247\n",
      "Iteration 30041 Loss: 0.8120902180671692\n",
      "Iteration 30042 Loss: 0.8634901642799377\n",
      "Iteration 30043 Loss: 0.9671804904937744\n",
      "Iteration 30044 Loss: 1.1000990867614746\n",
      "Iteration 30045 Loss: 1.1373693943023682\n",
      "Iteration 30046 Loss: 1.2604941129684448\n",
      "Iteration 30047 Loss: 0.6610077619552612\n",
      "Iteration 30048 Loss: 0.9579460620880127\n",
      "Iteration 30049 Loss: 0.7821491956710815\n",
      "Iteration 30049 Loss: 0.9745982885360718\n",
      "Iteration 30050 Loss: 0.9493757486343384\n",
      "Iteration 30051 Loss: 0.823790967464447\n",
      "Iteration 30052 Loss: 0.9418872594833374\n",
      "Iteration 30053 Loss: 0.7650861740112305\n",
      "Iteration 30054 Loss: 1.3952345848083496\n",
      "Iteration 30055 Loss: 1.3552160263061523\n",
      "Iteration 30056 Loss: 1.126413345336914\n",
      "Iteration 30057 Loss: 0.9711250066757202\n",
      "Iteration 30058 Loss: 0.8803983926773071\n",
      "Iteration 30059 Loss: 1.3514671325683594\n",
      "Iteration 30059 Loss: 1.0559993982315063\n",
      "Iteration 30060 Loss: 0.8942269682884216\n",
      "Iteration 30061 Loss: 1.2053111791610718\n",
      "Iteration 30062 Loss: 1.1780204772949219\n",
      "Iteration 30063 Loss: 1.1702994108200073\n",
      "Iteration 30064 Loss: 1.0200358629226685\n",
      "Iteration 30065 Loss: 1.2133811712265015\n",
      "Iteration 30066 Loss: 1.0979537963867188\n",
      "Iteration 30067 Loss: 0.7126418352127075\n",
      "Iteration 30068 Loss: 1.081549882888794\n",
      "Iteration 30069 Loss: 1.378082036972046\n",
      "Iteration 30069 Loss: 1.0951502323150635\n",
      "Iteration 30070 Loss: 1.0597785711288452\n",
      "Iteration 30071 Loss: 1.3626219034194946\n",
      "Iteration 30072 Loss: 0.7632391452789307\n",
      "Iteration 30073 Loss: 1.1879137754440308\n",
      "Iteration 30074 Loss: 1.0054134130477905\n",
      "Iteration 30075 Loss: 1.0685830116271973\n",
      "Iteration 30076 Loss: 1.3377492427825928\n",
      "Iteration 30077 Loss: 1.0417877435684204\n",
      "Iteration 30078 Loss: 1.2799108028411865\n",
      "Iteration 30079 Loss: 1.3211244344711304\n",
      "Iteration 30079 Loss: 1.1428122520446777\n",
      "Iteration 30080 Loss: 0.7952830195426941\n",
      "Iteration 30081 Loss: 0.9946774840354919\n",
      "Iteration 30082 Loss: 1.0643503665924072\n",
      "Iteration 30083 Loss: 1.0212067365646362\n",
      "Iteration 30084 Loss: 0.9734424352645874\n",
      "Iteration 30085 Loss: 0.9206113219261169\n",
      "Iteration 30086 Loss: 1.0567013025283813\n",
      "Iteration 30087 Loss: 1.1890668869018555\n",
      "Iteration 30088 Loss: 1.1523504257202148\n",
      "Iteration 30089 Loss: 0.8527899384498596\n",
      "Iteration 30089 Loss: 1.0020480155944824\n",
      "Iteration 30090 Loss: 0.8671321272850037\n",
      "Iteration 30091 Loss: 0.8032281398773193\n",
      "Iteration 30092 Loss: 1.0684154033660889\n",
      "Iteration 30093 Loss: 0.9537051320075989\n",
      "Iteration 30094 Loss: 0.9615022540092468\n",
      "Iteration 30095 Loss: 1.2725872993469238\n",
      "Iteration 30096 Loss: 1.4418025016784668\n",
      "Iteration 30097 Loss: 1.1811575889587402\n",
      "Iteration 30098 Loss: 1.2663882970809937\n",
      "Iteration 30099 Loss: 0.9286006093025208\n",
      "Iteration 30099 Loss: 1.0744520425796509\n",
      "Iteration 30100 Loss: 1.0028892755508423\n",
      "Iteration 30101 Loss: 0.9370600581169128\n",
      "Iteration 30102 Loss: 0.9620280265808105\n",
      "Iteration 30103 Loss: 0.9299220442771912\n",
      "Iteration 30104 Loss: 0.9846221804618835\n",
      "Iteration 30105 Loss: 0.9443188309669495\n",
      "Iteration 30106 Loss: 0.9473245143890381\n",
      "Iteration 30107 Loss: 1.0076199769973755\n",
      "Iteration 30108 Loss: 1.0188730955123901\n",
      "Iteration 30109 Loss: 0.8427327871322632\n",
      "Iteration 30109 Loss: 0.9577390551567078\n",
      "Iteration 30110 Loss: 0.9617635011672974\n",
      "Iteration 30111 Loss: 1.050333023071289\n",
      "Iteration 30112 Loss: 1.22779381275177\n",
      "Iteration 30113 Loss: 1.1589938402175903\n",
      "Iteration 30114 Loss: 0.9843325614929199\n",
      "Iteration 30115 Loss: 1.0094656944274902\n",
      "Iteration 30116 Loss: 1.0868645906448364\n",
      "Iteration 30117 Loss: 1.010939121246338\n",
      "Iteration 30118 Loss: 0.9793065190315247\n",
      "Iteration 30119 Loss: 0.8242499828338623\n",
      "Iteration 30119 Loss: 1.0294042825698853\n",
      "Iteration 30120 Loss: 1.2913057804107666\n",
      "Iteration 30121 Loss: 0.8711734414100647\n",
      "Iteration 30122 Loss: 1.2774758338928223\n",
      "Iteration 30123 Loss: 1.4929617643356323\n",
      "Iteration 30124 Loss: 1.115218162536621\n",
      "Iteration 30125 Loss: 0.7169736623764038\n",
      "Iteration 30126 Loss: 0.9003214240074158\n",
      "Iteration 30127 Loss: 0.8458943367004395\n",
      "Iteration 30128 Loss: 0.8558539152145386\n",
      "Iteration 30129 Loss: 0.821466863155365\n",
      "Iteration 30129 Loss: 1.018864393234253\n",
      "Iteration 30130 Loss: 1.066084623336792\n",
      "Iteration 30131 Loss: 0.8922216892242432\n",
      "Iteration 30132 Loss: 0.9535735249519348\n",
      "Iteration 30133 Loss: 0.9812685251235962\n",
      "Iteration 30134 Loss: 0.7923068404197693\n",
      "Iteration 30135 Loss: 1.17970609664917\n",
      "Iteration 30136 Loss: 1.3745977878570557\n",
      "Iteration 30137 Loss: 0.9942849278450012\n",
      "Iteration 30138 Loss: 0.8141344785690308\n",
      "Iteration 30139 Loss: 0.8555527329444885\n",
      "Iteration 30139 Loss: 0.9903731346130371\n",
      "Iteration 30140 Loss: 1.2589236497879028\n",
      "Iteration 30141 Loss: 1.2584388256072998\n",
      "Iteration 30142 Loss: 1.0195519924163818\n",
      "Iteration 30143 Loss: 1.4022389650344849\n",
      "Iteration 30144 Loss: 1.0241299867630005\n",
      "Iteration 30145 Loss: 0.9636319279670715\n",
      "Iteration 30146 Loss: 1.2026047706604004\n",
      "Iteration 30147 Loss: 1.029020071029663\n",
      "Iteration 30148 Loss: 0.9937374591827393\n",
      "Iteration 30149 Loss: 1.1014714241027832\n",
      "Iteration 30149 Loss: 1.1253750324249268\n",
      "Iteration 30150 Loss: 1.0017220973968506\n",
      "Iteration 30151 Loss: 0.8138887286186218\n",
      "Iteration 30152 Loss: 1.0106480121612549\n",
      "Iteration 30153 Loss: 0.9147078990936279\n",
      "Iteration 30154 Loss: 0.9902926087379456\n",
      "Iteration 30155 Loss: 0.706383228302002\n",
      "Iteration 30156 Loss: 1.1268424987792969\n",
      "Iteration 30157 Loss: 0.6030601263046265\n",
      "Iteration 30158 Loss: 1.0114506483078003\n",
      "Iteration 30159 Loss: 1.1949700117111206\n",
      "Iteration 30159 Loss: 0.9373966455459595\n",
      "Iteration 30160 Loss: 1.262630581855774\n",
      "Iteration 30161 Loss: 0.9073051810264587\n",
      "Iteration 30162 Loss: 0.7573803663253784\n",
      "Iteration 30163 Loss: 1.233278751373291\n",
      "Iteration 30164 Loss: 1.0176085233688354\n",
      "Iteration 30165 Loss: 1.1612852811813354\n",
      "Iteration 30166 Loss: 0.6909857988357544\n",
      "Iteration 30167 Loss: 0.6243244409561157\n",
      "Iteration 30168 Loss: 0.8226132988929749\n",
      "Iteration 30169 Loss: 1.152884602546692\n",
      "Iteration 30169 Loss: 0.9630297422409058\n",
      "Iteration 30170 Loss: 1.1559820175170898\n",
      "Iteration 30171 Loss: 0.9793443083763123\n",
      "Iteration 30172 Loss: 0.7384195327758789\n",
      "Iteration 30173 Loss: 1.1457701921463013\n",
      "Iteration 30174 Loss: 1.487103819847107\n",
      "Iteration 30175 Loss: 0.9141364097595215\n",
      "Iteration 30176 Loss: 1.132126808166504\n",
      "Iteration 30177 Loss: 1.5286005735397339\n",
      "Iteration 30178 Loss: 1.0254665613174438\n",
      "Iteration 30179 Loss: 0.7156444787979126\n",
      "Iteration 30179 Loss: 1.0822594165802002\n",
      "Iteration 30180 Loss: 1.0366369485855103\n",
      "Iteration 30181 Loss: 0.8602718114852905\n",
      "Iteration 30182 Loss: 1.0461660623550415\n",
      "Iteration 30183 Loss: 0.7073857188224792\n",
      "Iteration 30184 Loss: 1.064978837966919\n",
      "Iteration 30185 Loss: 0.7867021560668945\n",
      "Iteration 30186 Loss: 1.2449790239334106\n",
      "Iteration 30187 Loss: 0.8219255805015564\n",
      "Iteration 30188 Loss: 1.0394983291625977\n",
      "Iteration 30189 Loss: 1.191158652305603\n",
      "Iteration 30189 Loss: 0.9799702763557434\n",
      "Iteration 30190 Loss: 1.0049784183502197\n",
      "Iteration 30191 Loss: 0.963432252407074\n",
      "Iteration 30192 Loss: 1.3555151224136353\n",
      "Iteration 30193 Loss: 0.8009411692619324\n",
      "Iteration 30194 Loss: 1.38909113407135\n",
      "Iteration 30195 Loss: 0.6162660121917725\n",
      "Iteration 30196 Loss: 1.2576580047607422\n",
      "Iteration 30197 Loss: 0.5733668208122253\n",
      "Iteration 30198 Loss: 1.7267359495162964\n",
      "Iteration 30199 Loss: 1.0396175384521484\n",
      "Iteration 30199 Loss: 1.0727603435516357\n",
      "Iteration 30200 Loss: 1.503515601158142\n",
      "Iteration 30201 Loss: 0.818132221698761\n",
      "Iteration 30202 Loss: 1.1294927597045898\n",
      "Iteration 30203 Loss: 0.9774603843688965\n",
      "Iteration 30204 Loss: 0.8294063210487366\n",
      "Iteration 30205 Loss: 0.912105143070221\n",
      "Iteration 30206 Loss: 0.8514854311943054\n",
      "Iteration 30207 Loss: 1.3796894550323486\n",
      "Iteration 30208 Loss: 1.1051974296569824\n",
      "Iteration 30209 Loss: 1.0640065670013428\n",
      "Iteration 30209 Loss: 1.057049036026001\n",
      "Iteration 30210 Loss: 1.0356495380401611\n",
      "Iteration 30211 Loss: 0.9899765253067017\n",
      "Iteration 30212 Loss: 1.05473792552948\n",
      "Iteration 30213 Loss: 0.8589946627616882\n",
      "Iteration 30214 Loss: 1.1313903331756592\n",
      "Iteration 30215 Loss: 1.2841291427612305\n",
      "Iteration 30216 Loss: 0.9565667510032654\n",
      "Iteration 30217 Loss: 1.3351420164108276\n",
      "Iteration 30218 Loss: 1.4028184413909912\n",
      "Iteration 30219 Loss: 1.1317076683044434\n",
      "Iteration 30219 Loss: 1.1181113719940186\n",
      "Iteration 30220 Loss: 1.1366817951202393\n",
      "Iteration 30221 Loss: 0.9649924039840698\n",
      "Iteration 30222 Loss: 1.156555414199829\n",
      "Iteration 30223 Loss: 1.0511609315872192\n",
      "Iteration 30224 Loss: 0.9891086220741272\n",
      "Iteration 30225 Loss: 0.9136965870857239\n",
      "Iteration 30226 Loss: 0.8417396545410156\n",
      "Iteration 30227 Loss: 0.820488452911377\n",
      "Iteration 30228 Loss: 0.9646148681640625\n",
      "Iteration 30229 Loss: 1.0554143190383911\n",
      "Iteration 30229 Loss: 0.9894453287124634\n",
      "Iteration 30230 Loss: 1.0925483703613281\n",
      "Iteration 30231 Loss: 0.8298013210296631\n",
      "Iteration 30232 Loss: 1.1208064556121826\n",
      "Iteration 30233 Loss: 1.109484314918518\n",
      "Iteration 30234 Loss: 0.9502995610237122\n",
      "Iteration 30235 Loss: 1.0900956392288208\n",
      "Iteration 30236 Loss: 1.1214873790740967\n",
      "Iteration 30237 Loss: 1.1148242950439453\n",
      "Iteration 30238 Loss: 1.2180728912353516\n",
      "Iteration 30239 Loss: 1.1588948965072632\n",
      "Iteration 30239 Loss: 1.0806314945220947\n",
      "Iteration 30240 Loss: 1.2206333875656128\n",
      "Iteration 30241 Loss: 1.2699345350265503\n",
      "Iteration 30242 Loss: 1.0973241329193115\n",
      "Iteration 30243 Loss: 1.0754802227020264\n",
      "Iteration 30244 Loss: 0.8442074060440063\n",
      "Iteration 30245 Loss: 0.9665701389312744\n",
      "Iteration 30246 Loss: 1.2165613174438477\n",
      "Iteration 30247 Loss: 0.8396126627922058\n",
      "Iteration 30248 Loss: 0.8351479172706604\n",
      "Iteration 30249 Loss: 0.8193010091781616\n",
      "Iteration 30249 Loss: 1.0184773206710815\n",
      "Iteration 30250 Loss: 0.9638352394104004\n",
      "Iteration 30251 Loss: 1.177551031112671\n",
      "Iteration 30252 Loss: 0.7504202723503113\n",
      "Iteration 30253 Loss: 1.0493475198745728\n",
      "Iteration 30254 Loss: 1.1273635625839233\n",
      "Iteration 30255 Loss: 1.4146537780761719\n",
      "Iteration 30256 Loss: 0.5650207996368408\n",
      "Iteration 30257 Loss: 1.0821480751037598\n",
      "Iteration 30258 Loss: 0.9774313569068909\n",
      "Iteration 30259 Loss: 1.3520498275756836\n",
      "Iteration 30259 Loss: 1.0459821224212646\n",
      "Iteration 30260 Loss: 0.9536029100418091\n",
      "Iteration 30261 Loss: 1.1449390649795532\n",
      "Iteration 30262 Loss: 0.8882700204849243\n",
      "Iteration 30263 Loss: 1.0971043109893799\n",
      "Iteration 30264 Loss: 0.8092206120491028\n",
      "Iteration 30265 Loss: 1.3305553197860718\n",
      "Iteration 30266 Loss: 1.0621328353881836\n",
      "Iteration 30267 Loss: 1.255926251411438\n",
      "Iteration 30268 Loss: 1.0895516872406006\n",
      "Iteration 30269 Loss: 1.1377745866775513\n",
      "Iteration 30269 Loss: 1.076907753944397\n",
      "Iteration 30270 Loss: 1.107885479927063\n",
      "Iteration 30271 Loss: 1.01863694190979\n",
      "Iteration 30272 Loss: 1.0414810180664062\n",
      "Iteration 30273 Loss: 1.361892580986023\n",
      "Iteration 30274 Loss: 1.0274676084518433\n",
      "Iteration 30275 Loss: 1.3047082424163818\n",
      "Iteration 30276 Loss: 1.0806933641433716\n",
      "Iteration 30277 Loss: 1.16887366771698\n",
      "Iteration 30278 Loss: 1.0428248643875122\n",
      "Iteration 30279 Loss: 1.1651753187179565\n",
      "Iteration 30279 Loss: 1.1319639682769775\n",
      "Iteration 30280 Loss: 1.1215248107910156\n",
      "Iteration 30281 Loss: 1.139279842376709\n",
      "Iteration 30282 Loss: 1.2577520608901978\n",
      "Iteration 30283 Loss: 1.2622514963150024\n",
      "Iteration 30284 Loss: 1.3755029439926147\n",
      "Iteration 30285 Loss: 0.8449429869651794\n",
      "Iteration 30286 Loss: 1.0443729162216187\n",
      "Iteration 30287 Loss: 1.1720385551452637\n",
      "Iteration 30288 Loss: 0.9766973257064819\n",
      "Iteration 30289 Loss: 1.0351061820983887\n",
      "Iteration 30289 Loss: 1.1229469776153564\n",
      "Iteration 30290 Loss: 0.8670485615730286\n",
      "Iteration 30291 Loss: 1.1674280166625977\n",
      "Iteration 30292 Loss: 0.9101271629333496\n",
      "Iteration 30293 Loss: 1.1343623399734497\n",
      "Iteration 30294 Loss: 1.1294786930084229\n",
      "Iteration 30295 Loss: 1.3465501070022583\n",
      "Iteration 30296 Loss: 0.9871858954429626\n",
      "Iteration 30297 Loss: 0.8413480520248413\n",
      "Iteration 30298 Loss: 1.0853551626205444\n",
      "Iteration 30299 Loss: 0.8205442428588867\n",
      "Iteration 30299 Loss: 1.0289427042007446\n",
      "Iteration 30300 Loss: 1.0064725875854492\n",
      "Iteration 30301 Loss: 1.0970100164413452\n",
      "Iteration 30302 Loss: 0.7660103440284729\n",
      "Iteration 30303 Loss: 1.2757949829101562\n",
      "Iteration 30304 Loss: 1.2629958391189575\n",
      "Iteration 30305 Loss: 1.168635606765747\n",
      "Iteration 30306 Loss: 1.182800531387329\n",
      "Iteration 30307 Loss: 0.7445345520973206\n",
      "Iteration 30308 Loss: 0.9832533597946167\n",
      "Iteration 30309 Loss: 1.1526676416397095\n",
      "Iteration 30309 Loss: 1.0640175342559814\n",
      "Iteration 30310 Loss: 1.3355395793914795\n",
      "Iteration 30311 Loss: 1.210858941078186\n",
      "Iteration 30312 Loss: 0.9095298051834106\n",
      "Iteration 30313 Loss: 0.678453803062439\n",
      "Iteration 30314 Loss: 1.293099284172058\n",
      "Iteration 30315 Loss: 1.0298702716827393\n",
      "Iteration 30316 Loss: 0.9189170002937317\n",
      "Iteration 30317 Loss: 1.0680043697357178\n",
      "Iteration 30318 Loss: 1.2713871002197266\n",
      "Iteration 30319 Loss: 1.1074130535125732\n",
      "Iteration 30319 Loss: 1.0823073387145996\n",
      "Iteration 30320 Loss: 1.0772145986557007\n",
      "Iteration 30321 Loss: 0.8126921057701111\n",
      "Iteration 30322 Loss: 0.7635573744773865\n",
      "Iteration 30323 Loss: 0.8948714137077332\n",
      "Iteration 30324 Loss: 0.5337699055671692\n",
      "Iteration 30325 Loss: 1.234906554222107\n",
      "Iteration 30326 Loss: 1.202217698097229\n",
      "Iteration 30327 Loss: 1.2317935228347778\n",
      "Iteration 30328 Loss: 1.0035375356674194\n",
      "Iteration 30329 Loss: 1.0550049543380737\n",
      "Iteration 30329 Loss: 0.9809565544128418\n",
      "Iteration 30330 Loss: 0.945358395576477\n",
      "Iteration 30331 Loss: 1.292405366897583\n",
      "Iteration 30332 Loss: 1.3413840532302856\n",
      "Iteration 30333 Loss: 0.7874813675880432\n",
      "Iteration 30334 Loss: 0.8037608861923218\n",
      "Iteration 30335 Loss: 0.8982815146446228\n",
      "Iteration 30336 Loss: 1.0362753868103027\n",
      "Iteration 30337 Loss: 0.9160486459732056\n",
      "Iteration 30338 Loss: 0.8599883317947388\n",
      "Iteration 30339 Loss: 0.8681337833404541\n",
      "Iteration 30339 Loss: 0.9749118089675903\n",
      "Iteration 30340 Loss: 1.2101157903671265\n",
      "Iteration 30341 Loss: 0.6195049285888672\n",
      "Iteration 30342 Loss: 1.0538275241851807\n",
      "Iteration 30343 Loss: 0.904697060585022\n",
      "Iteration 30344 Loss: 1.168434739112854\n",
      "Iteration 30345 Loss: 0.9143203496932983\n",
      "Iteration 30346 Loss: 1.2148882150650024\n",
      "Iteration 30347 Loss: 1.1585255861282349\n",
      "Iteration 30348 Loss: 0.8962538242340088\n",
      "Iteration 30349 Loss: 1.204553484916687\n",
      "Iteration 30349 Loss: 1.0345121622085571\n",
      "Iteration 30350 Loss: 0.7540925145149231\n",
      "Iteration 30351 Loss: 0.7794177532196045\n",
      "Iteration 30352 Loss: 0.9768358469009399\n",
      "Iteration 30353 Loss: 0.990729808807373\n",
      "Iteration 30354 Loss: 1.349352478981018\n",
      "Iteration 30355 Loss: 1.0149016380310059\n",
      "Iteration 30356 Loss: 0.9235333800315857\n",
      "Iteration 30357 Loss: 0.8829203844070435\n",
      "Iteration 30358 Loss: 1.396667718887329\n",
      "Iteration 30359 Loss: 1.1261463165283203\n",
      "Iteration 30359 Loss: 1.0194597244262695\n",
      "Iteration 30360 Loss: 0.9530327916145325\n",
      "Iteration 30361 Loss: 0.941903293132782\n",
      "Iteration 30362 Loss: 1.0698460340499878\n",
      "Iteration 30363 Loss: 0.7895943522453308\n",
      "Iteration 30364 Loss: 0.7850680947303772\n",
      "Iteration 30365 Loss: 1.2420567274093628\n",
      "Iteration 30366 Loss: 1.2336395978927612\n",
      "Iteration 30367 Loss: 0.9748376607894897\n",
      "Iteration 30368 Loss: 0.9062748551368713\n",
      "Iteration 30369 Loss: 0.9854429960250854\n",
      "Iteration 30369 Loss: 0.9881695508956909\n",
      "Iteration 30370 Loss: 1.1786195039749146\n",
      "Iteration 30371 Loss: 1.1549779176712036\n",
      "Iteration 30372 Loss: 0.9048442840576172\n",
      "Iteration 30373 Loss: 1.16196608543396\n",
      "Iteration 30374 Loss: 1.1350523233413696\n",
      "Iteration 30375 Loss: 0.7159287929534912\n",
      "Iteration 30376 Loss: 1.309032917022705\n",
      "Iteration 30377 Loss: 0.9504327774047852\n",
      "Iteration 30378 Loss: 1.2949292659759521\n",
      "Iteration 30379 Loss: 0.8955883979797363\n",
      "Iteration 30379 Loss: 1.0701371431350708\n",
      "Iteration 30380 Loss: 0.7796906232833862\n",
      "Iteration 30381 Loss: 1.288251519203186\n",
      "Iteration 30382 Loss: 1.0653759241104126\n",
      "Iteration 30383 Loss: 1.3652986288070679\n",
      "Iteration 30384 Loss: 0.8224356174468994\n",
      "Iteration 30385 Loss: 0.847439706325531\n",
      "Iteration 30386 Loss: 0.9996469616889954\n",
      "Iteration 30387 Loss: 1.2182905673980713\n",
      "Iteration 30388 Loss: 1.0854696035385132\n",
      "Iteration 30389 Loss: 0.9473685026168823\n",
      "Iteration 30389 Loss: 1.0419267416000366\n",
      "Iteration 30390 Loss: 1.1783556938171387\n",
      "Iteration 30391 Loss: 0.7974317669868469\n",
      "Iteration 30392 Loss: 0.977996826171875\n",
      "Iteration 30393 Loss: 1.620769739151001\n",
      "Iteration 30394 Loss: 1.2466360330581665\n",
      "Iteration 30395 Loss: 0.8262194395065308\n",
      "Iteration 30396 Loss: 1.0551210641860962\n",
      "Iteration 30397 Loss: 1.081019401550293\n",
      "Iteration 30398 Loss: 1.0809484720230103\n",
      "Iteration 30399 Loss: 0.9096707105636597\n",
      "Iteration 30399 Loss: 1.0774168968200684\n",
      "Iteration 30400 Loss: 0.8344879150390625\n",
      "Iteration 30401 Loss: 0.876034140586853\n",
      "Iteration 30402 Loss: 1.0801154375076294\n",
      "Iteration 30403 Loss: 1.2068449258804321\n",
      "Iteration 30404 Loss: 1.069657564163208\n",
      "Iteration 30405 Loss: 0.9901444315910339\n",
      "Iteration 30406 Loss: 1.126317024230957\n",
      "Iteration 30407 Loss: 0.9749154448509216\n",
      "Iteration 30408 Loss: 0.6532215476036072\n",
      "Iteration 30409 Loss: 1.03622567653656\n",
      "Iteration 30409 Loss: 0.984796404838562\n",
      "Iteration 30410 Loss: 0.9110069870948792\n",
      "Iteration 30411 Loss: 0.8103808760643005\n",
      "Iteration 30412 Loss: 0.9286507368087769\n",
      "Iteration 30413 Loss: 1.2749189138412476\n",
      "Iteration 30414 Loss: 1.0484833717346191\n",
      "Iteration 30415 Loss: 0.8624408841133118\n",
      "Iteration 30416 Loss: 0.6952756643295288\n",
      "Iteration 30417 Loss: 0.6070984601974487\n",
      "Iteration 30418 Loss: 1.0474668741226196\n",
      "Iteration 30419 Loss: 1.2426272630691528\n",
      "Iteration 30419 Loss: 0.9428349733352661\n",
      "Iteration 30420 Loss: 1.1017310619354248\n",
      "Iteration 30421 Loss: 1.0924198627471924\n",
      "Iteration 30422 Loss: 1.1876519918441772\n",
      "Iteration 30423 Loss: 0.8661794662475586\n",
      "Iteration 30424 Loss: 1.2564358711242676\n",
      "Iteration 30425 Loss: 1.1807242631912231\n",
      "Iteration 30426 Loss: 1.0250455141067505\n",
      "Iteration 30427 Loss: 1.061550498008728\n",
      "Iteration 30428 Loss: 0.9566042423248291\n",
      "Iteration 30429 Loss: 0.9916955232620239\n",
      "Iteration 30429 Loss: 1.072003722190857\n",
      "Iteration 30430 Loss: 0.8296933174133301\n",
      "Iteration 30431 Loss: 1.0195859670639038\n",
      "Iteration 30432 Loss: 1.3190008401870728\n",
      "Iteration 30433 Loss: 1.0601736307144165\n",
      "Iteration 30434 Loss: 1.1736083030700684\n",
      "Iteration 30435 Loss: 1.2640143632888794\n",
      "Iteration 30436 Loss: 0.6905627250671387\n",
      "Iteration 30437 Loss: 0.8771186470985413\n",
      "Iteration 30438 Loss: 1.145463466644287\n",
      "Iteration 30439 Loss: 0.9884412884712219\n",
      "Iteration 30439 Loss: 1.0367662906646729\n",
      "Iteration 30440 Loss: 1.0141398906707764\n",
      "Iteration 30441 Loss: 1.1884477138519287\n",
      "Iteration 30442 Loss: 1.1107563972473145\n",
      "Iteration 30443 Loss: 1.1406972408294678\n",
      "Iteration 30444 Loss: 1.1765358448028564\n",
      "Iteration 30445 Loss: 1.2274954319000244\n",
      "Iteration 30446 Loss: 1.0882484912872314\n",
      "Iteration 30447 Loss: 0.7578938603401184\n",
      "Iteration 30448 Loss: 1.0322763919830322\n",
      "Iteration 30449 Loss: 1.1068644523620605\n",
      "Iteration 30449 Loss: 1.084335446357727\n",
      "Iteration 30450 Loss: 0.8936061263084412\n",
      "Iteration 30451 Loss: 0.5522128343582153\n",
      "Iteration 30452 Loss: 1.362857460975647\n",
      "Iteration 30453 Loss: 0.8723546266555786\n",
      "Iteration 30454 Loss: 0.8766441345214844\n",
      "Iteration 30455 Loss: 1.0642534494400024\n",
      "Iteration 30456 Loss: 0.675674319267273\n",
      "Iteration 30457 Loss: 1.1563974618911743\n",
      "Iteration 30458 Loss: 0.9217040538787842\n",
      "Iteration 30459 Loss: 1.0356158018112183\n",
      "Iteration 30459 Loss: 0.9411319494247437\n",
      "Iteration 30460 Loss: 0.6271511316299438\n",
      "Iteration 30461 Loss: 0.8392688035964966\n",
      "Iteration 30462 Loss: 1.4800862073898315\n",
      "Iteration 30463 Loss: 0.8676590323448181\n",
      "Iteration 30464 Loss: 1.0860017538070679\n",
      "Iteration 30465 Loss: 1.103387713432312\n",
      "Iteration 30466 Loss: 0.7396710515022278\n",
      "Iteration 30467 Loss: 1.2948975563049316\n",
      "Iteration 30468 Loss: 0.7243934273719788\n",
      "Iteration 30469 Loss: 0.7926162481307983\n",
      "Iteration 30469 Loss: 0.9555133581161499\n",
      "Iteration 30470 Loss: 0.8001689910888672\n",
      "Iteration 30471 Loss: 0.7170758843421936\n",
      "Iteration 30472 Loss: 0.9106658697128296\n",
      "Iteration 30473 Loss: 1.059463620185852\n",
      "Iteration 30474 Loss: 1.2054287195205688\n",
      "Iteration 30475 Loss: 1.2856225967407227\n",
      "Iteration 30476 Loss: 0.9720107316970825\n",
      "Iteration 30477 Loss: 1.3165576457977295\n",
      "Iteration 30478 Loss: 1.4424641132354736\n",
      "Iteration 30479 Loss: 1.134845495223999\n",
      "Iteration 30479 Loss: 1.0844303369522095\n",
      "Iteration 30480 Loss: 1.195601224899292\n",
      "Iteration 30481 Loss: 1.314591407775879\n",
      "Iteration 30482 Loss: 1.2061294317245483\n",
      "Iteration 30483 Loss: 0.9401670098304749\n",
      "Iteration 30484 Loss: 0.8570160865783691\n",
      "Iteration 30485 Loss: 1.0512608289718628\n",
      "Iteration 30486 Loss: 0.6968187689781189\n",
      "Iteration 30487 Loss: 1.1027477979660034\n",
      "Iteration 30488 Loss: 1.239222764968872\n",
      "Iteration 30489 Loss: 0.936405599117279\n",
      "Iteration 30489 Loss: 1.0539960861206055\n",
      "Iteration 30490 Loss: 0.7204850912094116\n",
      "Iteration 30491 Loss: 0.8937934637069702\n",
      "Iteration 30492 Loss: 0.5616185665130615\n",
      "Iteration 30493 Loss: 1.0518293380737305\n",
      "Iteration 30494 Loss: 1.5256547927856445\n",
      "Iteration 30495 Loss: 0.8914778232574463\n",
      "Iteration 30496 Loss: 0.6937298774719238\n",
      "Iteration 30497 Loss: 0.8016173243522644\n",
      "Iteration 30498 Loss: 1.0787261724472046\n",
      "Iteration 30499 Loss: 1.3327974081039429\n",
      "Iteration 30499 Loss: 0.9551730155944824\n",
      "Iteration 30500 Loss: 0.7325208187103271\n",
      "Iteration 30501 Loss: 1.1259539127349854\n",
      "Iteration 30502 Loss: 0.9135946035385132\n",
      "Iteration 30503 Loss: 1.4856737852096558\n",
      "Iteration 30504 Loss: 0.9239652156829834\n",
      "Iteration 30505 Loss: 1.0880427360534668\n",
      "Iteration 30506 Loss: 0.987552285194397\n",
      "Iteration 30507 Loss: 1.2676218748092651\n",
      "Iteration 30508 Loss: 0.8105148077011108\n",
      "Iteration 30509 Loss: 0.8589633703231812\n",
      "Iteration 30509 Loss: 1.0194404125213623\n",
      "Iteration 30510 Loss: 1.1934903860092163\n",
      "Iteration 30511 Loss: 0.8598602414131165\n",
      "Iteration 30512 Loss: 0.90941321849823\n",
      "Iteration 30513 Loss: 1.0225170850753784\n",
      "Iteration 30514 Loss: 0.8651560544967651\n",
      "Iteration 30515 Loss: 0.636140763759613\n",
      "Iteration 30516 Loss: 1.0534958839416504\n",
      "Iteration 30517 Loss: 1.20612370967865\n",
      "Iteration 30518 Loss: 0.7009299397468567\n",
      "Iteration 30519 Loss: 1.1244393587112427\n",
      "Iteration 30519 Loss: 0.9571566581726074\n",
      "Iteration 30520 Loss: 1.0352604389190674\n",
      "Iteration 30521 Loss: 0.8226482272148132\n",
      "Iteration 30522 Loss: 0.9738914370536804\n",
      "Iteration 30523 Loss: 0.9912615418434143\n",
      "Iteration 30524 Loss: 0.8175296187400818\n",
      "Iteration 30525 Loss: 0.7841647863388062\n",
      "Iteration 30526 Loss: 0.7028075456619263\n",
      "Iteration 30527 Loss: 1.0195107460021973\n",
      "Iteration 30528 Loss: 0.8749281764030457\n",
      "Iteration 30529 Loss: 0.8585548400878906\n",
      "Iteration 30529 Loss: 0.8880558013916016\n",
      "Iteration 30530 Loss: 1.2748911380767822\n",
      "Iteration 30531 Loss: 1.1846214532852173\n",
      "Iteration 30532 Loss: 0.8759158849716187\n",
      "Iteration 30533 Loss: 1.0318577289581299\n",
      "Iteration 30534 Loss: 0.9909214973449707\n",
      "Iteration 30535 Loss: 1.2199535369873047\n",
      "Iteration 30536 Loss: 0.9533860087394714\n",
      "Iteration 30537 Loss: 1.0199060440063477\n",
      "Iteration 30538 Loss: 1.1818138360977173\n",
      "Iteration 30539 Loss: 0.8681727647781372\n",
      "Iteration 30539 Loss: 1.0601439476013184\n",
      "Iteration 30540 Loss: 1.2081862688064575\n",
      "Iteration 30541 Loss: 1.3684778213500977\n",
      "Iteration 30542 Loss: 1.2358345985412598\n",
      "Iteration 30543 Loss: 1.3100169897079468\n",
      "Iteration 30544 Loss: 1.1263184547424316\n",
      "Iteration 30545 Loss: 0.7504914402961731\n",
      "Iteration 30546 Loss: 1.0812736749649048\n",
      "Iteration 30547 Loss: 0.603116512298584\n",
      "Iteration 30548 Loss: 0.8612721562385559\n",
      "Iteration 30549 Loss: 1.0089834928512573\n",
      "Iteration 30549 Loss: 1.0553972721099854\n",
      "Iteration 30550 Loss: 0.6533546447753906\n",
      "Iteration 30551 Loss: 0.9933812618255615\n",
      "Iteration 30552 Loss: 1.0144126415252686\n",
      "Iteration 30553 Loss: 1.1007574796676636\n",
      "Iteration 30554 Loss: 1.0320446491241455\n",
      "Iteration 30555 Loss: 0.9651680588722229\n",
      "Iteration 30556 Loss: 1.0913326740264893\n",
      "Iteration 30557 Loss: 0.7492526173591614\n",
      "Iteration 30558 Loss: 1.4412238597869873\n",
      "Iteration 30559 Loss: 1.1680021286010742\n",
      "Iteration 30559 Loss: 1.020892858505249\n",
      "Iteration 30560 Loss: 1.0292552709579468\n",
      "Iteration 30561 Loss: 1.2337579727172852\n",
      "Iteration 30562 Loss: 0.8412843942642212\n",
      "Iteration 30563 Loss: 1.1120392084121704\n",
      "Iteration 30564 Loss: 1.144162654876709\n",
      "Iteration 30565 Loss: 0.8191625475883484\n",
      "Iteration 30566 Loss: 0.9051787853240967\n",
      "Iteration 30567 Loss: 1.0052725076675415\n",
      "Iteration 30568 Loss: 1.1495649814605713\n",
      "Iteration 30569 Loss: 0.6710156798362732\n",
      "Iteration 30569 Loss: 0.9910694360733032\n",
      "Iteration 30570 Loss: 0.8790735006332397\n",
      "Iteration 30571 Loss: 1.0635740756988525\n",
      "Iteration 30572 Loss: 1.3026254177093506\n",
      "Iteration 30573 Loss: 0.9940246939659119\n",
      "Iteration 30574 Loss: 0.9854562282562256\n",
      "Iteration 30575 Loss: 0.8883486390113831\n",
      "Iteration 30576 Loss: 0.9029314517974854\n",
      "Iteration 30577 Loss: 0.9808266758918762\n",
      "Iteration 30578 Loss: 0.9628295302391052\n",
      "Iteration 30579 Loss: 1.2640962600708008\n",
      "Iteration 30579 Loss: 1.02237868309021\n",
      "Iteration 30580 Loss: 0.9086989164352417\n",
      "Iteration 30581 Loss: 0.8365115523338318\n",
      "Iteration 30582 Loss: 0.7261893153190613\n",
      "Iteration 30583 Loss: 0.7470065355300903\n",
      "Iteration 30584 Loss: 0.964057445526123\n",
      "Iteration 30585 Loss: 1.0139565467834473\n",
      "Iteration 30586 Loss: 0.8144389390945435\n",
      "Iteration 30587 Loss: 1.3891377449035645\n",
      "Iteration 30588 Loss: 1.1252737045288086\n",
      "Iteration 30589 Loss: 1.2854372262954712\n",
      "Iteration 30589 Loss: 0.9810706973075867\n",
      "Iteration 30590 Loss: 1.0175273418426514\n",
      "Iteration 30591 Loss: 1.059002161026001\n",
      "Iteration 30592 Loss: 1.154264211654663\n",
      "Iteration 30593 Loss: 0.9570882320404053\n",
      "Iteration 30594 Loss: 1.028704047203064\n",
      "Iteration 30595 Loss: 1.1635749340057373\n",
      "Iteration 30596 Loss: 1.0871288776397705\n",
      "Iteration 30597 Loss: 1.023518443107605\n",
      "Iteration 30598 Loss: 1.0902553796768188\n",
      "Iteration 30599 Loss: 1.3262453079223633\n",
      "Iteration 30599 Loss: 1.090730905532837\n",
      "Iteration 30600 Loss: 1.0494850873947144\n",
      "Iteration 30601 Loss: 1.1636463403701782\n",
      "Iteration 30602 Loss: 0.834662914276123\n",
      "Iteration 30603 Loss: 0.6712436079978943\n",
      "Iteration 30604 Loss: 0.8155748248100281\n",
      "Iteration 30605 Loss: 0.9726828932762146\n",
      "Iteration 30606 Loss: 1.0341534614562988\n",
      "Iteration 30607 Loss: 0.9002910852432251\n",
      "Iteration 30608 Loss: 1.1049408912658691\n",
      "Iteration 30609 Loss: 1.0508666038513184\n",
      "Iteration 30609 Loss: 0.9597547650337219\n",
      "Iteration 30610 Loss: 0.9513927698135376\n",
      "Iteration 30611 Loss: 0.5826448798179626\n",
      "Iteration 30612 Loss: 1.130077838897705\n",
      "Iteration 30613 Loss: 0.6507744193077087\n",
      "Iteration 30614 Loss: 1.087588906288147\n",
      "Iteration 30615 Loss: 0.9501730799674988\n",
      "Iteration 30616 Loss: 0.8824586272239685\n",
      "Iteration 30617 Loss: 0.9418224096298218\n",
      "Iteration 30618 Loss: 0.8374136686325073\n",
      "Iteration 30619 Loss: 0.7984977960586548\n",
      "Iteration 30619 Loss: 0.8812844157218933\n",
      "Iteration 30620 Loss: 1.052565097808838\n",
      "Iteration 30621 Loss: 0.8894394636154175\n",
      "Iteration 30622 Loss: 1.0551269054412842\n",
      "Iteration 30623 Loss: 1.1188750267028809\n",
      "Iteration 30624 Loss: 0.8953007459640503\n",
      "Iteration 30625 Loss: 1.187265396118164\n",
      "Iteration 30626 Loss: 0.794111967086792\n",
      "Iteration 30627 Loss: 0.8448203802108765\n",
      "Iteration 30628 Loss: 0.9527603387832642\n",
      "Iteration 30629 Loss: 0.9160129427909851\n",
      "Iteration 30629 Loss: 0.9706279039382935\n",
      "Iteration 30630 Loss: 1.1692237854003906\n",
      "Iteration 30631 Loss: 1.3434957265853882\n",
      "Iteration 30632 Loss: 1.176220178604126\n",
      "Iteration 30633 Loss: 0.8464028835296631\n",
      "Iteration 30634 Loss: 1.0029624700546265\n",
      "Iteration 30635 Loss: 1.6053773164749146\n",
      "Iteration 30636 Loss: 0.9787521958351135\n",
      "Iteration 30637 Loss: 1.04269540309906\n",
      "Iteration 30638 Loss: 1.1040617227554321\n",
      "Iteration 30639 Loss: 0.7056393027305603\n",
      "Iteration 30639 Loss: 1.0974831581115723\n",
      "Iteration 30640 Loss: 0.9136050939559937\n",
      "Iteration 30641 Loss: 1.040284276008606\n",
      "Iteration 30642 Loss: 0.7846153974533081\n",
      "Iteration 30643 Loss: 1.1175581216812134\n",
      "Iteration 30644 Loss: 1.3862535953521729\n",
      "Iteration 30645 Loss: 0.9366782307624817\n",
      "Iteration 30646 Loss: 1.0874265432357788\n",
      "Iteration 30647 Loss: 1.341772198677063\n",
      "Iteration 30648 Loss: 1.171846866607666\n",
      "Iteration 30649 Loss: 0.8807306885719299\n",
      "Iteration 30649 Loss: 1.0660769939422607\n",
      "Iteration 30650 Loss: 0.9266354441642761\n",
      "Iteration 30651 Loss: 1.0741174221038818\n",
      "Iteration 30652 Loss: 0.6901831030845642\n",
      "Iteration 30653 Loss: 1.0626565217971802\n",
      "Iteration 30654 Loss: 0.973531186580658\n",
      "Iteration 30655 Loss: 1.1583324670791626\n",
      "Iteration 30656 Loss: 1.238512635231018\n",
      "Iteration 30657 Loss: 1.182877540588379\n",
      "Iteration 30658 Loss: 0.92085200548172\n",
      "Iteration 30659 Loss: 1.2322529554367065\n",
      "Iteration 30659 Loss: 1.0459952354431152\n",
      "Iteration 30660 Loss: 1.0900871753692627\n",
      "Iteration 30661 Loss: 1.017665982246399\n",
      "Iteration 30662 Loss: 1.225784182548523\n",
      "Iteration 30663 Loss: 1.2846460342407227\n",
      "Iteration 30664 Loss: 1.0529661178588867\n",
      "Iteration 30665 Loss: 0.8544303178787231\n",
      "Iteration 30666 Loss: 0.9263687133789062\n",
      "Iteration 30667 Loss: 1.2524099349975586\n",
      "Iteration 30668 Loss: 1.084679365158081\n",
      "Iteration 30669 Loss: 1.193018913269043\n",
      "Iteration 30669 Loss: 1.0982056856155396\n",
      "Iteration 30670 Loss: 0.7103488445281982\n",
      "Iteration 30671 Loss: 0.7531777620315552\n",
      "Iteration 30672 Loss: 1.0534343719482422\n",
      "Iteration 30673 Loss: 1.1355993747711182\n",
      "Iteration 30674 Loss: 1.2781440019607544\n",
      "Iteration 30675 Loss: 1.1489757299423218\n",
      "Iteration 30676 Loss: 0.4847944974899292\n",
      "Iteration 30677 Loss: 1.0324482917785645\n",
      "Iteration 30678 Loss: 0.7270984053611755\n",
      "Iteration 30679 Loss: 0.8984588384628296\n",
      "Iteration 30679 Loss: 0.9222478866577148\n",
      "Iteration 30680 Loss: 1.151760220527649\n",
      "Iteration 30681 Loss: 1.2572150230407715\n",
      "Iteration 30682 Loss: 0.9594023823738098\n",
      "Iteration 30683 Loss: 0.9555906653404236\n",
      "Iteration 30684 Loss: 1.33027184009552\n",
      "Iteration 30685 Loss: 0.9505569934844971\n",
      "Iteration 30686 Loss: 0.9106018543243408\n",
      "Iteration 30687 Loss: 0.6770084500312805\n",
      "Iteration 30688 Loss: 1.3361868858337402\n",
      "Iteration 30689 Loss: 1.0687885284423828\n",
      "Iteration 30689 Loss: 1.059738278388977\n",
      "Iteration 30690 Loss: 1.2328492403030396\n",
      "Iteration 30691 Loss: 1.1178889274597168\n",
      "Iteration 30692 Loss: 1.1256989240646362\n",
      "Iteration 30693 Loss: 1.032540202140808\n",
      "Iteration 30694 Loss: 1.1098694801330566\n",
      "Iteration 30695 Loss: 0.8853206038475037\n",
      "Iteration 30696 Loss: 1.2934492826461792\n",
      "Iteration 30697 Loss: 1.0833569765090942\n",
      "Iteration 30698 Loss: 1.0001039505004883\n",
      "Iteration 30699 Loss: 0.9939743280410767\n",
      "Iteration 30699 Loss: 1.0875051021575928\n",
      "Iteration 30700 Loss: 0.6417036652565002\n",
      "Iteration 30701 Loss: 0.8277901411056519\n",
      "Iteration 30702 Loss: 1.3113486766815186\n",
      "Iteration 30703 Loss: 0.9788640737533569\n",
      "Iteration 30704 Loss: 1.0391589403152466\n",
      "Iteration 30705 Loss: 1.129475712776184\n",
      "Iteration 30706 Loss: 0.9380885362625122\n",
      "Iteration 30707 Loss: 1.0104305744171143\n",
      "Iteration 30708 Loss: 0.7768827080726624\n",
      "Iteration 30709 Loss: 1.0728033781051636\n",
      "Iteration 30709 Loss: 0.9726546406745911\n",
      "Iteration 30710 Loss: 0.76071697473526\n",
      "Iteration 30711 Loss: 0.8652188777923584\n",
      "Iteration 30712 Loss: 1.058585524559021\n",
      "Iteration 30713 Loss: 0.9835647344589233\n",
      "Iteration 30714 Loss: 0.560456395149231\n",
      "Iteration 30715 Loss: 1.1789517402648926\n",
      "Iteration 30716 Loss: 1.1780264377593994\n",
      "Iteration 30717 Loss: 0.9288160800933838\n",
      "Iteration 30718 Loss: 0.9027044773101807\n",
      "Iteration 30719 Loss: 0.6219401955604553\n",
      "Iteration 30719 Loss: 0.9038981199264526\n",
      "Iteration 30720 Loss: 0.9590404629707336\n",
      "Iteration 30721 Loss: 1.0581475496292114\n",
      "Iteration 30722 Loss: 1.121355652809143\n",
      "Iteration 30723 Loss: 0.8483203649520874\n",
      "Iteration 30724 Loss: 0.955289363861084\n",
      "Iteration 30725 Loss: 1.0053468942642212\n",
      "Iteration 30726 Loss: 1.0543478727340698\n",
      "Iteration 30727 Loss: 0.9343304634094238\n",
      "Iteration 30728 Loss: 1.1412173509597778\n",
      "Iteration 30729 Loss: 1.090279221534729\n",
      "Iteration 30729 Loss: 1.0167675018310547\n",
      "Iteration 30730 Loss: 1.074863314628601\n",
      "Iteration 30731 Loss: 0.9647144675254822\n",
      "Iteration 30732 Loss: 0.9708956480026245\n",
      "Iteration 30733 Loss: 1.303023099899292\n",
      "Iteration 30734 Loss: 0.8280950784683228\n",
      "Iteration 30735 Loss: 0.973936915397644\n",
      "Iteration 30736 Loss: 0.6259653568267822\n",
      "Iteration 30737 Loss: 1.1905524730682373\n",
      "Iteration 30738 Loss: 0.8620673418045044\n",
      "Iteration 30739 Loss: 0.9410295486450195\n",
      "Iteration 30739 Loss: 0.9735143780708313\n",
      "Iteration 30740 Loss: 1.1166363954544067\n",
      "Iteration 30741 Loss: 1.2683486938476562\n",
      "Iteration 30742 Loss: 0.6328052282333374\n",
      "Iteration 30743 Loss: 0.8849193453788757\n",
      "Iteration 30744 Loss: 1.1773407459259033\n",
      "Iteration 30745 Loss: 0.8491110801696777\n",
      "Iteration 30746 Loss: 1.1356064081192017\n",
      "Iteration 30747 Loss: 1.221543550491333\n",
      "Iteration 30748 Loss: 1.0500402450561523\n",
      "Iteration 30749 Loss: 1.3220245838165283\n",
      "Iteration 30749 Loss: 1.0658376216888428\n",
      "Iteration 30750 Loss: 0.7677600383758545\n",
      "Iteration 30751 Loss: 1.1357295513153076\n",
      "Iteration 30752 Loss: 1.2370251417160034\n",
      "Iteration 30753 Loss: 1.2095756530761719\n",
      "Iteration 30754 Loss: 0.8503629565238953\n",
      "Iteration 30755 Loss: 1.2184914350509644\n",
      "Iteration 30756 Loss: 1.0954720973968506\n",
      "Iteration 30757 Loss: 0.7353310585021973\n",
      "Iteration 30758 Loss: 1.1375362873077393\n",
      "Iteration 30759 Loss: 1.0026729106903076\n",
      "Iteration 30759 Loss: 1.0389957427978516\n",
      "Iteration 30760 Loss: 0.8760297894477844\n",
      "Iteration 30761 Loss: 1.045713186264038\n",
      "Iteration 30762 Loss: 0.5975677967071533\n",
      "Iteration 30763 Loss: 0.8727855682373047\n",
      "Iteration 30764 Loss: 1.1255052089691162\n",
      "Iteration 30765 Loss: 1.1796095371246338\n",
      "Iteration 30766 Loss: 1.021047830581665\n",
      "Iteration 30767 Loss: 0.8563739061355591\n",
      "Iteration 30768 Loss: 0.8407982587814331\n",
      "Iteration 30769 Loss: 0.999801754951477\n",
      "Iteration 30769 Loss: 0.9415232539176941\n",
      "Iteration 30770 Loss: 0.8106734752655029\n",
      "Iteration 30771 Loss: 0.8759636282920837\n",
      "Iteration 30772 Loss: 0.6398147344589233\n",
      "Iteration 30773 Loss: 0.9913515448570251\n",
      "Iteration 30774 Loss: 0.7947685122489929\n",
      "Iteration 30775 Loss: 1.031263828277588\n",
      "Iteration 30776 Loss: 0.5522326827049255\n",
      "Iteration 30777 Loss: 0.7868248224258423\n",
      "Iteration 30778 Loss: 1.1751232147216797\n",
      "Iteration 30779 Loss: 1.2107958793640137\n",
      "Iteration 30779 Loss: 0.8868812322616577\n",
      "Iteration 30780 Loss: 1.3108628988265991\n",
      "Iteration 30781 Loss: 0.9430877566337585\n",
      "Iteration 30782 Loss: 1.2130334377288818\n",
      "Iteration 30783 Loss: 1.1495047807693481\n",
      "Iteration 30784 Loss: 1.3181203603744507\n",
      "Iteration 30785 Loss: 1.049468994140625\n",
      "Iteration 30786 Loss: 1.3806370496749878\n",
      "Iteration 30787 Loss: 0.984815239906311\n",
      "Iteration 30788 Loss: 0.9618688225746155\n",
      "Iteration 30789 Loss: 0.7260986566543579\n",
      "Iteration 30789 Loss: 1.1037498712539673\n",
      "Iteration 30790 Loss: 1.1443158388137817\n",
      "Iteration 30791 Loss: 1.1356507539749146\n",
      "Iteration 30792 Loss: 0.9288359880447388\n",
      "Iteration 30793 Loss: 1.2614835500717163\n",
      "Iteration 30794 Loss: 1.086417317390442\n",
      "Iteration 30795 Loss: 1.0578972101211548\n",
      "Iteration 30796 Loss: 1.0924664735794067\n",
      "Iteration 30797 Loss: 1.1185026168823242\n",
      "Iteration 30798 Loss: 0.658235490322113\n",
      "Iteration 30799 Loss: 0.951601505279541\n",
      "Iteration 30799 Loss: 1.0435407161712646\n",
      "Iteration 30800 Loss: 0.9446858167648315\n",
      "Iteration 30801 Loss: 1.1317824125289917\n",
      "Iteration 30802 Loss: 1.0411158800125122\n",
      "Iteration 30803 Loss: 1.2418756484985352\n",
      "Iteration 30804 Loss: 1.0968860387802124\n",
      "Iteration 30805 Loss: 1.062191128730774\n",
      "Iteration 30806 Loss: 1.0416990518569946\n",
      "Iteration 30807 Loss: 1.0570813417434692\n",
      "Iteration 30808 Loss: 1.2560633420944214\n",
      "Iteration 30809 Loss: 0.7481911778450012\n",
      "Iteration 30809 Loss: 1.0621572732925415\n",
      "Iteration 30810 Loss: 1.2002208232879639\n",
      "Iteration 30811 Loss: 1.0322136878967285\n",
      "Iteration 30812 Loss: 0.6223835945129395\n",
      "Iteration 30813 Loss: 0.8165937662124634\n",
      "Iteration 30814 Loss: 0.8902906179428101\n",
      "Iteration 30815 Loss: 0.8473536968231201\n",
      "Iteration 30816 Loss: 1.104953408241272\n",
      "Iteration 30817 Loss: 1.1731330156326294\n",
      "Iteration 30818 Loss: 1.184741735458374\n",
      "Iteration 30819 Loss: 1.3882627487182617\n",
      "Iteration 30819 Loss: 1.0260146856307983\n",
      "Iteration 30820 Loss: 0.7975729703903198\n",
      "Iteration 30821 Loss: 1.2296937704086304\n",
      "Iteration 30822 Loss: 0.9991400241851807\n",
      "Iteration 30823 Loss: 1.1121524572372437\n",
      "Iteration 30824 Loss: 1.349488377571106\n",
      "Iteration 30825 Loss: 1.0979530811309814\n",
      "Iteration 30826 Loss: 1.1132293939590454\n",
      "Iteration 30827 Loss: 0.912007212638855\n",
      "Iteration 30828 Loss: 0.9199507236480713\n",
      "Iteration 30829 Loss: 1.265877366065979\n",
      "Iteration 30829 Loss: 1.0797066688537598\n",
      "Iteration 30830 Loss: 0.8403820395469666\n",
      "Iteration 30831 Loss: 1.0188385248184204\n",
      "Iteration 30832 Loss: 0.7472846508026123\n",
      "Iteration 30833 Loss: 0.9507099390029907\n",
      "Iteration 30834 Loss: 1.3276287317276\n",
      "Iteration 30835 Loss: 1.0454800128936768\n",
      "Iteration 30836 Loss: 1.0100295543670654\n",
      "Iteration 30837 Loss: 0.9831969738006592\n",
      "Iteration 30838 Loss: 0.9457474946975708\n",
      "Iteration 30839 Loss: 0.7374271750450134\n",
      "Iteration 30839 Loss: 0.9606724977493286\n",
      "Iteration 30840 Loss: 1.4401073455810547\n",
      "Iteration 30841 Loss: 1.0537981986999512\n",
      "Iteration 30842 Loss: 1.131156325340271\n",
      "Iteration 30843 Loss: 0.9045864939689636\n",
      "Iteration 30844 Loss: 0.9339296221733093\n",
      "Iteration 30845 Loss: 0.8211563229560852\n",
      "Iteration 30846 Loss: 1.177065134048462\n",
      "Iteration 30847 Loss: 1.0938752889633179\n",
      "Iteration 30848 Loss: 1.224891185760498\n",
      "Iteration 30849 Loss: 0.7846116423606873\n",
      "Iteration 30849 Loss: 1.0565177202224731\n",
      "Iteration 30850 Loss: 1.316723346710205\n",
      "Iteration 30851 Loss: 0.9850650429725647\n",
      "Iteration 30852 Loss: 1.227311611175537\n",
      "Iteration 30853 Loss: 1.3639973402023315\n",
      "Iteration 30854 Loss: 0.869104266166687\n",
      "Iteration 30855 Loss: 1.240602731704712\n",
      "Iteration 30856 Loss: 1.237291932106018\n",
      "Iteration 30857 Loss: 1.1403696537017822\n",
      "Iteration 30858 Loss: 0.855966329574585\n",
      "Iteration 30859 Loss: 1.2072646617889404\n",
      "Iteration 30859 Loss: 1.1443697214126587\n",
      "Iteration 30860 Loss: 1.333516240119934\n",
      "Iteration 30861 Loss: 1.0191985368728638\n",
      "Iteration 30862 Loss: 1.110548496246338\n",
      "Iteration 30863 Loss: 1.1128723621368408\n",
      "Iteration 30864 Loss: 0.7621313333511353\n",
      "Iteration 30865 Loss: 0.707450807094574\n",
      "Iteration 30866 Loss: 1.109529972076416\n",
      "Iteration 30867 Loss: 1.1225415468215942\n",
      "Iteration 30868 Loss: 1.1329336166381836\n",
      "Iteration 30869 Loss: 1.0392943620681763\n",
      "Iteration 30869 Loss: 1.0450016260147095\n",
      "Iteration 30870 Loss: 1.06248140335083\n",
      "Iteration 30871 Loss: 1.1212354898452759\n",
      "Iteration 30872 Loss: 1.0341181755065918\n",
      "Iteration 30873 Loss: 1.0112974643707275\n",
      "Iteration 30874 Loss: 0.9378741979598999\n",
      "Iteration 30875 Loss: 0.5181995034217834\n",
      "Iteration 30876 Loss: 0.7198659777641296\n",
      "Iteration 30877 Loss: 1.0834293365478516\n",
      "Iteration 30878 Loss: 0.8650435209274292\n",
      "Iteration 30879 Loss: 0.9073413610458374\n",
      "Iteration 30879 Loss: 0.9260886311531067\n",
      "Iteration 30880 Loss: 1.1494066715240479\n",
      "Iteration 30881 Loss: 0.7641376256942749\n",
      "Iteration 30882 Loss: 0.999915599822998\n",
      "Iteration 30883 Loss: 0.8313452005386353\n",
      "Iteration 30884 Loss: 1.0927587747573853\n",
      "Iteration 30885 Loss: 1.0428307056427002\n",
      "Iteration 30886 Loss: 1.2208514213562012\n",
      "Iteration 30887 Loss: 0.7645836472511292\n",
      "Iteration 30888 Loss: 0.681316614151001\n",
      "Iteration 30889 Loss: 0.8547476530075073\n",
      "Iteration 30889 Loss: 0.9401893615722656\n",
      "Iteration 30890 Loss: 1.079972505569458\n",
      "Iteration 30891 Loss: 0.9201705455780029\n",
      "Iteration 30892 Loss: 0.7995247840881348\n",
      "Iteration 30893 Loss: 0.9749257564544678\n",
      "Iteration 30894 Loss: 0.8294000029563904\n",
      "Iteration 30895 Loss: 1.2453186511993408\n",
      "Iteration 30896 Loss: 1.1260151863098145\n",
      "Iteration 30897 Loss: 1.1401207447052002\n",
      "Iteration 30898 Loss: 0.8232998251914978\n",
      "Iteration 30899 Loss: 0.8683207035064697\n",
      "Iteration 30899 Loss: 0.9807068705558777\n",
      "Iteration 30900 Loss: 0.9780890941619873\n",
      "Iteration 30901 Loss: 1.0171432495117188\n",
      "Iteration 30902 Loss: 1.0486575365066528\n",
      "Iteration 30903 Loss: 0.9999451637268066\n",
      "Iteration 30904 Loss: 1.0709257125854492\n",
      "Iteration 30905 Loss: 1.1148980855941772\n",
      "Iteration 30906 Loss: 0.8277892470359802\n",
      "Iteration 30907 Loss: 1.1004596948623657\n",
      "Iteration 30908 Loss: 0.8502199649810791\n",
      "Iteration 30909 Loss: 1.5118036270141602\n",
      "Iteration 30909 Loss: 1.0519931316375732\n",
      "Iteration 30910 Loss: 0.6087905168533325\n",
      "Iteration 30911 Loss: 0.7951440811157227\n",
      "Iteration 30912 Loss: 0.9666083455085754\n",
      "Iteration 30913 Loss: 1.269118309020996\n",
      "Iteration 30914 Loss: 1.1806918382644653\n",
      "Iteration 30915 Loss: 0.9178343415260315\n",
      "Iteration 30916 Loss: 1.058095932006836\n",
      "Iteration 30917 Loss: 0.67465740442276\n",
      "Iteration 30918 Loss: 0.8518479466438293\n",
      "Iteration 30919 Loss: 0.8572156429290771\n",
      "Iteration 30919 Loss: 0.9180004000663757\n",
      "Iteration 30920 Loss: 1.2434780597686768\n",
      "Iteration 30921 Loss: 1.138365387916565\n",
      "Iteration 30922 Loss: 0.959576427936554\n",
      "Iteration 30923 Loss: 0.4970185160636902\n",
      "Iteration 30924 Loss: 0.818280816078186\n",
      "Iteration 30925 Loss: 1.2736961841583252\n",
      "Iteration 30926 Loss: 1.1645231246948242\n",
      "Iteration 30927 Loss: 1.065503716468811\n",
      "Iteration 30928 Loss: 1.0056039094924927\n",
      "Iteration 30929 Loss: 1.4446661472320557\n",
      "Iteration 30929 Loss: 1.0610712766647339\n",
      "Iteration 30930 Loss: 0.9432727098464966\n",
      "Iteration 30931 Loss: 1.157187581062317\n",
      "Iteration 30932 Loss: 1.017535924911499\n",
      "Iteration 30933 Loss: 1.4651973247528076\n",
      "Iteration 30934 Loss: 0.9281665086746216\n",
      "Iteration 30935 Loss: 1.11530601978302\n",
      "Iteration 30936 Loss: 1.0609962940216064\n",
      "Iteration 30937 Loss: 1.0246741771697998\n",
      "Iteration 30938 Loss: 0.9833135604858398\n",
      "Iteration 30939 Loss: 0.7554516196250916\n",
      "Iteration 30939 Loss: 1.0451101064682007\n",
      "Iteration 30940 Loss: 1.0803982019424438\n",
      "Iteration 30941 Loss: 0.9630768299102783\n",
      "Iteration 30942 Loss: 1.034745454788208\n",
      "Iteration 30943 Loss: 1.1718473434448242\n",
      "Iteration 30944 Loss: 0.8858593106269836\n",
      "Iteration 30945 Loss: 1.0292834043502808\n",
      "Iteration 30946 Loss: 0.8072633147239685\n",
      "Iteration 30947 Loss: 0.8959599733352661\n",
      "Iteration 30948 Loss: 1.1775683164596558\n",
      "Iteration 30949 Loss: 0.9643036723136902\n",
      "Iteration 30949 Loss: 1.0010305643081665\n",
      "Iteration 30950 Loss: 1.0460249185562134\n",
      "Iteration 30951 Loss: 1.0060919523239136\n",
      "Iteration 30952 Loss: 0.7317475080490112\n",
      "Iteration 30953 Loss: 1.1481308937072754\n",
      "Iteration 30954 Loss: 0.9724708199501038\n",
      "Iteration 30955 Loss: 0.8082116842269897\n",
      "Iteration 30956 Loss: 1.2014776468276978\n",
      "Iteration 30957 Loss: 1.1005464792251587\n",
      "Iteration 30958 Loss: 0.9404603838920593\n",
      "Iteration 30959 Loss: 1.222743034362793\n",
      "Iteration 30959 Loss: 1.0177905559539795\n",
      "Iteration 30960 Loss: 1.055149793624878\n",
      "Iteration 30961 Loss: 1.0216467380523682\n",
      "Iteration 30962 Loss: 0.776385486125946\n",
      "Iteration 30963 Loss: 0.9559963941574097\n",
      "Iteration 30964 Loss: 1.3021352291107178\n",
      "Iteration 30965 Loss: 1.2496957778930664\n",
      "Iteration 30966 Loss: 1.1484516859054565\n",
      "Iteration 30967 Loss: 1.0205862522125244\n",
      "Iteration 30968 Loss: 1.2800312042236328\n",
      "Iteration 30969 Loss: 1.1541483402252197\n",
      "Iteration 30969 Loss: 1.0964226722717285\n",
      "Iteration 30970 Loss: 1.2201849222183228\n",
      "Iteration 30971 Loss: 1.0324259996414185\n",
      "Iteration 30972 Loss: 1.144087314605713\n",
      "Iteration 30973 Loss: 0.8042572736740112\n",
      "Iteration 30974 Loss: 1.1526834964752197\n",
      "Iteration 30975 Loss: 0.7317274808883667\n",
      "Iteration 30976 Loss: 0.9881815314292908\n",
      "Iteration 30977 Loss: 1.1358094215393066\n",
      "Iteration 30978 Loss: 1.0465635061264038\n",
      "Iteration 30979 Loss: 1.0076947212219238\n",
      "Iteration 30979 Loss: 1.0263614654541016\n",
      "Iteration 30980 Loss: 0.9261065721511841\n",
      "Iteration 30981 Loss: 1.0931071043014526\n",
      "Iteration 30982 Loss: 0.9210891127586365\n",
      "Iteration 30983 Loss: 1.1206891536712646\n",
      "Iteration 30984 Loss: 1.0815969705581665\n",
      "Iteration 30985 Loss: 1.1037571430206299\n",
      "Iteration 30986 Loss: 1.3820608854293823\n",
      "Iteration 30987 Loss: 0.9520449042320251\n",
      "Iteration 30988 Loss: 0.8707727193832397\n",
      "Iteration 30989 Loss: 1.1123133897781372\n",
      "Iteration 30989 Loss: 1.0563538074493408\n",
      "Iteration 30990 Loss: 1.3999909162521362\n",
      "Iteration 30991 Loss: 0.8375152945518494\n",
      "Iteration 30992 Loss: 1.0968588590621948\n",
      "Iteration 30993 Loss: 0.9742519855499268\n",
      "Iteration 30994 Loss: 0.899260938167572\n",
      "Iteration 30995 Loss: 1.053587555885315\n",
      "Iteration 30996 Loss: 0.8808851838111877\n",
      "Iteration 30997 Loss: 1.1520284414291382\n",
      "Iteration 30998 Loss: 0.932472825050354\n",
      "Iteration 30999 Loss: 0.8836646676063538\n",
      "Iteration 30999 Loss: 1.0110516548156738\n",
      "Iteration 31000 Loss: 0.9898332357406616\n",
      "Iteration 31001 Loss: 1.0960842370986938\n",
      "Iteration 31002 Loss: 1.055885910987854\n",
      "Iteration 31003 Loss: 1.1881929636001587\n",
      "Iteration 31004 Loss: 0.837234616279602\n",
      "Iteration 31005 Loss: 0.7243664264678955\n",
      "Iteration 31006 Loss: 1.1009360551834106\n",
      "Iteration 31007 Loss: 1.2967495918273926\n",
      "Iteration 31008 Loss: 1.223137617111206\n",
      "Iteration 31009 Loss: 1.0901070833206177\n",
      "Iteration 31009 Loss: 1.0602527856826782\n",
      "Iteration 31010 Loss: 1.004847764968872\n",
      "Iteration 31011 Loss: 1.0256544351577759\n",
      "Iteration 31012 Loss: 1.2883317470550537\n",
      "Iteration 31013 Loss: 1.3549500703811646\n",
      "Iteration 31014 Loss: 1.1179779767990112\n",
      "Iteration 31015 Loss: 1.010986089706421\n",
      "Iteration 31016 Loss: 0.7477598190307617\n",
      "Iteration 31017 Loss: 0.8816133737564087\n",
      "Iteration 31018 Loss: 1.1863734722137451\n",
      "Iteration 31019 Loss: 1.2081055641174316\n",
      "Iteration 31019 Loss: 1.0826600790023804\n",
      "Iteration 31020 Loss: 1.2179020643234253\n",
      "Iteration 31021 Loss: 0.8101903200149536\n",
      "Iteration 31022 Loss: 1.1233582496643066\n",
      "Iteration 31023 Loss: 1.2829015254974365\n",
      "Iteration 31024 Loss: 1.1078358888626099\n",
      "Iteration 31025 Loss: 0.975722074508667\n",
      "Iteration 31026 Loss: 1.0635284185409546\n",
      "Iteration 31027 Loss: 0.8639771938323975\n",
      "Iteration 31028 Loss: 1.2425254583358765\n",
      "Iteration 31029 Loss: 1.1898772716522217\n",
      "Iteration 31029 Loss: 1.0877817869186401\n",
      "Iteration 31030 Loss: 0.9799070358276367\n",
      "Iteration 31031 Loss: 0.8439578413963318\n",
      "Iteration 31032 Loss: 1.2881088256835938\n",
      "Iteration 31033 Loss: 1.346142053604126\n",
      "Iteration 31034 Loss: 1.0365444421768188\n",
      "Iteration 31035 Loss: 1.1133817434310913\n",
      "Iteration 31036 Loss: 0.5577391386032104\n",
      "Iteration 31037 Loss: 0.9863855242729187\n",
      "Iteration 31038 Loss: 0.9200073480606079\n",
      "Iteration 31039 Loss: 0.9470464587211609\n",
      "Iteration 31039 Loss: 1.0019220113754272\n",
      "Iteration 31040 Loss: 1.0980384349822998\n",
      "Iteration 31041 Loss: 0.9540116786956787\n",
      "Iteration 31042 Loss: 1.0241823196411133\n",
      "Iteration 31043 Loss: 1.2053544521331787\n",
      "Iteration 31044 Loss: 1.0321263074874878\n",
      "Iteration 31045 Loss: 1.2348271608352661\n",
      "Iteration 31046 Loss: 0.47332999110221863\n",
      "Iteration 31047 Loss: 0.9902758598327637\n",
      "Iteration 31048 Loss: 1.1589888334274292\n",
      "Iteration 31049 Loss: 0.7694871425628662\n",
      "Iteration 31049 Loss: 0.9940622448921204\n",
      "Iteration 31050 Loss: 0.9041894674301147\n",
      "Iteration 31051 Loss: 1.002307653427124\n",
      "Iteration 31052 Loss: 1.2235040664672852\n",
      "Iteration 31053 Loss: 0.7251058220863342\n",
      "Iteration 31054 Loss: 1.0493916273117065\n",
      "Iteration 31055 Loss: 1.1723792552947998\n",
      "Iteration 31056 Loss: 0.891532838344574\n",
      "Iteration 31057 Loss: 0.6871532797813416\n",
      "Iteration 31058 Loss: 1.0235892534255981\n",
      "Iteration 31059 Loss: 1.052883505821228\n",
      "Iteration 31059 Loss: 0.9732036590576172\n",
      "Iteration 31060 Loss: 1.0332216024398804\n",
      "Iteration 31061 Loss: 1.0158966779708862\n",
      "Iteration 31062 Loss: 0.9228236675262451\n",
      "Iteration 31063 Loss: 1.2267106771469116\n",
      "Iteration 31064 Loss: 1.1049188375473022\n",
      "Iteration 31065 Loss: 0.948908805847168\n",
      "Iteration 31066 Loss: 1.2083288431167603\n",
      "Iteration 31067 Loss: 1.4095184803009033\n",
      "Iteration 31068 Loss: 1.5109953880310059\n",
      "Iteration 31069 Loss: 0.9549350142478943\n",
      "Iteration 31069 Loss: 1.133625864982605\n",
      "Iteration 31070 Loss: 0.9126796722412109\n",
      "Iteration 31071 Loss: 1.1527743339538574\n",
      "Iteration 31072 Loss: 0.9839933514595032\n",
      "Iteration 31073 Loss: 1.1207334995269775\n",
      "Iteration 31074 Loss: 0.8229557871818542\n",
      "Iteration 31075 Loss: 1.273263692855835\n",
      "Iteration 31076 Loss: 1.0732059478759766\n",
      "Iteration 31077 Loss: 1.350009560585022\n",
      "Iteration 31078 Loss: 1.2258191108703613\n",
      "Iteration 31079 Loss: 1.0091688632965088\n",
      "Iteration 31079 Loss: 1.0924603939056396\n",
      "Iteration 31080 Loss: 1.3539265394210815\n",
      "Iteration 31081 Loss: 0.792361319065094\n",
      "Iteration 31082 Loss: 0.6523899435997009\n",
      "Iteration 31083 Loss: 1.2214272022247314\n",
      "Iteration 31084 Loss: 0.8570868372917175\n",
      "Iteration 31085 Loss: 0.5028843879699707\n",
      "Iteration 31086 Loss: 0.9364253878593445\n",
      "Iteration 31087 Loss: 1.0051443576812744\n",
      "Iteration 31088 Loss: 1.0572478771209717\n",
      "Iteration 31089 Loss: 0.7820779085159302\n",
      "Iteration 31089 Loss: 0.9160970449447632\n",
      "Iteration 31090 Loss: 0.9738195538520813\n",
      "Iteration 31091 Loss: 0.958848774433136\n",
      "Iteration 31092 Loss: 0.8328232169151306\n",
      "Iteration 31093 Loss: 1.1104875802993774\n",
      "Iteration 31094 Loss: 0.9465523958206177\n",
      "Iteration 31095 Loss: 0.7925649881362915\n",
      "Iteration 31096 Loss: 0.8107240796089172\n",
      "Iteration 31097 Loss: 1.067887783050537\n",
      "Iteration 31098 Loss: 0.8492960929870605\n",
      "Iteration 31099 Loss: 1.040402889251709\n",
      "Iteration 31099 Loss: 0.9383407831192017\n",
      "Iteration 31100 Loss: 1.1222045421600342\n",
      "Iteration 31101 Loss: 1.1924552917480469\n",
      "Iteration 31102 Loss: 1.024389624595642\n",
      "Iteration 31103 Loss: 1.3250911235809326\n",
      "Iteration 31104 Loss: 1.40492844581604\n",
      "Iteration 31105 Loss: 1.007981300354004\n",
      "Iteration 31106 Loss: 0.7026084661483765\n",
      "Iteration 31107 Loss: 0.8870081901550293\n",
      "Iteration 31108 Loss: 1.337051510810852\n",
      "Iteration 31109 Loss: 1.1459343433380127\n",
      "Iteration 31109 Loss: 1.1149652004241943\n",
      "Iteration 31110 Loss: 0.9574567079544067\n",
      "Iteration 31111 Loss: 0.7383398413658142\n",
      "Iteration 31112 Loss: 1.0640760660171509\n",
      "Iteration 31113 Loss: 1.0703411102294922\n",
      "Iteration 31114 Loss: 0.8576180338859558\n",
      "Iteration 31115 Loss: 1.0066044330596924\n",
      "Iteration 31116 Loss: 1.382908821105957\n",
      "Iteration 31117 Loss: 1.1695201396942139\n",
      "Iteration 31118 Loss: 1.1025949716567993\n",
      "Iteration 31119 Loss: 1.4134405851364136\n",
      "Iteration 31119 Loss: 1.0762900114059448\n",
      "Iteration 31120 Loss: 0.9224405884742737\n",
      "Iteration 31121 Loss: 1.2082947492599487\n",
      "Iteration 31122 Loss: 0.7144723534584045\n",
      "Iteration 31123 Loss: 1.014195442199707\n",
      "Iteration 31124 Loss: 1.03045654296875\n",
      "Iteration 31125 Loss: 1.007044792175293\n",
      "Iteration 31126 Loss: 0.9967266917228699\n",
      "Iteration 31127 Loss: 1.0319167375564575\n",
      "Iteration 31128 Loss: 1.0202150344848633\n",
      "Iteration 31129 Loss: 1.2158222198486328\n",
      "Iteration 31129 Loss: 1.0161585807800293\n",
      "Iteration 31130 Loss: 0.9034510850906372\n",
      "Iteration 31131 Loss: 1.0899215936660767\n",
      "Iteration 31132 Loss: 0.7156525254249573\n",
      "Iteration 31133 Loss: 1.1015770435333252\n",
      "Iteration 31134 Loss: 1.0796730518341064\n",
      "Iteration 31135 Loss: 1.0256218910217285\n",
      "Iteration 31136 Loss: 0.9269278645515442\n",
      "Iteration 31137 Loss: 1.0424052476882935\n",
      "Iteration 31138 Loss: 1.0187320709228516\n",
      "Iteration 31139 Loss: 0.9661102294921875\n",
      "Iteration 31139 Loss: 0.9870072603225708\n",
      "Iteration 31140 Loss: 0.9801737070083618\n",
      "Iteration 31141 Loss: 0.9686421155929565\n",
      "Iteration 31142 Loss: 0.8089343905448914\n",
      "Iteration 31143 Loss: 0.7977575659751892\n",
      "Iteration 31144 Loss: 1.0534286499023438\n",
      "Iteration 31145 Loss: 1.3843342065811157\n",
      "Iteration 31146 Loss: 0.9251238107681274\n",
      "Iteration 31147 Loss: 1.103477954864502\n",
      "Iteration 31148 Loss: 1.031815528869629\n",
      "Iteration 31149 Loss: 0.8267882466316223\n",
      "Iteration 31149 Loss: 0.9880475997924805\n",
      "Iteration 31150 Loss: 0.9818356037139893\n",
      "Iteration 31151 Loss: 0.921077311038971\n",
      "Iteration 31152 Loss: 1.111716628074646\n",
      "Iteration 31153 Loss: 1.0809110403060913\n",
      "Iteration 31154 Loss: 0.8510661125183105\n",
      "Iteration 31155 Loss: 1.0457179546356201\n",
      "Iteration 31156 Loss: 1.3485406637191772\n",
      "Iteration 31157 Loss: 0.7950475811958313\n",
      "Iteration 31158 Loss: 0.9484916925430298\n",
      "Iteration 31159 Loss: 1.1352174282073975\n",
      "Iteration 31159 Loss: 1.0219621658325195\n",
      "Iteration 31160 Loss: 0.9853174090385437\n",
      "Iteration 31161 Loss: 1.304174780845642\n",
      "Iteration 31162 Loss: 1.0368201732635498\n",
      "Iteration 31163 Loss: 0.8129724264144897\n",
      "Iteration 31164 Loss: 1.0668736696243286\n",
      "Iteration 31165 Loss: 0.9580308198928833\n",
      "Iteration 31166 Loss: 1.0099365711212158\n",
      "Iteration 31167 Loss: 0.8645225167274475\n",
      "Iteration 31168 Loss: 0.9898501038551331\n",
      "Iteration 31169 Loss: 0.7755162119865417\n",
      "Iteration 31169 Loss: 0.9804015159606934\n",
      "Iteration 31170 Loss: 1.1237597465515137\n",
      "Iteration 31171 Loss: 0.8796666264533997\n",
      "Iteration 31172 Loss: 0.8664279580116272\n",
      "Iteration 31173 Loss: 1.0502638816833496\n",
      "Iteration 31174 Loss: 1.0794938802719116\n",
      "Iteration 31175 Loss: 1.0831973552703857\n",
      "Iteration 31176 Loss: 0.9160920977592468\n",
      "Iteration 31177 Loss: 1.0117794275283813\n",
      "Iteration 31178 Loss: 1.2206153869628906\n",
      "Iteration 31179 Loss: 1.1974533796310425\n",
      "Iteration 31179 Loss: 1.042875051498413\n",
      "Iteration 31180 Loss: 0.9327348470687866\n",
      "Iteration 31181 Loss: 0.9800776243209839\n",
      "Iteration 31182 Loss: 0.7936075925827026\n",
      "Iteration 31183 Loss: 0.831602931022644\n",
      "Iteration 31184 Loss: 0.833692729473114\n",
      "Iteration 31185 Loss: 1.1575785875320435\n",
      "Iteration 31186 Loss: 1.3730648756027222\n",
      "Iteration 31187 Loss: 1.1025503873825073\n",
      "Iteration 31188 Loss: 0.9825654625892639\n",
      "Iteration 31189 Loss: 1.1040579080581665\n",
      "Iteration 31189 Loss: 1.0091532468795776\n",
      "Iteration 31190 Loss: 1.0114624500274658\n",
      "Iteration 31191 Loss: 1.1281440258026123\n",
      "Iteration 31192 Loss: 1.2982591390609741\n",
      "Iteration 31193 Loss: 0.8790332078933716\n",
      "Iteration 31194 Loss: 1.0232760906219482\n",
      "Iteration 31195 Loss: 0.9686717391014099\n",
      "Iteration 31196 Loss: 1.1708182096481323\n",
      "Iteration 31197 Loss: 0.8676233887672424\n",
      "Iteration 31198 Loss: 1.029536247253418\n",
      "Iteration 31199 Loss: 0.7421874403953552\n",
      "Iteration 31199 Loss: 1.0119011402130127\n",
      "Iteration 31200 Loss: 1.1255881786346436\n",
      "Iteration 31201 Loss: 1.0775675773620605\n",
      "Iteration 31202 Loss: 1.2597273588180542\n",
      "Iteration 31203 Loss: 1.0086296796798706\n",
      "Iteration 31204 Loss: 1.3661913871765137\n",
      "Iteration 31205 Loss: 1.128684639930725\n",
      "Iteration 31206 Loss: 1.180371880531311\n",
      "Iteration 31207 Loss: 0.7421045899391174\n",
      "Iteration 31208 Loss: 1.188258409500122\n",
      "Iteration 31209 Loss: 0.8640450835227966\n",
      "Iteration 31209 Loss: 1.0941169261932373\n",
      "Iteration 31210 Loss: 0.7554540038108826\n",
      "Iteration 31211 Loss: 0.8371321558952332\n",
      "Iteration 31212 Loss: 1.1958597898483276\n",
      "Iteration 31213 Loss: 1.286204218864441\n",
      "Iteration 31214 Loss: 1.1531894207000732\n",
      "Iteration 31215 Loss: 1.3911645412445068\n",
      "Iteration 31216 Loss: 0.71277916431427\n",
      "Iteration 31217 Loss: 1.0384975671768188\n",
      "Iteration 31218 Loss: 1.1779800653457642\n",
      "Iteration 31219 Loss: 0.9108115434646606\n",
      "Iteration 31219 Loss: 1.0459072589874268\n",
      "Iteration 31220 Loss: 1.1223169565200806\n",
      "Iteration 31221 Loss: 1.232997179031372\n",
      "Iteration 31222 Loss: 1.0809671878814697\n",
      "Iteration 31223 Loss: 0.9995366334915161\n",
      "Iteration 31224 Loss: 0.9043330550193787\n",
      "Iteration 31225 Loss: 1.1474111080169678\n",
      "Iteration 31226 Loss: 0.923243522644043\n",
      "Iteration 31227 Loss: 0.8416316509246826\n",
      "Iteration 31228 Loss: 0.84626704454422\n",
      "Iteration 31229 Loss: 1.0510228872299194\n",
      "Iteration 31229 Loss: 1.0149728059768677\n",
      "Iteration 31230 Loss: 1.1883431673049927\n",
      "Iteration 31231 Loss: 1.1457332372665405\n",
      "Iteration 31232 Loss: 1.189915657043457\n",
      "Iteration 31233 Loss: 1.2417339086532593\n",
      "Iteration 31234 Loss: 1.072852373123169\n",
      "Iteration 31235 Loss: 0.804366946220398\n",
      "Iteration 31236 Loss: 0.9367476105690002\n",
      "Iteration 31237 Loss: 1.2758183479309082\n",
      "Iteration 31238 Loss: 1.079193115234375\n",
      "Iteration 31239 Loss: 0.889799952507019\n",
      "Iteration 31239 Loss: 1.08245050907135\n",
      "Iteration 31240 Loss: 1.2063552141189575\n",
      "Iteration 31241 Loss: 0.8815183043479919\n",
      "Iteration 31242 Loss: 0.9320999383926392\n",
      "Iteration 31243 Loss: 1.1434509754180908\n",
      "Iteration 31244 Loss: 0.9998964071273804\n",
      "Iteration 31245 Loss: 0.7958947420120239\n",
      "Iteration 31246 Loss: 0.9035645127296448\n",
      "Iteration 31247 Loss: 0.6835145950317383\n",
      "Iteration 31248 Loss: 1.1813589334487915\n",
      "Iteration 31249 Loss: 1.0773464441299438\n",
      "Iteration 31249 Loss: 0.9805000424385071\n",
      "Iteration 31250 Loss: 0.9155381321907043\n",
      "Iteration 31251 Loss: 1.1846345663070679\n",
      "Iteration 31252 Loss: 1.0423063039779663\n",
      "Iteration 31253 Loss: 0.6517770886421204\n",
      "Iteration 31254 Loss: 1.0104212760925293\n",
      "Iteration 31255 Loss: 1.2753175497055054\n",
      "Iteration 31256 Loss: 0.997972309589386\n",
      "Iteration 31257 Loss: 1.0187408924102783\n",
      "Iteration 31258 Loss: 1.3999834060668945\n",
      "Iteration 31259 Loss: 1.1530321836471558\n",
      "Iteration 31259 Loss: 1.0649724006652832\n",
      "Iteration 31260 Loss: 0.9814599752426147\n",
      "Iteration 31261 Loss: 0.9592216610908508\n",
      "Iteration 31262 Loss: 0.5791354775428772\n",
      "Iteration 31263 Loss: 1.0566997528076172\n",
      "Iteration 31264 Loss: 1.0043758153915405\n",
      "Iteration 31265 Loss: 0.8358187675476074\n",
      "Iteration 31266 Loss: 1.1986385583877563\n",
      "Iteration 31267 Loss: 1.0450949668884277\n",
      "Iteration 31268 Loss: 0.9787584543228149\n",
      "Iteration 31269 Loss: 0.8793047070503235\n",
      "Iteration 31269 Loss: 0.9518508911132812\n",
      "Iteration 31270 Loss: 0.8399762511253357\n",
      "Iteration 31271 Loss: 1.1124814748764038\n",
      "Iteration 31272 Loss: 1.5506407022476196\n",
      "Iteration 31273 Loss: 1.242913007736206\n",
      "Iteration 31274 Loss: 0.9724411964416504\n",
      "Iteration 31275 Loss: 0.8625478148460388\n",
      "Iteration 31276 Loss: 0.5716967582702637\n",
      "Iteration 31277 Loss: 1.043884038925171\n",
      "Iteration 31278 Loss: 1.1085448265075684\n",
      "Iteration 31279 Loss: 1.2333102226257324\n",
      "Iteration 31279 Loss: 1.0538437366485596\n",
      "Iteration 31280 Loss: 1.0020027160644531\n",
      "Iteration 31281 Loss: 0.8303827047348022\n",
      "Iteration 31282 Loss: 0.972181499004364\n",
      "Iteration 31283 Loss: 1.0946781635284424\n",
      "Iteration 31284 Loss: 1.476365566253662\n",
      "Iteration 31285 Loss: 1.0350041389465332\n",
      "Iteration 31286 Loss: 0.9383305311203003\n",
      "Iteration 31287 Loss: 0.9176965355873108\n",
      "Iteration 31288 Loss: 1.3314770460128784\n",
      "Iteration 31289 Loss: 1.2071422338485718\n",
      "Iteration 31289 Loss: 1.0805261135101318\n",
      "Iteration 31290 Loss: 1.2765368223190308\n",
      "Iteration 31291 Loss: 1.0472184419631958\n",
      "Iteration 31292 Loss: 1.238120675086975\n",
      "Iteration 31293 Loss: 0.7867233157157898\n",
      "Iteration 31294 Loss: 0.9678314924240112\n",
      "Iteration 31295 Loss: 1.1393567323684692\n",
      "Iteration 31296 Loss: 1.0228320360183716\n",
      "Iteration 31297 Loss: 1.121791124343872\n",
      "Iteration 31298 Loss: 1.0937291383743286\n",
      "Iteration 31299 Loss: 1.0906404256820679\n",
      "Iteration 31299 Loss: 1.0784779787063599\n",
      "Iteration 31300 Loss: 0.9783412218093872\n",
      "Iteration 31301 Loss: 1.0904427766799927\n",
      "Iteration 31302 Loss: 0.9358041882514954\n",
      "Iteration 31303 Loss: 1.2706400156021118\n",
      "Iteration 31304 Loss: 0.8727894425392151\n",
      "Iteration 31305 Loss: 1.0113308429718018\n",
      "Iteration 31306 Loss: 0.8655038475990295\n",
      "Iteration 31307 Loss: 0.8584101796150208\n",
      "Iteration 31308 Loss: 0.9231957793235779\n",
      "Iteration 31309 Loss: 0.9063578248023987\n",
      "Iteration 31309 Loss: 0.9712816476821899\n",
      "Iteration 31310 Loss: 1.10826575756073\n",
      "Iteration 31311 Loss: 1.0431971549987793\n",
      "Iteration 31312 Loss: 0.9336922764778137\n",
      "Iteration 31313 Loss: 1.0913569927215576\n",
      "Iteration 31314 Loss: 1.1207406520843506\n",
      "Iteration 31315 Loss: 0.7012186050415039\n",
      "Iteration 31316 Loss: 1.1759206056594849\n",
      "Iteration 31317 Loss: 0.9848998785018921\n",
      "Iteration 31318 Loss: 1.3034210205078125\n",
      "Iteration 31319 Loss: 0.6752693057060242\n",
      "Iteration 31319 Loss: 1.0137982368469238\n",
      "Iteration 31320 Loss: 1.1780250072479248\n",
      "Iteration 31321 Loss: 0.7301377058029175\n",
      "Iteration 31322 Loss: 1.1163800954818726\n",
      "Iteration 31323 Loss: 0.8963640928268433\n",
      "Iteration 31324 Loss: 0.9571836590766907\n",
      "Iteration 31325 Loss: 0.756775975227356\n",
      "Iteration 31326 Loss: 1.4943382740020752\n",
      "Iteration 31327 Loss: 1.0180624723434448\n",
      "Iteration 31328 Loss: 0.9388811588287354\n",
      "Iteration 31329 Loss: 0.7322756052017212\n",
      "Iteration 31329 Loss: 0.9818423986434937\n",
      "Iteration 31330 Loss: 0.9074872732162476\n",
      "Iteration 31331 Loss: 1.0062243938446045\n",
      "Iteration 31332 Loss: 1.1629865169525146\n",
      "Iteration 31333 Loss: 1.0005816221237183\n",
      "Iteration 31334 Loss: 0.9606462121009827\n",
      "Iteration 31335 Loss: 1.027622103691101\n",
      "Iteration 31336 Loss: 1.0366647243499756\n",
      "Iteration 31337 Loss: 0.9231764674186707\n",
      "Iteration 31338 Loss: 0.8925506472587585\n",
      "Iteration 31339 Loss: 0.8602466583251953\n",
      "Iteration 31339 Loss: 0.9778186678886414\n",
      "Iteration 31340 Loss: 0.7047736048698425\n",
      "Iteration 31341 Loss: 1.1266133785247803\n",
      "Iteration 31342 Loss: 0.8382315635681152\n",
      "Iteration 31343 Loss: 0.7123199701309204\n",
      "Iteration 31344 Loss: 0.90869140625\n",
      "Iteration 31345 Loss: 0.8581936359405518\n",
      "Iteration 31346 Loss: 0.8135402202606201\n",
      "Iteration 31347 Loss: 0.684723973274231\n",
      "Iteration 31348 Loss: 1.4540505409240723\n",
      "Iteration 31349 Loss: 0.8822492957115173\n",
      "Iteration 31349 Loss: 0.898338794708252\n",
      "Iteration 31350 Loss: 1.1944522857666016\n",
      "Iteration 31351 Loss: 1.0342191457748413\n",
      "Iteration 31352 Loss: 0.920037031173706\n",
      "Iteration 31353 Loss: 0.8939776420593262\n",
      "Iteration 31354 Loss: 1.2017667293548584\n",
      "Iteration 31355 Loss: 0.8487908840179443\n",
      "Iteration 31356 Loss: 0.9731613993644714\n",
      "Iteration 31357 Loss: 0.4987024664878845\n",
      "Iteration 31358 Loss: 0.9810658693313599\n",
      "Iteration 31359 Loss: 0.6767992377281189\n",
      "Iteration 31359 Loss: 0.9222972989082336\n",
      "Iteration 31360 Loss: 0.8120987415313721\n",
      "Iteration 31361 Loss: 1.341768503189087\n",
      "Iteration 31362 Loss: 1.1321089267730713\n",
      "Iteration 31363 Loss: 0.909709632396698\n",
      "Iteration 31364 Loss: 1.0677776336669922\n",
      "Iteration 31365 Loss: 0.9209341406822205\n",
      "Iteration 31366 Loss: 0.9391158223152161\n",
      "Iteration 31367 Loss: 0.683839738368988\n",
      "Iteration 31368 Loss: 1.162481665611267\n",
      "Iteration 31369 Loss: 0.5225486159324646\n",
      "Iteration 31369 Loss: 0.9492383003234863\n",
      "Iteration 31370 Loss: 1.27836012840271\n",
      "Iteration 31371 Loss: 1.0989476442337036\n",
      "Iteration 31372 Loss: 0.7983661890029907\n",
      "Iteration 31373 Loss: 1.25171959400177\n",
      "Iteration 31374 Loss: 0.9693736433982849\n",
      "Iteration 31375 Loss: 0.8979927897453308\n",
      "Iteration 31376 Loss: 1.0529590845108032\n",
      "Iteration 31377 Loss: 0.8848924040794373\n",
      "Iteration 31378 Loss: 1.1257433891296387\n",
      "Iteration 31379 Loss: 0.8951032161712646\n",
      "Iteration 31379 Loss: 1.025345802307129\n",
      "Iteration 31380 Loss: 1.0520155429840088\n",
      "Iteration 31381 Loss: 0.8576958775520325\n",
      "Iteration 31382 Loss: 0.9655554294586182\n",
      "Iteration 31383 Loss: 0.8526017665863037\n",
      "Iteration 31384 Loss: 0.6591594219207764\n",
      "Iteration 31385 Loss: 1.0793651342391968\n",
      "Iteration 31386 Loss: 1.3877595663070679\n",
      "Iteration 31387 Loss: 0.840382993221283\n",
      "Iteration 31388 Loss: 1.1504735946655273\n",
      "Iteration 31389 Loss: 0.8490651845932007\n",
      "Iteration 31389 Loss: 0.9694074392318726\n",
      "Iteration 31390 Loss: 1.6156057119369507\n",
      "Iteration 31391 Loss: 1.292402744293213\n",
      "Iteration 31392 Loss: 1.0803848505020142\n",
      "Iteration 31393 Loss: 0.7876882553100586\n",
      "Iteration 31394 Loss: 1.1723164319992065\n",
      "Iteration 31395 Loss: 0.9487515687942505\n",
      "Iteration 31396 Loss: 1.2173418998718262\n",
      "Iteration 31397 Loss: 1.0035277605056763\n",
      "Iteration 31398 Loss: 1.2486298084259033\n",
      "Iteration 31399 Loss: 1.1785807609558105\n",
      "Iteration 31399 Loss: 1.1545230150222778\n",
      "Iteration 31400 Loss: 1.0018000602722168\n",
      "Iteration 31401 Loss: 0.7128463983535767\n",
      "Iteration 31402 Loss: 1.233221173286438\n",
      "Iteration 31403 Loss: 1.1859722137451172\n",
      "Iteration 31404 Loss: 1.0524886846542358\n",
      "Iteration 31405 Loss: 0.9102587103843689\n",
      "Iteration 31406 Loss: 0.848281979560852\n",
      "Iteration 31407 Loss: 0.6204912662506104\n",
      "Iteration 31408 Loss: 0.9119892120361328\n",
      "Iteration 31409 Loss: 0.9651702046394348\n",
      "Iteration 31409 Loss: 0.9442520141601562\n",
      "Iteration 31410 Loss: 1.0513911247253418\n",
      "Iteration 31411 Loss: 1.0362319946289062\n",
      "Iteration 31412 Loss: 1.1793900728225708\n",
      "Iteration 31413 Loss: 1.1016310453414917\n",
      "Iteration 31414 Loss: 1.3463867902755737\n",
      "Iteration 31415 Loss: 0.8595693707466125\n",
      "Iteration 31416 Loss: 0.8901171088218689\n",
      "Iteration 31417 Loss: 0.6646707057952881\n",
      "Iteration 31418 Loss: 0.9570960998535156\n",
      "Iteration 31419 Loss: 1.2410731315612793\n",
      "Iteration 31419 Loss: 1.032755732536316\n",
      "Iteration 31420 Loss: 1.1945019960403442\n",
      "Iteration 31421 Loss: 0.9806564450263977\n",
      "Iteration 31422 Loss: 1.1012343168258667\n",
      "Iteration 31423 Loss: 0.8860686421394348\n",
      "Iteration 31424 Loss: 0.9915377497673035\n",
      "Iteration 31425 Loss: 1.0298280715942383\n",
      "Iteration 31426 Loss: 1.1395381689071655\n",
      "Iteration 31427 Loss: 0.848395049571991\n",
      "Iteration 31428 Loss: 0.9141106009483337\n",
      "Iteration 31429 Loss: 0.9532513618469238\n",
      "Iteration 31429 Loss: 1.0039122104644775\n",
      "Iteration 31430 Loss: 0.6907055377960205\n",
      "Iteration 31431 Loss: 1.259883999824524\n",
      "Iteration 31432 Loss: 1.2962521314620972\n",
      "Iteration 31433 Loss: 0.8965792655944824\n",
      "Iteration 31434 Loss: 1.0511133670806885\n",
      "Iteration 31435 Loss: 0.9253379702568054\n",
      "Iteration 31436 Loss: 0.6067543029785156\n",
      "Iteration 31437 Loss: 0.9990295171737671\n",
      "Iteration 31438 Loss: 0.777992308139801\n",
      "Iteration 31439 Loss: 0.7465680837631226\n",
      "Iteration 31439 Loss: 0.9250216484069824\n",
      "Iteration 31440 Loss: 0.9517617225646973\n",
      "Iteration 31441 Loss: 0.9334081411361694\n",
      "Iteration 31442 Loss: 0.8078803420066833\n",
      "Iteration 31443 Loss: 1.0987296104431152\n",
      "Iteration 31444 Loss: 1.1467995643615723\n",
      "Iteration 31445 Loss: 1.0700527429580688\n",
      "Iteration 31446 Loss: 0.7018907070159912\n",
      "Iteration 31447 Loss: 1.052940845489502\n",
      "Iteration 31448 Loss: 0.9275538325309753\n",
      "Iteration 31449 Loss: 0.8580625057220459\n",
      "Iteration 31449 Loss: 0.954908013343811\n",
      "Iteration 31450 Loss: 0.6939157247543335\n",
      "Iteration 31451 Loss: 1.1760262250900269\n",
      "Iteration 31452 Loss: 1.358634114265442\n",
      "Iteration 31453 Loss: 0.6375464200973511\n",
      "Iteration 31454 Loss: 1.0434495210647583\n",
      "Iteration 31455 Loss: 0.9714177846908569\n",
      "Iteration 31456 Loss: 0.9064168930053711\n",
      "Iteration 31457 Loss: 1.2785331010818481\n",
      "Iteration 31458 Loss: 0.8400593996047974\n",
      "Iteration 31459 Loss: 0.9916496276855469\n",
      "Iteration 31459 Loss: 0.9897648692131042\n",
      "Iteration 31460 Loss: 1.0895670652389526\n",
      "Iteration 31461 Loss: 0.8710638880729675\n",
      "Iteration 31462 Loss: 1.2150554656982422\n",
      "Iteration 31463 Loss: 0.9518221616744995\n",
      "Iteration 31464 Loss: 0.9706332087516785\n",
      "Iteration 31465 Loss: 1.289953589439392\n",
      "Iteration 31466 Loss: 1.0757145881652832\n",
      "Iteration 31467 Loss: 1.1779813766479492\n",
      "Iteration 31468 Loss: 1.2187443971633911\n",
      "Iteration 31469 Loss: 0.8261299729347229\n",
      "Iteration 31469 Loss: 1.0686665773391724\n",
      "Iteration 31470 Loss: 0.8418622016906738\n",
      "Iteration 31471 Loss: 0.8924964666366577\n",
      "Iteration 31472 Loss: 0.9959501028060913\n",
      "Iteration 31473 Loss: 0.8517240881919861\n",
      "Iteration 31474 Loss: 1.1617870330810547\n",
      "Iteration 31475 Loss: 1.153438925743103\n",
      "Iteration 31476 Loss: 0.8219120502471924\n",
      "Iteration 31477 Loss: 1.1781959533691406\n",
      "Iteration 31478 Loss: 1.1128591299057007\n",
      "Iteration 31479 Loss: 1.0972102880477905\n",
      "Iteration 31479 Loss: 1.0107436180114746\n",
      "Iteration 31480 Loss: 0.9551048278808594\n",
      "Iteration 31481 Loss: 1.1076680421829224\n",
      "Iteration 31482 Loss: 1.1221258640289307\n",
      "Iteration 31483 Loss: 1.0900368690490723\n",
      "Iteration 31484 Loss: 1.1934705972671509\n",
      "Iteration 31485 Loss: 1.3533501625061035\n",
      "Iteration 31486 Loss: 1.129529356956482\n",
      "Iteration 31487 Loss: 0.9883532524108887\n",
      "Iteration 31488 Loss: 0.7003161311149597\n",
      "Iteration 31489 Loss: 1.0508384704589844\n",
      "Iteration 31489 Loss: 1.0690791606903076\n",
      "Iteration 31490 Loss: 1.075886607170105\n",
      "Iteration 31491 Loss: 0.719579815864563\n",
      "Iteration 31492 Loss: 1.179506778717041\n",
      "Iteration 31493 Loss: 1.105000376701355\n",
      "Iteration 31494 Loss: 1.0586460828781128\n",
      "Iteration 31495 Loss: 1.1222225427627563\n",
      "Iteration 31496 Loss: 0.7872731685638428\n",
      "Iteration 31497 Loss: 1.1757307052612305\n",
      "Iteration 31498 Loss: 1.293553352355957\n",
      "Iteration 31499 Loss: 0.9256715774536133\n",
      "Iteration 31499 Loss: 1.0443071126937866\n",
      "Iteration 31500 Loss: 0.9795436859130859\n",
      "Iteration 31501 Loss: 1.56281578540802\n",
      "Iteration 31502 Loss: 1.2567403316497803\n",
      "Iteration 31503 Loss: 1.106300950050354\n",
      "Iteration 31504 Loss: 0.93927001953125\n",
      "Iteration 31505 Loss: 1.3193336725234985\n",
      "Iteration 31506 Loss: 0.8844587206840515\n",
      "Iteration 31507 Loss: 0.8564706444740295\n",
      "Iteration 31508 Loss: 1.054192304611206\n",
      "Iteration 31509 Loss: 0.9797302484512329\n",
      "Iteration 31509 Loss: 1.0938856601715088\n",
      "Iteration 31510 Loss: 0.7146245241165161\n",
      "Iteration 31511 Loss: 1.2512943744659424\n",
      "Iteration 31512 Loss: 0.6255850791931152\n",
      "Iteration 31513 Loss: 1.026965618133545\n",
      "Iteration 31514 Loss: 0.8920778632164001\n",
      "Iteration 31515 Loss: 1.0081971883773804\n",
      "Iteration 31516 Loss: 0.8766457438468933\n",
      "Iteration 31517 Loss: 0.8856815695762634\n",
      "Iteration 31518 Loss: 1.0863728523254395\n",
      "Iteration 31519 Loss: 0.7065465450286865\n",
      "Iteration 31519 Loss: 0.90739905834198\n",
      "Iteration 31520 Loss: 1.3875186443328857\n",
      "Iteration 31521 Loss: 1.0629249811172485\n",
      "Iteration 31522 Loss: 1.383676290512085\n",
      "Iteration 31523 Loss: 1.137227177619934\n",
      "Iteration 31524 Loss: 0.9995720982551575\n",
      "Iteration 31525 Loss: 0.808862030506134\n",
      "Iteration 31526 Loss: 0.9730602502822876\n",
      "Iteration 31527 Loss: 0.9251480102539062\n",
      "Iteration 31528 Loss: 0.9120153784751892\n",
      "Iteration 31529 Loss: 1.2243573665618896\n",
      "Iteration 31529 Loss: 1.0814361572265625\n",
      "Iteration 31530 Loss: 0.8086041212081909\n",
      "Iteration 31531 Loss: 0.798522412776947\n",
      "Iteration 31532 Loss: 0.885554313659668\n",
      "Iteration 31533 Loss: 0.877094030380249\n",
      "Iteration 31534 Loss: 0.7816217541694641\n",
      "Iteration 31535 Loss: 0.8852430582046509\n",
      "Iteration 31536 Loss: 1.0527414083480835\n",
      "Iteration 31537 Loss: 1.1359508037567139\n",
      "Iteration 31538 Loss: 1.4293123483657837\n",
      "Iteration 31539 Loss: 1.1895554065704346\n",
      "Iteration 31539 Loss: 0.9844200015068054\n",
      "Iteration 31540 Loss: 0.8308209180831909\n",
      "Iteration 31541 Loss: 1.2068963050842285\n",
      "Iteration 31542 Loss: 1.053774118423462\n",
      "Iteration 31543 Loss: 1.1764966249465942\n",
      "Iteration 31544 Loss: 1.0194004774093628\n",
      "Iteration 31545 Loss: 0.7054360508918762\n",
      "Iteration 31546 Loss: 1.1117546558380127\n",
      "Iteration 31547 Loss: 0.8341485261917114\n",
      "Iteration 31548 Loss: 0.8962604403495789\n",
      "Iteration 31549 Loss: 1.072003960609436\n",
      "Iteration 31549 Loss: 0.9906991720199585\n",
      "Iteration 31550 Loss: 0.8861333131790161\n",
      "Iteration 31551 Loss: 0.8985885381698608\n",
      "Iteration 31552 Loss: 1.1440218687057495\n",
      "Iteration 31553 Loss: 1.2222994565963745\n",
      "Iteration 31554 Loss: 0.7474809885025024\n",
      "Iteration 31555 Loss: 0.9852335453033447\n",
      "Iteration 31556 Loss: 1.1983250379562378\n",
      "Iteration 31557 Loss: 1.0799459218978882\n",
      "Iteration 31558 Loss: 1.126455307006836\n",
      "Iteration 31559 Loss: 1.114810824394226\n",
      "Iteration 31559 Loss: 1.0403294563293457\n",
      "Iteration 31560 Loss: 0.954345166683197\n",
      "Iteration 31561 Loss: 1.27557373046875\n",
      "Iteration 31562 Loss: 1.2103753089904785\n",
      "Iteration 31563 Loss: 0.6076881289482117\n",
      "Iteration 31564 Loss: 0.9168742299079895\n",
      "Iteration 31565 Loss: 1.2654316425323486\n",
      "Iteration 31566 Loss: 1.4177830219268799\n",
      "Iteration 31567 Loss: 1.134963035583496\n",
      "Iteration 31568 Loss: 0.7366870641708374\n",
      "Iteration 31569 Loss: 0.826412558555603\n",
      "Iteration 31569 Loss: 1.0346133708953857\n",
      "Iteration 31570 Loss: 1.244678020477295\n",
      "Iteration 31571 Loss: 1.2378766536712646\n",
      "Iteration 31572 Loss: 1.1076595783233643\n",
      "Iteration 31573 Loss: 1.1233453750610352\n",
      "Iteration 31574 Loss: 1.0855048894882202\n",
      "Iteration 31575 Loss: 1.004429578781128\n",
      "Iteration 31576 Loss: 1.106731653213501\n",
      "Iteration 31577 Loss: 0.9742709994316101\n",
      "Iteration 31578 Loss: 0.8005473017692566\n",
      "Iteration 31579 Loss: 0.9562386274337769\n",
      "Iteration 31579 Loss: 1.0641281604766846\n",
      "Iteration 31580 Loss: 0.9995226263999939\n",
      "Iteration 31581 Loss: 0.6533134579658508\n",
      "Iteration 31582 Loss: 1.0218331813812256\n",
      "Iteration 31583 Loss: 1.040238380432129\n",
      "Iteration 31584 Loss: 0.9971016049385071\n",
      "Iteration 31585 Loss: 1.3009533882141113\n",
      "Iteration 31586 Loss: 1.1004960536956787\n",
      "Iteration 31587 Loss: 1.1807962656021118\n",
      "Iteration 31588 Loss: 0.9517618417739868\n",
      "Iteration 31589 Loss: 0.6307663917541504\n",
      "Iteration 31589 Loss: 0.9876783490180969\n",
      "Iteration 31590 Loss: 1.234898328781128\n",
      "Iteration 31591 Loss: 1.0747121572494507\n",
      "Iteration 31592 Loss: 0.9273484945297241\n",
      "Iteration 31593 Loss: 1.0153734683990479\n",
      "Iteration 31594 Loss: 0.9303107857704163\n",
      "Iteration 31595 Loss: 1.1121882200241089\n",
      "Iteration 31596 Loss: 0.8084951639175415\n",
      "Iteration 31597 Loss: 0.8720179200172424\n",
      "Iteration 31598 Loss: 1.3303478956222534\n",
      "Iteration 31599 Loss: 0.7106072306632996\n",
      "Iteration 31599 Loss: 1.0016299486160278\n",
      "Iteration 31600 Loss: 1.140319585800171\n",
      "Iteration 31601 Loss: 0.929323673248291\n",
      "Iteration 31602 Loss: 0.8095434308052063\n",
      "Iteration 31603 Loss: 1.0147149562835693\n",
      "Iteration 31604 Loss: 1.0197950601577759\n",
      "Iteration 31605 Loss: 0.8209382891654968\n",
      "Iteration 31606 Loss: 0.8702908158302307\n",
      "Iteration 31607 Loss: 1.0472584962844849\n",
      "Iteration 31608 Loss: 0.9983728528022766\n",
      "Iteration 31609 Loss: 1.2388454675674438\n",
      "Iteration 31609 Loss: 0.9889403581619263\n",
      "Iteration 31610 Loss: 1.0454214811325073\n",
      "Iteration 31611 Loss: 1.4081393480300903\n",
      "Iteration 31612 Loss: 1.1695852279663086\n",
      "Iteration 31613 Loss: 1.2523412704467773\n",
      "Iteration 31614 Loss: 1.1513339281082153\n",
      "Iteration 31615 Loss: 0.6046062707901001\n",
      "Iteration 31616 Loss: 1.057126760482788\n",
      "Iteration 31617 Loss: 0.9941573739051819\n",
      "Iteration 31618 Loss: 1.3242896795272827\n",
      "Iteration 31619 Loss: 0.9261996150016785\n",
      "Iteration 31619 Loss: 1.0933201313018799\n",
      "Iteration 31620 Loss: 1.1587269306182861\n",
      "Iteration 31621 Loss: 1.1030769348144531\n",
      "Iteration 31622 Loss: 0.9587736129760742\n",
      "Iteration 31623 Loss: 0.8588541150093079\n",
      "Iteration 31624 Loss: 1.0393284559249878\n",
      "Iteration 31625 Loss: 0.8847766518592834\n",
      "Iteration 31626 Loss: 0.8313722610473633\n",
      "Iteration 31627 Loss: 0.8481757044792175\n",
      "Iteration 31628 Loss: 1.447266936302185\n",
      "Iteration 31629 Loss: 1.0699974298477173\n",
      "Iteration 31629 Loss: 1.0200350284576416\n",
      "Iteration 31630 Loss: 0.7803189158439636\n",
      "Iteration 31631 Loss: 0.9856506586074829\n",
      "Iteration 31632 Loss: 1.1426990032196045\n",
      "Iteration 31633 Loss: 0.6929367780685425\n",
      "Iteration 31634 Loss: 0.6138713359832764\n",
      "Iteration 31635 Loss: 0.9786995053291321\n",
      "Iteration 31636 Loss: 0.7064575552940369\n",
      "Iteration 31637 Loss: 1.1803869009017944\n",
      "Iteration 31638 Loss: 0.7918777465820312\n",
      "Iteration 31639 Loss: 1.3497506380081177\n",
      "Iteration 31639 Loss: 0.9222648739814758\n",
      "Iteration 31640 Loss: 0.9919823408126831\n",
      "Iteration 31641 Loss: 0.8006515502929688\n",
      "Iteration 31642 Loss: 1.1158480644226074\n",
      "Iteration 31643 Loss: 1.1554218530654907\n",
      "Iteration 31644 Loss: 0.8058196306228638\n",
      "Iteration 31645 Loss: 0.8956083059310913\n",
      "Iteration 31646 Loss: 1.157880425453186\n",
      "Iteration 31647 Loss: 0.6152545809745789\n",
      "Iteration 31648 Loss: 1.1566593647003174\n",
      "Iteration 31649 Loss: 1.0635286569595337\n",
      "Iteration 31649 Loss: 0.9758654832839966\n",
      "Iteration 31650 Loss: 1.0449496507644653\n",
      "Iteration 31651 Loss: 1.0114613771438599\n",
      "Iteration 31652 Loss: 1.0712169408798218\n",
      "Iteration 31653 Loss: 1.2331268787384033\n",
      "Iteration 31654 Loss: 1.2336390018463135\n",
      "Iteration 31655 Loss: 0.8262493014335632\n",
      "Iteration 31656 Loss: 0.8332752585411072\n",
      "Iteration 31657 Loss: 0.9736554622650146\n",
      "Iteration 31658 Loss: 1.2062625885009766\n",
      "Iteration 31659 Loss: 1.3747622966766357\n",
      "Iteration 31659 Loss: 1.080859899520874\n",
      "Iteration 31660 Loss: 1.2228114604949951\n",
      "Iteration 31661 Loss: 0.9715664386749268\n",
      "Iteration 31662 Loss: 0.8814196586608887\n",
      "Iteration 31663 Loss: 1.0335593223571777\n",
      "Iteration 31664 Loss: 1.4351426362991333\n",
      "Iteration 31665 Loss: 0.9130059480667114\n",
      "Iteration 31666 Loss: 1.1799074411392212\n",
      "Iteration 31667 Loss: 0.7687404155731201\n",
      "Iteration 31668 Loss: 0.9807247519493103\n",
      "Iteration 31669 Loss: 1.207715392112732\n",
      "Iteration 31669 Loss: 1.0594594478607178\n",
      "Iteration 31670 Loss: 0.9527386426925659\n",
      "Iteration 31671 Loss: 1.2419685125350952\n",
      "Iteration 31672 Loss: 1.0622509717941284\n",
      "Iteration 31673 Loss: 1.0681055784225464\n",
      "Iteration 31674 Loss: 0.7283068299293518\n",
      "Iteration 31675 Loss: 1.063001036643982\n",
      "Iteration 31676 Loss: 1.090038537979126\n",
      "Iteration 31677 Loss: 1.1087241172790527\n",
      "Iteration 31678 Loss: 0.9659767150878906\n",
      "Iteration 31679 Loss: 1.1868822574615479\n",
      "Iteration 31679 Loss: 1.0467993021011353\n",
      "Iteration 31680 Loss: 0.9960113763809204\n",
      "Iteration 31681 Loss: 1.1639270782470703\n",
      "Iteration 31682 Loss: 1.3615089654922485\n",
      "Iteration 31683 Loss: 1.1058682203292847\n",
      "Iteration 31684 Loss: 1.0335264205932617\n",
      "Iteration 31685 Loss: 1.3927093744277954\n",
      "Iteration 31686 Loss: 0.8870513439178467\n",
      "Iteration 31687 Loss: 0.7932465076446533\n",
      "Iteration 31688 Loss: 1.1526941061019897\n",
      "Iteration 31689 Loss: 1.2408978939056396\n",
      "Iteration 31689 Loss: 1.1127440929412842\n",
      "Iteration 31690 Loss: 0.8513262867927551\n",
      "Iteration 31691 Loss: 0.8665395379066467\n",
      "Iteration 31692 Loss: 1.1401227712631226\n",
      "Iteration 31693 Loss: 1.2340086698532104\n",
      "Iteration 31694 Loss: 1.1287670135498047\n",
      "Iteration 31695 Loss: 1.0866385698318481\n",
      "Iteration 31696 Loss: 0.871618926525116\n",
      "Iteration 31697 Loss: 1.1099671125411987\n",
      "Iteration 31698 Loss: 1.1865217685699463\n",
      "Iteration 31699 Loss: 1.1010380983352661\n",
      "Iteration 31699 Loss: 1.0576549768447876\n",
      "Iteration 31700 Loss: 1.2618277072906494\n",
      "Iteration 31701 Loss: 0.9517233967781067\n",
      "Iteration 31702 Loss: 0.9836167097091675\n",
      "Iteration 31703 Loss: 0.9218710064888\n",
      "Iteration 31704 Loss: 0.8525018692016602\n",
      "Iteration 31705 Loss: 0.9100338220596313\n",
      "Iteration 31706 Loss: 0.9567016363143921\n",
      "Iteration 31707 Loss: 0.71071457862854\n",
      "Iteration 31708 Loss: 0.8979170918464661\n",
      "Iteration 31709 Loss: 0.9611325860023499\n",
      "Iteration 31709 Loss: 0.9408040046691895\n",
      "Iteration 31710 Loss: 0.9131788611412048\n",
      "Iteration 31711 Loss: 0.8445539474487305\n",
      "Iteration 31712 Loss: 1.2149091958999634\n",
      "Iteration 31713 Loss: 1.0793370008468628\n",
      "Iteration 31714 Loss: 1.0982459783554077\n",
      "Iteration 31715 Loss: 0.9734802842140198\n",
      "Iteration 31716 Loss: 0.6282564401626587\n",
      "Iteration 31717 Loss: 1.1121726036071777\n",
      "Iteration 31718 Loss: 1.4737671613693237\n",
      "Iteration 31719 Loss: 1.0685558319091797\n",
      "Iteration 31719 Loss: 1.0406458377838135\n",
      "Iteration 31720 Loss: 1.1885567903518677\n",
      "Iteration 31721 Loss: 1.0253887176513672\n",
      "Iteration 31722 Loss: 1.1205170154571533\n",
      "Iteration 31723 Loss: 1.0245190858840942\n",
      "Iteration 31724 Loss: 0.8184601068496704\n",
      "Iteration 31725 Loss: 1.1831177473068237\n",
      "Iteration 31726 Loss: 1.219768762588501\n",
      "Iteration 31727 Loss: 1.3691990375518799\n",
      "Iteration 31728 Loss: 1.243108868598938\n",
      "Iteration 31729 Loss: 1.066598892211914\n",
      "Iteration 31729 Loss: 1.1259233951568604\n",
      "Iteration 31730 Loss: 0.9657388925552368\n",
      "Iteration 31731 Loss: 0.8041316866874695\n",
      "Iteration 31732 Loss: 1.216204285621643\n",
      "Iteration 31733 Loss: 1.1925135850906372\n",
      "Iteration 31734 Loss: 1.1799888610839844\n",
      "Iteration 31735 Loss: 0.9563380479812622\n",
      "Iteration 31736 Loss: 0.6950947642326355\n",
      "Iteration 31737 Loss: 1.2783653736114502\n",
      "Iteration 31738 Loss: 0.7320334315299988\n",
      "Iteration 31739 Loss: 1.088808536529541\n",
      "Iteration 31739 Loss: 1.0109217166900635\n",
      "Iteration 31740 Loss: 0.7753716111183167\n",
      "Iteration 31741 Loss: 1.3992893695831299\n",
      "Iteration 31742 Loss: 1.3068770170211792\n",
      "Iteration 31743 Loss: 1.2339872121810913\n",
      "Iteration 31744 Loss: 0.9147281050682068\n",
      "Iteration 31745 Loss: 0.8479612469673157\n",
      "Iteration 31746 Loss: 0.9177022576332092\n",
      "Iteration 31747 Loss: 1.0186076164245605\n",
      "Iteration 31748 Loss: 0.9189247488975525\n",
      "Iteration 31749 Loss: 0.869746208190918\n",
      "Iteration 31749 Loss: 1.0203195810317993\n",
      "Iteration 31750 Loss: 1.1611186265945435\n",
      "Iteration 31751 Loss: 0.7577550411224365\n",
      "Iteration 31752 Loss: 0.7303128242492676\n",
      "Iteration 31753 Loss: 0.7362577319145203\n",
      "Iteration 31754 Loss: 1.2978284358978271\n",
      "Iteration 31755 Loss: 1.1828501224517822\n",
      "Iteration 31756 Loss: 1.5819474458694458\n",
      "Iteration 31757 Loss: 1.32919442653656\n",
      "Iteration 31758 Loss: 1.0067111253738403\n",
      "Iteration 31759 Loss: 1.1599665880203247\n",
      "Iteration 31759 Loss: 1.0943940877914429\n",
      "Iteration 31760 Loss: 0.7503620386123657\n",
      "Iteration 31761 Loss: 1.0382137298583984\n",
      "Iteration 31762 Loss: 1.067578911781311\n",
      "Iteration 31763 Loss: 0.9822882413864136\n",
      "Iteration 31764 Loss: 1.0612881183624268\n",
      "Iteration 31765 Loss: 0.9772420525550842\n",
      "Iteration 31766 Loss: 0.7909301519393921\n",
      "Iteration 31767 Loss: 0.769223153591156\n",
      "Iteration 31768 Loss: 0.6552270650863647\n",
      "Iteration 31769 Loss: 1.1991856098175049\n",
      "Iteration 31769 Loss: 0.9291539192199707\n",
      "Iteration 31770 Loss: 1.3009690046310425\n",
      "Iteration 31771 Loss: 1.3319193124771118\n",
      "Iteration 31772 Loss: 1.0651775598526\n",
      "Iteration 31773 Loss: 1.1931406259536743\n",
      "Iteration 31774 Loss: 0.9767704606056213\n",
      "Iteration 31775 Loss: 1.1965444087982178\n",
      "Iteration 31776 Loss: 0.9477944374084473\n",
      "Iteration 31777 Loss: 1.101438283920288\n",
      "Iteration 31778 Loss: 0.8853194713592529\n",
      "Iteration 31779 Loss: 0.9538722634315491\n",
      "Iteration 31779 Loss: 1.0952945947647095\n",
      "Iteration 31780 Loss: 0.8312375545501709\n",
      "Iteration 31781 Loss: 0.724806547164917\n",
      "Iteration 31782 Loss: 1.298485279083252\n",
      "Iteration 31783 Loss: 1.0922045707702637\n",
      "Iteration 31784 Loss: 1.2371355295181274\n",
      "Iteration 31785 Loss: 1.0791722536087036\n",
      "Iteration 31786 Loss: 1.2215676307678223\n",
      "Iteration 31787 Loss: 0.830275297164917\n",
      "Iteration 31788 Loss: 1.0880998373031616\n",
      "Iteration 31789 Loss: 0.8422821164131165\n",
      "Iteration 31789 Loss: 1.0245267152786255\n",
      "Iteration 31790 Loss: 0.7104954123497009\n",
      "Iteration 31791 Loss: 0.8455120921134949\n",
      "Iteration 31792 Loss: 0.8202239871025085\n",
      "Iteration 31793 Loss: 0.8442773818969727\n",
      "Iteration 31794 Loss: 1.0020414590835571\n",
      "Iteration 31795 Loss: 1.494394063949585\n",
      "Iteration 31796 Loss: 1.1007670164108276\n",
      "Iteration 31797 Loss: 0.8991999626159668\n",
      "Iteration 31798 Loss: 1.2523130178451538\n",
      "Iteration 31799 Loss: 1.0827730894088745\n",
      "Iteration 31799 Loss: 1.0051997900009155\n",
      "Iteration 31800 Loss: 0.7439578771591187\n",
      "Iteration 31801 Loss: 1.0828219652175903\n",
      "Iteration 31802 Loss: 1.1943244934082031\n",
      "Iteration 31803 Loss: 0.8936027884483337\n",
      "Iteration 31804 Loss: 0.8752802610397339\n",
      "Iteration 31805 Loss: 1.1991828680038452\n",
      "Iteration 31806 Loss: 1.3905713558197021\n",
      "Iteration 31807 Loss: 0.987297773361206\n",
      "Iteration 31808 Loss: 1.2712444067001343\n",
      "Iteration 31809 Loss: 1.44913911819458\n",
      "Iteration 31809 Loss: 1.108742356300354\n",
      "Iteration 31810 Loss: 1.1498515605926514\n",
      "Iteration 31811 Loss: 0.9290700554847717\n",
      "Iteration 31812 Loss: 1.0775041580200195\n",
      "Iteration 31813 Loss: 1.0557388067245483\n",
      "Iteration 31814 Loss: 1.1069270372390747\n",
      "Iteration 31815 Loss: 1.0761559009552002\n",
      "Iteration 31816 Loss: 0.8727692365646362\n",
      "Iteration 31817 Loss: 1.0334219932556152\n",
      "Iteration 31818 Loss: 0.7096359133720398\n",
      "Iteration 31819 Loss: 0.7267565131187439\n",
      "Iteration 31819 Loss: 0.9737831354141235\n",
      "Iteration 31820 Loss: 0.7946515083312988\n",
      "Iteration 31821 Loss: 1.3548986911773682\n",
      "Iteration 31822 Loss: 1.3358780145645142\n",
      "Iteration 31823 Loss: 1.4034836292266846\n",
      "Iteration 31824 Loss: 1.181861162185669\n",
      "Iteration 31825 Loss: 1.0548036098480225\n",
      "Iteration 31826 Loss: 1.3751757144927979\n",
      "Iteration 31827 Loss: 0.7560585737228394\n",
      "Iteration 31828 Loss: 1.2184133529663086\n",
      "Iteration 31829 Loss: 1.3333182334899902\n",
      "Iteration 31829 Loss: 1.1808542013168335\n",
      "Iteration 31830 Loss: 1.042184829711914\n",
      "Iteration 31831 Loss: 0.8123175501823425\n",
      "Iteration 31832 Loss: 0.9236183762550354\n",
      "Iteration 31833 Loss: 1.0085238218307495\n",
      "Iteration 31834 Loss: 0.9828433990478516\n",
      "Iteration 31835 Loss: 1.0111891031265259\n",
      "Iteration 31836 Loss: 1.1683080196380615\n",
      "Iteration 31837 Loss: 0.9594846963882446\n",
      "Iteration 31838 Loss: 0.8800087571144104\n",
      "Iteration 31839 Loss: 0.8498906493186951\n",
      "Iteration 31839 Loss: 0.9638369679450989\n",
      "Iteration 31840 Loss: 1.2881795167922974\n",
      "Iteration 31841 Loss: 0.8321311473846436\n",
      "Iteration 31842 Loss: 1.2450673580169678\n",
      "Iteration 31843 Loss: 0.998716413974762\n",
      "Iteration 31844 Loss: 1.162866234779358\n",
      "Iteration 31845 Loss: 1.1854338645935059\n",
      "Iteration 31846 Loss: 0.9355356693267822\n",
      "Iteration 31847 Loss: 1.0435843467712402\n",
      "Iteration 31848 Loss: 1.2147090435028076\n",
      "Iteration 31849 Loss: 1.3521313667297363\n",
      "Iteration 31849 Loss: 1.125835657119751\n",
      "Iteration 31850 Loss: 0.7087116837501526\n",
      "Iteration 31851 Loss: 0.9668978452682495\n",
      "Iteration 31852 Loss: 0.9604063034057617\n",
      "Iteration 31853 Loss: 1.2644461393356323\n",
      "Iteration 31854 Loss: 1.241376519203186\n",
      "Iteration 31855 Loss: 0.8889830708503723\n",
      "Iteration 31856 Loss: 0.8798015713691711\n",
      "Iteration 31857 Loss: 1.1809651851654053\n",
      "Iteration 31858 Loss: 0.9848383069038391\n",
      "Iteration 31859 Loss: 1.2121363878250122\n",
      "Iteration 31859 Loss: 1.0288563966751099\n",
      "Iteration 31860 Loss: 0.9520334601402283\n",
      "Iteration 31861 Loss: 0.9260407090187073\n",
      "Iteration 31862 Loss: 1.0997223854064941\n",
      "Iteration 31863 Loss: 0.7479957342147827\n",
      "Iteration 31864 Loss: 0.9959545135498047\n",
      "Iteration 31865 Loss: 0.9715220928192139\n",
      "Iteration 31866 Loss: 0.7771270275115967\n",
      "Iteration 31867 Loss: 1.2305030822753906\n",
      "Iteration 31868 Loss: 0.7356745600700378\n",
      "Iteration 31869 Loss: 0.9336236715316772\n",
      "Iteration 31869 Loss: 0.9370197057723999\n",
      "Iteration 31870 Loss: 0.8054381012916565\n",
      "Iteration 31871 Loss: 0.7012783288955688\n",
      "Iteration 31872 Loss: 1.0615156888961792\n",
      "Iteration 31873 Loss: 1.133409023284912\n",
      "Iteration 31874 Loss: 0.9783883690834045\n",
      "Iteration 31875 Loss: 1.1560542583465576\n",
      "Iteration 31876 Loss: 1.0197433233261108\n",
      "Iteration 31877 Loss: 1.0241780281066895\n",
      "Iteration 31878 Loss: 1.0050907135009766\n",
      "Iteration 31879 Loss: 0.6961190104484558\n",
      "Iteration 31879 Loss: 0.9581214785575867\n",
      "Iteration 31880 Loss: 1.237795114517212\n",
      "Iteration 31881 Loss: 1.112418293952942\n",
      "Iteration 31882 Loss: 1.3784093856811523\n",
      "Iteration 31883 Loss: 1.2538188695907593\n",
      "Iteration 31884 Loss: 1.0721718072891235\n",
      "Iteration 31885 Loss: 0.9442840218544006\n",
      "Iteration 31886 Loss: 1.1626290082931519\n",
      "Iteration 31887 Loss: 0.7895386815071106\n",
      "Iteration 31888 Loss: 1.2155555486679077\n",
      "Iteration 31889 Loss: 1.3738754987716675\n",
      "Iteration 31889 Loss: 1.1540496349334717\n",
      "Iteration 31890 Loss: 1.147212028503418\n",
      "Iteration 31891 Loss: 1.0487546920776367\n",
      "Iteration 31892 Loss: 0.9553618431091309\n",
      "Iteration 31893 Loss: 1.1568658351898193\n",
      "Iteration 31894 Loss: 1.1308624744415283\n",
      "Iteration 31895 Loss: 1.2214828729629517\n",
      "Iteration 31896 Loss: 0.7810837626457214\n",
      "Iteration 31897 Loss: 0.9605631828308105\n",
      "Iteration 31898 Loss: 1.2300108671188354\n",
      "Iteration 31899 Loss: 1.133476734161377\n",
      "Iteration 31899 Loss: 1.0765674114227295\n",
      "Iteration 31900 Loss: 1.054137945175171\n",
      "Iteration 31901 Loss: 0.5492182374000549\n",
      "Iteration 31902 Loss: 0.9396405816078186\n",
      "Iteration 31903 Loss: 1.175096035003662\n",
      "Iteration 31904 Loss: 1.1669080257415771\n",
      "Iteration 31905 Loss: 1.0195114612579346\n",
      "Iteration 31906 Loss: 0.6418992280960083\n",
      "Iteration 31907 Loss: 1.1224710941314697\n",
      "Iteration 31908 Loss: 1.036831259727478\n",
      "Iteration 31909 Loss: 1.0059415102005005\n",
      "Iteration 31909 Loss: 0.9711654782295227\n",
      "Iteration 31910 Loss: 1.0652930736541748\n",
      "Iteration 31911 Loss: 1.3573774099349976\n",
      "Iteration 31912 Loss: 1.1411027908325195\n",
      "Iteration 31913 Loss: 0.8622791767120361\n",
      "Iteration 31914 Loss: 1.0881456136703491\n",
      "Iteration 31915 Loss: 1.0224772691726685\n",
      "Iteration 31916 Loss: 1.2747418880462646\n",
      "Iteration 31917 Loss: 0.8964051604270935\n",
      "Iteration 31918 Loss: 1.3624963760375977\n",
      "Iteration 31919 Loss: 0.961276113986969\n",
      "Iteration 31919 Loss: 1.1031595468521118\n",
      "Iteration 31920 Loss: 0.8231873512268066\n",
      "Iteration 31921 Loss: 0.8180880546569824\n",
      "Iteration 31922 Loss: 0.9617330431938171\n",
      "Iteration 31923 Loss: 1.1719688177108765\n",
      "Iteration 31924 Loss: 0.7598474621772766\n",
      "Iteration 31925 Loss: 1.2372169494628906\n",
      "Iteration 31926 Loss: 1.1589411497116089\n",
      "Iteration 31927 Loss: 1.0778567790985107\n",
      "Iteration 31928 Loss: 1.2534188032150269\n",
      "Iteration 31929 Loss: 0.798337996006012\n",
      "Iteration 31929 Loss: 1.0060596466064453\n",
      "Iteration 31930 Loss: 1.1253299713134766\n",
      "Iteration 31931 Loss: 0.8867233991622925\n",
      "Iteration 31932 Loss: 0.8554840087890625\n",
      "Iteration 31933 Loss: 1.0495896339416504\n",
      "Iteration 31934 Loss: 1.1638412475585938\n",
      "Iteration 31935 Loss: 1.2105247974395752\n",
      "Iteration 31936 Loss: 0.9322018027305603\n",
      "Iteration 31937 Loss: 1.0819933414459229\n",
      "Iteration 31938 Loss: 1.1767770051956177\n",
      "Iteration 31939 Loss: 1.2697038650512695\n",
      "Iteration 31939 Loss: 1.0752168893814087\n",
      "Iteration 31940 Loss: 0.9258713722229004\n",
      "Iteration 31941 Loss: 0.83619624376297\n",
      "Iteration 31942 Loss: 0.9593307971954346\n",
      "Iteration 31943 Loss: 1.1559852361679077\n",
      "Iteration 31944 Loss: 1.1393963098526\n",
      "Iteration 31945 Loss: 1.1109708547592163\n",
      "Iteration 31946 Loss: 1.080905556678772\n",
      "Iteration 31947 Loss: 0.8274508714675903\n",
      "Iteration 31948 Loss: 0.893153965473175\n",
      "Iteration 31949 Loss: 1.1815000772476196\n",
      "Iteration 31949 Loss: 1.0110762119293213\n",
      "Iteration 31950 Loss: 0.9073857665061951\n",
      "Iteration 31951 Loss: 0.8842262029647827\n",
      "Iteration 31952 Loss: 0.8096795082092285\n",
      "Iteration 31953 Loss: 0.6951664686203003\n",
      "Iteration 31954 Loss: 1.071423888206482\n",
      "Iteration 31955 Loss: 0.871603786945343\n",
      "Iteration 31956 Loss: 0.8930150866508484\n",
      "Iteration 31957 Loss: 1.1265637874603271\n",
      "Iteration 31958 Loss: 1.408103585243225\n",
      "Iteration 31959 Loss: 0.8757414221763611\n",
      "Iteration 31959 Loss: 0.954291045665741\n",
      "Iteration 31960 Loss: 0.8136481046676636\n",
      "Iteration 31961 Loss: 1.0012173652648926\n",
      "Iteration 31962 Loss: 1.0002142190933228\n",
      "Iteration 31963 Loss: 1.0438926219940186\n",
      "Iteration 31964 Loss: 0.8352026343345642\n",
      "Iteration 31965 Loss: 1.1259369850158691\n",
      "Iteration 31966 Loss: 1.051682949066162\n",
      "Iteration 31967 Loss: 1.093634009361267\n",
      "Iteration 31968 Loss: 0.8556738495826721\n",
      "Iteration 31969 Loss: 1.1179684400558472\n",
      "Iteration 31969 Loss: 0.9939071536064148\n",
      "Iteration 31970 Loss: 1.095180630683899\n",
      "Iteration 31971 Loss: 1.0401184558868408\n",
      "Iteration 31972 Loss: 1.1036523580551147\n",
      "Iteration 31973 Loss: 0.8706422448158264\n",
      "Iteration 31974 Loss: 0.8519719243049622\n",
      "Iteration 31975 Loss: 1.1007676124572754\n",
      "Iteration 31976 Loss: 0.8634142875671387\n",
      "Iteration 31977 Loss: 0.8757137656211853\n",
      "Iteration 31978 Loss: 0.7719597816467285\n",
      "Iteration 31979 Loss: 0.8077638745307922\n",
      "Iteration 31979 Loss: 0.9381184577941895\n",
      "Iteration 31980 Loss: 0.5180184245109558\n",
      "Iteration 31981 Loss: 0.9343236684799194\n",
      "Iteration 31982 Loss: 1.3583530187606812\n",
      "Iteration 31983 Loss: 0.7426876425743103\n",
      "Iteration 31984 Loss: 1.1070362329483032\n",
      "Iteration 31985 Loss: 0.9526271224021912\n",
      "Iteration 31986 Loss: 1.0553538799285889\n",
      "Iteration 31987 Loss: 0.8345407247543335\n",
      "Iteration 31988 Loss: 0.8562304973602295\n",
      "Iteration 31989 Loss: 1.2305233478546143\n",
      "Iteration 31989 Loss: 0.9589694142341614\n",
      "Iteration 31990 Loss: 1.1503512859344482\n",
      "Iteration 31991 Loss: 0.7490643858909607\n",
      "Iteration 31992 Loss: 1.2642329931259155\n",
      "Iteration 31993 Loss: 1.4052029848098755\n",
      "Iteration 31994 Loss: 1.0423946380615234\n",
      "Iteration 31995 Loss: 0.8928095102310181\n",
      "Iteration 31996 Loss: 1.1784260272979736\n",
      "Iteration 31997 Loss: 1.4337636232376099\n",
      "Iteration 31998 Loss: 0.9503244161605835\n",
      "Iteration 31999 Loss: 0.9189067482948303\n",
      "Iteration 31999 Loss: 1.0985476970672607\n",
      "Iteration 32000 Loss: 1.213921308517456\n",
      "Iteration 32001 Loss: 1.010387659072876\n",
      "Iteration 32002 Loss: 1.0469467639923096\n",
      "Iteration 32003 Loss: 0.8915854096412659\n",
      "Iteration 32004 Loss: 1.2913401126861572\n",
      "Iteration 32005 Loss: 1.0452697277069092\n",
      "Iteration 32006 Loss: 0.9835752248764038\n",
      "Iteration 32007 Loss: 1.3142608404159546\n",
      "Iteration 32008 Loss: 1.0409417152404785\n",
      "Iteration 32009 Loss: 0.951429009437561\n",
      "Iteration 32009 Loss: 1.078965663909912\n",
      "Iteration 32010 Loss: 1.0443010330200195\n",
      "Iteration 32011 Loss: 1.2352405786514282\n",
      "Iteration 32012 Loss: 0.9354764819145203\n",
      "Iteration 32013 Loss: 1.008832573890686\n",
      "Iteration 32014 Loss: 1.0448472499847412\n",
      "Iteration 32015 Loss: 0.9760622382164001\n",
      "Iteration 32016 Loss: 0.8470503091812134\n",
      "Iteration 32017 Loss: 0.8681858777999878\n",
      "Iteration 32018 Loss: 1.6991842985153198\n",
      "Iteration 32019 Loss: 1.1049044132232666\n",
      "Iteration 32019 Loss: 1.0764085054397583\n",
      "Iteration 32020 Loss: 1.1302591562271118\n",
      "Iteration 32021 Loss: 0.9923802614212036\n",
      "Iteration 32022 Loss: 0.9336619973182678\n",
      "Iteration 32023 Loss: 1.1118899583816528\n",
      "Iteration 32024 Loss: 0.9252803325653076\n",
      "Iteration 32025 Loss: 1.1029212474822998\n",
      "Iteration 32026 Loss: 1.3349055051803589\n",
      "Iteration 32027 Loss: 0.794036865234375\n",
      "Iteration 32028 Loss: 0.9882849454879761\n",
      "Iteration 32029 Loss: 0.8415694236755371\n",
      "Iteration 32029 Loss: 1.0155189037322998\n",
      "Iteration 32030 Loss: 1.2138631343841553\n",
      "Iteration 32031 Loss: 1.3409364223480225\n",
      "Iteration 32032 Loss: 0.8904346823692322\n",
      "Iteration 32033 Loss: 0.9194225072860718\n",
      "Iteration 32034 Loss: 0.9966537952423096\n",
      "Iteration 32035 Loss: 0.8592555522918701\n",
      "Iteration 32036 Loss: 0.9508716464042664\n",
      "Iteration 32037 Loss: 1.0859284400939941\n",
      "Iteration 32038 Loss: 0.7343608140945435\n",
      "Iteration 32039 Loss: 1.0713223218917847\n",
      "Iteration 32039 Loss: 1.0063049793243408\n",
      "Iteration 32040 Loss: 1.0280933380126953\n",
      "Iteration 32041 Loss: 0.9851387143135071\n",
      "Iteration 32042 Loss: 0.6371449828147888\n",
      "Iteration 32043 Loss: 1.0969064235687256\n",
      "Iteration 32044 Loss: 1.0713987350463867\n",
      "Iteration 32045 Loss: 1.1558846235275269\n",
      "Iteration 32046 Loss: 1.3523378372192383\n",
      "Iteration 32047 Loss: 0.8174697756767273\n",
      "Iteration 32048 Loss: 0.7057801485061646\n",
      "Iteration 32049 Loss: 1.1899399757385254\n",
      "Iteration 32049 Loss: 1.004009485244751\n",
      "Iteration 32050 Loss: 1.2818944454193115\n",
      "Iteration 32051 Loss: 1.2337923049926758\n",
      "Iteration 32052 Loss: 0.9736030697822571\n",
      "Iteration 32053 Loss: 1.1560264825820923\n",
      "Iteration 32054 Loss: 0.8576768636703491\n",
      "Iteration 32055 Loss: 1.2473505735397339\n",
      "Iteration 32056 Loss: 0.9499874711036682\n",
      "Iteration 32057 Loss: 1.0890324115753174\n",
      "Iteration 32058 Loss: 1.0228676795959473\n",
      "Iteration 32059 Loss: 1.1581273078918457\n",
      "Iteration 32059 Loss: 1.0970358848571777\n",
      "Iteration 32060 Loss: 1.303444266319275\n",
      "Iteration 32061 Loss: 1.0026346445083618\n",
      "Iteration 32062 Loss: 0.6773860454559326\n",
      "Iteration 32063 Loss: 1.174729585647583\n",
      "Iteration 32064 Loss: 1.4157434701919556\n",
      "Iteration 32065 Loss: 0.8796544075012207\n",
      "Iteration 32066 Loss: 0.8028422594070435\n",
      "Iteration 32067 Loss: 0.9772072434425354\n",
      "Iteration 32068 Loss: 0.8662538528442383\n",
      "Iteration 32069 Loss: 0.5234860181808472\n",
      "Iteration 32069 Loss: 0.9623381495475769\n",
      "Iteration 32070 Loss: 0.9755392670631409\n",
      "Iteration 32071 Loss: 0.9463754892349243\n",
      "Iteration 32072 Loss: 0.8815400004386902\n",
      "Iteration 32073 Loss: 1.142187237739563\n",
      "Iteration 32074 Loss: 1.1020543575286865\n",
      "Iteration 32075 Loss: 0.8954201936721802\n",
      "Iteration 32076 Loss: 0.7412963509559631\n",
      "Iteration 32077 Loss: 1.1032049655914307\n",
      "Iteration 32078 Loss: 1.1315257549285889\n",
      "Iteration 32079 Loss: 0.9449902772903442\n",
      "Iteration 32079 Loss: 0.9864133596420288\n",
      "Iteration 32080 Loss: 1.0621130466461182\n",
      "Iteration 32081 Loss: 1.0359822511672974\n",
      "Iteration 32082 Loss: 0.8225414752960205\n",
      "Iteration 32083 Loss: 1.1162292957305908\n",
      "Iteration 32084 Loss: 1.2668042182922363\n",
      "Iteration 32085 Loss: 1.019378423690796\n",
      "Iteration 32086 Loss: 1.1780734062194824\n",
      "Iteration 32087 Loss: 1.0647578239440918\n",
      "Iteration 32088 Loss: 0.9697445631027222\n",
      "Iteration 32089 Loss: 1.1880037784576416\n",
      "Iteration 32089 Loss: 1.0723628997802734\n",
      "Iteration 32090 Loss: 0.9375600814819336\n",
      "Iteration 32091 Loss: 0.669739842414856\n",
      "Iteration 32092 Loss: 0.8147899508476257\n",
      "Iteration 32093 Loss: 0.8745306134223938\n",
      "Iteration 32094 Loss: 1.2739938497543335\n",
      "Iteration 32095 Loss: 0.7753807306289673\n",
      "Iteration 32096 Loss: 0.9451809525489807\n",
      "Iteration 32097 Loss: 1.277260661125183\n",
      "Iteration 32098 Loss: 1.029599666595459\n",
      "Iteration 32099 Loss: 0.7727087140083313\n",
      "Iteration 32099 Loss: 0.9370745420455933\n",
      "Iteration 32100 Loss: 0.9502047896385193\n",
      "Iteration 32101 Loss: 1.201263666152954\n",
      "Iteration 32102 Loss: 0.8694921135902405\n",
      "Iteration 32103 Loss: 1.1838035583496094\n",
      "Iteration 32104 Loss: 0.7669929265975952\n",
      "Iteration 32105 Loss: 1.0159430503845215\n",
      "Iteration 32106 Loss: 1.1411715745925903\n",
      "Iteration 32107 Loss: 0.5874172449111938\n",
      "Iteration 32108 Loss: 1.192199468612671\n",
      "Iteration 32109 Loss: 0.9644454121589661\n",
      "Iteration 32109 Loss: 0.9872933626174927\n",
      "Iteration 32110 Loss: 1.2581062316894531\n",
      "Iteration 32111 Loss: 0.9878049492835999\n",
      "Iteration 32112 Loss: 0.6330839991569519\n",
      "Iteration 32113 Loss: 1.1283477544784546\n",
      "Iteration 32114 Loss: 1.1703758239746094\n",
      "Iteration 32115 Loss: 1.2140382528305054\n",
      "Iteration 32116 Loss: 1.116716980934143\n",
      "Iteration 32117 Loss: 1.2509993314743042\n",
      "Iteration 32118 Loss: 0.7900938391685486\n",
      "Iteration 32119 Loss: 1.0770176649093628\n",
      "Iteration 32119 Loss: 1.0626585483551025\n",
      "Iteration 32120 Loss: 1.119917631149292\n",
      "Iteration 32121 Loss: 1.3355721235275269\n",
      "Iteration 32122 Loss: 1.4635223150253296\n",
      "Iteration 32123 Loss: 1.2045196294784546\n",
      "Iteration 32124 Loss: 0.8865564465522766\n",
      "Iteration 32125 Loss: 1.1965519189834595\n",
      "Iteration 32126 Loss: 1.2852519750595093\n",
      "Iteration 32127 Loss: 1.142295241355896\n",
      "Iteration 32128 Loss: 1.0074037313461304\n",
      "Iteration 32129 Loss: 1.0878397226333618\n",
      "Iteration 32129 Loss: 1.1729429960250854\n",
      "Iteration 32130 Loss: 0.9716479182243347\n",
      "Iteration 32131 Loss: 0.7799107432365417\n",
      "Iteration 32132 Loss: 1.1968578100204468\n",
      "Iteration 32133 Loss: 1.2383513450622559\n",
      "Iteration 32134 Loss: 0.6228141188621521\n",
      "Iteration 32135 Loss: 1.0702879428863525\n",
      "Iteration 32136 Loss: 1.1849788427352905\n",
      "Iteration 32137 Loss: 0.9436908960342407\n",
      "Iteration 32138 Loss: 0.8088629245758057\n",
      "Iteration 32139 Loss: 0.8726439476013184\n",
      "Iteration 32139 Loss: 0.9690046310424805\n",
      "Iteration 32140 Loss: 1.153818130493164\n",
      "Iteration 32141 Loss: 1.1203582286834717\n",
      "Iteration 32142 Loss: 1.133687973022461\n",
      "Iteration 32143 Loss: 1.3965407609939575\n",
      "Iteration 32144 Loss: 0.7909047603607178\n",
      "Iteration 32145 Loss: 1.030422329902649\n",
      "Iteration 32146 Loss: 0.8352318406105042\n",
      "Iteration 32147 Loss: 0.8126605749130249\n",
      "Iteration 32148 Loss: 1.1901812553405762\n",
      "Iteration 32149 Loss: 0.8098430037498474\n",
      "Iteration 32149 Loss: 1.0273648500442505\n",
      "Iteration 32150 Loss: 0.9971185326576233\n",
      "Iteration 32151 Loss: 0.9148726463317871\n",
      "Iteration 32152 Loss: 0.8047199249267578\n",
      "Iteration 32153 Loss: 1.2198822498321533\n",
      "Iteration 32154 Loss: 0.9680742621421814\n",
      "Iteration 32155 Loss: 0.8924570083618164\n",
      "Iteration 32156 Loss: 0.7052901387214661\n",
      "Iteration 32157 Loss: 1.1725441217422485\n",
      "Iteration 32158 Loss: 1.1653821468353271\n",
      "Iteration 32159 Loss: 1.0211902856826782\n",
      "Iteration 32159 Loss: 0.986153244972229\n",
      "Iteration 32160 Loss: 0.6361082792282104\n",
      "Iteration 32161 Loss: 1.057578444480896\n",
      "Iteration 32162 Loss: 1.091339111328125\n",
      "Iteration 32163 Loss: 0.797377347946167\n",
      "Iteration 32164 Loss: 1.2465744018554688\n",
      "Iteration 32165 Loss: 0.7334030866622925\n",
      "Iteration 32166 Loss: 0.5729723572731018\n",
      "Iteration 32167 Loss: 0.90260910987854\n",
      "Iteration 32168 Loss: 0.8403043746948242\n",
      "Iteration 32169 Loss: 1.3445732593536377\n",
      "Iteration 32169 Loss: 0.922283947467804\n",
      "Iteration 32170 Loss: 0.8426827788352966\n",
      "Iteration 32171 Loss: 0.6199195384979248\n",
      "Iteration 32172 Loss: 0.9972352981567383\n",
      "Iteration 32173 Loss: 0.8082830905914307\n",
      "Iteration 32174 Loss: 1.0390924215316772\n",
      "Iteration 32175 Loss: 0.993961751461029\n",
      "Iteration 32176 Loss: 0.8002442717552185\n",
      "Iteration 32177 Loss: 1.2243444919586182\n",
      "Iteration 32178 Loss: 1.3163175582885742\n",
      "Iteration 32179 Loss: 0.9428033828735352\n",
      "Iteration 32179 Loss: 0.9584884643554688\n",
      "Iteration 32180 Loss: 0.8996408581733704\n",
      "Iteration 32181 Loss: 0.9778768420219421\n",
      "Iteration 32182 Loss: 0.867536187171936\n",
      "Iteration 32183 Loss: 0.9706047177314758\n",
      "Iteration 32184 Loss: 1.522862195968628\n",
      "Iteration 32185 Loss: 0.8690842986106873\n",
      "Iteration 32186 Loss: 1.098950743675232\n",
      "Iteration 32187 Loss: 1.0495836734771729\n",
      "Iteration 32188 Loss: 0.8697646260261536\n",
      "Iteration 32189 Loss: 1.1481934785842896\n",
      "Iteration 32189 Loss: 1.0274097919464111\n",
      "Iteration 32190 Loss: 0.9167705774307251\n",
      "Iteration 32191 Loss: 0.9075063467025757\n",
      "Iteration 32192 Loss: 0.6313371658325195\n",
      "Iteration 32193 Loss: 1.4991751909255981\n",
      "Iteration 32194 Loss: 0.7536333203315735\n",
      "Iteration 32195 Loss: 0.9883729815483093\n",
      "Iteration 32196 Loss: 1.1203900575637817\n",
      "Iteration 32197 Loss: 0.9026933908462524\n",
      "Iteration 32198 Loss: 0.929375410079956\n",
      "Iteration 32199 Loss: 1.3180363178253174\n",
      "Iteration 32199 Loss: 0.9967290759086609\n",
      "Iteration 32200 Loss: 1.6465939283370972\n",
      "Iteration 32201 Loss: 0.9784042835235596\n",
      "Iteration 32202 Loss: 0.8786779046058655\n",
      "Iteration 32203 Loss: 0.9247502088546753\n",
      "Iteration 32204 Loss: 0.72403883934021\n",
      "Iteration 32205 Loss: 1.1476720571517944\n",
      "Iteration 32206 Loss: 0.9619259238243103\n",
      "Iteration 32207 Loss: 0.7955718040466309\n",
      "Iteration 32208 Loss: 0.8365296721458435\n",
      "Iteration 32209 Loss: 0.7731587290763855\n",
      "Iteration 32209 Loss: 0.966732382774353\n",
      "Iteration 32210 Loss: 0.9243401288986206\n",
      "Iteration 32211 Loss: 1.1657650470733643\n",
      "Iteration 32212 Loss: 1.3603333234786987\n",
      "Iteration 32213 Loss: 0.7897030115127563\n",
      "Iteration 32214 Loss: 1.065859317779541\n",
      "Iteration 32215 Loss: 0.870802640914917\n",
      "Iteration 32216 Loss: 1.2017600536346436\n",
      "Iteration 32217 Loss: 0.8887354731559753\n",
      "Iteration 32218 Loss: 1.3044991493225098\n",
      "Iteration 32219 Loss: 0.7401177287101746\n",
      "Iteration 32219 Loss: 1.0311915874481201\n",
      "Iteration 32220 Loss: 0.9955955147743225\n",
      "Iteration 32221 Loss: 0.9364821910858154\n",
      "Iteration 32222 Loss: 0.6948163509368896\n",
      "Iteration 32223 Loss: 1.092688798904419\n",
      "Iteration 32224 Loss: 0.7612167000770569\n",
      "Iteration 32225 Loss: 1.1111527681350708\n",
      "Iteration 32226 Loss: 0.9008452892303467\n",
      "Iteration 32227 Loss: 0.9656248688697815\n",
      "Iteration 32228 Loss: 1.050123691558838\n",
      "Iteration 32229 Loss: 0.7893017530441284\n",
      "Iteration 32229 Loss: 0.9297847747802734\n",
      "Iteration 32230 Loss: 0.8631032705307007\n",
      "Iteration 32231 Loss: 1.2963719367980957\n",
      "Iteration 32232 Loss: 0.5132502317428589\n",
      "Iteration 32233 Loss: 0.981338381767273\n",
      "Iteration 32234 Loss: 1.225428581237793\n",
      "Iteration 32235 Loss: 0.9969706535339355\n",
      "Iteration 32236 Loss: 1.0817856788635254\n",
      "Iteration 32237 Loss: 0.9491714239120483\n",
      "Iteration 32238 Loss: 1.280481219291687\n",
      "Iteration 32239 Loss: 0.7829894423484802\n",
      "Iteration 32239 Loss: 0.9970890283584595\n",
      "Iteration 32240 Loss: 1.109474778175354\n",
      "Iteration 32241 Loss: 0.9394403696060181\n",
      "Iteration 32242 Loss: 1.3684126138687134\n",
      "Iteration 32243 Loss: 0.9682154059410095\n",
      "Iteration 32244 Loss: 0.7614532113075256\n",
      "Iteration 32245 Loss: 0.9478391408920288\n",
      "Iteration 32246 Loss: 1.0757801532745361\n",
      "Iteration 32247 Loss: 0.9719427227973938\n",
      "Iteration 32248 Loss: 1.1738955974578857\n",
      "Iteration 32249 Loss: 0.710992157459259\n",
      "Iteration 32249 Loss: 1.0027445554733276\n",
      "Iteration 32250 Loss: 0.9125348329544067\n",
      "Iteration 32251 Loss: 1.0657941102981567\n",
      "Iteration 32252 Loss: 1.043027400970459\n",
      "Iteration 32253 Loss: 1.0356433391571045\n",
      "Iteration 32254 Loss: 1.0872657299041748\n",
      "Iteration 32255 Loss: 1.0266847610473633\n",
      "Iteration 32256 Loss: 1.223090648651123\n",
      "Iteration 32257 Loss: 0.6236526370048523\n",
      "Iteration 32258 Loss: 1.3675007820129395\n",
      "Iteration 32259 Loss: 0.8597512245178223\n",
      "Iteration 32259 Loss: 1.0244944095611572\n",
      "Iteration 32260 Loss: 1.0726736783981323\n",
      "Iteration 32261 Loss: 1.0347766876220703\n",
      "Iteration 32262 Loss: 0.9014976620674133\n",
      "Iteration 32263 Loss: 0.6144585609436035\n",
      "Iteration 32264 Loss: 1.1498076915740967\n",
      "Iteration 32265 Loss: 0.8727403879165649\n",
      "Iteration 32266 Loss: 1.0903503894805908\n",
      "Iteration 32267 Loss: 1.2187843322753906\n",
      "Iteration 32268 Loss: 0.9450419545173645\n",
      "Iteration 32269 Loss: 0.9698450565338135\n",
      "Iteration 32269 Loss: 0.9869977235794067\n",
      "Iteration 32270 Loss: 0.9771562218666077\n",
      "Iteration 32271 Loss: 1.1107107400894165\n",
      "Iteration 32272 Loss: 1.041458010673523\n",
      "Iteration 32273 Loss: 1.0592886209487915\n",
      "Iteration 32274 Loss: 1.3146789073944092\n",
      "Iteration 32275 Loss: 1.1289478540420532\n",
      "Iteration 32276 Loss: 1.0272153615951538\n",
      "Iteration 32277 Loss: 1.012562870979309\n",
      "Iteration 32278 Loss: 1.301505208015442\n",
      "Iteration 32279 Loss: 1.0779249668121338\n",
      "Iteration 32279 Loss: 1.1051448583602905\n",
      "Iteration 32280 Loss: 1.1322131156921387\n",
      "Iteration 32281 Loss: 1.087619423866272\n",
      "Iteration 32282 Loss: 1.1532050371170044\n",
      "Iteration 32283 Loss: 1.0649436712265015\n",
      "Iteration 32284 Loss: 1.0247677564620972\n",
      "Iteration 32285 Loss: 0.7298119068145752\n",
      "Iteration 32286 Loss: 1.1006886959075928\n",
      "Iteration 32287 Loss: 0.8609949350357056\n",
      "Iteration 32288 Loss: 1.0574945211410522\n",
      "Iteration 32289 Loss: 1.4793740510940552\n",
      "Iteration 32289 Loss: 1.0691113471984863\n",
      "Iteration 32290 Loss: 1.1925628185272217\n",
      "Iteration 32291 Loss: 1.0028021335601807\n",
      "Iteration 32292 Loss: 1.087429404258728\n",
      "Iteration 32293 Loss: 1.0660748481750488\n",
      "Iteration 32294 Loss: 1.2262699604034424\n",
      "Iteration 32295 Loss: 0.7827922701835632\n",
      "Iteration 32296 Loss: 0.9923911094665527\n",
      "Iteration 32297 Loss: 1.3510277271270752\n",
      "Iteration 32298 Loss: 1.084840178489685\n",
      "Iteration 32299 Loss: 1.2809606790542603\n",
      "Iteration 32299 Loss: 1.1067150831222534\n",
      "Iteration 32300 Loss: 0.9325704574584961\n",
      "Iteration 32301 Loss: 0.9988794326782227\n",
      "Iteration 32302 Loss: 0.775201141834259\n",
      "Iteration 32303 Loss: 1.340558648109436\n",
      "Iteration 32304 Loss: 1.0121513605117798\n",
      "Iteration 32305 Loss: 0.5665478110313416\n",
      "Iteration 32306 Loss: 1.0003238916397095\n",
      "Iteration 32307 Loss: 0.8333131074905396\n",
      "Iteration 32308 Loss: 1.1904103755950928\n",
      "Iteration 32309 Loss: 0.7574275135993958\n",
      "Iteration 32309 Loss: 0.9407383799552917\n",
      "Iteration 32310 Loss: 1.0419790744781494\n",
      "Iteration 32311 Loss: 0.9996690154075623\n",
      "Iteration 32312 Loss: 1.0419551134109497\n",
      "Iteration 32313 Loss: 1.0848078727722168\n",
      "Iteration 32314 Loss: 0.9044942855834961\n",
      "Iteration 32315 Loss: 1.04538893699646\n",
      "Iteration 32316 Loss: 1.2228083610534668\n",
      "Iteration 32317 Loss: 0.9556724429130554\n",
      "Iteration 32318 Loss: 1.1190060377120972\n",
      "Iteration 32319 Loss: 1.1151918172836304\n",
      "Iteration 32319 Loss: 1.0530972480773926\n",
      "Iteration 32320 Loss: 1.432932734489441\n",
      "Iteration 32321 Loss: 1.1660350561141968\n",
      "Iteration 32322 Loss: 0.9121837615966797\n",
      "Iteration 32323 Loss: 0.9505227208137512\n",
      "Iteration 32324 Loss: 0.96302729845047\n",
      "Iteration 32325 Loss: 0.9006067514419556\n",
      "Iteration 32326 Loss: 0.7881910800933838\n",
      "Iteration 32327 Loss: 1.0588427782058716\n",
      "Iteration 32328 Loss: 0.8350580930709839\n",
      "Iteration 32329 Loss: 0.7216896414756775\n",
      "Iteration 32329 Loss: 0.9729089736938477\n",
      "Iteration 32330 Loss: 0.9525431394577026\n",
      "Iteration 32331 Loss: 1.0148286819458008\n",
      "Iteration 32332 Loss: 0.7369185090065002\n",
      "Iteration 32333 Loss: 0.9012789130210876\n",
      "Iteration 32334 Loss: 0.9826947450637817\n",
      "Iteration 32335 Loss: 0.876468300819397\n",
      "Iteration 32336 Loss: 1.1234413385391235\n",
      "Iteration 32337 Loss: 0.8247942924499512\n",
      "Iteration 32338 Loss: 1.040610432624817\n",
      "Iteration 32339 Loss: 0.7506769299507141\n",
      "Iteration 32339 Loss: 0.9204255938529968\n",
      "Iteration 32340 Loss: 1.0200815200805664\n",
      "Iteration 32341 Loss: 0.9424917697906494\n",
      "Iteration 32342 Loss: 1.370449185371399\n",
      "Iteration 32343 Loss: 0.777485191822052\n",
      "Iteration 32344 Loss: 0.83140629529953\n",
      "Iteration 32345 Loss: 1.1074081659317017\n",
      "Iteration 32346 Loss: 0.7431918382644653\n",
      "Iteration 32347 Loss: 1.0556135177612305\n",
      "Iteration 32348 Loss: 0.9526519775390625\n",
      "Iteration 32349 Loss: 1.2558008432388306\n",
      "Iteration 32349 Loss: 1.0056580305099487\n",
      "Iteration 32350 Loss: 1.2451564073562622\n",
      "Iteration 32351 Loss: 1.5014606714248657\n",
      "Iteration 32352 Loss: 0.9994171857833862\n",
      "Iteration 32353 Loss: 0.6628265380859375\n",
      "Iteration 32354 Loss: 0.9219057559967041\n",
      "Iteration 32355 Loss: 0.872831404209137\n",
      "Iteration 32356 Loss: 0.8464727401733398\n",
      "Iteration 32357 Loss: 0.78882896900177\n",
      "Iteration 32358 Loss: 0.8352543115615845\n",
      "Iteration 32359 Loss: 1.3210612535476685\n",
      "Iteration 32359 Loss: 0.9995215535163879\n",
      "Iteration 32360 Loss: 0.7758829593658447\n",
      "Iteration 32361 Loss: 0.8525000214576721\n",
      "Iteration 32362 Loss: 0.8944829106330872\n",
      "Iteration 32363 Loss: 1.0917184352874756\n",
      "Iteration 32364 Loss: 1.2038239240646362\n",
      "Iteration 32365 Loss: 1.3737545013427734\n",
      "Iteration 32366 Loss: 0.7364528179168701\n",
      "Iteration 32367 Loss: 1.1682074069976807\n",
      "Iteration 32368 Loss: 0.7580878734588623\n",
      "Iteration 32369 Loss: 0.9237638115882874\n",
      "Iteration 32369 Loss: 0.9778674840927124\n",
      "Iteration 32370 Loss: 0.8549402356147766\n",
      "Iteration 32371 Loss: 0.9705705046653748\n",
      "Iteration 32372 Loss: 1.004921793937683\n",
      "Iteration 32373 Loss: 0.7869729995727539\n",
      "Iteration 32374 Loss: 1.0871124267578125\n",
      "Iteration 32375 Loss: 1.1922844648361206\n",
      "Iteration 32376 Loss: 1.093867301940918\n",
      "Iteration 32377 Loss: 1.2098828554153442\n",
      "Iteration 32378 Loss: 1.1352384090423584\n",
      "Iteration 32379 Loss: 0.735871434211731\n",
      "Iteration 32379 Loss: 1.0071662664413452\n",
      "Iteration 32380 Loss: 0.8713909983634949\n",
      "Iteration 32381 Loss: 1.1987501382827759\n",
      "Iteration 32382 Loss: 0.889389157295227\n",
      "Iteration 32383 Loss: 1.0522046089172363\n",
      "Iteration 32384 Loss: 1.0470397472381592\n",
      "Iteration 32385 Loss: 0.6368821263313293\n",
      "Iteration 32386 Loss: 1.2000278234481812\n",
      "Iteration 32387 Loss: 1.0643482208251953\n",
      "Iteration 32388 Loss: 1.195851445198059\n",
      "Iteration 32389 Loss: 0.8245541453361511\n",
      "Iteration 32389 Loss: 0.9980438351631165\n",
      "Iteration 32390 Loss: 1.2796642780303955\n",
      "Iteration 32391 Loss: 1.1833829879760742\n",
      "Iteration 32392 Loss: 0.9366206526756287\n",
      "Iteration 32393 Loss: 0.6686633825302124\n",
      "Iteration 32394 Loss: 0.7307751774787903\n",
      "Iteration 32395 Loss: 1.138571858406067\n",
      "Iteration 32396 Loss: 1.184574842453003\n",
      "Iteration 32397 Loss: 0.841672956943512\n",
      "Iteration 32398 Loss: 1.0479127168655396\n",
      "Iteration 32399 Loss: 0.9672459363937378\n",
      "Iteration 32399 Loss: 0.9979084730148315\n",
      "Iteration 32400 Loss: 1.0173190832138062\n",
      "Iteration 32401 Loss: 1.169118881225586\n",
      "Iteration 32402 Loss: 0.8923630118370056\n",
      "Iteration 32403 Loss: 0.931719958782196\n",
      "Iteration 32404 Loss: 1.1114661693572998\n",
      "Iteration 32405 Loss: 0.9071834087371826\n",
      "Iteration 32406 Loss: 0.6520380973815918\n",
      "Iteration 32407 Loss: 1.0663243532180786\n",
      "Iteration 32408 Loss: 0.8043304681777954\n",
      "Iteration 32409 Loss: 1.2188971042633057\n",
      "Iteration 32409 Loss: 0.9770761728286743\n",
      "Iteration 32410 Loss: 1.2992935180664062\n",
      "Iteration 32411 Loss: 0.8050412535667419\n",
      "Iteration 32412 Loss: 1.2152734994888306\n",
      "Iteration 32413 Loss: 0.8079285621643066\n",
      "Iteration 32414 Loss: 0.8316253423690796\n",
      "Iteration 32415 Loss: 1.1241741180419922\n",
      "Iteration 32416 Loss: 1.0959327220916748\n",
      "Iteration 32417 Loss: 1.034281849861145\n",
      "Iteration 32418 Loss: 0.9367848634719849\n",
      "Iteration 32419 Loss: 0.844994306564331\n",
      "Iteration 32419 Loss: 0.9995329976081848\n",
      "Iteration 32420 Loss: 1.3140904903411865\n",
      "Iteration 32421 Loss: 0.9247913956642151\n",
      "Iteration 32422 Loss: 0.9968512058258057\n",
      "Iteration 32423 Loss: 1.2080193758010864\n",
      "Iteration 32424 Loss: 1.2863550186157227\n",
      "Iteration 32425 Loss: 0.7985162138938904\n",
      "Iteration 32426 Loss: 0.9984248876571655\n",
      "Iteration 32427 Loss: 1.0423133373260498\n",
      "Iteration 32428 Loss: 1.1461389064788818\n",
      "Iteration 32429 Loss: 1.2943216562271118\n",
      "Iteration 32429 Loss: 1.1009823083877563\n",
      "Iteration 32430 Loss: 0.7394108772277832\n",
      "Iteration 32431 Loss: 0.8879395127296448\n",
      "Iteration 32432 Loss: 0.9096602201461792\n",
      "Iteration 32433 Loss: 1.0604689121246338\n",
      "Iteration 32434 Loss: 0.7386799454689026\n",
      "Iteration 32435 Loss: 0.8715863227844238\n",
      "Iteration 32436 Loss: 1.4231994152069092\n",
      "Iteration 32437 Loss: 1.0319136381149292\n",
      "Iteration 32438 Loss: 0.7328330874443054\n",
      "Iteration 32439 Loss: 0.8068756461143494\n",
      "Iteration 32439 Loss: 0.9202567338943481\n",
      "Iteration 32440 Loss: 1.3598445653915405\n",
      "Iteration 32441 Loss: 1.087538480758667\n",
      "Iteration 32442 Loss: 1.0264102220535278\n",
      "Iteration 32443 Loss: 1.024081826210022\n",
      "Iteration 32444 Loss: 0.7029964923858643\n",
      "Iteration 32445 Loss: 1.2384555339813232\n",
      "Iteration 32446 Loss: 0.9402819275856018\n",
      "Iteration 32447 Loss: 0.8673860430717468\n",
      "Iteration 32448 Loss: 1.1347260475158691\n",
      "Iteration 32449 Loss: 1.1860084533691406\n",
      "Iteration 32449 Loss: 1.0567729473114014\n",
      "Iteration 32450 Loss: 0.8292220830917358\n",
      "Iteration 32451 Loss: 1.0045706033706665\n",
      "Iteration 32452 Loss: 1.1415272951126099\n",
      "Iteration 32453 Loss: 1.0748276710510254\n",
      "Iteration 32454 Loss: 1.08173406124115\n",
      "Iteration 32455 Loss: 1.0297815799713135\n",
      "Iteration 32456 Loss: 1.2369554042816162\n",
      "Iteration 32457 Loss: 0.9036884307861328\n",
      "Iteration 32458 Loss: 1.4655038118362427\n",
      "Iteration 32459 Loss: 1.1033920049667358\n",
      "Iteration 32459 Loss: 1.0871202945709229\n",
      "Iteration 32460 Loss: 1.0805439949035645\n",
      "Iteration 32461 Loss: 0.7563789486885071\n",
      "Iteration 32462 Loss: 0.9927417039871216\n",
      "Iteration 32463 Loss: 1.174865961074829\n",
      "Iteration 32464 Loss: 1.2320261001586914\n",
      "Iteration 32465 Loss: 1.115566611289978\n",
      "Iteration 32466 Loss: 1.0851627588272095\n",
      "Iteration 32467 Loss: 0.7586548328399658\n",
      "Iteration 32468 Loss: 1.1194287538528442\n",
      "Iteration 32469 Loss: 0.9011306762695312\n",
      "Iteration 32469 Loss: 1.0216500759124756\n",
      "Iteration 32470 Loss: 0.6845629215240479\n",
      "Iteration 32471 Loss: 1.1703088283538818\n",
      "Iteration 32472 Loss: 0.9550684094429016\n",
      "Iteration 32473 Loss: 0.8170078992843628\n",
      "Iteration 32474 Loss: 1.0726481676101685\n",
      "Iteration 32475 Loss: 1.189922571182251\n",
      "Iteration 32476 Loss: 1.1541413068771362\n",
      "Iteration 32477 Loss: 0.8960452079772949\n",
      "Iteration 32478 Loss: 0.8386275768280029\n",
      "Iteration 32479 Loss: 0.8993865251541138\n",
      "Iteration 32479 Loss: 0.9677718877792358\n",
      "Iteration 32480 Loss: 1.0985336303710938\n",
      "Iteration 32481 Loss: 0.7616251111030579\n",
      "Iteration 32482 Loss: 1.3071013689041138\n",
      "Iteration 32483 Loss: 1.218932867050171\n",
      "Iteration 32484 Loss: 1.1550263166427612\n",
      "Iteration 32485 Loss: 0.7070363163948059\n",
      "Iteration 32486 Loss: 0.9028221964836121\n",
      "Iteration 32487 Loss: 1.303614616394043\n",
      "Iteration 32488 Loss: 1.0658546686172485\n",
      "Iteration 32489 Loss: 1.3476269245147705\n",
      "Iteration 32489 Loss: 1.0868175029754639\n",
      "Iteration 32490 Loss: 0.9544312357902527\n",
      "Iteration 32491 Loss: 1.5681166648864746\n",
      "Iteration 32492 Loss: 0.9664429426193237\n",
      "Iteration 32493 Loss: 1.028848648071289\n",
      "Iteration 32494 Loss: 1.0515319108963013\n",
      "Iteration 32495 Loss: 1.1243393421173096\n",
      "Iteration 32496 Loss: 0.7865957021713257\n",
      "Iteration 32497 Loss: 1.3912866115570068\n",
      "Iteration 32498 Loss: 1.3388200998306274\n",
      "Iteration 32499 Loss: 0.675584614276886\n",
      "Iteration 32499 Loss: 1.0885998010635376\n",
      "Iteration 32500 Loss: 1.035499095916748\n",
      "Iteration 32501 Loss: 0.9454574584960938\n",
      "Iteration 32502 Loss: 0.783341646194458\n",
      "Iteration 32503 Loss: 0.8906108736991882\n",
      "Iteration 32504 Loss: 1.1629137992858887\n",
      "Iteration 32505 Loss: 0.897812008857727\n",
      "Iteration 32506 Loss: 1.3264789581298828\n",
      "Iteration 32507 Loss: 0.9197303056716919\n",
      "Iteration 32508 Loss: 0.9120376110076904\n",
      "Iteration 32509 Loss: 1.0796887874603271\n",
      "Iteration 32509 Loss: 0.9953570365905762\n",
      "Iteration 32510 Loss: 0.9778828024864197\n",
      "Iteration 32511 Loss: 1.1777769327163696\n",
      "Iteration 32512 Loss: 1.1483408212661743\n",
      "Iteration 32513 Loss: 0.8422465324401855\n",
      "Iteration 32514 Loss: 1.1014341115951538\n",
      "Iteration 32515 Loss: 1.1352946758270264\n",
      "Iteration 32516 Loss: 0.9450502395629883\n",
      "Iteration 32517 Loss: 0.9269672632217407\n",
      "Iteration 32518 Loss: 0.6791709661483765\n",
      "Iteration 32519 Loss: 0.8744254112243652\n",
      "Iteration 32519 Loss: 0.9808589816093445\n",
      "Iteration 32520 Loss: 0.8916458487510681\n",
      "Iteration 32521 Loss: 0.9554944038391113\n",
      "Iteration 32522 Loss: 0.8172007203102112\n",
      "Iteration 32523 Loss: 1.170729160308838\n",
      "Iteration 32524 Loss: 0.8244282007217407\n",
      "Iteration 32525 Loss: 1.0603758096694946\n",
      "Iteration 32526 Loss: 0.5774648189544678\n",
      "Iteration 32527 Loss: 0.8959680199623108\n",
      "Iteration 32528 Loss: 1.0582399368286133\n",
      "Iteration 32529 Loss: 0.983870804309845\n",
      "Iteration 32529 Loss: 0.9235418438911438\n",
      "Iteration 32530 Loss: 1.1568523645401\n",
      "Iteration 32531 Loss: 0.9099379777908325\n",
      "Iteration 32532 Loss: 0.9569498896598816\n",
      "Iteration 32533 Loss: 1.3868368864059448\n",
      "Iteration 32534 Loss: 1.0657174587249756\n",
      "Iteration 32535 Loss: 1.0390994548797607\n",
      "Iteration 32536 Loss: 0.9869296550750732\n",
      "Iteration 32537 Loss: 0.9044987559318542\n",
      "Iteration 32538 Loss: 1.0292344093322754\n",
      "Iteration 32539 Loss: 1.307018756866455\n",
      "Iteration 32539 Loss: 1.0743076801300049\n",
      "Iteration 32540 Loss: 1.2354336977005005\n",
      "Iteration 32541 Loss: 1.0989980697631836\n",
      "Iteration 32542 Loss: 1.2582120895385742\n",
      "Iteration 32543 Loss: 1.0451914072036743\n",
      "Iteration 32544 Loss: 1.0460290908813477\n",
      "Iteration 32545 Loss: 1.0252667665481567\n",
      "Iteration 32546 Loss: 1.1025140285491943\n",
      "Iteration 32547 Loss: 0.8402847647666931\n",
      "Iteration 32548 Loss: 0.9397108554840088\n",
      "Iteration 32549 Loss: 1.086997628211975\n",
      "Iteration 32549 Loss: 1.0678638219833374\n",
      "Iteration 32550 Loss: 0.9469662308692932\n",
      "Iteration 32551 Loss: 1.1504435539245605\n",
      "Iteration 32552 Loss: 0.8399598598480225\n",
      "Iteration 32553 Loss: 1.105069875717163\n",
      "Iteration 32554 Loss: 0.8019826412200928\n",
      "Iteration 32555 Loss: 1.040055274963379\n",
      "Iteration 32556 Loss: 1.188805103302002\n",
      "Iteration 32557 Loss: 1.1478358507156372\n",
      "Iteration 32558 Loss: 0.9865736961364746\n",
      "Iteration 32559 Loss: 0.8769458532333374\n",
      "Iteration 32559 Loss: 1.008463740348816\n",
      "Iteration 32560 Loss: 1.0999983549118042\n",
      "Iteration 32561 Loss: 1.0211888551712036\n",
      "Iteration 32562 Loss: 0.9744740724563599\n",
      "Iteration 32563 Loss: 1.1073359251022339\n",
      "Iteration 32564 Loss: 0.7281005382537842\n",
      "Iteration 32565 Loss: 1.2713420391082764\n",
      "Iteration 32566 Loss: 0.9778828024864197\n",
      "Iteration 32567 Loss: 0.9185552597045898\n",
      "Iteration 32568 Loss: 1.1986985206604004\n",
      "Iteration 32569 Loss: 0.940515398979187\n",
      "Iteration 32569 Loss: 1.0238091945648193\n",
      "Iteration 32570 Loss: 0.9765618443489075\n",
      "Iteration 32571 Loss: 0.9779552817344666\n",
      "Iteration 32572 Loss: 1.1360089778900146\n",
      "Iteration 32573 Loss: 1.1134483814239502\n",
      "Iteration 32574 Loss: 1.087233304977417\n",
      "Iteration 32575 Loss: 0.717197060585022\n",
      "Iteration 32576 Loss: 0.7470021843910217\n",
      "Iteration 32577 Loss: 0.6376285552978516\n",
      "Iteration 32578 Loss: 0.9941413402557373\n",
      "Iteration 32579 Loss: 1.2475762367248535\n",
      "Iteration 32579 Loss: 0.9634754061698914\n",
      "Iteration 32580 Loss: 0.973013162612915\n",
      "Iteration 32581 Loss: 1.3379782438278198\n",
      "Iteration 32582 Loss: 1.0198400020599365\n",
      "Iteration 32583 Loss: 1.101778507232666\n",
      "Iteration 32584 Loss: 0.6471366286277771\n",
      "Iteration 32585 Loss: 1.048574447631836\n",
      "Iteration 32586 Loss: 1.4086014032363892\n",
      "Iteration 32587 Loss: 0.9886645078659058\n",
      "Iteration 32588 Loss: 0.5160083770751953\n",
      "Iteration 32589 Loss: 0.7863119840621948\n",
      "Iteration 32589 Loss: 0.9827907681465149\n",
      "Iteration 32590 Loss: 0.8282188773155212\n",
      "Iteration 32591 Loss: 1.0899723768234253\n",
      "Iteration 32592 Loss: 1.231033444404602\n",
      "Iteration 32593 Loss: 0.7879921793937683\n",
      "Iteration 32594 Loss: 1.0371851921081543\n",
      "Iteration 32595 Loss: 1.2336617708206177\n",
      "Iteration 32596 Loss: 1.3565406799316406\n",
      "Iteration 32597 Loss: 1.2369440793991089\n",
      "Iteration 32598 Loss: 0.701661229133606\n",
      "Iteration 32599 Loss: 1.2126476764678955\n",
      "Iteration 32599 Loss: 1.071585774421692\n",
      "Iteration 32600 Loss: 0.8351059556007385\n",
      "Iteration 32601 Loss: 0.9730720520019531\n",
      "Iteration 32602 Loss: 1.1640610694885254\n",
      "Iteration 32603 Loss: 0.8680655360221863\n",
      "Iteration 32604 Loss: 1.0710946321487427\n",
      "Iteration 32605 Loss: 1.313139796257019\n",
      "Iteration 32606 Loss: 0.9161485433578491\n",
      "Iteration 32607 Loss: 0.8479881882667542\n",
      "Iteration 32608 Loss: 1.2011358737945557\n",
      "Iteration 32609 Loss: 0.9950154423713684\n",
      "Iteration 32609 Loss: 1.0184825658798218\n",
      "Iteration 32610 Loss: 1.2101951837539673\n",
      "Iteration 32611 Loss: 1.050070881843567\n",
      "Iteration 32612 Loss: 0.9814029932022095\n",
      "Iteration 32613 Loss: 1.0191718339920044\n",
      "Iteration 32614 Loss: 0.791511595249176\n",
      "Iteration 32615 Loss: 1.155505895614624\n",
      "Iteration 32616 Loss: 1.0841809511184692\n",
      "Iteration 32617 Loss: 0.905794084072113\n",
      "Iteration 32618 Loss: 0.5993521809577942\n",
      "Iteration 32619 Loss: 0.973624587059021\n",
      "Iteration 32619 Loss: 0.9770810008049011\n",
      "Iteration 32620 Loss: 1.1083065271377563\n",
      "Iteration 32621 Loss: 1.168623924255371\n",
      "Iteration 32622 Loss: 1.0863842964172363\n",
      "Iteration 32623 Loss: 0.37756797671318054\n",
      "Iteration 32624 Loss: 0.9491552710533142\n",
      "Iteration 32625 Loss: 0.7246987223625183\n",
      "Iteration 32626 Loss: 0.7843225002288818\n",
      "Iteration 32627 Loss: 0.8262652158737183\n",
      "Iteration 32628 Loss: 1.1958140134811401\n",
      "Iteration 32629 Loss: 0.8537061214447021\n",
      "Iteration 32629 Loss: 0.9074844121932983\n",
      "Iteration 32630 Loss: 1.0154943466186523\n",
      "Iteration 32631 Loss: 1.0341157913208008\n",
      "Iteration 32632 Loss: 1.0082502365112305\n",
      "Iteration 32633 Loss: 1.14972984790802\n",
      "Iteration 32634 Loss: 0.9280720353126526\n",
      "Iteration 32635 Loss: 0.9695082902908325\n",
      "Iteration 32636 Loss: 0.7597941160202026\n",
      "Iteration 32637 Loss: 1.1463679075241089\n",
      "Iteration 32638 Loss: 0.6912131309509277\n",
      "Iteration 32639 Loss: 0.7439742684364319\n",
      "Iteration 32639 Loss: 0.9446519613265991\n",
      "Iteration 32640 Loss: 1.123812198638916\n",
      "Iteration 32641 Loss: 0.7412176132202148\n",
      "Iteration 32642 Loss: 0.9054574966430664\n",
      "Iteration 32643 Loss: 1.0132153034210205\n",
      "Iteration 32644 Loss: 1.0337870121002197\n",
      "Iteration 32645 Loss: 0.7531787753105164\n",
      "Iteration 32646 Loss: 0.9652538895606995\n",
      "Iteration 32647 Loss: 0.8979772925376892\n",
      "Iteration 32648 Loss: 0.9563292860984802\n",
      "Iteration 32649 Loss: 1.2344691753387451\n",
      "Iteration 32649 Loss: 0.9624696969985962\n",
      "Iteration 32650 Loss: 1.2389769554138184\n",
      "Iteration 32651 Loss: 0.9232237935066223\n",
      "Iteration 32652 Loss: 0.6736247539520264\n",
      "Iteration 32653 Loss: 1.0782387256622314\n",
      "Iteration 32654 Loss: 1.470018982887268\n",
      "Iteration 32655 Loss: 1.0044997930526733\n",
      "Iteration 32656 Loss: 1.0696924924850464\n",
      "Iteration 32657 Loss: 1.129103660583496\n",
      "Iteration 32658 Loss: 0.9859672784805298\n",
      "Iteration 32659 Loss: 0.9201450347900391\n",
      "Iteration 32659 Loss: 1.049349069595337\n",
      "Iteration 32660 Loss: 1.1891332864761353\n",
      "Iteration 32661 Loss: 0.8398143649101257\n",
      "Iteration 32662 Loss: 1.2267272472381592\n",
      "Iteration 32663 Loss: 0.8547588586807251\n",
      "Iteration 32664 Loss: 1.0583927631378174\n",
      "Iteration 32665 Loss: 1.1236848831176758\n",
      "Iteration 32666 Loss: 1.0030009746551514\n",
      "Iteration 32667 Loss: 0.7855586409568787\n",
      "Iteration 32668 Loss: 1.0040836334228516\n",
      "Iteration 32669 Loss: 0.9720494151115417\n",
      "Iteration 32669 Loss: 1.0057203769683838\n",
      "Iteration 32670 Loss: 0.9288910627365112\n",
      "Iteration 32671 Loss: 1.351701259613037\n",
      "Iteration 32672 Loss: 1.001734733581543\n",
      "Iteration 32673 Loss: 0.9945278167724609\n",
      "Iteration 32674 Loss: 0.9803740978240967\n",
      "Iteration 32675 Loss: 1.0823299884796143\n",
      "Iteration 32676 Loss: 0.7320699095726013\n",
      "Iteration 32677 Loss: 1.0827659368515015\n",
      "Iteration 32678 Loss: 1.2742704153060913\n",
      "Iteration 32679 Loss: 1.106702208518982\n",
      "Iteration 32679 Loss: 1.0535366535186768\n",
      "Iteration 32680 Loss: 0.8077790141105652\n",
      "Iteration 32681 Loss: 1.1566890478134155\n",
      "Iteration 32682 Loss: 1.2023639678955078\n",
      "Iteration 32683 Loss: 0.8330237865447998\n",
      "Iteration 32684 Loss: 1.0806474685668945\n",
      "Iteration 32685 Loss: 1.3601614236831665\n",
      "Iteration 32686 Loss: 1.1166048049926758\n",
      "Iteration 32687 Loss: 1.090187430381775\n",
      "Iteration 32688 Loss: 0.9589693546295166\n",
      "Iteration 32689 Loss: 1.4502322673797607\n",
      "Iteration 32689 Loss: 1.105665922164917\n",
      "Iteration 32690 Loss: 1.1484122276306152\n",
      "Iteration 32691 Loss: 1.0792758464813232\n",
      "Iteration 32692 Loss: 0.9652658104896545\n",
      "Iteration 32693 Loss: 1.1959706544876099\n",
      "Iteration 32694 Loss: 0.9683894515037537\n",
      "Iteration 32695 Loss: 1.0411854982376099\n",
      "Iteration 32696 Loss: 1.1679301261901855\n",
      "Iteration 32697 Loss: 1.2160696983337402\n",
      "Iteration 32698 Loss: 1.0995980501174927\n",
      "Iteration 32699 Loss: 1.1166261434555054\n",
      "Iteration 32699 Loss: 1.099872350692749\n",
      "Iteration 32700 Loss: 1.1642117500305176\n",
      "Iteration 32701 Loss: 0.8647204041481018\n",
      "Iteration 32702 Loss: 0.9850687980651855\n",
      "Iteration 32703 Loss: 0.9713460803031921\n",
      "Iteration 32704 Loss: 0.971018373966217\n",
      "Iteration 32705 Loss: 1.2706834077835083\n",
      "Iteration 32706 Loss: 0.8259857296943665\n",
      "Iteration 32707 Loss: 0.8195214867591858\n",
      "Iteration 32708 Loss: 1.2394856214523315\n",
      "Iteration 32709 Loss: 0.9156323075294495\n",
      "Iteration 32709 Loss: 1.0027674436569214\n",
      "Iteration 32710 Loss: 0.7546676397323608\n",
      "Iteration 32711 Loss: 0.9938796758651733\n",
      "Iteration 32712 Loss: 0.7503017783164978\n",
      "Iteration 32713 Loss: 0.9432100057601929\n",
      "Iteration 32714 Loss: 0.7757247686386108\n",
      "Iteration 32715 Loss: 0.8027324080467224\n",
      "Iteration 32716 Loss: 1.410483956336975\n",
      "Iteration 32717 Loss: 0.7566330432891846\n",
      "Iteration 32718 Loss: 1.4015898704528809\n",
      "Iteration 32719 Loss: 1.084984540939331\n",
      "Iteration 32719 Loss: 0.967420756816864\n",
      "Iteration 32720 Loss: 0.8521939516067505\n",
      "Iteration 32721 Loss: 1.2708146572113037\n",
      "Iteration 32722 Loss: 0.7817482948303223\n",
      "Iteration 32723 Loss: 1.1270248889923096\n",
      "Iteration 32724 Loss: 0.8343726396560669\n",
      "Iteration 32725 Loss: 0.7017116546630859\n",
      "Iteration 32726 Loss: 1.0266035795211792\n",
      "Iteration 32727 Loss: 1.1775095462799072\n",
      "Iteration 32728 Loss: 1.0798882246017456\n",
      "Iteration 32729 Loss: 1.04570734500885\n",
      "Iteration 32729 Loss: 0.9897574186325073\n",
      "Iteration 32730 Loss: 0.8477551341056824\n",
      "Iteration 32731 Loss: 0.9975227117538452\n",
      "Iteration 32732 Loss: 0.9374401569366455\n",
      "Iteration 32733 Loss: 1.1457703113555908\n",
      "Iteration 32734 Loss: 0.7476822733879089\n",
      "Iteration 32735 Loss: 0.9690062999725342\n",
      "Iteration 32736 Loss: 1.141017198562622\n",
      "Iteration 32737 Loss: 0.71054607629776\n",
      "Iteration 32738 Loss: 0.9231563806533813\n",
      "Iteration 32739 Loss: 1.0371131896972656\n",
      "Iteration 32739 Loss: 0.9457009434700012\n",
      "Iteration 32740 Loss: 0.8438816666603088\n",
      "Iteration 32741 Loss: 1.0124138593673706\n",
      "Iteration 32742 Loss: 1.2320369482040405\n",
      "Iteration 32743 Loss: 0.733609676361084\n",
      "Iteration 32744 Loss: 1.2914224863052368\n",
      "Iteration 32745 Loss: 1.1719413995742798\n",
      "Iteration 32746 Loss: 0.8852162957191467\n",
      "Iteration 32747 Loss: 1.0654654502868652\n",
      "Iteration 32748 Loss: 0.7571485042572021\n",
      "Iteration 32749 Loss: 0.9200306534767151\n",
      "Iteration 32749 Loss: 0.9913166165351868\n",
      "Iteration 32750 Loss: 1.1981234550476074\n",
      "Iteration 32751 Loss: 0.9709502458572388\n",
      "Iteration 32752 Loss: 0.927008330821991\n",
      "Iteration 32753 Loss: 1.1560916900634766\n",
      "Iteration 32754 Loss: 0.6883494853973389\n",
      "Iteration 32755 Loss: 0.90785813331604\n",
      "Iteration 32756 Loss: 0.8196646571159363\n",
      "Iteration 32757 Loss: 1.0017915964126587\n",
      "Iteration 32758 Loss: 1.2573928833007812\n",
      "Iteration 32759 Loss: 1.4805270433425903\n",
      "Iteration 32759 Loss: 1.0407757759094238\n",
      "Iteration 32760 Loss: 0.8742783665657043\n",
      "Iteration 32761 Loss: 1.3863416910171509\n",
      "Iteration 32762 Loss: 1.0753183364868164\n",
      "Iteration 32763 Loss: 1.0756421089172363\n",
      "Iteration 32764 Loss: 0.9784447550773621\n",
      "Iteration 32765 Loss: 0.7019370198249817\n",
      "Iteration 32766 Loss: 0.8510523438453674\n",
      "Iteration 32767 Loss: 1.3137001991271973\n",
      "Iteration 32768 Loss: 1.105074405670166\n",
      "Iteration 32769 Loss: 1.165322184562683\n",
      "Iteration 32769 Loss: 1.0527111291885376\n",
      "Iteration 32770 Loss: 0.8774226903915405\n",
      "Iteration 32771 Loss: 0.9417359828948975\n",
      "Iteration 32772 Loss: 1.0517464876174927\n",
      "Iteration 32773 Loss: 1.2297636270523071\n",
      "Iteration 32774 Loss: 0.9200814962387085\n",
      "Iteration 32775 Loss: 1.2830554246902466\n",
      "Iteration 32776 Loss: 0.874167799949646\n",
      "Iteration 32777 Loss: 0.7514018416404724\n",
      "Iteration 32778 Loss: 1.0605558156967163\n",
      "Iteration 32779 Loss: 0.7459920048713684\n",
      "Iteration 32779 Loss: 0.9735922813415527\n",
      "Iteration 32780 Loss: 0.8073126077651978\n",
      "Iteration 32781 Loss: 0.9050074219703674\n",
      "Iteration 32782 Loss: 0.9973605275154114\n",
      "Iteration 32783 Loss: 1.1910126209259033\n",
      "Iteration 32784 Loss: 0.6550433039665222\n",
      "Iteration 32785 Loss: 1.179253101348877\n",
      "Iteration 32786 Loss: 0.9254428744316101\n",
      "Iteration 32787 Loss: 1.1940213441848755\n",
      "Iteration 32788 Loss: 1.1601409912109375\n",
      "Iteration 32789 Loss: 0.9846900105476379\n",
      "Iteration 32789 Loss: 0.9999284744262695\n",
      "Iteration 32790 Loss: 0.8132317662239075\n",
      "Iteration 32791 Loss: 0.8087506890296936\n",
      "Iteration 32792 Loss: 1.2357155084609985\n",
      "Iteration 32793 Loss: 0.8880218267440796\n",
      "Iteration 32794 Loss: 1.151577115058899\n",
      "Iteration 32795 Loss: 1.1147183179855347\n",
      "Iteration 32796 Loss: 0.8558662533760071\n",
      "Iteration 32797 Loss: 0.9029262065887451\n",
      "Iteration 32798 Loss: 1.2788972854614258\n",
      "Iteration 32799 Loss: 1.092129111289978\n",
      "Iteration 32799 Loss: 1.0141834020614624\n",
      "Iteration 32800 Loss: 0.9128217697143555\n",
      "Iteration 32801 Loss: 0.9199762940406799\n",
      "Iteration 32802 Loss: 0.9146021008491516\n",
      "Iteration 32803 Loss: 0.8901758790016174\n",
      "Iteration 32804 Loss: 0.918651282787323\n",
      "Iteration 32805 Loss: 0.7762999534606934\n",
      "Iteration 32806 Loss: 0.9513334035873413\n",
      "Iteration 32807 Loss: 0.9081882238388062\n",
      "Iteration 32808 Loss: 0.9139963984489441\n",
      "Iteration 32809 Loss: 1.228939175605774\n",
      "Iteration 32809 Loss: 0.9334983825683594\n",
      "Iteration 32810 Loss: 0.866712749004364\n",
      "Iteration 32811 Loss: 1.0782593488693237\n",
      "Iteration 32812 Loss: 1.2494045495986938\n",
      "Iteration 32813 Loss: 0.8323108553886414\n",
      "Iteration 32814 Loss: 1.306922197341919\n",
      "Iteration 32815 Loss: 1.140491008758545\n",
      "Iteration 32816 Loss: 1.1816383600234985\n",
      "Iteration 32817 Loss: 1.2206196784973145\n",
      "Iteration 32818 Loss: 0.759448230266571\n",
      "Iteration 32819 Loss: 0.7071870565414429\n",
      "Iteration 32819 Loss: 1.0342992544174194\n",
      "Iteration 32820 Loss: 1.234622597694397\n",
      "Iteration 32821 Loss: 1.0029085874557495\n",
      "Iteration 32822 Loss: 1.0009793043136597\n",
      "Iteration 32823 Loss: 0.8441203236579895\n",
      "Iteration 32824 Loss: 0.7285099029541016\n",
      "Iteration 32825 Loss: 0.6777645349502563\n",
      "Iteration 32826 Loss: 0.8202744722366333\n",
      "Iteration 32827 Loss: 0.8267519474029541\n",
      "Iteration 32828 Loss: 1.236506700515747\n",
      "Iteration 32829 Loss: 0.878336489200592\n",
      "Iteration 32829 Loss: 0.9250775575637817\n",
      "Iteration 32830 Loss: 0.9274538159370422\n",
      "Iteration 32831 Loss: 1.0978177785873413\n",
      "Iteration 32832 Loss: 1.1631600856781006\n",
      "Iteration 32833 Loss: 1.4014861583709717\n",
      "Iteration 32834 Loss: 1.2428094148635864\n",
      "Iteration 32835 Loss: 1.0223830938339233\n",
      "Iteration 32836 Loss: 0.7193482518196106\n",
      "Iteration 32837 Loss: 1.3878313302993774\n",
      "Iteration 32838 Loss: 0.9210891723632812\n",
      "Iteration 32839 Loss: 0.9871691465377808\n",
      "Iteration 32839 Loss: 1.0870548486709595\n",
      "Iteration 32840 Loss: 0.9927874207496643\n",
      "Iteration 32841 Loss: 0.9534579515457153\n",
      "Iteration 32842 Loss: 0.5655525326728821\n",
      "Iteration 32843 Loss: 1.0636186599731445\n",
      "Iteration 32844 Loss: 1.3208215236663818\n",
      "Iteration 32845 Loss: 0.8944908976554871\n",
      "Iteration 32846 Loss: 1.0883686542510986\n",
      "Iteration 32847 Loss: 1.206239938735962\n",
      "Iteration 32848 Loss: 0.7849052548408508\n",
      "Iteration 32849 Loss: 1.0321274995803833\n",
      "Iteration 32849 Loss: 0.9902370572090149\n",
      "Iteration 32850 Loss: 0.8995563387870789\n",
      "Iteration 32851 Loss: 1.0097180604934692\n",
      "Iteration 32852 Loss: 1.0013670921325684\n",
      "Iteration 32853 Loss: 0.8577440977096558\n",
      "Iteration 32854 Loss: 1.0958081483840942\n",
      "Iteration 32855 Loss: 1.23335862159729\n",
      "Iteration 32856 Loss: 0.6996695399284363\n",
      "Iteration 32857 Loss: 1.1571980714797974\n",
      "Iteration 32858 Loss: 0.8562960624694824\n",
      "Iteration 32859 Loss: 0.7689114809036255\n",
      "Iteration 32859 Loss: 0.9579628109931946\n",
      "Iteration 32860 Loss: 0.962755024433136\n",
      "Iteration 32861 Loss: 1.2188308238983154\n",
      "Iteration 32862 Loss: 0.9343857765197754\n",
      "Iteration 32863 Loss: 1.1381492614746094\n",
      "Iteration 32864 Loss: 0.9055901169776917\n",
      "Iteration 32865 Loss: 0.9789159297943115\n",
      "Iteration 32866 Loss: 1.0904524326324463\n",
      "Iteration 32867 Loss: 1.129473328590393\n",
      "Iteration 32868 Loss: 1.0304793119430542\n",
      "Iteration 32869 Loss: 1.1136317253112793\n",
      "Iteration 32869 Loss: 1.0502663850784302\n",
      "Iteration 32870 Loss: 1.0243130922317505\n",
      "Iteration 32871 Loss: 0.973271906375885\n",
      "Iteration 32872 Loss: 1.1700985431671143\n",
      "Iteration 32873 Loss: 0.6096351146697998\n",
      "Iteration 32874 Loss: 1.241435170173645\n",
      "Iteration 32875 Loss: 1.1923067569732666\n",
      "Iteration 32876 Loss: 0.8821829557418823\n",
      "Iteration 32877 Loss: 0.8519365787506104\n",
      "Iteration 32878 Loss: 1.0583666563034058\n",
      "Iteration 32879 Loss: 1.2673990726470947\n",
      "Iteration 32879 Loss: 1.0270946025848389\n",
      "Iteration 32880 Loss: 1.0589673519134521\n",
      "Iteration 32881 Loss: 1.2754367589950562\n",
      "Iteration 32882 Loss: 1.1600505113601685\n",
      "Iteration 32883 Loss: 0.7706903219223022\n",
      "Iteration 32884 Loss: 1.0928330421447754\n",
      "Iteration 32885 Loss: 0.6911728382110596\n",
      "Iteration 32886 Loss: 1.0439516305923462\n",
      "Iteration 32887 Loss: 0.9997052550315857\n",
      "Iteration 32888 Loss: 0.7683866024017334\n",
      "Iteration 32889 Loss: 0.8702074289321899\n",
      "Iteration 32889 Loss: 0.9731402397155762\n",
      "Iteration 32890 Loss: 0.9954025149345398\n",
      "Iteration 32891 Loss: 0.9530493021011353\n",
      "Iteration 32892 Loss: 1.0302605628967285\n",
      "Iteration 32893 Loss: 0.4789428412914276\n",
      "Iteration 32894 Loss: 1.0751208066940308\n",
      "Iteration 32895 Loss: 0.9613171219825745\n",
      "Iteration 32896 Loss: 0.8669385313987732\n",
      "Iteration 32897 Loss: 0.9757838249206543\n",
      "Iteration 32898 Loss: 0.9718829989433289\n",
      "Iteration 32899 Loss: 0.7064109444618225\n",
      "Iteration 32899 Loss: 0.901511013507843\n",
      "Iteration 32900 Loss: 1.0247682332992554\n",
      "Iteration 32901 Loss: 0.9885017275810242\n",
      "Iteration 32902 Loss: 0.9128864407539368\n",
      "Iteration 32903 Loss: 0.7318267226219177\n",
      "Iteration 32904 Loss: 1.1402819156646729\n",
      "Iteration 32905 Loss: 0.8572604060173035\n",
      "Iteration 32906 Loss: 1.2270491123199463\n",
      "Iteration 32907 Loss: 0.7192944288253784\n",
      "Iteration 32908 Loss: 0.6745109558105469\n",
      "Iteration 32909 Loss: 0.8252964615821838\n",
      "Iteration 32909 Loss: 0.9101676940917969\n",
      "Iteration 32910 Loss: 0.9032495617866516\n",
      "Iteration 32911 Loss: 1.0774906873703003\n",
      "Iteration 32912 Loss: 0.882262647151947\n",
      "Iteration 32913 Loss: 0.8847702145576477\n",
      "Iteration 32914 Loss: 1.2585527896881104\n",
      "Iteration 32915 Loss: 1.3633126020431519\n",
      "Iteration 32916 Loss: 1.1437675952911377\n",
      "Iteration 32917 Loss: 1.1138036251068115\n",
      "Iteration 32918 Loss: 0.8804270625114441\n",
      "Iteration 32919 Loss: 1.2710081338882446\n",
      "Iteration 32919 Loss: 1.0778645277023315\n",
      "Iteration 32920 Loss: 0.7913610339164734\n",
      "Iteration 32921 Loss: 1.0596070289611816\n",
      "Iteration 32922 Loss: 1.0996677875518799\n",
      "Iteration 32923 Loss: 0.8394789695739746\n",
      "Iteration 32924 Loss: 1.2825766801834106\n",
      "Iteration 32925 Loss: 0.6843593120574951\n",
      "Iteration 32926 Loss: 1.1165902614593506\n",
      "Iteration 32927 Loss: 0.6469918489456177\n",
      "Iteration 32928 Loss: 0.894096314907074\n",
      "Iteration 32929 Loss: 1.3288617134094238\n",
      "Iteration 32929 Loss: 0.9743590354919434\n",
      "Iteration 32930 Loss: 1.3434594869613647\n",
      "Iteration 32931 Loss: 1.0078046321868896\n",
      "Iteration 32932 Loss: 0.7535823583602905\n",
      "Iteration 32933 Loss: 0.852542519569397\n",
      "Iteration 32934 Loss: 0.6643608808517456\n",
      "Iteration 32935 Loss: 0.812835693359375\n",
      "Iteration 32936 Loss: 0.9062296748161316\n",
      "Iteration 32937 Loss: 1.2820653915405273\n",
      "Iteration 32938 Loss: 0.8794167041778564\n",
      "Iteration 32939 Loss: 1.1389689445495605\n",
      "Iteration 32939 Loss: 0.964126706123352\n",
      "Iteration 32940 Loss: 0.9793417453765869\n",
      "Iteration 32941 Loss: 1.2644741535186768\n",
      "Iteration 32942 Loss: 1.0851513147354126\n",
      "Iteration 32943 Loss: 1.0608084201812744\n",
      "Iteration 32944 Loss: 1.046136498451233\n",
      "Iteration 32945 Loss: 0.9549090266227722\n",
      "Iteration 32946 Loss: 0.9217208027839661\n",
      "Iteration 32947 Loss: 0.9371442198753357\n",
      "Iteration 32948 Loss: 0.9448584318161011\n",
      "Iteration 32949 Loss: 0.8376721143722534\n",
      "Iteration 32949 Loss: 1.0032216310501099\n",
      "Iteration 32950 Loss: 1.2001487016677856\n",
      "Iteration 32951 Loss: 0.9639745354652405\n",
      "Iteration 32952 Loss: 1.0545333623886108\n",
      "Iteration 32953 Loss: 1.3435109853744507\n",
      "Iteration 32954 Loss: 1.19948148727417\n",
      "Iteration 32955 Loss: 1.3090031147003174\n",
      "Iteration 32956 Loss: 1.39325749874115\n",
      "Iteration 32957 Loss: 0.8099932074546814\n",
      "Iteration 32958 Loss: 0.937171995639801\n",
      "Iteration 32959 Loss: 1.1344072818756104\n",
      "Iteration 32959 Loss: 1.1345481872558594\n",
      "Iteration 32960 Loss: 0.8179330229759216\n",
      "Iteration 32961 Loss: 0.8828966021537781\n",
      "Iteration 32962 Loss: 1.2410670518875122\n",
      "Iteration 32963 Loss: 1.0121232271194458\n",
      "Iteration 32964 Loss: 1.0495197772979736\n",
      "Iteration 32965 Loss: 0.9691073298454285\n",
      "Iteration 32966 Loss: 1.095698356628418\n",
      "Iteration 32967 Loss: 0.9793868064880371\n",
      "Iteration 32968 Loss: 0.9694994688034058\n",
      "Iteration 32969 Loss: 1.1178245544433594\n",
      "Iteration 32969 Loss: 1.0135056972503662\n",
      "Iteration 32970 Loss: 1.1405906677246094\n",
      "Iteration 32971 Loss: 1.2000623941421509\n",
      "Iteration 32972 Loss: 0.6137687563896179\n",
      "Iteration 32973 Loss: 0.6648192405700684\n",
      "Iteration 32974 Loss: 1.0705379247665405\n",
      "Iteration 32975 Loss: 1.0233248472213745\n",
      "Iteration 32976 Loss: 0.8967587947845459\n",
      "Iteration 32977 Loss: 1.307788610458374\n",
      "Iteration 32978 Loss: 1.0324828624725342\n",
      "Iteration 32979 Loss: 1.2932604551315308\n",
      "Iteration 32979 Loss: 1.0243394374847412\n",
      "Iteration 32980 Loss: 0.6758766174316406\n",
      "Iteration 32981 Loss: 0.7189514636993408\n",
      "Iteration 32982 Loss: 0.9950088858604431\n",
      "Iteration 32983 Loss: 1.1112053394317627\n",
      "Iteration 32984 Loss: 0.7768973708152771\n",
      "Iteration 32985 Loss: 0.7866232991218567\n",
      "Iteration 32986 Loss: 0.7544553279876709\n",
      "Iteration 32987 Loss: 1.342091679573059\n",
      "Iteration 32988 Loss: 1.048600435256958\n",
      "Iteration 32989 Loss: 1.1239635944366455\n",
      "Iteration 32989 Loss: 0.9333673715591431\n",
      "Iteration 32990 Loss: 0.8724664449691772\n",
      "Iteration 32991 Loss: 0.9876957535743713\n",
      "Iteration 32992 Loss: 0.72963547706604\n",
      "Iteration 32993 Loss: 1.2470508813858032\n",
      "Iteration 32994 Loss: 1.019237995147705\n",
      "Iteration 32995 Loss: 0.9934794306755066\n",
      "Iteration 32996 Loss: 0.9087849259376526\n",
      "Iteration 32997 Loss: 0.996234655380249\n",
      "Iteration 32998 Loss: 0.9150654077529907\n",
      "Iteration 32999 Loss: 1.050554633140564\n",
      "Iteration 32999 Loss: 0.9720206260681152\n",
      "Iteration 33000 Loss: 0.9734882712364197\n",
      "Iteration 33001 Loss: 1.045902967453003\n",
      "Iteration 33002 Loss: 1.0968923568725586\n",
      "Iteration 33003 Loss: 1.2070821523666382\n",
      "Iteration 33004 Loss: 1.101347804069519\n",
      "Iteration 33005 Loss: 1.311996340751648\n",
      "Iteration 33006 Loss: 1.144703984260559\n",
      "Iteration 33007 Loss: 0.8329350352287292\n",
      "Iteration 33008 Loss: 1.2420623302459717\n",
      "Iteration 33009 Loss: 1.180783748626709\n",
      "Iteration 33009 Loss: 1.1137195825576782\n",
      "Iteration 33010 Loss: 1.1200804710388184\n",
      "Iteration 33011 Loss: 0.9025586843490601\n",
      "Iteration 33012 Loss: 0.8782874941825867\n",
      "Iteration 33013 Loss: 1.0280745029449463\n",
      "Iteration 33014 Loss: 0.8318741321563721\n",
      "Iteration 33015 Loss: 1.351173758506775\n",
      "Iteration 33016 Loss: 0.7567004561424255\n",
      "Iteration 33017 Loss: 0.9818598031997681\n",
      "Iteration 33018 Loss: 1.0330274105072021\n",
      "Iteration 33019 Loss: 1.0226991176605225\n",
      "Iteration 33019 Loss: 0.9906336069107056\n",
      "Iteration 33020 Loss: 0.8926581144332886\n",
      "Iteration 33021 Loss: 0.9838201999664307\n",
      "Iteration 33022 Loss: 1.1322979927062988\n",
      "Iteration 33023 Loss: 0.8507468104362488\n",
      "Iteration 33024 Loss: 1.0253469944000244\n",
      "Iteration 33025 Loss: 1.021114706993103\n",
      "Iteration 33026 Loss: 0.9776970148086548\n",
      "Iteration 33027 Loss: 0.9016684889793396\n",
      "Iteration 33028 Loss: 1.0875805616378784\n",
      "Iteration 33029 Loss: 0.7302131652832031\n",
      "Iteration 33029 Loss: 0.9603143930435181\n",
      "Iteration 33030 Loss: 0.9461353421211243\n",
      "Iteration 33031 Loss: 0.9288987517356873\n",
      "Iteration 33032 Loss: 0.9534447193145752\n",
      "Iteration 33033 Loss: 1.5191078186035156\n",
      "Iteration 33034 Loss: 1.1264148950576782\n",
      "Iteration 33035 Loss: 1.0879908800125122\n",
      "Iteration 33036 Loss: 0.8665785789489746\n",
      "Iteration 33037 Loss: 1.0154342651367188\n",
      "Iteration 33038 Loss: 0.7091386914253235\n",
      "Iteration 33039 Loss: 1.2516032457351685\n",
      "Iteration 33039 Loss: 1.0404746532440186\n",
      "Iteration 33040 Loss: 0.8651901483535767\n",
      "Iteration 33041 Loss: 0.7567795515060425\n",
      "Iteration 33042 Loss: 1.1775460243225098\n",
      "Iteration 33043 Loss: 0.9234007596969604\n",
      "Iteration 33044 Loss: 0.8338865637779236\n",
      "Iteration 33045 Loss: 0.9460381865501404\n",
      "Iteration 33046 Loss: 1.3659274578094482\n",
      "Iteration 33047 Loss: 1.0579317808151245\n",
      "Iteration 33048 Loss: 0.9191931486129761\n",
      "Iteration 33049 Loss: 1.2568200826644897\n",
      "Iteration 33049 Loss: 1.010271430015564\n",
      "Iteration 33050 Loss: 1.0005391836166382\n",
      "Iteration 33051 Loss: 0.7366413474082947\n",
      "Iteration 33052 Loss: 0.9692533612251282\n",
      "Iteration 33053 Loss: 1.0339018106460571\n",
      "Iteration 33054 Loss: 0.988430380821228\n",
      "Iteration 33055 Loss: 1.014296054840088\n",
      "Iteration 33056 Loss: 1.3047133684158325\n",
      "Iteration 33057 Loss: 1.1553657054901123\n",
      "Iteration 33058 Loss: 1.0529125928878784\n",
      "Iteration 33059 Loss: 1.0955384969711304\n",
      "Iteration 33059 Loss: 1.0351593494415283\n",
      "Iteration 33060 Loss: 0.8340657353401184\n",
      "Iteration 33061 Loss: 1.0130280256271362\n",
      "Iteration 33062 Loss: 1.1951653957366943\n",
      "Iteration 33063 Loss: 1.265073537826538\n",
      "Iteration 33064 Loss: 0.7259515523910522\n",
      "Iteration 33065 Loss: 0.7351640462875366\n",
      "Iteration 33066 Loss: 1.189586877822876\n",
      "Iteration 33067 Loss: 0.8794851899147034\n",
      "Iteration 33068 Loss: 1.1882129907608032\n",
      "Iteration 33069 Loss: 0.778266966342926\n",
      "Iteration 33069 Loss: 0.9804000854492188\n",
      "Iteration 33070 Loss: 1.0442842245101929\n",
      "Iteration 33071 Loss: 1.3422538042068481\n",
      "Iteration 33072 Loss: 0.49049845337867737\n",
      "Iteration 33073 Loss: 1.0370433330535889\n",
      "Iteration 33074 Loss: 0.807020366191864\n",
      "Iteration 33075 Loss: 1.4815372228622437\n",
      "Iteration 33076 Loss: 1.1779289245605469\n",
      "Iteration 33077 Loss: 1.1102216243743896\n",
      "Iteration 33078 Loss: 1.005180835723877\n",
      "Iteration 33079 Loss: 1.1548893451690674\n",
      "Iteration 33079 Loss: 1.065085768699646\n",
      "Iteration 33080 Loss: 0.6915637254714966\n",
      "Iteration 33081 Loss: 1.1241570711135864\n",
      "Iteration 33082 Loss: 0.9778229594230652\n",
      "Iteration 33083 Loss: 0.9738695025444031\n",
      "Iteration 33084 Loss: 1.3865660429000854\n",
      "Iteration 33085 Loss: 1.14932119846344\n",
      "Iteration 33086 Loss: 1.2475252151489258\n",
      "Iteration 33087 Loss: 0.884987473487854\n",
      "Iteration 33088 Loss: 0.7729185819625854\n",
      "Iteration 33089 Loss: 0.9343937039375305\n",
      "Iteration 33089 Loss: 1.014312505722046\n",
      "Iteration 33090 Loss: 0.7426705360412598\n",
      "Iteration 33091 Loss: 1.0245088338851929\n",
      "Iteration 33092 Loss: 0.8586793541908264\n",
      "Iteration 33093 Loss: 0.799181342124939\n",
      "Iteration 33094 Loss: 1.4784337282180786\n",
      "Iteration 33095 Loss: 0.9836955070495605\n",
      "Iteration 33096 Loss: 0.8550189137458801\n",
      "Iteration 33097 Loss: 1.0242832899093628\n",
      "Iteration 33098 Loss: 1.0360769033432007\n",
      "Iteration 33099 Loss: 1.108726978302002\n",
      "Iteration 33099 Loss: 0.9911274909973145\n",
      "Iteration 33100 Loss: 1.160928726196289\n",
      "Iteration 33101 Loss: 0.6888295412063599\n",
      "Iteration 33102 Loss: 0.8112087249755859\n",
      "Iteration 33103 Loss: 1.1413311958312988\n",
      "Iteration 33104 Loss: 0.974480390548706\n",
      "Iteration 33105 Loss: 1.3274174928665161\n",
      "Iteration 33106 Loss: 0.9713315367698669\n",
      "Iteration 33107 Loss: 0.7334306240081787\n",
      "Iteration 33108 Loss: 1.243233561515808\n",
      "Iteration 33109 Loss: 1.1843706369400024\n",
      "Iteration 33109 Loss: 1.0236562490463257\n",
      "Iteration 33110 Loss: 0.9982679486274719\n",
      "Iteration 33111 Loss: 1.1288275718688965\n",
      "Iteration 33112 Loss: 0.9826703667640686\n",
      "Iteration 33113 Loss: 0.8708693385124207\n",
      "Iteration 33114 Loss: 1.0296496152877808\n",
      "Iteration 33115 Loss: 0.9741516709327698\n",
      "Iteration 33116 Loss: 0.8187016844749451\n",
      "Iteration 33117 Loss: 1.0601909160614014\n",
      "Iteration 33118 Loss: 1.2120304107666016\n",
      "Iteration 33119 Loss: 1.176120638847351\n",
      "Iteration 33119 Loss: 1.0251480340957642\n",
      "Iteration 33120 Loss: 1.0659520626068115\n",
      "Iteration 33121 Loss: 0.8115473985671997\n",
      "Iteration 33122 Loss: 1.0683104991912842\n",
      "Iteration 33123 Loss: 0.6704790592193604\n",
      "Iteration 33124 Loss: 0.9202152490615845\n",
      "Iteration 33125 Loss: 0.7445178031921387\n",
      "Iteration 33126 Loss: 0.8880751132965088\n",
      "Iteration 33127 Loss: 1.16948664188385\n",
      "Iteration 33128 Loss: 1.3106952905654907\n",
      "Iteration 33129 Loss: 0.982425332069397\n",
      "Iteration 33129 Loss: 0.9631704092025757\n",
      "Iteration 33130 Loss: 0.9000275135040283\n",
      "Iteration 33131 Loss: 1.0857207775115967\n",
      "Iteration 33132 Loss: 0.8511114120483398\n",
      "Iteration 33133 Loss: 0.5906259417533875\n",
      "Iteration 33134 Loss: 1.1418485641479492\n",
      "Iteration 33135 Loss: 0.6273804903030396\n",
      "Iteration 33136 Loss: 1.0963563919067383\n",
      "Iteration 33137 Loss: 0.9599004983901978\n",
      "Iteration 33138 Loss: 1.0475518703460693\n",
      "Iteration 33139 Loss: 1.2215814590454102\n",
      "Iteration 33139 Loss: 0.952210545539856\n",
      "Iteration 33140 Loss: 1.2399557828903198\n",
      "Iteration 33141 Loss: 0.47963806986808777\n",
      "Iteration 33142 Loss: 1.1219558715820312\n",
      "Iteration 33143 Loss: 1.2708029747009277\n",
      "Iteration 33144 Loss: 1.0053902864456177\n",
      "Iteration 33145 Loss: 0.6963678002357483\n",
      "Iteration 33146 Loss: 0.8780125975608826\n",
      "Iteration 33147 Loss: 1.03048574924469\n",
      "Iteration 33148 Loss: 1.330536961555481\n",
      "Iteration 33149 Loss: 0.8645305633544922\n",
      "Iteration 33149 Loss: 0.9917677044868469\n",
      "Iteration 33150 Loss: 0.6292075514793396\n",
      "Iteration 33151 Loss: 1.2414511442184448\n",
      "Iteration 33152 Loss: 0.9616318345069885\n",
      "Iteration 33153 Loss: 0.7659689784049988\n",
      "Iteration 33154 Loss: 0.7865166068077087\n",
      "Iteration 33155 Loss: 0.783216655254364\n",
      "Iteration 33156 Loss: 1.180234670639038\n",
      "Iteration 33157 Loss: 1.2305433750152588\n",
      "Iteration 33158 Loss: 0.7489645481109619\n",
      "Iteration 33159 Loss: 0.9787719249725342\n",
      "Iteration 33159 Loss: 0.9306507110595703\n",
      "Iteration 33160 Loss: 1.344481348991394\n",
      "Iteration 33161 Loss: 0.9351161122322083\n",
      "Iteration 33162 Loss: 0.948557436466217\n",
      "Iteration 33163 Loss: 0.8901914954185486\n",
      "Iteration 33164 Loss: 0.5585182309150696\n",
      "Iteration 33165 Loss: 0.6459406614303589\n",
      "Iteration 33166 Loss: 0.823409378528595\n",
      "Iteration 33167 Loss: 1.0410276651382446\n",
      "Iteration 33168 Loss: 0.9597629904747009\n",
      "Iteration 33169 Loss: 1.2010775804519653\n",
      "Iteration 33169 Loss: 0.9348083734512329\n",
      "Iteration 33170 Loss: 0.9512478113174438\n",
      "Iteration 33171 Loss: 1.057058572769165\n",
      "Iteration 33172 Loss: 0.8677131533622742\n",
      "Iteration 33173 Loss: 1.2771233320236206\n",
      "Iteration 33174 Loss: 1.091433048248291\n",
      "Iteration 33175 Loss: 1.1251065731048584\n",
      "Iteration 33176 Loss: 0.9849500060081482\n",
      "Iteration 33177 Loss: 1.2689603567123413\n",
      "Iteration 33178 Loss: 0.8163505792617798\n",
      "Iteration 33179 Loss: 0.8970350623130798\n",
      "Iteration 33179 Loss: 1.0336978435516357\n",
      "Iteration 33180 Loss: 1.024277687072754\n",
      "Iteration 33181 Loss: 1.084663987159729\n",
      "Iteration 33182 Loss: 1.1028436422348022\n",
      "Iteration 33183 Loss: 1.0236997604370117\n",
      "Iteration 33184 Loss: 0.9121652245521545\n",
      "Iteration 33185 Loss: 0.9478544592857361\n",
      "Iteration 33186 Loss: 1.1301189661026\n",
      "Iteration 33187 Loss: 1.0609883069992065\n",
      "Iteration 33188 Loss: 0.7101457715034485\n",
      "Iteration 33189 Loss: 1.0221894979476929\n",
      "Iteration 33189 Loss: 1.0018947124481201\n",
      "Iteration 33190 Loss: 0.9862592220306396\n",
      "Iteration 33191 Loss: 1.2592865228652954\n",
      "Iteration 33192 Loss: 0.8896374702453613\n",
      "Iteration 33193 Loss: 0.8342909216880798\n",
      "Iteration 33194 Loss: 0.5473431348800659\n",
      "Iteration 33195 Loss: 1.1122783422470093\n",
      "Iteration 33196 Loss: 0.8340243101119995\n",
      "Iteration 33197 Loss: 0.9615422487258911\n",
      "Iteration 33198 Loss: 1.040554165840149\n",
      "Iteration 33199 Loss: 1.0038657188415527\n",
      "Iteration 33199 Loss: 0.946908175945282\n",
      "Iteration 33200 Loss: 1.3005824089050293\n",
      "Iteration 33201 Loss: 1.25277841091156\n",
      "Iteration 33202 Loss: 1.160987377166748\n",
      "Iteration 33203 Loss: 1.2350695133209229\n",
      "Iteration 33204 Loss: 1.1066994667053223\n",
      "Iteration 33205 Loss: 0.9806057810783386\n",
      "Iteration 33206 Loss: 1.2448649406433105\n",
      "Iteration 33207 Loss: 1.143993616104126\n",
      "Iteration 33208 Loss: 0.9791073799133301\n",
      "Iteration 33209 Loss: 1.0558066368103027\n",
      "Iteration 33209 Loss: 1.1460497379302979\n",
      "Iteration 33210 Loss: 1.2881197929382324\n",
      "Iteration 33211 Loss: 0.8466647863388062\n",
      "Iteration 33212 Loss: 0.8383068442344666\n",
      "Iteration 33213 Loss: 1.315527081489563\n",
      "Iteration 33214 Loss: 1.1742777824401855\n",
      "Iteration 33215 Loss: 1.217361330986023\n",
      "Iteration 33216 Loss: 1.1272854804992676\n",
      "Iteration 33217 Loss: 1.0141397714614868\n",
      "Iteration 33218 Loss: 1.1138437986373901\n",
      "Iteration 33219 Loss: 1.0169988870620728\n",
      "Iteration 33219 Loss: 1.095252513885498\n",
      "Iteration 33220 Loss: 1.2932515144348145\n",
      "Iteration 33221 Loss: 1.1903454065322876\n",
      "Iteration 33222 Loss: 1.362794041633606\n",
      "Iteration 33223 Loss: 0.8188141584396362\n",
      "Iteration 33224 Loss: 1.0793174505233765\n",
      "Iteration 33225 Loss: 0.9141736030578613\n",
      "Iteration 33226 Loss: 0.8974878787994385\n",
      "Iteration 33227 Loss: 1.2302367687225342\n",
      "Iteration 33228 Loss: 0.9405154585838318\n",
      "Iteration 33229 Loss: 1.1378705501556396\n",
      "Iteration 33229 Loss: 1.086480736732483\n",
      "Iteration 33230 Loss: 0.701034426689148\n",
      "Iteration 33231 Loss: 0.773794949054718\n",
      "Iteration 33232 Loss: 1.0464674234390259\n",
      "Iteration 33233 Loss: 1.302173137664795\n",
      "Iteration 33234 Loss: 0.8816930055618286\n",
      "Iteration 33235 Loss: 1.0914362668991089\n",
      "Iteration 33236 Loss: 1.2569093704223633\n",
      "Iteration 33237 Loss: 1.068472981452942\n",
      "Iteration 33238 Loss: 0.6344119310379028\n",
      "Iteration 33239 Loss: 1.0910669565200806\n",
      "Iteration 33239 Loss: 0.9847460985183716\n",
      "Iteration 33240 Loss: 0.734776496887207\n",
      "Iteration 33241 Loss: 1.1389771699905396\n",
      "Iteration 33242 Loss: 0.9712166786193848\n",
      "Iteration 33243 Loss: 1.0896506309509277\n",
      "Iteration 33244 Loss: 0.9811593890190125\n",
      "Iteration 33245 Loss: 1.0543849468231201\n",
      "Iteration 33246 Loss: 1.462099313735962\n",
      "Iteration 33247 Loss: 1.1802266836166382\n",
      "Iteration 33248 Loss: 0.6385904550552368\n",
      "Iteration 33249 Loss: 1.1415820121765137\n",
      "Iteration 33249 Loss: 1.0392663478851318\n",
      "Iteration 33250 Loss: 0.6599302887916565\n",
      "Iteration 33251 Loss: 0.8251469731330872\n",
      "Iteration 33252 Loss: 1.2924240827560425\n",
      "Iteration 33253 Loss: 1.1304060220718384\n",
      "Iteration 33254 Loss: 0.8129159808158875\n",
      "Iteration 33255 Loss: 0.8001174926757812\n",
      "Iteration 33256 Loss: 0.8648870587348938\n",
      "Iteration 33257 Loss: 1.3246175050735474\n",
      "Iteration 33258 Loss: 1.1440600156784058\n",
      "Iteration 33259 Loss: 0.9843544363975525\n",
      "Iteration 33259 Loss: 0.9838859438896179\n",
      "Iteration 33260 Loss: 0.8927712440490723\n",
      "Iteration 33261 Loss: 1.1151891946792603\n",
      "Iteration 33262 Loss: 0.6405102014541626\n",
      "Iteration 33263 Loss: 0.9792922139167786\n",
      "Iteration 33264 Loss: 0.9651645421981812\n",
      "Iteration 33265 Loss: 0.8790357708930969\n",
      "Iteration 33266 Loss: 1.1905769109725952\n",
      "Iteration 33267 Loss: 0.8750483393669128\n",
      "Iteration 33268 Loss: 0.9707632660865784\n",
      "Iteration 33269 Loss: 1.1694926023483276\n",
      "Iteration 33269 Loss: 0.9677845239639282\n",
      "Iteration 33270 Loss: 1.0116713047027588\n",
      "Iteration 33271 Loss: 1.0045268535614014\n",
      "Iteration 33272 Loss: 1.0332204103469849\n",
      "Iteration 33273 Loss: 0.44493359327316284\n",
      "Iteration 33274 Loss: 1.2122997045516968\n",
      "Iteration 33275 Loss: 0.9040272831916809\n",
      "Iteration 33276 Loss: 0.8961349725723267\n",
      "Iteration 33277 Loss: 1.0678468942642212\n",
      "Iteration 33278 Loss: 1.141950249671936\n",
      "Iteration 33279 Loss: 0.9132429957389832\n",
      "Iteration 33279 Loss: 0.9629853963851929\n",
      "Iteration 33280 Loss: 1.1558845043182373\n",
      "Iteration 33281 Loss: 1.2139902114868164\n",
      "Iteration 33282 Loss: 0.889800488948822\n",
      "Iteration 33283 Loss: 1.4456137418746948\n",
      "Iteration 33284 Loss: 1.2017961740493774\n",
      "Iteration 33285 Loss: 1.136210560798645\n",
      "Iteration 33286 Loss: 0.5991905331611633\n",
      "Iteration 33287 Loss: 1.0164884328842163\n",
      "Iteration 33288 Loss: 1.2634873390197754\n",
      "Iteration 33289 Loss: 0.9778767228126526\n",
      "Iteration 33289 Loss: 1.090033769607544\n",
      "Iteration 33290 Loss: 1.1255745887756348\n",
      "Iteration 33291 Loss: 0.8755673766136169\n",
      "Iteration 33292 Loss: 0.8044580817222595\n",
      "Iteration 33293 Loss: 1.1581422090530396\n",
      "Iteration 33294 Loss: 1.1746478080749512\n",
      "Iteration 33295 Loss: 0.9576338529586792\n",
      "Iteration 33296 Loss: 1.161329984664917\n",
      "Iteration 33297 Loss: 0.8876869678497314\n",
      "Iteration 33298 Loss: 1.3132333755493164\n",
      "Iteration 33299 Loss: 0.7816588878631592\n",
      "Iteration 33299 Loss: 1.0239932537078857\n",
      "Iteration 33300 Loss: 0.6824777126312256\n",
      "Iteration 33301 Loss: 0.6885144114494324\n",
      "Iteration 33302 Loss: 1.0427544116973877\n",
      "Iteration 33303 Loss: 0.9956371188163757\n",
      "Iteration 33304 Loss: 0.7987688779830933\n",
      "Iteration 33305 Loss: 1.2305148839950562\n",
      "Iteration 33306 Loss: 1.3330398797988892\n",
      "Iteration 33307 Loss: 1.2437679767608643\n",
      "Iteration 33308 Loss: 1.0757243633270264\n",
      "Iteration 33309 Loss: 0.9322318434715271\n",
      "Iteration 33309 Loss: 1.0023431777954102\n",
      "Iteration 33310 Loss: 0.8624753952026367\n",
      "Iteration 33311 Loss: 1.0352144241333008\n",
      "Iteration 33312 Loss: 1.0197381973266602\n",
      "Iteration 33313 Loss: 1.0102797746658325\n",
      "Iteration 33314 Loss: 1.149069905281067\n",
      "Iteration 33315 Loss: 1.0203875303268433\n",
      "Iteration 33316 Loss: 0.9392512440681458\n",
      "Iteration 33317 Loss: 1.0257376432418823\n",
      "Iteration 33318 Loss: 0.9504656791687012\n",
      "Iteration 33319 Loss: 1.3211485147476196\n",
      "Iteration 33319 Loss: 1.0333768129348755\n",
      "Iteration 33320 Loss: 1.381460428237915\n",
      "Iteration 33321 Loss: 1.1703572273254395\n",
      "Iteration 33322 Loss: 1.4130860567092896\n",
      "Iteration 33323 Loss: 0.7783817648887634\n",
      "Iteration 33324 Loss: 0.8670575022697449\n",
      "Iteration 33325 Loss: 1.3194218873977661\n",
      "Iteration 33326 Loss: 0.9156128168106079\n",
      "Iteration 33327 Loss: 1.2070504426956177\n",
      "Iteration 33328 Loss: 1.8416036367416382\n",
      "Iteration 33329 Loss: 0.7530524134635925\n",
      "Iteration 33329 Loss: 1.1647083759307861\n",
      "Iteration 33330 Loss: 0.7891571521759033\n",
      "Iteration 33331 Loss: 1.0578454732894897\n",
      "Iteration 33332 Loss: 0.8795590400695801\n",
      "Iteration 33333 Loss: 1.1930471658706665\n",
      "Iteration 33334 Loss: 1.2237597703933716\n",
      "Iteration 33335 Loss: 0.9413651823997498\n",
      "Iteration 33336 Loss: 1.2222284078598022\n",
      "Iteration 33337 Loss: 1.1563273668289185\n",
      "Iteration 33338 Loss: 0.8880725502967834\n",
      "Iteration 33339 Loss: 1.120348334312439\n",
      "Iteration 33339 Loss: 1.047170877456665\n",
      "Iteration 33340 Loss: 0.9463374018669128\n",
      "Iteration 33341 Loss: 0.9352177381515503\n",
      "Iteration 33342 Loss: 1.063230276107788\n",
      "Iteration 33343 Loss: 1.1081278324127197\n",
      "Iteration 33344 Loss: 0.9324496984481812\n",
      "Iteration 33345 Loss: 1.0321046113967896\n",
      "Iteration 33346 Loss: 0.9536727070808411\n",
      "Iteration 33347 Loss: 0.8265060782432556\n",
      "Iteration 33348 Loss: 1.275381326675415\n",
      "Iteration 33349 Loss: 1.3098939657211304\n",
      "Iteration 33349 Loss: 1.0382921695709229\n",
      "Iteration 33350 Loss: 1.1106568574905396\n",
      "Iteration 33351 Loss: 0.7477574944496155\n",
      "Iteration 33352 Loss: 0.92824786901474\n",
      "Iteration 33353 Loss: 1.3124363422393799\n",
      "Iteration 33354 Loss: 0.7481635212898254\n",
      "Iteration 33355 Loss: 0.8754095435142517\n",
      "Iteration 33356 Loss: 1.2669398784637451\n",
      "Iteration 33357 Loss: 0.8874160051345825\n",
      "Iteration 33358 Loss: 0.9594234228134155\n",
      "Iteration 33359 Loss: 1.0201159715652466\n",
      "Iteration 33359 Loss: 0.98565673828125\n",
      "Iteration 33360 Loss: 1.2050310373306274\n",
      "Iteration 33361 Loss: 0.8686971068382263\n",
      "Iteration 33362 Loss: 0.9228667616844177\n",
      "Iteration 33363 Loss: 1.2063997983932495\n",
      "Iteration 33364 Loss: 1.2374459505081177\n",
      "Iteration 33365 Loss: 1.0525072813034058\n",
      "Iteration 33366 Loss: 0.9133915901184082\n",
      "Iteration 33367 Loss: 0.8342442512512207\n",
      "Iteration 33368 Loss: 1.1323915719985962\n",
      "Iteration 33369 Loss: 0.7997146248817444\n",
      "Iteration 33369 Loss: 1.0172688961029053\n",
      "Iteration 33370 Loss: 0.7847623229026794\n",
      "Iteration 33371 Loss: 0.945549726486206\n",
      "Iteration 33372 Loss: 0.9769033193588257\n",
      "Iteration 33373 Loss: 0.9166748523712158\n",
      "Iteration 33374 Loss: 1.1275733709335327\n",
      "Iteration 33375 Loss: 1.2227314710617065\n",
      "Iteration 33376 Loss: 0.8053739070892334\n",
      "Iteration 33377 Loss: 1.0892823934555054\n",
      "Iteration 33378 Loss: 1.0376654863357544\n",
      "Iteration 33379 Loss: 1.1915676593780518\n",
      "Iteration 33379 Loss: 1.0098084211349487\n",
      "Iteration 33380 Loss: 1.1656039953231812\n",
      "Iteration 33381 Loss: 1.0089384317398071\n",
      "Iteration 33382 Loss: 0.9658133387565613\n",
      "Iteration 33383 Loss: 0.9470309615135193\n",
      "Iteration 33384 Loss: 0.6940205693244934\n",
      "Iteration 33385 Loss: 1.3990164995193481\n",
      "Iteration 33386 Loss: 0.8935762047767639\n",
      "Iteration 33387 Loss: 1.207898497581482\n",
      "Iteration 33388 Loss: 1.1016160249710083\n",
      "Iteration 33389 Loss: 1.013864278793335\n",
      "Iteration 33389 Loss: 1.0397379398345947\n",
      "Iteration 33390 Loss: 1.2059437036514282\n",
      "Iteration 33391 Loss: 0.9812151193618774\n",
      "Iteration 33392 Loss: 1.189611792564392\n",
      "Iteration 33393 Loss: 0.9157297015190125\n",
      "Iteration 33394 Loss: 0.7403615713119507\n",
      "Iteration 33395 Loss: 1.5488684177398682\n",
      "Iteration 33396 Loss: 1.2883999347686768\n",
      "Iteration 33397 Loss: 1.0211400985717773\n",
      "Iteration 33398 Loss: 0.8096806406974792\n",
      "Iteration 33399 Loss: 1.128912091255188\n",
      "Iteration 33399 Loss: 1.0829862356185913\n",
      "Iteration 33400 Loss: 0.890153169631958\n",
      "Iteration 33401 Loss: 1.0421799421310425\n",
      "Iteration 33402 Loss: 1.193640112876892\n",
      "Iteration 33403 Loss: 0.8072829246520996\n",
      "Iteration 33404 Loss: 0.7286690473556519\n",
      "Iteration 33405 Loss: 1.2985625267028809\n",
      "Iteration 33406 Loss: 1.0225156545639038\n",
      "Iteration 33407 Loss: 0.680557906627655\n",
      "Iteration 33408 Loss: 1.0481992959976196\n",
      "Iteration 33409 Loss: 0.8539882898330688\n",
      "Iteration 33409 Loss: 0.9565749168395996\n",
      "Iteration 33410 Loss: 1.2468702793121338\n",
      "Iteration 33411 Loss: 1.2446672916412354\n",
      "Iteration 33412 Loss: 1.0076011419296265\n",
      "Iteration 33413 Loss: 0.8202553987503052\n",
      "Iteration 33414 Loss: 1.0719672441482544\n",
      "Iteration 33415 Loss: 0.8762633800506592\n",
      "Iteration 33416 Loss: 0.9684930443763733\n",
      "Iteration 33417 Loss: 1.1843940019607544\n",
      "Iteration 33418 Loss: 0.6177908778190613\n",
      "Iteration 33419 Loss: 0.6964176297187805\n",
      "Iteration 33419 Loss: 0.973471999168396\n",
      "Iteration 33420 Loss: 0.8549566268920898\n",
      "Iteration 33421 Loss: 1.2069910764694214\n",
      "Iteration 33422 Loss: 1.3103458881378174\n",
      "Iteration 33423 Loss: 1.005079746246338\n",
      "Iteration 33424 Loss: 1.1338889598846436\n",
      "Iteration 33425 Loss: 1.000368595123291\n",
      "Iteration 33426 Loss: 0.9454078674316406\n",
      "Iteration 33427 Loss: 1.1030628681182861\n",
      "Iteration 33428 Loss: 1.282065987586975\n",
      "Iteration 33429 Loss: 0.8570938110351562\n",
      "Iteration 33429 Loss: 1.069926142692566\n",
      "Iteration 33430 Loss: 1.0897760391235352\n",
      "Iteration 33431 Loss: 0.7159989476203918\n",
      "Iteration 33432 Loss: 0.9951068162918091\n",
      "Iteration 33433 Loss: 1.0333702564239502\n",
      "Iteration 33434 Loss: 1.1065884828567505\n",
      "Iteration 33435 Loss: 1.1084312200546265\n",
      "Iteration 33436 Loss: 1.344601035118103\n",
      "Iteration 33437 Loss: 0.758405327796936\n",
      "Iteration 33438 Loss: 1.0608726739883423\n",
      "Iteration 33439 Loss: 0.7026348114013672\n",
      "Iteration 33439 Loss: 0.9915785789489746\n",
      "Iteration 33440 Loss: 1.4733654260635376\n",
      "Iteration 33441 Loss: 1.2493051290512085\n",
      "Iteration 33442 Loss: 0.7644084692001343\n",
      "Iteration 33443 Loss: 0.7507588863372803\n",
      "Iteration 33444 Loss: 0.9069700241088867\n",
      "Iteration 33445 Loss: 1.1504839658737183\n",
      "Iteration 33446 Loss: 1.0391180515289307\n",
      "Iteration 33447 Loss: 1.043799638748169\n",
      "Iteration 33448 Loss: 0.9465416073799133\n",
      "Iteration 33449 Loss: 1.034201741218567\n",
      "Iteration 33449 Loss: 1.0358952283859253\n",
      "Iteration 33450 Loss: 1.1179287433624268\n",
      "Iteration 33451 Loss: 0.8557113409042358\n",
      "Iteration 33452 Loss: 1.0871623754501343\n",
      "Iteration 33453 Loss: 1.2591551542282104\n",
      "Iteration 33454 Loss: 0.9273743629455566\n",
      "Iteration 33455 Loss: 1.0651732683181763\n",
      "Iteration 33456 Loss: 1.1424719095230103\n",
      "Iteration 33457 Loss: 1.2136180400848389\n",
      "Iteration 33458 Loss: 0.8733174204826355\n",
      "Iteration 33459 Loss: 1.0515741109848022\n",
      "Iteration 33459 Loss: 1.0593488216400146\n",
      "Iteration 33460 Loss: 0.9419485926628113\n",
      "Iteration 33461 Loss: 1.0328879356384277\n",
      "Iteration 33462 Loss: 1.187548041343689\n",
      "Iteration 33463 Loss: 1.3320081233978271\n",
      "Iteration 33464 Loss: 1.409222960472107\n",
      "Iteration 33465 Loss: 1.2686396837234497\n",
      "Iteration 33466 Loss: 1.222511887550354\n",
      "Iteration 33467 Loss: 0.9220610857009888\n",
      "Iteration 33468 Loss: 0.6359429955482483\n",
      "Iteration 33469 Loss: 0.8287426233291626\n",
      "Iteration 33469 Loss: 1.0781514644622803\n",
      "Iteration 33470 Loss: 1.0975770950317383\n",
      "Iteration 33471 Loss: 0.9756710529327393\n",
      "Iteration 33472 Loss: 1.4070228338241577\n",
      "Iteration 33473 Loss: 1.0549567937850952\n",
      "Iteration 33474 Loss: 0.9035750031471252\n",
      "Iteration 33475 Loss: 1.1526085138320923\n",
      "Iteration 33476 Loss: 0.884462296962738\n",
      "Iteration 33477 Loss: 0.6639740467071533\n",
      "Iteration 33478 Loss: 0.978399932384491\n",
      "Iteration 33479 Loss: 0.890590250492096\n",
      "Iteration 33479 Loss: 1.0008838176727295\n",
      "Iteration 33480 Loss: 0.7939544320106506\n",
      "Iteration 33481 Loss: 1.1854987144470215\n",
      "Iteration 33482 Loss: 0.9457216262817383\n",
      "Iteration 33483 Loss: 0.7336926460266113\n",
      "Iteration 33484 Loss: 1.0938704013824463\n",
      "Iteration 33485 Loss: 0.6293182373046875\n",
      "Iteration 33486 Loss: 1.049431324005127\n",
      "Iteration 33487 Loss: 1.2061549425125122\n",
      "Iteration 33488 Loss: 0.9237209558486938\n",
      "Iteration 33489 Loss: 1.1688754558563232\n",
      "Iteration 33489 Loss: 0.9730238914489746\n",
      "Iteration 33490 Loss: 0.9851391911506653\n",
      "Iteration 33491 Loss: 1.0288487672805786\n",
      "Iteration 33492 Loss: 1.272180438041687\n",
      "Iteration 33493 Loss: 1.166749358177185\n",
      "Iteration 33494 Loss: 0.8484203815460205\n",
      "Iteration 33495 Loss: 0.9022060632705688\n",
      "Iteration 33496 Loss: 1.1438387632369995\n",
      "Iteration 33497 Loss: 0.660237729549408\n",
      "Iteration 33498 Loss: 1.2785146236419678\n",
      "Iteration 33499 Loss: 1.2975163459777832\n",
      "Iteration 33499 Loss: 1.0583651065826416\n",
      "Iteration 33500 Loss: 1.143592357635498\n",
      "Iteration 33501 Loss: 0.8800986409187317\n",
      "Iteration 33502 Loss: 1.0347929000854492\n",
      "Iteration 33503 Loss: 1.0749059915542603\n",
      "Iteration 33504 Loss: 0.8649248480796814\n",
      "Iteration 33505 Loss: 0.9196369647979736\n",
      "Iteration 33506 Loss: 0.568486750125885\n",
      "Iteration 33507 Loss: 1.170738935470581\n",
      "Iteration 33508 Loss: 0.9652170538902283\n",
      "Iteration 33509 Loss: 0.8476576805114746\n",
      "Iteration 33509 Loss: 0.9470052719116211\n",
      "Iteration 33510 Loss: 0.9356197714805603\n",
      "Iteration 33511 Loss: 0.8298728466033936\n",
      "Iteration 33512 Loss: 0.7906705737113953\n",
      "Iteration 33513 Loss: 1.1340352296829224\n",
      "Iteration 33514 Loss: 1.1599806547164917\n",
      "Iteration 33515 Loss: 1.212472677230835\n",
      "Iteration 33516 Loss: 1.077343225479126\n",
      "Iteration 33517 Loss: 0.7353927493095398\n",
      "Iteration 33518 Loss: 0.9470606446266174\n",
      "Iteration 33519 Loss: 0.8821433186531067\n",
      "Iteration 33519 Loss: 0.9704591035842896\n",
      "Iteration 33520 Loss: 1.173291563987732\n",
      "Iteration 33521 Loss: 0.9936316609382629\n",
      "Iteration 33522 Loss: 0.9500595927238464\n",
      "Iteration 33523 Loss: 1.0396991968154907\n",
      "Iteration 33524 Loss: 1.0754576921463013\n",
      "Iteration 33525 Loss: 1.0086015462875366\n",
      "Iteration 33526 Loss: 0.6971933245658875\n",
      "Iteration 33527 Loss: 1.2360942363739014\n",
      "Iteration 33528 Loss: 1.2036380767822266\n",
      "Iteration 33529 Loss: 1.3506793975830078\n",
      "Iteration 33529 Loss: 1.0728346109390259\n",
      "Iteration 33530 Loss: 0.7256218791007996\n",
      "Iteration 33531 Loss: 0.8897550106048584\n",
      "Iteration 33532 Loss: 1.0197685956954956\n",
      "Iteration 33533 Loss: 0.8887165784835815\n",
      "Iteration 33534 Loss: 0.8525424003601074\n",
      "Iteration 33535 Loss: 1.1083803176879883\n",
      "Iteration 33536 Loss: 1.1275273561477661\n",
      "Iteration 33537 Loss: 0.8738787770271301\n",
      "Iteration 33538 Loss: 0.9477209448814392\n",
      "Iteration 33539 Loss: 1.2748826742172241\n",
      "Iteration 33539 Loss: 0.9708794355392456\n",
      "Iteration 33540 Loss: 0.9836705923080444\n",
      "Iteration 33541 Loss: 0.8739064931869507\n",
      "Iteration 33542 Loss: 1.1338512897491455\n",
      "Iteration 33543 Loss: 0.9366799592971802\n",
      "Iteration 33544 Loss: 0.7824500203132629\n",
      "Iteration 33545 Loss: 0.7318992018699646\n",
      "Iteration 33546 Loss: 0.985633909702301\n",
      "Iteration 33547 Loss: 0.8711827397346497\n",
      "Iteration 33548 Loss: 1.172669768333435\n",
      "Iteration 33549 Loss: 1.0948460102081299\n",
      "Iteration 33549 Loss: 0.9566790461540222\n",
      "Iteration 33550 Loss: 0.8324915766716003\n",
      "Iteration 33551 Loss: 1.5283130407333374\n",
      "Iteration 33552 Loss: 1.1793372631072998\n",
      "Iteration 33553 Loss: 1.003108024597168\n",
      "Iteration 33554 Loss: 0.9648162126541138\n",
      "Iteration 33555 Loss: 0.9499198794364929\n",
      "Iteration 33556 Loss: 0.9366194009780884\n",
      "Iteration 33557 Loss: 0.8093944191932678\n",
      "Iteration 33558 Loss: 0.7364312410354614\n",
      "Iteration 33559 Loss: 0.9710056185722351\n",
      "Iteration 33559 Loss: 0.9911437034606934\n",
      "Iteration 33560 Loss: 1.2130111455917358\n",
      "Iteration 33561 Loss: 1.0089150667190552\n",
      "Iteration 33562 Loss: 1.1958879232406616\n",
      "Iteration 33563 Loss: 1.2235561609268188\n",
      "Iteration 33564 Loss: 0.8413112759590149\n",
      "Iteration 33565 Loss: 1.0518680810928345\n",
      "Iteration 33566 Loss: 1.0384901762008667\n",
      "Iteration 33567 Loss: 1.1177139282226562\n",
      "Iteration 33568 Loss: 0.8967198133468628\n",
      "Iteration 33569 Loss: 1.130604863166809\n",
      "Iteration 33569 Loss: 1.071807861328125\n",
      "Iteration 33570 Loss: 1.1466349363327026\n",
      "Iteration 33571 Loss: 0.7534844875335693\n",
      "Iteration 33572 Loss: 0.8354744911193848\n",
      "Iteration 33573 Loss: 0.9840598702430725\n",
      "Iteration 33574 Loss: 1.2869209051132202\n",
      "Iteration 33575 Loss: 1.1131222248077393\n",
      "Iteration 33576 Loss: 0.9239236116409302\n",
      "Iteration 33577 Loss: 0.9862159490585327\n",
      "Iteration 33578 Loss: 1.0360156297683716\n",
      "Iteration 33579 Loss: 1.134779453277588\n",
      "Iteration 33579 Loss: 1.0200631618499756\n",
      "Iteration 33580 Loss: 1.0179011821746826\n",
      "Iteration 33581 Loss: 1.0523580312728882\n",
      "Iteration 33582 Loss: 1.075147032737732\n",
      "Iteration 33583 Loss: 0.9460062384605408\n",
      "Iteration 33584 Loss: 1.2432875633239746\n",
      "Iteration 33585 Loss: 0.771910548210144\n",
      "Iteration 33586 Loss: 1.311140537261963\n",
      "Iteration 33587 Loss: 0.8178607225418091\n",
      "Iteration 33588 Loss: 1.0530842542648315\n",
      "Iteration 33589 Loss: 1.0908081531524658\n",
      "Iteration 33589 Loss: 1.0379503965377808\n",
      "Iteration 33590 Loss: 0.994331419467926\n",
      "Iteration 33591 Loss: 1.0005292892456055\n",
      "Iteration 33592 Loss: 1.2404921054840088\n",
      "Iteration 33593 Loss: 0.9422632455825806\n",
      "Iteration 33594 Loss: 1.4289532899856567\n",
      "Iteration 33595 Loss: 0.7678250074386597\n",
      "Iteration 33596 Loss: 1.0954344272613525\n",
      "Iteration 33597 Loss: 1.0735735893249512\n",
      "Iteration 33598 Loss: 0.7665817737579346\n",
      "Iteration 33599 Loss: 0.7189546823501587\n",
      "Iteration 33599 Loss: 1.0028938055038452\n",
      "Iteration 33600 Loss: 1.0307753086090088\n",
      "Iteration 33601 Loss: 0.9803414344787598\n",
      "Iteration 33602 Loss: 1.2658007144927979\n",
      "Iteration 33603 Loss: 1.3078844547271729\n",
      "Iteration 33604 Loss: 1.1199742555618286\n",
      "Iteration 33605 Loss: 1.130051612854004\n",
      "Iteration 33606 Loss: 1.286341905593872\n",
      "Iteration 33607 Loss: 1.0090616941452026\n",
      "Iteration 33608 Loss: 1.5826566219329834\n",
      "Iteration 33609 Loss: 0.7096347808837891\n",
      "Iteration 33609 Loss: 1.1422522068023682\n",
      "Iteration 33610 Loss: 1.1360976696014404\n",
      "Iteration 33611 Loss: 0.950160562992096\n",
      "Iteration 33612 Loss: 0.6999757289886475\n",
      "Iteration 33613 Loss: 1.1588853597640991\n",
      "Iteration 33614 Loss: 1.1274223327636719\n",
      "Iteration 33615 Loss: 1.1788784265518188\n",
      "Iteration 33616 Loss: 0.7516103982925415\n",
      "Iteration 33617 Loss: 0.978945255279541\n",
      "Iteration 33618 Loss: 1.1591378450393677\n",
      "Iteration 33619 Loss: 1.064939022064209\n",
      "Iteration 33619 Loss: 1.0206053256988525\n",
      "Iteration 33620 Loss: 1.1426904201507568\n",
      "Iteration 33621 Loss: 0.7114925384521484\n",
      "Iteration 33622 Loss: 1.3011409044265747\n",
      "Iteration 33623 Loss: 1.1689741611480713\n",
      "Iteration 33624 Loss: 0.9936658143997192\n",
      "Iteration 33625 Loss: 0.590164303779602\n",
      "Iteration 33626 Loss: 1.3398840427398682\n",
      "Iteration 33627 Loss: 1.210290551185608\n",
      "Iteration 33628 Loss: 1.1619733572006226\n",
      "Iteration 33629 Loss: 1.0324983596801758\n",
      "Iteration 33629 Loss: 1.065277338027954\n",
      "Iteration 33630 Loss: 1.127498984336853\n",
      "Iteration 33631 Loss: 0.8549851179122925\n",
      "Iteration 33632 Loss: 1.0179994106292725\n",
      "Iteration 33633 Loss: 1.0706322193145752\n",
      "Iteration 33634 Loss: 1.058167576789856\n",
      "Iteration 33635 Loss: 0.7234247922897339\n",
      "Iteration 33636 Loss: 0.727116584777832\n",
      "Iteration 33637 Loss: 1.2023926973342896\n",
      "Iteration 33638 Loss: 1.2664299011230469\n",
      "Iteration 33639 Loss: 0.9945406317710876\n",
      "Iteration 33639 Loss: 1.0043188333511353\n",
      "Iteration 33640 Loss: 0.7943974137306213\n",
      "Iteration 33641 Loss: 0.949026882648468\n",
      "Iteration 33642 Loss: 1.1894370317459106\n",
      "Iteration 33643 Loss: 0.8960192799568176\n",
      "Iteration 33644 Loss: 1.0392024517059326\n",
      "Iteration 33645 Loss: 1.0463483333587646\n",
      "Iteration 33646 Loss: 0.8623328804969788\n",
      "Iteration 33647 Loss: 0.999418318271637\n",
      "Iteration 33648 Loss: 1.185029149055481\n",
      "Iteration 33649 Loss: 0.9284746646881104\n",
      "Iteration 33649 Loss: 0.9889687299728394\n",
      "Iteration 33650 Loss: 0.9157405495643616\n",
      "Iteration 33651 Loss: 1.0643603801727295\n",
      "Iteration 33652 Loss: 1.0216293334960938\n",
      "Iteration 33653 Loss: 0.838503897190094\n",
      "Iteration 33654 Loss: 1.1284815073013306\n",
      "Iteration 33655 Loss: 0.9475221037864685\n",
      "Iteration 33656 Loss: 1.2544499635696411\n",
      "Iteration 33657 Loss: 1.1944478750228882\n",
      "Iteration 33658 Loss: 0.8777310848236084\n",
      "Iteration 33659 Loss: 1.073302984237671\n",
      "Iteration 33659 Loss: 1.0316169261932373\n",
      "Iteration 33660 Loss: 0.9420440196990967\n",
      "Iteration 33661 Loss: 1.1081080436706543\n",
      "Iteration 33662 Loss: 1.6054719686508179\n",
      "Iteration 33663 Loss: 0.7184228897094727\n",
      "Iteration 33664 Loss: 0.7010947465896606\n",
      "Iteration 33665 Loss: 1.0558165311813354\n",
      "Iteration 33666 Loss: 1.20990788936615\n",
      "Iteration 33667 Loss: 0.8917498588562012\n",
      "Iteration 33668 Loss: 1.0844407081604004\n",
      "Iteration 33669 Loss: 1.118058681488037\n",
      "Iteration 33669 Loss: 1.0435113906860352\n",
      "Iteration 33670 Loss: 1.004156231880188\n",
      "Iteration 33671 Loss: 1.0212308168411255\n",
      "Iteration 33672 Loss: 1.001315712928772\n",
      "Iteration 33673 Loss: 0.9752889275550842\n",
      "Iteration 33674 Loss: 0.9813194870948792\n",
      "Iteration 33675 Loss: 0.9416698813438416\n",
      "Iteration 33676 Loss: 1.2366971969604492\n",
      "Iteration 33677 Loss: 0.8987295627593994\n",
      "Iteration 33678 Loss: 1.1689996719360352\n",
      "Iteration 33679 Loss: 1.3000625371932983\n",
      "Iteration 33679 Loss: 1.052946925163269\n",
      "Iteration 33680 Loss: 1.2067856788635254\n",
      "Iteration 33681 Loss: 0.9636370539665222\n",
      "Iteration 33682 Loss: 1.0167872905731201\n",
      "Iteration 33683 Loss: 0.8144993782043457\n",
      "Iteration 33684 Loss: 0.9031979441642761\n",
      "Iteration 33685 Loss: 0.9926059246063232\n",
      "Iteration 33686 Loss: 0.9838892221450806\n",
      "Iteration 33687 Loss: 0.9494435787200928\n",
      "Iteration 33688 Loss: 1.0561872720718384\n",
      "Iteration 33689 Loss: 0.8458408117294312\n",
      "Iteration 33689 Loss: 0.9732874631881714\n",
      "Iteration 33690 Loss: 1.033196210861206\n",
      "Iteration 33691 Loss: 1.1518232822418213\n",
      "Iteration 33692 Loss: 0.711670458316803\n",
      "Iteration 33693 Loss: 0.8601823449134827\n",
      "Iteration 33694 Loss: 1.4516472816467285\n",
      "Iteration 33695 Loss: 0.6442834138870239\n",
      "Iteration 33696 Loss: 0.7358715534210205\n",
      "Iteration 33697 Loss: 0.6816741824150085\n",
      "Iteration 33698 Loss: 0.9354286193847656\n",
      "Iteration 33699 Loss: 1.1154996156692505\n",
      "Iteration 33699 Loss: 0.9321276545524597\n",
      "Iteration 33700 Loss: 1.014674186706543\n",
      "Iteration 33701 Loss: 0.8010480403900146\n",
      "Iteration 33702 Loss: 1.1163376569747925\n",
      "Iteration 33703 Loss: 1.0352786779403687\n",
      "Iteration 33704 Loss: 0.6805843710899353\n",
      "Iteration 33705 Loss: 1.1995066404342651\n",
      "Iteration 33706 Loss: 0.9341319799423218\n",
      "Iteration 33707 Loss: 0.9842391014099121\n",
      "Iteration 33708 Loss: 1.0936840772628784\n",
      "Iteration 33709 Loss: 0.68178790807724\n",
      "Iteration 33709 Loss: 0.954127311706543\n",
      "Iteration 33710 Loss: 1.1312175989151\n",
      "Iteration 33711 Loss: 0.5115526914596558\n",
      "Iteration 33712 Loss: 1.1110295057296753\n",
      "Iteration 33713 Loss: 0.6254661679267883\n",
      "Iteration 33714 Loss: 0.8685265183448792\n",
      "Iteration 33715 Loss: 1.0063362121582031\n",
      "Iteration 33716 Loss: 1.195446252822876\n",
      "Iteration 33717 Loss: 1.32805335521698\n",
      "Iteration 33718 Loss: 0.849131166934967\n",
      "Iteration 33719 Loss: 1.0239711999893188\n",
      "Iteration 33719 Loss: 0.9650731086730957\n",
      "Iteration 33720 Loss: 1.059611439704895\n",
      "Iteration 33721 Loss: 0.8866321444511414\n",
      "Iteration 33722 Loss: 1.0266834497451782\n",
      "Iteration 33723 Loss: 1.430992603302002\n",
      "Iteration 33724 Loss: 0.8689814805984497\n",
      "Iteration 33725 Loss: 0.70578533411026\n",
      "Iteration 33726 Loss: 0.9903711676597595\n",
      "Iteration 33727 Loss: 1.2437217235565186\n",
      "Iteration 33728 Loss: 1.076804757118225\n",
      "Iteration 33729 Loss: 0.8653048276901245\n",
      "Iteration 33729 Loss: 1.015488862991333\n",
      "Iteration 33730 Loss: 0.8366982340812683\n",
      "Iteration 33731 Loss: 1.399970531463623\n",
      "Iteration 33732 Loss: 0.7886627316474915\n",
      "Iteration 33733 Loss: 1.1157093048095703\n",
      "Iteration 33734 Loss: 1.1601780652999878\n",
      "Iteration 33735 Loss: 0.8248929381370544\n",
      "Iteration 33736 Loss: 0.5465710163116455\n",
      "Iteration 33737 Loss: 0.8297250270843506\n",
      "Iteration 33738 Loss: 0.6726826429367065\n",
      "Iteration 33739 Loss: 0.6904848217964172\n",
      "Iteration 33739 Loss: 0.8865575790405273\n",
      "Iteration 33740 Loss: 0.6049813032150269\n",
      "Iteration 33741 Loss: 1.046257734298706\n",
      "Iteration 33742 Loss: 0.848401665687561\n",
      "Iteration 33743 Loss: 0.950605034828186\n",
      "Iteration 33744 Loss: 0.8210474252700806\n",
      "Iteration 33745 Loss: 1.0770173072814941\n",
      "Iteration 33746 Loss: 0.8223205208778381\n",
      "Iteration 33747 Loss: 1.0989432334899902\n",
      "Iteration 33748 Loss: 1.4809868335723877\n",
      "Iteration 33749 Loss: 0.9201952219009399\n",
      "Iteration 33749 Loss: 0.9670757055282593\n",
      "Iteration 33750 Loss: 0.6237596869468689\n",
      "Iteration 33751 Loss: 0.8380158543586731\n",
      "Iteration 33752 Loss: 1.110843300819397\n",
      "Iteration 33753 Loss: 1.0515989065170288\n",
      "Iteration 33754 Loss: 1.08302640914917\n",
      "Iteration 33755 Loss: 0.7685588598251343\n",
      "Iteration 33756 Loss: 0.9383716583251953\n",
      "Iteration 33757 Loss: 1.1913330554962158\n",
      "Iteration 33758 Loss: 0.9342741370201111\n",
      "Iteration 33759 Loss: 1.06162691116333\n",
      "Iteration 33759 Loss: 0.9601408839225769\n",
      "Iteration 33760 Loss: 1.197829008102417\n",
      "Iteration 33761 Loss: 1.152160882949829\n",
      "Iteration 33762 Loss: 0.8869268298149109\n",
      "Iteration 33763 Loss: 0.7626194357872009\n",
      "Iteration 33764 Loss: 0.8097134828567505\n",
      "Iteration 33765 Loss: 0.9126845002174377\n",
      "Iteration 33766 Loss: 1.1451901197433472\n",
      "Iteration 33767 Loss: 0.7222890853881836\n",
      "Iteration 33768 Loss: 1.2876728773117065\n",
      "Iteration 33769 Loss: 0.9196621179580688\n",
      "Iteration 33769 Loss: 0.9796748161315918\n",
      "Iteration 33770 Loss: 0.8874966502189636\n",
      "Iteration 33771 Loss: 1.1831778287887573\n",
      "Iteration 33772 Loss: 0.9121114611625671\n",
      "Iteration 33773 Loss: 0.9869944453239441\n",
      "Iteration 33774 Loss: 1.0165923833847046\n",
      "Iteration 33775 Loss: 0.9529082179069519\n",
      "Iteration 33776 Loss: 0.9011906385421753\n",
      "Iteration 33777 Loss: 1.2748267650604248\n",
      "Iteration 33778 Loss: 0.8264432549476624\n",
      "Iteration 33779 Loss: 0.9951564073562622\n",
      "Iteration 33779 Loss: 0.9936898350715637\n",
      "Iteration 33780 Loss: 0.8865461349487305\n",
      "Iteration 33781 Loss: 1.3923206329345703\n",
      "Iteration 33782 Loss: 0.9362266063690186\n",
      "Iteration 33783 Loss: 0.8746570348739624\n",
      "Iteration 33784 Loss: 0.9764646291732788\n",
      "Iteration 33785 Loss: 0.8097425103187561\n",
      "Iteration 33786 Loss: 0.7684634327888489\n",
      "Iteration 33787 Loss: 1.2589952945709229\n",
      "Iteration 33788 Loss: 1.1233971118927002\n",
      "Iteration 33789 Loss: 1.0524792671203613\n",
      "Iteration 33789 Loss: 1.0079293251037598\n",
      "Iteration 33790 Loss: 0.9641579389572144\n",
      "Iteration 33791 Loss: 0.985369861125946\n",
      "Iteration 33792 Loss: 1.216102957725525\n",
      "Iteration 33793 Loss: 1.2146583795547485\n",
      "Iteration 33794 Loss: 0.8448081016540527\n",
      "Iteration 33795 Loss: 1.2803043127059937\n",
      "Iteration 33796 Loss: 0.9410567283630371\n",
      "Iteration 33797 Loss: 0.7253502607345581\n",
      "Iteration 33798 Loss: 0.8298789262771606\n",
      "Iteration 33799 Loss: 0.9147382378578186\n",
      "Iteration 33799 Loss: 0.9916424751281738\n",
      "Iteration 33800 Loss: 1.1949665546417236\n",
      "Iteration 33801 Loss: 0.9558943510055542\n",
      "Iteration 33802 Loss: 0.9220275282859802\n",
      "Iteration 33803 Loss: 0.9715178608894348\n",
      "Iteration 33804 Loss: 1.1542145013809204\n",
      "Iteration 33805 Loss: 1.3791486024856567\n",
      "Iteration 33806 Loss: 0.6109123229980469\n",
      "Iteration 33807 Loss: 1.229102611541748\n",
      "Iteration 33808 Loss: 0.8224399089813232\n",
      "Iteration 33809 Loss: 1.058341383934021\n",
      "Iteration 33809 Loss: 1.0298566818237305\n",
      "Iteration 33810 Loss: 0.9220950603485107\n",
      "Iteration 33811 Loss: 1.0836642980575562\n",
      "Iteration 33812 Loss: 1.1509240865707397\n",
      "Iteration 33813 Loss: 1.0750685930252075\n",
      "Iteration 33814 Loss: 0.737690806388855\n",
      "Iteration 33815 Loss: 1.0755555629730225\n",
      "Iteration 33816 Loss: 0.9443426728248596\n",
      "Iteration 33817 Loss: 0.6932543516159058\n",
      "Iteration 33818 Loss: 0.8855022192001343\n",
      "Iteration 33819 Loss: 1.2128854990005493\n",
      "Iteration 33819 Loss: 0.9780982732772827\n",
      "Iteration 33820 Loss: 1.0393174886703491\n",
      "Iteration 33821 Loss: 0.8185973167419434\n",
      "Iteration 33822 Loss: 1.081880807876587\n",
      "Iteration 33823 Loss: 0.8829193115234375\n",
      "Iteration 33824 Loss: 0.8395340442657471\n",
      "Iteration 33825 Loss: 0.7871111631393433\n",
      "Iteration 33826 Loss: 1.0179721117019653\n",
      "Iteration 33827 Loss: 1.077419400215149\n",
      "Iteration 33828 Loss: 0.8405182957649231\n",
      "Iteration 33829 Loss: 0.6935685276985168\n",
      "Iteration 33829 Loss: 0.9078838229179382\n",
      "Iteration 33830 Loss: 0.9243882894515991\n",
      "Iteration 33831 Loss: 0.7901286482810974\n",
      "Iteration 33832 Loss: 1.2339489459991455\n",
      "Iteration 33833 Loss: 0.8844938278198242\n",
      "Iteration 33834 Loss: 1.3165640830993652\n",
      "Iteration 33835 Loss: 1.0498906373977661\n",
      "Iteration 33836 Loss: 0.6854321360588074\n",
      "Iteration 33837 Loss: 0.8253271579742432\n",
      "Iteration 33838 Loss: 0.710703432559967\n",
      "Iteration 33839 Loss: 0.9021392464637756\n",
      "Iteration 33839 Loss: 0.9323016405105591\n",
      "Iteration 33840 Loss: 1.3348495960235596\n",
      "Iteration 33841 Loss: 0.9258219599723816\n",
      "Iteration 33842 Loss: 0.9380230903625488\n",
      "Iteration 33843 Loss: 0.9781990051269531\n",
      "Iteration 33844 Loss: 1.3390098810195923\n",
      "Iteration 33845 Loss: 0.9123768210411072\n",
      "Iteration 33846 Loss: 0.9942192435264587\n",
      "Iteration 33847 Loss: 0.9404152035713196\n",
      "Iteration 33848 Loss: 0.7307703495025635\n",
      "Iteration 33849 Loss: 0.7413806915283203\n",
      "Iteration 33849 Loss: 0.9835065603256226\n",
      "Iteration 33850 Loss: 0.9345060586929321\n",
      "Iteration 33851 Loss: 0.9815984964370728\n",
      "Iteration 33852 Loss: 0.9910574555397034\n",
      "Iteration 33853 Loss: 0.9969591498374939\n",
      "Iteration 33854 Loss: 1.047805905342102\n",
      "Iteration 33855 Loss: 0.6980143785476685\n",
      "Iteration 33856 Loss: 1.1467368602752686\n",
      "Iteration 33857 Loss: 1.1656370162963867\n",
      "Iteration 33858 Loss: 0.8724352121353149\n",
      "Iteration 33859 Loss: 1.0034499168395996\n",
      "Iteration 33859 Loss: 0.9838200807571411\n",
      "Iteration 33860 Loss: 0.7355169653892517\n",
      "Iteration 33861 Loss: 0.8862716555595398\n",
      "Iteration 33862 Loss: 1.0500352382659912\n",
      "Iteration 33863 Loss: 1.0251543521881104\n",
      "Iteration 33864 Loss: 1.5757720470428467\n",
      "Iteration 33865 Loss: 0.8098530769348145\n",
      "Iteration 33866 Loss: 1.063187599182129\n",
      "Iteration 33867 Loss: 0.7894634008407593\n",
      "Iteration 33868 Loss: 1.3303240537643433\n",
      "Iteration 33869 Loss: 1.3919310569763184\n",
      "Iteration 33869 Loss: 1.0657509565353394\n",
      "Iteration 33870 Loss: 0.7444906830787659\n",
      "Iteration 33871 Loss: 0.945116400718689\n",
      "Iteration 33872 Loss: 0.895753026008606\n",
      "Iteration 33873 Loss: 1.1778031587600708\n",
      "Iteration 33874 Loss: 0.861445963382721\n",
      "Iteration 33875 Loss: 1.1151927709579468\n",
      "Iteration 33876 Loss: 1.048119068145752\n",
      "Iteration 33877 Loss: 1.1488661766052246\n",
      "Iteration 33878 Loss: 0.8277754187583923\n",
      "Iteration 33879 Loss: 1.0747846364974976\n",
      "Iteration 33879 Loss: 0.983934760093689\n",
      "Iteration 33880 Loss: 1.1915253400802612\n",
      "Iteration 33881 Loss: 1.2388594150543213\n",
      "Iteration 33882 Loss: 0.8438654541969299\n",
      "Iteration 33883 Loss: 0.7906580567359924\n",
      "Iteration 33884 Loss: 1.2279998064041138\n",
      "Iteration 33885 Loss: 1.5205392837524414\n",
      "Iteration 33886 Loss: 1.0887120962142944\n",
      "Iteration 33887 Loss: 1.0167063474655151\n",
      "Iteration 33888 Loss: 0.9947806596755981\n",
      "Iteration 33889 Loss: 0.5518074035644531\n",
      "Iteration 33889 Loss: 1.0465452671051025\n",
      "Iteration 33890 Loss: 1.151110291481018\n",
      "Iteration 33891 Loss: 0.9468531608581543\n",
      "Iteration 33892 Loss: 0.5115566849708557\n",
      "Iteration 33893 Loss: 1.1651721000671387\n",
      "Iteration 33894 Loss: 0.9347009062767029\n",
      "Iteration 33895 Loss: 1.1238939762115479\n",
      "Iteration 33896 Loss: 1.2589846849441528\n",
      "Iteration 33897 Loss: 0.9779448509216309\n",
      "Iteration 33898 Loss: 0.7257801294326782\n",
      "Iteration 33899 Loss: 1.0234616994857788\n",
      "Iteration 33899 Loss: 0.981945812702179\n",
      "Iteration 33900 Loss: 1.0951614379882812\n",
      "Iteration 33901 Loss: 0.9576945900917053\n",
      "Iteration 33902 Loss: 1.5038275718688965\n",
      "Iteration 33903 Loss: 0.8909883499145508\n",
      "Iteration 33904 Loss: 1.2481741905212402\n",
      "Iteration 33905 Loss: 0.8013561367988586\n",
      "Iteration 33906 Loss: 1.0766547918319702\n",
      "Iteration 33907 Loss: 0.9581906199455261\n",
      "Iteration 33908 Loss: 1.0558538436889648\n",
      "Iteration 33909 Loss: 1.0220056772232056\n",
      "Iteration 33909 Loss: 1.0609906911849976\n",
      "Iteration 33910 Loss: 0.8851901292800903\n",
      "Iteration 33911 Loss: 1.0939106941223145\n",
      "Iteration 33912 Loss: 1.2102916240692139\n",
      "Iteration 33913 Loss: 0.9004951119422913\n",
      "Iteration 33914 Loss: 1.1247992515563965\n",
      "Iteration 33915 Loss: 1.0413423776626587\n",
      "Iteration 33916 Loss: 1.2794830799102783\n",
      "Iteration 33917 Loss: 0.725324809551239\n",
      "Iteration 33918 Loss: 0.9287421703338623\n",
      "Iteration 33919 Loss: 0.9614753723144531\n",
      "Iteration 33919 Loss: 1.0151054859161377\n",
      "Iteration 33920 Loss: 1.1028459072113037\n",
      "Iteration 33921 Loss: 1.1545114517211914\n",
      "Iteration 33922 Loss: 0.90120530128479\n",
      "Iteration 33923 Loss: 0.7381330132484436\n",
      "Iteration 33924 Loss: 1.218416452407837\n",
      "Iteration 33925 Loss: 0.8364327549934387\n",
      "Iteration 33926 Loss: 0.9195135235786438\n",
      "Iteration 33927 Loss: 0.8738484978675842\n",
      "Iteration 33928 Loss: 0.9225630760192871\n",
      "Iteration 33929 Loss: 0.9425122737884521\n",
      "Iteration 33929 Loss: 0.9609982371330261\n",
      "Iteration 33930 Loss: 1.429821252822876\n",
      "Iteration 33931 Loss: 1.0948147773742676\n",
      "Iteration 33932 Loss: 0.8062419295310974\n",
      "Iteration 33933 Loss: 1.2407333850860596\n",
      "Iteration 33934 Loss: 1.2925708293914795\n",
      "Iteration 33935 Loss: 0.7910670638084412\n",
      "Iteration 33936 Loss: 0.9807676672935486\n",
      "Iteration 33937 Loss: 0.9796836972236633\n",
      "Iteration 33938 Loss: 1.1290746927261353\n",
      "Iteration 33939 Loss: 1.0079950094223022\n",
      "Iteration 33939 Loss: 1.0752770900726318\n",
      "Iteration 33940 Loss: 0.6860557794570923\n",
      "Iteration 33941 Loss: 1.1157764196395874\n",
      "Iteration 33942 Loss: 1.0700677633285522\n",
      "Iteration 33943 Loss: 0.9534856081008911\n",
      "Iteration 33944 Loss: 1.1250888109207153\n",
      "Iteration 33945 Loss: 0.7836835384368896\n",
      "Iteration 33946 Loss: 0.9720224738121033\n",
      "Iteration 33947 Loss: 0.8411855101585388\n",
      "Iteration 33948 Loss: 1.1282074451446533\n",
      "Iteration 33949 Loss: 1.001180648803711\n",
      "Iteration 33949 Loss: 0.9676753878593445\n",
      "Iteration 33950 Loss: 0.8440878987312317\n",
      "Iteration 33951 Loss: 1.4538646936416626\n",
      "Iteration 33952 Loss: 0.8115525841712952\n",
      "Iteration 33953 Loss: 1.0522207021713257\n",
      "Iteration 33954 Loss: 0.960152268409729\n",
      "Iteration 33955 Loss: 0.8777734041213989\n",
      "Iteration 33956 Loss: 1.0794894695281982\n",
      "Iteration 33957 Loss: 0.7951974272727966\n",
      "Iteration 33958 Loss: 0.9816873669624329\n",
      "Iteration 33959 Loss: 1.2586243152618408\n",
      "Iteration 33959 Loss: 1.011465072631836\n",
      "Iteration 33960 Loss: 1.105209231376648\n",
      "Iteration 33961 Loss: 1.0155949592590332\n",
      "Iteration 33962 Loss: 0.5902169942855835\n",
      "Iteration 33963 Loss: 1.218414306640625\n",
      "Iteration 33964 Loss: 1.1362754106521606\n",
      "Iteration 33965 Loss: 0.7885233759880066\n",
      "Iteration 33966 Loss: 0.9280634522438049\n",
      "Iteration 33967 Loss: 1.1871784925460815\n",
      "Iteration 33968 Loss: 0.9321038722991943\n",
      "Iteration 33969 Loss: 1.126655101776123\n",
      "Iteration 33969 Loss: 1.0028235912322998\n",
      "Iteration 33970 Loss: 0.7011072039604187\n",
      "Iteration 33971 Loss: 0.9918268322944641\n",
      "Iteration 33972 Loss: 1.2068837881088257\n",
      "Iteration 33973 Loss: 0.7645391821861267\n",
      "Iteration 33974 Loss: 1.1574556827545166\n",
      "Iteration 33975 Loss: 1.1760098934173584\n",
      "Iteration 33976 Loss: 0.8778554797172546\n",
      "Iteration 33977 Loss: 0.7938524484634399\n",
      "Iteration 33978 Loss: 1.0170174837112427\n",
      "Iteration 33979 Loss: 0.9018737077713013\n",
      "Iteration 33979 Loss: 0.9588421583175659\n",
      "Iteration 33980 Loss: 0.8622093200683594\n",
      "Iteration 33981 Loss: 1.0733193159103394\n",
      "Iteration 33982 Loss: 1.0225629806518555\n",
      "Iteration 33983 Loss: 1.0262807607650757\n",
      "Iteration 33984 Loss: 1.1712123155593872\n",
      "Iteration 33985 Loss: 0.7912406921386719\n",
      "Iteration 33986 Loss: 0.44560471177101135\n",
      "Iteration 33987 Loss: 0.9452653527259827\n",
      "Iteration 33988 Loss: 1.0218404531478882\n",
      "Iteration 33989 Loss: 1.0233187675476074\n",
      "Iteration 33989 Loss: 0.9382855296134949\n",
      "Iteration 33990 Loss: 0.9582861661911011\n",
      "Iteration 33991 Loss: 0.9938040971755981\n",
      "Iteration 33992 Loss: 0.8563838601112366\n",
      "Iteration 33993 Loss: 0.8940325379371643\n",
      "Iteration 33994 Loss: 0.7360442280769348\n",
      "Iteration 33995 Loss: 0.9651927947998047\n",
      "Iteration 33996 Loss: 1.2843847274780273\n",
      "Iteration 33997 Loss: 1.0700948238372803\n",
      "Iteration 33998 Loss: 0.804953932762146\n",
      "Iteration 33999 Loss: 0.7930098176002502\n",
      "Iteration 33999 Loss: 0.9356187582015991\n",
      "Iteration 34000 Loss: 1.5894355773925781\n",
      "Iteration 34001 Loss: 0.7238518595695496\n",
      "Iteration 34002 Loss: 1.220228672027588\n",
      "Iteration 34003 Loss: 0.7026553153991699\n",
      "Iteration 34004 Loss: 0.939462423324585\n",
      "Iteration 34005 Loss: 0.8654885292053223\n",
      "Iteration 34006 Loss: 0.7245862483978271\n",
      "Iteration 34007 Loss: 0.8141184449195862\n",
      "Iteration 34008 Loss: 1.0629223585128784\n",
      "Iteration 34009 Loss: 1.2320586442947388\n",
      "Iteration 34009 Loss: 0.9874808192253113\n",
      "Iteration 34010 Loss: 0.9611146450042725\n",
      "Iteration 34011 Loss: 1.033312439918518\n",
      "Iteration 34012 Loss: 1.0298576354980469\n",
      "Iteration 34013 Loss: 0.9767100811004639\n",
      "Iteration 34014 Loss: 0.9382326006889343\n",
      "Iteration 34015 Loss: 0.7585254311561584\n",
      "Iteration 34016 Loss: 0.7637618780136108\n",
      "Iteration 34017 Loss: 1.0630449056625366\n",
      "Iteration 34018 Loss: 0.9774872660636902\n",
      "Iteration 34019 Loss: 1.1126550436019897\n",
      "Iteration 34019 Loss: 0.9614701271057129\n",
      "Iteration 34020 Loss: 1.0473947525024414\n",
      "Iteration 34021 Loss: 1.1514110565185547\n",
      "Iteration 34022 Loss: 0.6779598593711853\n",
      "Iteration 34023 Loss: 1.1727408170700073\n",
      "Iteration 34024 Loss: 0.8361574411392212\n",
      "Iteration 34025 Loss: 1.0112979412078857\n",
      "Iteration 34026 Loss: 1.390263557434082\n",
      "Iteration 34027 Loss: 1.1766771078109741\n",
      "Iteration 34028 Loss: 0.9614506959915161\n",
      "Iteration 34029 Loss: 0.9694785475730896\n",
      "Iteration 34029 Loss: 1.0394831895828247\n",
      "Iteration 34030 Loss: 1.1966578960418701\n",
      "Iteration 34031 Loss: 1.1296710968017578\n",
      "Iteration 34032 Loss: 1.2634135484695435\n",
      "Iteration 34033 Loss: 0.8284253478050232\n",
      "Iteration 34034 Loss: 0.6197684407234192\n",
      "Iteration 34035 Loss: 0.9123745560646057\n",
      "Iteration 34036 Loss: 0.9258685111999512\n",
      "Iteration 34037 Loss: 0.9646130204200745\n",
      "Iteration 34038 Loss: 0.8912123441696167\n",
      "Iteration 34039 Loss: 0.7066916227340698\n",
      "Iteration 34039 Loss: 0.9438695907592773\n",
      "Iteration 34040 Loss: 1.1315488815307617\n",
      "Iteration 34041 Loss: 1.0595579147338867\n",
      "Iteration 34042 Loss: 0.7283471822738647\n",
      "Iteration 34043 Loss: 0.8575652837753296\n",
      "Iteration 34044 Loss: 1.3604273796081543\n",
      "Iteration 34045 Loss: 0.9861705303192139\n",
      "Iteration 34046 Loss: 1.0905535221099854\n",
      "Iteration 34047 Loss: 0.947110116481781\n",
      "Iteration 34048 Loss: 1.123870849609375\n",
      "Iteration 34049 Loss: 0.9314572215080261\n",
      "Iteration 34049 Loss: 1.0216609239578247\n",
      "Iteration 34050 Loss: 1.3200863599777222\n",
      "Iteration 34051 Loss: 1.2001802921295166\n",
      "Iteration 34052 Loss: 1.2190485000610352\n",
      "Iteration 34053 Loss: 1.1249206066131592\n",
      "Iteration 34054 Loss: 0.9784305095672607\n",
      "Iteration 34055 Loss: 1.2705506086349487\n",
      "Iteration 34056 Loss: 0.7996035218238831\n",
      "Iteration 34057 Loss: 1.2635300159454346\n",
      "Iteration 34058 Loss: 0.9756922721862793\n",
      "Iteration 34059 Loss: 1.1814491748809814\n",
      "Iteration 34059 Loss: 1.1333491802215576\n",
      "Iteration 34060 Loss: 0.8862573504447937\n",
      "Iteration 34061 Loss: 1.003783941268921\n",
      "Iteration 34062 Loss: 1.1082478761672974\n",
      "Iteration 34063 Loss: 1.1406372785568237\n",
      "Iteration 34064 Loss: 0.8628826141357422\n",
      "Iteration 34065 Loss: 0.9931241273880005\n",
      "Iteration 34066 Loss: 1.1673972606658936\n",
      "Iteration 34067 Loss: 1.0941449403762817\n",
      "Iteration 34068 Loss: 1.1453454494476318\n",
      "Iteration 34069 Loss: 1.0393006801605225\n",
      "Iteration 34069 Loss: 1.044112205505371\n",
      "Iteration 34070 Loss: 0.9831046462059021\n",
      "Iteration 34071 Loss: 0.8057198524475098\n",
      "Iteration 34072 Loss: 0.8932954668998718\n",
      "Iteration 34073 Loss: 0.8356956243515015\n",
      "Iteration 34074 Loss: 1.0969605445861816\n",
      "Iteration 34075 Loss: 1.1915866136550903\n",
      "Iteration 34076 Loss: 1.020354986190796\n",
      "Iteration 34077 Loss: 1.3340991735458374\n",
      "Iteration 34078 Loss: 0.9601236581802368\n",
      "Iteration 34079 Loss: 0.9371626377105713\n",
      "Iteration 34079 Loss: 1.005810260772705\n",
      "Iteration 34080 Loss: 1.035865068435669\n",
      "Iteration 34081 Loss: 1.0237065553665161\n",
      "Iteration 34082 Loss: 0.930156409740448\n",
      "Iteration 34083 Loss: 1.2424702644348145\n",
      "Iteration 34084 Loss: 0.868000328540802\n",
      "Iteration 34085 Loss: 1.223466157913208\n",
      "Iteration 34086 Loss: 0.917686402797699\n",
      "Iteration 34087 Loss: 1.0834074020385742\n",
      "Iteration 34088 Loss: 1.0429385900497437\n",
      "Iteration 34089 Loss: 0.7493560910224915\n",
      "Iteration 34089 Loss: 1.0117052793502808\n",
      "Iteration 34090 Loss: 0.8039206862449646\n",
      "Iteration 34091 Loss: 0.8799486756324768\n",
      "Iteration 34092 Loss: 1.115708351135254\n",
      "Iteration 34093 Loss: 1.2474068403244019\n",
      "Iteration 34094 Loss: 0.9065678715705872\n",
      "Iteration 34095 Loss: 0.9986975193023682\n",
      "Iteration 34096 Loss: 1.0523957014083862\n",
      "Iteration 34097 Loss: 1.074741244316101\n",
      "Iteration 34098 Loss: 0.6881828308105469\n",
      "Iteration 34099 Loss: 0.7835298180580139\n",
      "Iteration 34099 Loss: 0.9551099538803101\n",
      "Iteration 34100 Loss: 1.0492311716079712\n",
      "Iteration 34101 Loss: 1.3166805505752563\n",
      "Iteration 34102 Loss: 1.1521517038345337\n",
      "Iteration 34103 Loss: 1.1482537984848022\n",
      "Iteration 34104 Loss: 0.9788156747817993\n",
      "Iteration 34105 Loss: 0.9238401055335999\n",
      "Iteration 34106 Loss: 0.8095234036445618\n",
      "Iteration 34107 Loss: 1.1087396144866943\n",
      "Iteration 34108 Loss: 0.7925078272819519\n",
      "Iteration 34109 Loss: 0.7233316898345947\n",
      "Iteration 34109 Loss: 1.000307559967041\n",
      "Iteration 34110 Loss: 0.8094808459281921\n",
      "Iteration 34111 Loss: 0.9882152676582336\n",
      "Iteration 34112 Loss: 1.086136817932129\n",
      "Iteration 34113 Loss: 1.106130599975586\n",
      "Iteration 34114 Loss: 0.8625974655151367\n",
      "Iteration 34115 Loss: 0.8170148730278015\n",
      "Iteration 34116 Loss: 0.8701525330543518\n",
      "Iteration 34117 Loss: 0.8695082664489746\n",
      "Iteration 34118 Loss: 1.1506330966949463\n",
      "Iteration 34119 Loss: 1.0139195919036865\n",
      "Iteration 34119 Loss: 0.9573789834976196\n",
      "Iteration 34120 Loss: 0.9443075060844421\n",
      "Iteration 34121 Loss: 1.1277437210083008\n",
      "Iteration 34122 Loss: 0.9985884428024292\n",
      "Iteration 34123 Loss: 0.8059459924697876\n",
      "Iteration 34124 Loss: 0.692570686340332\n",
      "Iteration 34125 Loss: 0.8983422517776489\n",
      "Iteration 34126 Loss: 0.7972736954689026\n",
      "Iteration 34127 Loss: 0.8441599011421204\n",
      "Iteration 34128 Loss: 1.102547287940979\n",
      "Iteration 34129 Loss: 1.0014187097549438\n",
      "Iteration 34129 Loss: 0.9212898015975952\n",
      "Iteration 34130 Loss: 1.0912368297576904\n",
      "Iteration 34131 Loss: 1.035649299621582\n",
      "Iteration 34132 Loss: 0.7891806364059448\n",
      "Iteration 34133 Loss: 0.8968393206596375\n",
      "Iteration 34134 Loss: 1.2144842147827148\n",
      "Iteration 34135 Loss: 1.175168514251709\n",
      "Iteration 34136 Loss: 0.861985981464386\n",
      "Iteration 34137 Loss: 0.8562598824501038\n",
      "Iteration 34138 Loss: 1.0413085222244263\n",
      "Iteration 34139 Loss: 0.904117226600647\n",
      "Iteration 34139 Loss: 0.9866231083869934\n",
      "Iteration 34140 Loss: 0.9226230978965759\n",
      "Iteration 34141 Loss: 1.02896249294281\n",
      "Iteration 34142 Loss: 0.8933411836624146\n",
      "Iteration 34143 Loss: 0.8499966859817505\n",
      "Iteration 34144 Loss: 0.8847684264183044\n",
      "Iteration 34145 Loss: 1.3151309490203857\n",
      "Iteration 34146 Loss: 0.950840175151825\n",
      "Iteration 34147 Loss: 1.0133538246154785\n",
      "Iteration 34148 Loss: 0.9853106141090393\n",
      "Iteration 34149 Loss: 0.8370939493179321\n",
      "Iteration 34149 Loss: 0.9681421518325806\n",
      "Iteration 34150 Loss: 0.7359010577201843\n",
      "Iteration 34151 Loss: 0.8201535940170288\n",
      "Iteration 34152 Loss: 0.7760981321334839\n",
      "Iteration 34153 Loss: 0.8084668517112732\n",
      "Iteration 34154 Loss: 0.9940305948257446\n",
      "Iteration 34155 Loss: 1.0379750728607178\n",
      "Iteration 34156 Loss: 1.171281337738037\n",
      "Iteration 34157 Loss: 0.9212602972984314\n",
      "Iteration 34158 Loss: 1.286422848701477\n",
      "Iteration 34159 Loss: 1.0763553380966187\n",
      "Iteration 34159 Loss: 0.9627944231033325\n",
      "Iteration 34160 Loss: 0.8993141651153564\n",
      "Iteration 34161 Loss: 1.056419014930725\n",
      "Iteration 34162 Loss: 0.7879072427749634\n",
      "Iteration 34163 Loss: 0.9123847484588623\n",
      "Iteration 34164 Loss: 0.8785653710365295\n",
      "Iteration 34165 Loss: 1.2484424114227295\n",
      "Iteration 34166 Loss: 1.1790955066680908\n",
      "Iteration 34167 Loss: 0.9881207942962646\n",
      "Iteration 34168 Loss: 0.9316369295120239\n",
      "Iteration 34169 Loss: 1.092159390449524\n",
      "Iteration 34169 Loss: 0.9974044561386108\n",
      "Iteration 34170 Loss: 0.9736461639404297\n",
      "Iteration 34171 Loss: 0.8682283759117126\n",
      "Iteration 34172 Loss: 1.0910979509353638\n",
      "Iteration 34173 Loss: 1.0038150548934937\n",
      "Iteration 34174 Loss: 1.2556931972503662\n",
      "Iteration 34175 Loss: 1.263603925704956\n",
      "Iteration 34176 Loss: 0.9491745233535767\n",
      "Iteration 34177 Loss: 0.8800607919692993\n",
      "Iteration 34178 Loss: 0.807036280632019\n",
      "Iteration 34179 Loss: 0.8696063756942749\n",
      "Iteration 34179 Loss: 0.9961963891983032\n",
      "Iteration 34180 Loss: 0.9193196892738342\n",
      "Iteration 34181 Loss: 1.3713645935058594\n",
      "Iteration 34182 Loss: 0.8714911937713623\n",
      "Iteration 34183 Loss: 0.8990963697433472\n",
      "Iteration 34184 Loss: 1.3097771406173706\n",
      "Iteration 34185 Loss: 1.2277660369873047\n",
      "Iteration 34186 Loss: 1.0904757976531982\n",
      "Iteration 34187 Loss: 0.6460423469543457\n",
      "Iteration 34188 Loss: 1.2015933990478516\n",
      "Iteration 34189 Loss: 1.1135536432266235\n",
      "Iteration 34189 Loss: 1.0650479793548584\n",
      "Iteration 34190 Loss: 0.9956494569778442\n",
      "Iteration 34191 Loss: 0.8982297778129578\n",
      "Iteration 34192 Loss: 1.005621314048767\n",
      "Iteration 34193 Loss: 0.784356415271759\n",
      "Iteration 34194 Loss: 1.2459321022033691\n",
      "Iteration 34195 Loss: 0.7588914632797241\n",
      "Iteration 34196 Loss: 0.6756831407546997\n",
      "Iteration 34197 Loss: 1.1931017637252808\n",
      "Iteration 34198 Loss: 1.000287413597107\n",
      "Iteration 34199 Loss: 0.9702206254005432\n",
      "Iteration 34199 Loss: 0.9527974128723145\n",
      "Iteration 34200 Loss: 1.4305050373077393\n",
      "Iteration 34201 Loss: 0.6134150624275208\n",
      "Iteration 34202 Loss: 1.1027324199676514\n",
      "Iteration 34203 Loss: 1.3584569692611694\n",
      "Iteration 34204 Loss: 0.8559178709983826\n",
      "Iteration 34205 Loss: 1.0895347595214844\n",
      "Iteration 34206 Loss: 0.9686818718910217\n",
      "Iteration 34207 Loss: 0.6267964839935303\n",
      "Iteration 34208 Loss: 0.7228750586509705\n",
      "Iteration 34209 Loss: 1.0987857580184937\n",
      "Iteration 34209 Loss: 0.9867701530456543\n",
      "Iteration 34210 Loss: 1.0114117860794067\n",
      "Iteration 34211 Loss: 1.0846755504608154\n",
      "Iteration 34212 Loss: 0.9166436791419983\n",
      "Iteration 34213 Loss: 1.0598742961883545\n",
      "Iteration 34214 Loss: 1.2281348705291748\n",
      "Iteration 34215 Loss: 1.0429013967514038\n",
      "Iteration 34216 Loss: 1.19216787815094\n",
      "Iteration 34217 Loss: 1.1046366691589355\n",
      "Iteration 34218 Loss: 1.1857261657714844\n",
      "Iteration 34219 Loss: 0.8571224212646484\n",
      "Iteration 34219 Loss: 1.0683294534683228\n",
      "Iteration 34220 Loss: 1.1067620515823364\n",
      "Iteration 34221 Loss: 0.8248525857925415\n",
      "Iteration 34222 Loss: 0.9129248857498169\n",
      "Iteration 34223 Loss: 0.8272736072540283\n",
      "Iteration 34224 Loss: 1.1586555242538452\n",
      "Iteration 34225 Loss: 0.9609456658363342\n",
      "Iteration 34226 Loss: 1.0637128353118896\n",
      "Iteration 34227 Loss: 1.3032159805297852\n",
      "Iteration 34228 Loss: 0.7532744407653809\n",
      "Iteration 34229 Loss: 0.9873107671737671\n",
      "Iteration 34229 Loss: 0.989892840385437\n",
      "Iteration 34230 Loss: 1.19918954372406\n",
      "Iteration 34231 Loss: 0.9807116985321045\n",
      "Iteration 34232 Loss: 0.7942591309547424\n",
      "Iteration 34233 Loss: 1.114654541015625\n",
      "Iteration 34234 Loss: 0.8995965719223022\n",
      "Iteration 34235 Loss: 1.1263011693954468\n",
      "Iteration 34236 Loss: 1.1487911939620972\n",
      "Iteration 34237 Loss: 0.645379900932312\n",
      "Iteration 34238 Loss: 0.9020810723304749\n",
      "Iteration 34239 Loss: 1.0607335567474365\n",
      "Iteration 34239 Loss: 0.9871698617935181\n",
      "Iteration 34240 Loss: 1.2104259729385376\n",
      "Iteration 34241 Loss: 1.3124418258666992\n",
      "Iteration 34242 Loss: 1.274760127067566\n",
      "Iteration 34243 Loss: 1.229749321937561\n",
      "Iteration 34244 Loss: 0.7816455960273743\n",
      "Iteration 34245 Loss: 1.0001479387283325\n",
      "Iteration 34246 Loss: 0.7808784246444702\n",
      "Iteration 34247 Loss: 0.9404720067977905\n",
      "Iteration 34248 Loss: 1.0614866018295288\n",
      "Iteration 34249 Loss: 1.033064603805542\n",
      "Iteration 34249 Loss: 1.062507152557373\n",
      "Iteration 34250 Loss: 0.9509016871452332\n",
      "Iteration 34251 Loss: 1.0107614994049072\n",
      "Iteration 34252 Loss: 0.9289998412132263\n",
      "Iteration 34253 Loss: 0.5699541568756104\n",
      "Iteration 34254 Loss: 0.9198647737503052\n",
      "Iteration 34255 Loss: 0.9019841551780701\n",
      "Iteration 34256 Loss: 0.8514082431793213\n",
      "Iteration 34257 Loss: 1.4374229907989502\n",
      "Iteration 34258 Loss: 1.0043060779571533\n",
      "Iteration 34259 Loss: 0.6725252866744995\n",
      "Iteration 34259 Loss: 0.9248127937316895\n",
      "Iteration 34260 Loss: 0.8450794219970703\n",
      "Iteration 34261 Loss: 0.9734808802604675\n",
      "Iteration 34262 Loss: 1.1998765468597412\n",
      "Iteration 34263 Loss: 1.4172277450561523\n",
      "Iteration 34264 Loss: 1.1818283796310425\n",
      "Iteration 34265 Loss: 1.3109941482543945\n",
      "Iteration 34266 Loss: 1.1221860647201538\n",
      "Iteration 34267 Loss: 0.9573810696601868\n",
      "Iteration 34268 Loss: 0.9784128665924072\n",
      "Iteration 34269 Loss: 1.128012776374817\n",
      "Iteration 34269 Loss: 1.111448049545288\n",
      "Iteration 34270 Loss: 0.8676648736000061\n",
      "Iteration 34271 Loss: 1.0238873958587646\n",
      "Iteration 34272 Loss: 0.8949208855628967\n",
      "Iteration 34273 Loss: 0.8114712834358215\n",
      "Iteration 34274 Loss: 0.9629143476486206\n",
      "Iteration 34275 Loss: 1.2835559844970703\n",
      "Iteration 34276 Loss: 0.9296343326568604\n",
      "Iteration 34277 Loss: 1.114341378211975\n",
      "Iteration 34278 Loss: 0.8620424270629883\n",
      "Iteration 34279 Loss: 1.2775628566741943\n",
      "Iteration 34279 Loss: 1.0027996301651\n",
      "Iteration 34280 Loss: 1.091774582862854\n",
      "Iteration 34281 Loss: 0.9422985315322876\n",
      "Iteration 34282 Loss: 0.8103627562522888\n",
      "Iteration 34283 Loss: 0.9820241332054138\n",
      "Iteration 34284 Loss: 0.8933457136154175\n",
      "Iteration 34285 Loss: 1.107493281364441\n",
      "Iteration 34286 Loss: 0.8147094249725342\n",
      "Iteration 34287 Loss: 1.3267790079116821\n",
      "Iteration 34288 Loss: 0.8325653672218323\n",
      "Iteration 34289 Loss: 1.2236703634262085\n",
      "Iteration 34289 Loss: 1.0025023221969604\n",
      "Iteration 34290 Loss: 1.0631828308105469\n",
      "Iteration 34291 Loss: 0.935498833656311\n",
      "Iteration 34292 Loss: 1.0524084568023682\n",
      "Iteration 34293 Loss: 1.1684454679489136\n",
      "Iteration 34294 Loss: 0.8718584775924683\n",
      "Iteration 34295 Loss: 0.8835205435752869\n",
      "Iteration 34296 Loss: 0.9141530394554138\n",
      "Iteration 34297 Loss: 0.8172426223754883\n",
      "Iteration 34298 Loss: 1.0518031120300293\n",
      "Iteration 34299 Loss: 1.045305609703064\n",
      "Iteration 34299 Loss: 0.980341911315918\n",
      "Iteration 34300 Loss: 0.9696415066719055\n",
      "Iteration 34301 Loss: 0.8630741834640503\n",
      "Iteration 34302 Loss: 0.9593955278396606\n",
      "Iteration 34303 Loss: 1.09040367603302\n",
      "Iteration 34304 Loss: 0.948647677898407\n",
      "Iteration 34305 Loss: 0.9837228655815125\n",
      "Iteration 34306 Loss: 1.125591516494751\n",
      "Iteration 34307 Loss: 0.9682316780090332\n",
      "Iteration 34308 Loss: 0.8864418864250183\n",
      "Iteration 34309 Loss: 1.184999942779541\n",
      "Iteration 34309 Loss: 0.9980150461196899\n",
      "Iteration 34310 Loss: 1.6060324907302856\n",
      "Iteration 34311 Loss: 0.8915996551513672\n",
      "Iteration 34312 Loss: 1.01802396774292\n",
      "Iteration 34313 Loss: 1.0780110359191895\n",
      "Iteration 34314 Loss: 1.0073999166488647\n",
      "Iteration 34315 Loss: 1.1021573543548584\n",
      "Iteration 34316 Loss: 0.7262833118438721\n",
      "Iteration 34317 Loss: 0.9136890769004822\n",
      "Iteration 34318 Loss: 1.0319820642471313\n",
      "Iteration 34319 Loss: 0.7367775440216064\n",
      "Iteration 34319 Loss: 1.0111955404281616\n",
      "Iteration 34320 Loss: 1.069616436958313\n",
      "Iteration 34321 Loss: 1.0940563678741455\n",
      "Iteration 34322 Loss: 0.8060139417648315\n",
      "Iteration 34323 Loss: 0.9120519161224365\n",
      "Iteration 34324 Loss: 1.053166389465332\n",
      "Iteration 34325 Loss: 0.7226870059967041\n",
      "Iteration 34326 Loss: 0.6308562159538269\n",
      "Iteration 34327 Loss: 1.2653334140777588\n",
      "Iteration 34328 Loss: 0.8124240636825562\n",
      "Iteration 34329 Loss: 0.8038408756256104\n",
      "Iteration 34329 Loss: 0.9170047044754028\n",
      "Iteration 34330 Loss: 0.9258142709732056\n",
      "Iteration 34331 Loss: 0.9644976854324341\n",
      "Iteration 34332 Loss: 0.9522525668144226\n",
      "Iteration 34333 Loss: 0.6774299740791321\n",
      "Iteration 34334 Loss: 1.0125929117202759\n",
      "Iteration 34335 Loss: 1.171700119972229\n",
      "Iteration 34336 Loss: 1.4625917673110962\n",
      "Iteration 34337 Loss: 0.6618512272834778\n",
      "Iteration 34338 Loss: 1.0197700262069702\n",
      "Iteration 34339 Loss: 1.1262046098709106\n",
      "Iteration 34339 Loss: 0.997470498085022\n",
      "Iteration 34340 Loss: 1.069641351699829\n",
      "Iteration 34341 Loss: 0.8421705961227417\n",
      "Iteration 34342 Loss: 0.9156851768493652\n",
      "Iteration 34343 Loss: 1.1079926490783691\n",
      "Iteration 34344 Loss: 0.5522345304489136\n",
      "Iteration 34345 Loss: 0.8405067920684814\n",
      "Iteration 34346 Loss: 0.898547887802124\n",
      "Iteration 34347 Loss: 0.9581577777862549\n",
      "Iteration 34348 Loss: 0.7367141246795654\n",
      "Iteration 34349 Loss: 1.0128717422485352\n",
      "Iteration 34349 Loss: 0.8934522867202759\n",
      "Iteration 34350 Loss: 0.9213376641273499\n",
      "Iteration 34351 Loss: 0.5321357250213623\n",
      "Iteration 34352 Loss: 1.0162620544433594\n",
      "Iteration 34353 Loss: 1.1257795095443726\n",
      "Iteration 34354 Loss: 0.9907791614532471\n",
      "Iteration 34355 Loss: 0.8114072680473328\n",
      "Iteration 34356 Loss: 1.1486152410507202\n",
      "Iteration 34357 Loss: 0.9236793518066406\n",
      "Iteration 34358 Loss: 0.8650971055030823\n",
      "Iteration 34359 Loss: 1.2003477811813354\n",
      "Iteration 34359 Loss: 0.953544020652771\n",
      "Iteration 34360 Loss: 0.8745418190956116\n",
      "Iteration 34361 Loss: 0.9661463499069214\n",
      "Iteration 34362 Loss: 0.6421751379966736\n",
      "Iteration 34363 Loss: 0.8808625936508179\n",
      "Iteration 34364 Loss: 0.8387690782546997\n",
      "Iteration 34365 Loss: 0.8526254892349243\n",
      "Iteration 34366 Loss: 0.9058919548988342\n",
      "Iteration 34367 Loss: 0.6759273409843445\n",
      "Iteration 34368 Loss: 0.9973692297935486\n",
      "Iteration 34369 Loss: 1.1180204153060913\n",
      "Iteration 34369 Loss: 0.8752328753471375\n",
      "Iteration 34370 Loss: 0.695378303527832\n",
      "Iteration 34371 Loss: 0.8436207175254822\n",
      "Iteration 34372 Loss: 0.9677631855010986\n",
      "Iteration 34373 Loss: 1.0015558004379272\n",
      "Iteration 34374 Loss: 1.1240566968917847\n",
      "Iteration 34375 Loss: 1.0423789024353027\n",
      "Iteration 34376 Loss: 1.0285964012145996\n",
      "Iteration 34377 Loss: 0.7246150970458984\n",
      "Iteration 34378 Loss: 1.2190748453140259\n",
      "Iteration 34379 Loss: 0.7021811604499817\n",
      "Iteration 34379 Loss: 0.9349222183227539\n",
      "Iteration 34380 Loss: 0.8708599209785461\n",
      "Iteration 34381 Loss: 0.8783385157585144\n",
      "Iteration 34382 Loss: 0.7206111550331116\n",
      "Iteration 34383 Loss: 0.6853187084197998\n",
      "Iteration 34384 Loss: 0.8992944955825806\n",
      "Iteration 34385 Loss: 0.9565893411636353\n",
      "Iteration 34386 Loss: 1.4874963760375977\n",
      "Iteration 34387 Loss: 1.3539338111877441\n",
      "Iteration 34388 Loss: 1.0223486423492432\n",
      "Iteration 34389 Loss: 1.1914494037628174\n",
      "Iteration 34389 Loss: 1.0066239833831787\n",
      "Iteration 34390 Loss: 1.2269530296325684\n",
      "Iteration 34391 Loss: 1.2400048971176147\n",
      "Iteration 34392 Loss: 1.3240209817886353\n",
      "Iteration 34393 Loss: 1.0479083061218262\n",
      "Iteration 34394 Loss: 1.0875539779663086\n",
      "Iteration 34395 Loss: 0.8373755216598511\n",
      "Iteration 34396 Loss: 1.195572853088379\n",
      "Iteration 34397 Loss: 0.8827900886535645\n",
      "Iteration 34398 Loss: 0.8551388382911682\n",
      "Iteration 34399 Loss: 1.1043585538864136\n",
      "Iteration 34399 Loss: 1.0801677703857422\n",
      "Iteration 34400 Loss: 1.1822789907455444\n",
      "Iteration 34401 Loss: 0.8071797490119934\n",
      "Iteration 34402 Loss: 1.0307949781417847\n",
      "Iteration 34403 Loss: 1.032029151916504\n",
      "Iteration 34404 Loss: 0.825871467590332\n",
      "Iteration 34405 Loss: 0.8441590070724487\n",
      "Iteration 34406 Loss: 0.980736255645752\n",
      "Iteration 34407 Loss: 0.903154194355011\n",
      "Iteration 34408 Loss: 1.3337316513061523\n",
      "Iteration 34409 Loss: 0.8511619567871094\n",
      "Iteration 34409 Loss: 0.9791097640991211\n",
      "Iteration 34410 Loss: 0.6759786605834961\n",
      "Iteration 34411 Loss: 0.6303704380989075\n",
      "Iteration 34412 Loss: 1.0722777843475342\n",
      "Iteration 34413 Loss: 0.9935256242752075\n",
      "Iteration 34414 Loss: 1.3233286142349243\n",
      "Iteration 34415 Loss: 1.0374902486801147\n",
      "Iteration 34416 Loss: 1.0941088199615479\n",
      "Iteration 34417 Loss: 0.9508382678031921\n",
      "Iteration 34418 Loss: 1.10182523727417\n",
      "Iteration 34419 Loss: 0.9284188747406006\n",
      "Iteration 34419 Loss: 0.9808161854743958\n",
      "Iteration 34420 Loss: 0.9694815874099731\n",
      "Iteration 34421 Loss: 0.7634406089782715\n",
      "Iteration 34422 Loss: 0.9434820413589478\n",
      "Iteration 34423 Loss: 0.923901379108429\n",
      "Iteration 34424 Loss: 1.0722854137420654\n",
      "Iteration 34425 Loss: 1.1309012174606323\n",
      "Iteration 34426 Loss: 1.1650928258895874\n",
      "Iteration 34427 Loss: 0.8708135485649109\n",
      "Iteration 34428 Loss: 1.15647554397583\n",
      "Iteration 34429 Loss: 0.9398335218429565\n",
      "Iteration 34429 Loss: 0.9935706853866577\n",
      "Iteration 34430 Loss: 0.884816586971283\n",
      "Iteration 34431 Loss: 1.1183061599731445\n",
      "Iteration 34432 Loss: 1.054818868637085\n",
      "Iteration 34433 Loss: 0.9873079657554626\n",
      "Iteration 34434 Loss: 1.1986466646194458\n",
      "Iteration 34435 Loss: 1.0227117538452148\n",
      "Iteration 34436 Loss: 1.1058998107910156\n",
      "Iteration 34437 Loss: 0.8816628456115723\n",
      "Iteration 34438 Loss: 0.9608952403068542\n",
      "Iteration 34439 Loss: 1.1692848205566406\n",
      "Iteration 34439 Loss: 1.0384349822998047\n",
      "Iteration 34440 Loss: 1.1889593601226807\n",
      "Iteration 34441 Loss: 0.5444430112838745\n",
      "Iteration 34442 Loss: 1.2315223217010498\n",
      "Iteration 34443 Loss: 1.2435622215270996\n",
      "Iteration 34444 Loss: 1.0436729192733765\n",
      "Iteration 34445 Loss: 1.13363778591156\n",
      "Iteration 34446 Loss: 1.0759258270263672\n",
      "Iteration 34447 Loss: 1.3124048709869385\n",
      "Iteration 34448 Loss: 0.8988721966743469\n",
      "Iteration 34449 Loss: 1.345155119895935\n",
      "Iteration 34449 Loss: 1.1018154621124268\n",
      "Iteration 34450 Loss: 0.9526481628417969\n",
      "Iteration 34451 Loss: 0.9162164330482483\n",
      "Iteration 34452 Loss: 0.899703323841095\n",
      "Iteration 34453 Loss: 0.8476428985595703\n",
      "Iteration 34454 Loss: 0.89225172996521\n",
      "Iteration 34455 Loss: 0.9216557741165161\n",
      "Iteration 34456 Loss: 0.7146445512771606\n",
      "Iteration 34457 Loss: 0.769188404083252\n",
      "Iteration 34458 Loss: 0.8244001865386963\n",
      "Iteration 34459 Loss: 0.9725512266159058\n",
      "Iteration 34459 Loss: 0.871090292930603\n",
      "Iteration 34460 Loss: 1.0765067338943481\n",
      "Iteration 34461 Loss: 0.6357381939888\n",
      "Iteration 34462 Loss: 1.3166834115982056\n",
      "Iteration 34463 Loss: 1.2336558103561401\n",
      "Iteration 34464 Loss: 1.0914801359176636\n",
      "Iteration 34465 Loss: 0.7938162684440613\n",
      "Iteration 34466 Loss: 0.843507707118988\n",
      "Iteration 34467 Loss: 1.0676054954528809\n",
      "Iteration 34468 Loss: 1.0646157264709473\n",
      "Iteration 34469 Loss: 1.0383583307266235\n",
      "Iteration 34469 Loss: 1.016196846961975\n",
      "Iteration 34470 Loss: 1.4258544445037842\n",
      "Iteration 34471 Loss: 1.128446340560913\n",
      "Iteration 34472 Loss: 0.861666738986969\n",
      "Iteration 34473 Loss: 1.057987093925476\n",
      "Iteration 34474 Loss: 1.087251901626587\n",
      "Iteration 34475 Loss: 0.9294745922088623\n",
      "Iteration 34476 Loss: 1.2084757089614868\n",
      "Iteration 34477 Loss: 1.2558798789978027\n",
      "Iteration 34478 Loss: 1.0439411401748657\n",
      "Iteration 34479 Loss: 0.8549385070800781\n",
      "Iteration 34479 Loss: 1.085391640663147\n",
      "Iteration 34480 Loss: 1.0709055662155151\n",
      "Iteration 34481 Loss: 1.1153042316436768\n",
      "Iteration 34482 Loss: 0.8081353306770325\n",
      "Iteration 34483 Loss: 1.0668240785598755\n",
      "Iteration 34484 Loss: 1.1448523998260498\n",
      "Iteration 34485 Loss: 0.7697413563728333\n",
      "Iteration 34486 Loss: 1.0252964496612549\n",
      "Iteration 34487 Loss: 0.8775861859321594\n",
      "Iteration 34488 Loss: 0.9403303265571594\n",
      "Iteration 34489 Loss: 0.9862533807754517\n",
      "Iteration 34489 Loss: 0.9805229902267456\n",
      "Iteration 34490 Loss: 0.879072368144989\n",
      "Iteration 34491 Loss: 1.0013025999069214\n",
      "Iteration 34492 Loss: 1.1897064447402954\n",
      "Iteration 34493 Loss: 0.9541550874710083\n",
      "Iteration 34494 Loss: 0.9641714692115784\n",
      "Iteration 34495 Loss: 0.9758524894714355\n",
      "Iteration 34496 Loss: 1.20578932762146\n",
      "Iteration 34497 Loss: 0.8283808827400208\n",
      "Iteration 34498 Loss: 1.088985562324524\n",
      "Iteration 34499 Loss: 0.8625903725624084\n",
      "Iteration 34499 Loss: 0.9950006604194641\n",
      "Iteration 34500 Loss: 0.9954784512519836\n",
      "Iteration 34501 Loss: 0.8548759818077087\n",
      "Iteration 34502 Loss: 0.9170575737953186\n",
      "Iteration 34503 Loss: 0.6937212944030762\n",
      "Iteration 34504 Loss: 1.1667611598968506\n",
      "Iteration 34505 Loss: 0.8898338079452515\n",
      "Iteration 34506 Loss: 0.9633007645606995\n",
      "Iteration 34507 Loss: 0.7507113814353943\n",
      "Iteration 34508 Loss: 0.7135645747184753\n",
      "Iteration 34509 Loss: 0.9147890210151672\n",
      "Iteration 34509 Loss: 0.8860095143318176\n",
      "Iteration 34510 Loss: 1.2162549495697021\n",
      "Iteration 34511 Loss: 0.9305087924003601\n",
      "Iteration 34512 Loss: 0.9422106742858887\n",
      "Iteration 34513 Loss: 0.6631126403808594\n",
      "Iteration 34514 Loss: 1.073624849319458\n",
      "Iteration 34515 Loss: 1.107107162475586\n",
      "Iteration 34516 Loss: 0.9827497005462646\n",
      "Iteration 34517 Loss: 0.9926657676696777\n",
      "Iteration 34518 Loss: 0.7378715872764587\n",
      "Iteration 34519 Loss: 1.1729365587234497\n",
      "Iteration 34519 Loss: 0.9819042086601257\n",
      "Iteration 34520 Loss: 0.6776698231697083\n",
      "Iteration 34521 Loss: 0.9824131727218628\n",
      "Iteration 34522 Loss: 0.687826931476593\n",
      "Iteration 34523 Loss: 1.0059843063354492\n",
      "Iteration 34524 Loss: 1.0696536302566528\n",
      "Iteration 34525 Loss: 0.9701511859893799\n",
      "Iteration 34526 Loss: 1.2342472076416016\n",
      "Iteration 34527 Loss: 1.0978699922561646\n",
      "Iteration 34528 Loss: 0.6018481254577637\n",
      "Iteration 34529 Loss: 1.015743374824524\n",
      "Iteration 34529 Loss: 0.93434077501297\n",
      "Iteration 34530 Loss: 1.032178521156311\n",
      "Iteration 34531 Loss: 1.1577759981155396\n",
      "Iteration 34532 Loss: 1.1379363536834717\n",
      "Iteration 34533 Loss: 0.6848360300064087\n",
      "Iteration 34534 Loss: 1.5478928089141846\n",
      "Iteration 34535 Loss: 1.3118104934692383\n",
      "Iteration 34536 Loss: 0.8746294975280762\n",
      "Iteration 34537 Loss: 0.9659051895141602\n",
      "Iteration 34538 Loss: 1.4346630573272705\n",
      "Iteration 34539 Loss: 0.7274922728538513\n",
      "Iteration 34539 Loss: 1.0875121355056763\n",
      "Iteration 34540 Loss: 0.6888923645019531\n",
      "Iteration 34541 Loss: 0.8550317287445068\n",
      "Iteration 34542 Loss: 1.0094879865646362\n",
      "Iteration 34543 Loss: 1.0483660697937012\n",
      "Iteration 34544 Loss: 0.7442569136619568\n",
      "Iteration 34545 Loss: 1.098158359527588\n",
      "Iteration 34546 Loss: 1.0414351224899292\n",
      "Iteration 34547 Loss: 0.7518269419670105\n",
      "Iteration 34548 Loss: 0.7814059853553772\n",
      "Iteration 34549 Loss: 0.9008516669273376\n",
      "Iteration 34549 Loss: 0.8919712901115417\n",
      "Iteration 34550 Loss: 0.9128196239471436\n",
      "Iteration 34551 Loss: 1.0505421161651611\n",
      "Iteration 34552 Loss: 1.0230740308761597\n",
      "Iteration 34553 Loss: 1.067759394645691\n",
      "Iteration 34554 Loss: 0.9226385951042175\n",
      "Iteration 34555 Loss: 0.7377342581748962\n",
      "Iteration 34556 Loss: 0.9047298431396484\n",
      "Iteration 34557 Loss: 1.257772445678711\n",
      "Iteration 34558 Loss: 1.1461851596832275\n",
      "Iteration 34559 Loss: 0.744418740272522\n",
      "Iteration 34559 Loss: 0.976767361164093\n",
      "Iteration 34560 Loss: 0.9675353765487671\n",
      "Iteration 34561 Loss: 1.1133451461791992\n",
      "Iteration 34562 Loss: 0.9851568937301636\n",
      "Iteration 34563 Loss: 0.8010186553001404\n",
      "Iteration 34564 Loss: 0.8779698610305786\n",
      "Iteration 34565 Loss: 1.1182880401611328\n",
      "Iteration 34566 Loss: 0.8377918601036072\n",
      "Iteration 34567 Loss: 1.0449845790863037\n",
      "Iteration 34568 Loss: 1.1773309707641602\n",
      "Iteration 34569 Loss: 0.8783098459243774\n",
      "Iteration 34569 Loss: 0.9801731109619141\n",
      "Iteration 34570 Loss: 0.7293257713317871\n",
      "Iteration 34571 Loss: 0.7537340521812439\n",
      "Iteration 34572 Loss: 0.9276843070983887\n",
      "Iteration 34573 Loss: 0.9081495404243469\n",
      "Iteration 34574 Loss: 1.1769986152648926\n",
      "Iteration 34575 Loss: 0.7486029267311096\n",
      "Iteration 34576 Loss: 1.1324760913848877\n",
      "Iteration 34577 Loss: 0.8358104825019836\n",
      "Iteration 34578 Loss: 0.7793092727661133\n",
      "Iteration 34579 Loss: 1.050970196723938\n",
      "Iteration 34579 Loss: 0.9043061137199402\n",
      "Iteration 34580 Loss: 1.208444595336914\n",
      "Iteration 34581 Loss: 1.1754099130630493\n",
      "Iteration 34582 Loss: 0.8954693078994751\n",
      "Iteration 34583 Loss: 1.0285961627960205\n",
      "Iteration 34584 Loss: 1.0146491527557373\n",
      "Iteration 34585 Loss: 1.0618228912353516\n",
      "Iteration 34586 Loss: 1.263918161392212\n",
      "Iteration 34587 Loss: 0.9621220231056213\n",
      "Iteration 34588 Loss: 0.8338574171066284\n",
      "Iteration 34589 Loss: 1.1807297468185425\n",
      "Iteration 34589 Loss: 1.0625019073486328\n",
      "Iteration 34590 Loss: 1.2598941326141357\n",
      "Iteration 34591 Loss: 1.0707013607025146\n",
      "Iteration 34592 Loss: 1.1919406652450562\n",
      "Iteration 34593 Loss: 0.9516783356666565\n",
      "Iteration 34594 Loss: 0.9268171191215515\n",
      "Iteration 34595 Loss: 0.491462379693985\n",
      "Iteration 34596 Loss: 1.2307727336883545\n",
      "Iteration 34597 Loss: 0.9224606156349182\n",
      "Iteration 34598 Loss: 0.786632239818573\n",
      "Iteration 34599 Loss: 1.110314965248108\n",
      "Iteration 34599 Loss: 0.994267463684082\n",
      "Iteration 34600 Loss: 1.1779356002807617\n",
      "Iteration 34601 Loss: 0.778239369392395\n",
      "Iteration 34602 Loss: 1.0206775665283203\n",
      "Iteration 34603 Loss: 0.8321055173873901\n",
      "Iteration 34604 Loss: 1.089931845664978\n",
      "Iteration 34605 Loss: 1.1195615530014038\n",
      "Iteration 34606 Loss: 1.176966905593872\n",
      "Iteration 34607 Loss: 1.0143884420394897\n",
      "Iteration 34608 Loss: 1.2076482772827148\n",
      "Iteration 34609 Loss: 1.1034388542175293\n",
      "Iteration 34609 Loss: 1.0520893335342407\n",
      "Iteration 34610 Loss: 0.8971518278121948\n",
      "Iteration 34611 Loss: 0.9045743346214294\n",
      "Iteration 34612 Loss: 0.8410550951957703\n",
      "Iteration 34613 Loss: 1.0646443367004395\n",
      "Iteration 34614 Loss: 1.0284498929977417\n",
      "Iteration 34615 Loss: 1.2204623222351074\n",
      "Iteration 34616 Loss: 1.1831626892089844\n",
      "Iteration 34617 Loss: 1.0213727951049805\n",
      "Iteration 34618 Loss: 0.9855579137802124\n",
      "Iteration 34619 Loss: 1.124503493309021\n",
      "Iteration 34619 Loss: 1.0270934104919434\n",
      "Iteration 34620 Loss: 1.2300546169281006\n",
      "Iteration 34621 Loss: 0.834851861000061\n",
      "Iteration 34622 Loss: 0.7874419689178467\n",
      "Iteration 34623 Loss: 1.0466150045394897\n",
      "Iteration 34624 Loss: 1.281058430671692\n",
      "Iteration 34625 Loss: 1.291067361831665\n",
      "Iteration 34626 Loss: 1.3432656526565552\n",
      "Iteration 34627 Loss: 1.1807284355163574\n",
      "Iteration 34628 Loss: 0.9097053408622742\n",
      "Iteration 34629 Loss: 1.169340968132019\n",
      "Iteration 34629 Loss: 1.1074129343032837\n",
      "Iteration 34630 Loss: 0.9522294402122498\n",
      "Iteration 34631 Loss: 0.8461509943008423\n",
      "Iteration 34632 Loss: 1.0311099290847778\n",
      "Iteration 34633 Loss: 0.8242955207824707\n",
      "Iteration 34634 Loss: 1.0086466073989868\n",
      "Iteration 34635 Loss: 1.1670275926589966\n",
      "Iteration 34636 Loss: 1.2527544498443604\n",
      "Iteration 34637 Loss: 1.2113642692565918\n",
      "Iteration 34638 Loss: 1.0051321983337402\n",
      "Iteration 34639 Loss: 0.6462787389755249\n",
      "Iteration 34639 Loss: 0.9944990277290344\n",
      "Iteration 34640 Loss: 0.9598587155342102\n",
      "Iteration 34641 Loss: 1.0954259634017944\n",
      "Iteration 34642 Loss: 1.0615289211273193\n",
      "Iteration 34643 Loss: 0.7563977241516113\n",
      "Iteration 34644 Loss: 1.1827776432037354\n",
      "Iteration 34645 Loss: 0.8272186517715454\n",
      "Iteration 34646 Loss: 0.6239262223243713\n",
      "Iteration 34647 Loss: 1.239305853843689\n",
      "Iteration 34648 Loss: 1.0413504838943481\n",
      "Iteration 34649 Loss: 0.9407346844673157\n",
      "Iteration 34649 Loss: 0.9728525280952454\n",
      "Iteration 34650 Loss: 1.2672511339187622\n",
      "Iteration 34651 Loss: 1.0796031951904297\n",
      "Iteration 34652 Loss: 1.0446127653121948\n",
      "Iteration 34653 Loss: 0.8525522947311401\n",
      "Iteration 34654 Loss: 0.7723745703697205\n",
      "Iteration 34655 Loss: 0.8042334914207458\n",
      "Iteration 34656 Loss: 1.0580058097839355\n",
      "Iteration 34657 Loss: 1.014664649963379\n",
      "Iteration 34658 Loss: 1.2467859983444214\n",
      "Iteration 34659 Loss: 0.9001136422157288\n",
      "Iteration 34659 Loss: 1.004019856452942\n",
      "Iteration 34660 Loss: 1.0077859163284302\n",
      "Iteration 34661 Loss: 0.884282112121582\n",
      "Iteration 34662 Loss: 1.1500115394592285\n",
      "Iteration 34663 Loss: 0.976193368434906\n",
      "Iteration 34664 Loss: 0.957688570022583\n",
      "Iteration 34665 Loss: 1.0383492708206177\n",
      "Iteration 34666 Loss: 1.1829839944839478\n",
      "Iteration 34667 Loss: 0.9272551536560059\n",
      "Iteration 34668 Loss: 0.9364789128303528\n",
      "Iteration 34669 Loss: 1.2069487571716309\n",
      "Iteration 34669 Loss: 1.026797890663147\n",
      "Iteration 34670 Loss: 1.034299612045288\n",
      "Iteration 34671 Loss: 0.9176610708236694\n",
      "Iteration 34672 Loss: 1.0336676836013794\n",
      "Iteration 34673 Loss: 1.0582743883132935\n",
      "Iteration 34674 Loss: 1.190917730331421\n",
      "Iteration 34675 Loss: 0.5892287492752075\n",
      "Iteration 34676 Loss: 0.9913195371627808\n",
      "Iteration 34677 Loss: 1.1567957401275635\n",
      "Iteration 34678 Loss: 0.8858532905578613\n",
      "Iteration 34679 Loss: 0.958441972732544\n",
      "Iteration 34679 Loss: 0.981645941734314\n",
      "Iteration 34680 Loss: 1.0268616676330566\n",
      "Iteration 34681 Loss: 0.5955994129180908\n",
      "Iteration 34682 Loss: 1.0349737405776978\n",
      "Iteration 34683 Loss: 1.0810129642486572\n",
      "Iteration 34684 Loss: 1.035286545753479\n",
      "Iteration 34685 Loss: 0.639877200126648\n",
      "Iteration 34686 Loss: 1.0023642778396606\n",
      "Iteration 34687 Loss: 1.148949384689331\n",
      "Iteration 34688 Loss: 0.954428493976593\n",
      "Iteration 34689 Loss: 0.9931566715240479\n",
      "Iteration 34689 Loss: 0.9512511491775513\n",
      "Iteration 34690 Loss: 0.8446366190910339\n",
      "Iteration 34691 Loss: 0.5968891978263855\n",
      "Iteration 34692 Loss: 1.035184621810913\n",
      "Iteration 34693 Loss: 1.070972204208374\n",
      "Iteration 34694 Loss: 0.9373692870140076\n",
      "Iteration 34695 Loss: 1.285217523574829\n",
      "Iteration 34696 Loss: 1.437431812286377\n",
      "Iteration 34697 Loss: 1.19948148727417\n",
      "Iteration 34698 Loss: 1.2832930088043213\n",
      "Iteration 34699 Loss: 0.867163360118866\n",
      "Iteration 34699 Loss: 1.0557639598846436\n",
      "Iteration 34700 Loss: 1.0564461946487427\n",
      "Iteration 34701 Loss: 1.6308702230453491\n",
      "Iteration 34702 Loss: 0.7632288336753845\n",
      "Iteration 34703 Loss: 1.3421881198883057\n",
      "Iteration 34704 Loss: 1.1135112047195435\n",
      "Iteration 34705 Loss: 0.9049306511878967\n",
      "Iteration 34706 Loss: 0.9459013938903809\n",
      "Iteration 34707 Loss: 0.8928813338279724\n",
      "Iteration 34708 Loss: 0.8859215378761292\n",
      "Iteration 34709 Loss: 0.9705275297164917\n",
      "Iteration 34709 Loss: 1.0506407022476196\n",
      "Iteration 34710 Loss: 1.211028814315796\n",
      "Iteration 34711 Loss: 0.9247400164604187\n",
      "Iteration 34712 Loss: 1.3740558624267578\n",
      "Iteration 34713 Loss: 1.362311601638794\n",
      "Iteration 34714 Loss: 0.9171502590179443\n",
      "Iteration 34715 Loss: 1.1522960662841797\n",
      "Iteration 34716 Loss: 1.3287793397903442\n",
      "Iteration 34717 Loss: 0.8051029443740845\n",
      "Iteration 34718 Loss: 0.9817879796028137\n",
      "Iteration 34719 Loss: 0.7518776655197144\n",
      "Iteration 34719 Loss: 1.0809130668640137\n",
      "Iteration 34720 Loss: 0.9904199838638306\n",
      "Iteration 34721 Loss: 0.8516336679458618\n",
      "Iteration 34722 Loss: 1.0647993087768555\n",
      "Iteration 34723 Loss: 0.6525254249572754\n",
      "Iteration 34724 Loss: 0.9491825699806213\n",
      "Iteration 34725 Loss: 0.902334451675415\n",
      "Iteration 34726 Loss: 0.9347942471504211\n",
      "Iteration 34727 Loss: 0.996239423751831\n",
      "Iteration 34728 Loss: 1.0375796556472778\n",
      "Iteration 34729 Loss: 0.8481060266494751\n",
      "Iteration 34729 Loss: 0.9227615594863892\n",
      "Iteration 34730 Loss: 0.6568620800971985\n",
      "Iteration 34731 Loss: 1.0917184352874756\n",
      "Iteration 34732 Loss: 0.9701710343360901\n",
      "Iteration 34733 Loss: 1.209789514541626\n",
      "Iteration 34734 Loss: 1.1039862632751465\n",
      "Iteration 34735 Loss: 0.9499869346618652\n",
      "Iteration 34736 Loss: 1.0512559413909912\n",
      "Iteration 34737 Loss: 1.0334246158599854\n",
      "Iteration 34738 Loss: 0.9515857696533203\n",
      "Iteration 34739 Loss: 0.7482375502586365\n",
      "Iteration 34739 Loss: 0.9767018556594849\n",
      "Iteration 34740 Loss: 0.8826464414596558\n",
      "Iteration 34741 Loss: 1.2365343570709229\n",
      "Iteration 34742 Loss: 0.8851336240768433\n",
      "Iteration 34743 Loss: 1.1724869012832642\n",
      "Iteration 34744 Loss: 0.7026373147964478\n",
      "Iteration 34745 Loss: 0.8288202285766602\n",
      "Iteration 34746 Loss: 0.9113323092460632\n",
      "Iteration 34747 Loss: 0.7388594746589661\n",
      "Iteration 34748 Loss: 0.7675814032554626\n",
      "Iteration 34749 Loss: 1.028226613998413\n",
      "Iteration 34749 Loss: 0.9154257774353027\n",
      "Iteration 34750 Loss: 1.0741498470306396\n",
      "Iteration 34751 Loss: 0.7596096992492676\n",
      "Iteration 34752 Loss: 1.2107139825820923\n",
      "Iteration 34753 Loss: 1.146462082862854\n",
      "Iteration 34754 Loss: 0.7510472536087036\n",
      "Iteration 34755 Loss: 1.1862678527832031\n",
      "Iteration 34756 Loss: 0.7899176478385925\n",
      "Iteration 34757 Loss: 1.155652642250061\n",
      "Iteration 34758 Loss: 1.5876586437225342\n",
      "Iteration 34759 Loss: 0.8070114254951477\n",
      "Iteration 34759 Loss: 1.0468491315841675\n",
      "Iteration 34760 Loss: 0.9928951263427734\n",
      "Iteration 34761 Loss: 0.8960272669792175\n",
      "Iteration 34762 Loss: 0.9715119004249573\n",
      "Iteration 34763 Loss: 1.0388247966766357\n",
      "Iteration 34764 Loss: 1.0148634910583496\n",
      "Iteration 34765 Loss: 1.3247380256652832\n",
      "Iteration 34766 Loss: 1.1327191591262817\n",
      "Iteration 34767 Loss: 0.831580400466919\n",
      "Iteration 34768 Loss: 1.087601661682129\n",
      "Iteration 34769 Loss: 1.2471883296966553\n",
      "Iteration 34769 Loss: 1.0537949800491333\n",
      "Iteration 34770 Loss: 0.8614614605903625\n",
      "Iteration 34771 Loss: 1.1322084665298462\n",
      "Iteration 34772 Loss: 1.1932411193847656\n",
      "Iteration 34773 Loss: 1.0731141567230225\n",
      "Iteration 34774 Loss: 1.0757269859313965\n",
      "Iteration 34775 Loss: 1.0038105249404907\n",
      "Iteration 34776 Loss: 0.8322205543518066\n",
      "Iteration 34777 Loss: 0.7656074166297913\n",
      "Iteration 34778 Loss: 1.0920467376708984\n",
      "Iteration 34779 Loss: 0.8781617879867554\n",
      "Iteration 34779 Loss: 0.9907600283622742\n",
      "Iteration 34780 Loss: 0.9187459945678711\n",
      "Iteration 34781 Loss: 0.8167684674263\n",
      "Iteration 34782 Loss: 0.9577270150184631\n",
      "Iteration 34783 Loss: 0.8507784605026245\n",
      "Iteration 34784 Loss: 0.9359902143478394\n",
      "Iteration 34785 Loss: 1.0581663846969604\n",
      "Iteration 34786 Loss: 1.1449599266052246\n",
      "Iteration 34787 Loss: 0.9384399056434631\n",
      "Iteration 34788 Loss: 0.9728634357452393\n",
      "Iteration 34789 Loss: 1.2359191179275513\n",
      "Iteration 34789 Loss: 0.9830360412597656\n",
      "Iteration 34790 Loss: 1.1601271629333496\n",
      "Iteration 34791 Loss: 0.994196355342865\n",
      "Iteration 34792 Loss: 1.10774827003479\n",
      "Iteration 34793 Loss: 1.4946304559707642\n",
      "Iteration 34794 Loss: 1.0019890069961548\n",
      "Iteration 34795 Loss: 0.8921146392822266\n",
      "Iteration 34796 Loss: 0.9959505200386047\n",
      "Iteration 34797 Loss: 1.0044834613800049\n",
      "Iteration 34798 Loss: 1.0971943140029907\n",
      "Iteration 34799 Loss: 0.6365642547607422\n",
      "Iteration 34799 Loss: 1.0384998321533203\n",
      "Iteration 34800 Loss: 0.6846678256988525\n",
      "Iteration 34801 Loss: 0.8766556978225708\n",
      "Iteration 34802 Loss: 1.3519412279129028\n",
      "Iteration 34803 Loss: 0.7106366753578186\n",
      "Iteration 34804 Loss: 1.1205133199691772\n",
      "Iteration 34805 Loss: 0.9972848892211914\n",
      "Iteration 34806 Loss: 0.921097457408905\n",
      "Iteration 34807 Loss: 0.9135568141937256\n",
      "Iteration 34808 Loss: 1.0406116247177124\n",
      "Iteration 34809 Loss: 1.1042286157608032\n",
      "Iteration 34809 Loss: 0.9721194505691528\n",
      "Iteration 34810 Loss: 0.7801524996757507\n",
      "Iteration 34811 Loss: 0.8867219090461731\n",
      "Iteration 34812 Loss: 0.7498302459716797\n",
      "Iteration 34813 Loss: 0.8296416997909546\n",
      "Iteration 34814 Loss: 1.2328550815582275\n",
      "Iteration 34815 Loss: 1.185166835784912\n",
      "Iteration 34816 Loss: 1.1394567489624023\n",
      "Iteration 34817 Loss: 0.9674410820007324\n",
      "Iteration 34818 Loss: 0.901170551776886\n",
      "Iteration 34819 Loss: 1.0746550559997559\n",
      "Iteration 34819 Loss: 0.9747093319892883\n",
      "Iteration 34820 Loss: 0.7656528353691101\n",
      "Iteration 34821 Loss: 1.1724460124969482\n",
      "Iteration 34822 Loss: 1.2500568628311157\n",
      "Iteration 34823 Loss: 0.9978708624839783\n",
      "Iteration 34824 Loss: 1.4528982639312744\n",
      "Iteration 34825 Loss: 0.4532136023044586\n",
      "Iteration 34826 Loss: 1.2792187929153442\n",
      "Iteration 34827 Loss: 1.0721532106399536\n",
      "Iteration 34828 Loss: 1.0178728103637695\n",
      "Iteration 34829 Loss: 1.0765300989151\n",
      "Iteration 34829 Loss: 1.0537912845611572\n",
      "Iteration 34830 Loss: 1.1120214462280273\n",
      "Iteration 34831 Loss: 0.7877044081687927\n",
      "Iteration 34832 Loss: 1.1898908615112305\n",
      "Iteration 34833 Loss: 0.9882887601852417\n",
      "Iteration 34834 Loss: 1.1983904838562012\n",
      "Iteration 34835 Loss: 0.9305262565612793\n",
      "Iteration 34836 Loss: 1.1565961837768555\n",
      "Iteration 34837 Loss: 1.0270313024520874\n",
      "Iteration 34838 Loss: 0.9617482423782349\n",
      "Iteration 34839 Loss: 0.8018465638160706\n",
      "Iteration 34839 Loss: 1.015404462814331\n",
      "Iteration 34840 Loss: 0.7496325373649597\n",
      "Iteration 34841 Loss: 1.2432962656021118\n",
      "Iteration 34842 Loss: 0.9204617142677307\n",
      "Iteration 34843 Loss: 0.9738714098930359\n",
      "Iteration 34844 Loss: 0.956601619720459\n",
      "Iteration 34845 Loss: 0.8322249054908752\n",
      "Iteration 34846 Loss: 0.860468864440918\n",
      "Iteration 34847 Loss: 1.1653170585632324\n",
      "Iteration 34848 Loss: 0.6866425275802612\n",
      "Iteration 34849 Loss: 1.1503698825836182\n",
      "Iteration 34849 Loss: 0.9538887143135071\n",
      "Iteration 34850 Loss: 1.0521535873413086\n",
      "Iteration 34851 Loss: 0.9969398975372314\n",
      "Iteration 34852 Loss: 1.4482741355895996\n",
      "Iteration 34853 Loss: 0.8011574745178223\n",
      "Iteration 34854 Loss: 0.9221818447113037\n",
      "Iteration 34855 Loss: 0.8770533204078674\n",
      "Iteration 34856 Loss: 1.1021125316619873\n",
      "Iteration 34857 Loss: 0.9204994440078735\n",
      "Iteration 34858 Loss: 1.0971101522445679\n",
      "Iteration 34859 Loss: 0.9536038637161255\n",
      "Iteration 34859 Loss: 1.017108678817749\n",
      "Iteration 34860 Loss: 0.9839075803756714\n",
      "Iteration 34861 Loss: 0.6939226388931274\n",
      "Iteration 34862 Loss: 0.8735994100570679\n",
      "Iteration 34863 Loss: 0.7836260199546814\n",
      "Iteration 34864 Loss: 0.7465368509292603\n",
      "Iteration 34865 Loss: 1.2336066961288452\n",
      "Iteration 34866 Loss: 1.0172126293182373\n",
      "Iteration 34867 Loss: 1.2751365900039673\n",
      "Iteration 34868 Loss: 1.1777971982955933\n",
      "Iteration 34869 Loss: 0.7241374850273132\n",
      "Iteration 34869 Loss: 0.9509483575820923\n",
      "Iteration 34870 Loss: 0.8681977391242981\n",
      "Iteration 34871 Loss: 1.0661206245422363\n",
      "Iteration 34872 Loss: 1.053188443183899\n",
      "Iteration 34873 Loss: 0.8231663107872009\n",
      "Iteration 34874 Loss: 0.8712610006332397\n",
      "Iteration 34875 Loss: 0.9051962494850159\n",
      "Iteration 34876 Loss: 1.1337971687316895\n",
      "Iteration 34877 Loss: 1.1810698509216309\n",
      "Iteration 34878 Loss: 1.2043594121932983\n",
      "Iteration 34879 Loss: 0.9254931807518005\n",
      "Iteration 34879 Loss: 1.0031850337982178\n",
      "Iteration 34880 Loss: 1.1894569396972656\n",
      "Iteration 34881 Loss: 1.0766462087631226\n",
      "Iteration 34882 Loss: 0.9583725929260254\n",
      "Iteration 34883 Loss: 0.8145309090614319\n",
      "Iteration 34884 Loss: 0.6738064289093018\n",
      "Iteration 34885 Loss: 1.1308239698410034\n",
      "Iteration 34886 Loss: 0.7850042581558228\n",
      "Iteration 34887 Loss: 0.8310120701789856\n",
      "Iteration 34888 Loss: 0.9491837620735168\n",
      "Iteration 34889 Loss: 0.9397977590560913\n",
      "Iteration 34889 Loss: 0.9348634481430054\n",
      "Iteration 34890 Loss: 1.0484718084335327\n",
      "Iteration 34891 Loss: 1.1722086668014526\n",
      "Iteration 34892 Loss: 0.8340983986854553\n",
      "Iteration 34893 Loss: 1.356339931488037\n",
      "Iteration 34894 Loss: 0.9946374297142029\n",
      "Iteration 34895 Loss: 1.0132784843444824\n",
      "Iteration 34896 Loss: 0.7162908911705017\n",
      "Iteration 34897 Loss: 1.0075291395187378\n",
      "Iteration 34898 Loss: 1.0527561902999878\n",
      "Iteration 34899 Loss: 0.6709151268005371\n",
      "Iteration 34899 Loss: 0.9866525530815125\n",
      "Iteration 34900 Loss: 1.0930441617965698\n",
      "Iteration 34901 Loss: 0.9818020462989807\n",
      "Iteration 34902 Loss: 1.1023645401000977\n",
      "Iteration 34903 Loss: 1.1989744901657104\n",
      "Iteration 34904 Loss: 1.2803783416748047\n",
      "Iteration 34905 Loss: 1.1001145839691162\n",
      "Iteration 34906 Loss: 0.979913592338562\n",
      "Iteration 34907 Loss: 0.9047802686691284\n",
      "Iteration 34908 Loss: 0.8486356735229492\n",
      "Iteration 34909 Loss: 0.6740366220474243\n",
      "Iteration 34909 Loss: 1.016404390335083\n",
      "Iteration 34910 Loss: 1.0821772813796997\n",
      "Iteration 34911 Loss: 0.7484255433082581\n",
      "Iteration 34912 Loss: 1.0013765096664429\n",
      "Iteration 34913 Loss: 0.9716663956642151\n",
      "Iteration 34914 Loss: 1.115729808807373\n",
      "Iteration 34915 Loss: 0.9362123608589172\n",
      "Iteration 34916 Loss: 1.1886907815933228\n",
      "Iteration 34917 Loss: 1.1218373775482178\n",
      "Iteration 34918 Loss: 1.0654420852661133\n",
      "Iteration 34919 Loss: 0.7748079299926758\n",
      "Iteration 34919 Loss: 1.0006366968154907\n",
      "Iteration 34920 Loss: 0.9274975061416626\n",
      "Iteration 34921 Loss: 0.9527664184570312\n",
      "Iteration 34922 Loss: 1.1867499351501465\n",
      "Iteration 34923 Loss: 0.9793030619621277\n",
      "Iteration 34924 Loss: 0.9055418968200684\n",
      "Iteration 34925 Loss: 0.8024908900260925\n",
      "Iteration 34926 Loss: 0.9737637639045715\n",
      "Iteration 34927 Loss: 1.0428390502929688\n",
      "Iteration 34928 Loss: 1.2364760637283325\n",
      "Iteration 34929 Loss: 0.9185706377029419\n",
      "Iteration 34929 Loss: 0.9925998449325562\n",
      "Iteration 34930 Loss: 0.7701227068901062\n",
      "Iteration 34931 Loss: 1.216385841369629\n",
      "Iteration 34932 Loss: 1.0671360492706299\n",
      "Iteration 34933 Loss: 0.9088268280029297\n",
      "Iteration 34934 Loss: 0.8032101988792419\n",
      "Iteration 34935 Loss: 1.5469276905059814\n",
      "Iteration 34936 Loss: 0.9219180345535278\n",
      "Iteration 34937 Loss: 0.8670885562896729\n",
      "Iteration 34938 Loss: 1.6029925346374512\n",
      "Iteration 34939 Loss: 1.0938044786453247\n",
      "Iteration 34939 Loss: 1.0798412561416626\n",
      "Iteration 34940 Loss: 0.948989748954773\n",
      "Iteration 34941 Loss: 1.152957797050476\n",
      "Iteration 34942 Loss: 0.8736674189567566\n",
      "Iteration 34943 Loss: 0.9697911739349365\n",
      "Iteration 34944 Loss: 1.2497153282165527\n",
      "Iteration 34945 Loss: 1.0074516534805298\n",
      "Iteration 34946 Loss: 0.9523661136627197\n",
      "Iteration 34947 Loss: 1.2029385566711426\n",
      "Iteration 34948 Loss: 1.0853198766708374\n",
      "Iteration 34949 Loss: 0.9324324727058411\n",
      "Iteration 34949 Loss: 1.0375629663467407\n",
      "Iteration 34950 Loss: 0.9169406294822693\n",
      "Iteration 34951 Loss: 0.8744089007377625\n",
      "Iteration 34952 Loss: 1.0040647983551025\n",
      "Iteration 34953 Loss: 1.0708515644073486\n",
      "Iteration 34954 Loss: 0.8859297633171082\n",
      "Iteration 34955 Loss: 0.9166186451911926\n",
      "Iteration 34956 Loss: 1.1047941446304321\n",
      "Iteration 34957 Loss: 1.081637978553772\n",
      "Iteration 34958 Loss: 1.0667763948440552\n",
      "Iteration 34959 Loss: 1.243261456489563\n",
      "Iteration 34959 Loss: 1.0165284872055054\n",
      "Iteration 34960 Loss: 1.1605479717254639\n",
      "Iteration 34961 Loss: 1.161752462387085\n",
      "Iteration 34962 Loss: 0.929105818271637\n",
      "Iteration 34963 Loss: 1.004359245300293\n",
      "Iteration 34964 Loss: 1.132277488708496\n",
      "Iteration 34965 Loss: 1.309923529624939\n",
      "Iteration 34966 Loss: 1.0269819498062134\n",
      "Iteration 34967 Loss: 0.9219655990600586\n",
      "Iteration 34968 Loss: 0.703541100025177\n",
      "Iteration 34969 Loss: 0.9867112636566162\n",
      "Iteration 34969 Loss: 1.0337166786193848\n",
      "Iteration 34970 Loss: 0.9522138237953186\n",
      "Iteration 34971 Loss: 1.0428797006607056\n",
      "Iteration 34972 Loss: 0.6696136593818665\n",
      "Iteration 34973 Loss: 0.9590016007423401\n",
      "Iteration 34974 Loss: 0.7753406763076782\n",
      "Iteration 34975 Loss: 1.0472277402877808\n",
      "Iteration 34976 Loss: 0.9443759322166443\n",
      "Iteration 34977 Loss: 1.0424383878707886\n",
      "Iteration 34978 Loss: 0.6514950394630432\n",
      "Iteration 34979 Loss: 1.3023056983947754\n",
      "Iteration 34979 Loss: 0.9386892318725586\n",
      "Iteration 34980 Loss: 0.9649167060852051\n",
      "Iteration 34981 Loss: 0.7648322582244873\n",
      "Iteration 34982 Loss: 1.1645898818969727\n",
      "Iteration 34983 Loss: 1.0109052658081055\n",
      "Iteration 34984 Loss: 0.9022735357284546\n",
      "Iteration 34985 Loss: 0.6752519011497498\n",
      "Iteration 34986 Loss: 0.9014772176742554\n",
      "Iteration 34987 Loss: 1.0059009790420532\n",
      "Iteration 34988 Loss: 1.1121678352355957\n",
      "Iteration 34989 Loss: 0.7784717679023743\n",
      "Iteration 34989 Loss: 0.9280787706375122\n",
      "Iteration 34990 Loss: 0.9544012546539307\n",
      "Iteration 34991 Loss: 0.9638740420341492\n",
      "Iteration 34992 Loss: 1.0021932125091553\n",
      "Iteration 34993 Loss: 1.1090754270553589\n",
      "Iteration 34994 Loss: 0.8499743938446045\n",
      "Iteration 34995 Loss: 1.0363867282867432\n",
      "Iteration 34996 Loss: 1.0995311737060547\n",
      "Iteration 34997 Loss: 0.6200945973396301\n",
      "Iteration 34998 Loss: 1.23898184299469\n",
      "Iteration 34999 Loss: 1.039628505706787\n",
      "Iteration 34999 Loss: 0.9914140701293945\n",
      "Iteration 35000 Loss: 1.1200644969940186\n",
      "Iteration 35001 Loss: 0.5445767641067505\n",
      "Iteration 35002 Loss: 0.9210992455482483\n",
      "Iteration 35003 Loss: 0.8189259767532349\n",
      "Iteration 35004 Loss: 1.1971962451934814\n",
      "Iteration 35005 Loss: 0.9180456399917603\n",
      "Iteration 35006 Loss: 1.0902960300445557\n",
      "Iteration 35007 Loss: 0.994584858417511\n",
      "Iteration 35008 Loss: 1.0813179016113281\n",
      "Iteration 35009 Loss: 1.0542505979537964\n",
      "Iteration 35009 Loss: 0.9740358591079712\n",
      "Iteration 35010 Loss: 1.0809279680252075\n",
      "Iteration 35011 Loss: 1.1226720809936523\n",
      "Iteration 35012 Loss: 1.2594925165176392\n",
      "Iteration 35013 Loss: 1.0856560468673706\n",
      "Iteration 35014 Loss: 1.087969422340393\n",
      "Iteration 35015 Loss: 1.019755244255066\n",
      "Iteration 35016 Loss: 0.9423802495002747\n",
      "Iteration 35017 Loss: 1.0440666675567627\n",
      "Iteration 35018 Loss: 0.8825970888137817\n",
      "Iteration 35019 Loss: 1.148735523223877\n",
      "Iteration 35019 Loss: 1.0674251317977905\n",
      "Iteration 35020 Loss: 1.1513606309890747\n",
      "Iteration 35021 Loss: 0.9397547841072083\n",
      "Iteration 35022 Loss: 0.8582319617271423\n",
      "Iteration 35023 Loss: 0.9138756394386292\n",
      "Iteration 35024 Loss: 1.3452951908111572\n",
      "Iteration 35025 Loss: 0.9560311436653137\n",
      "Iteration 35026 Loss: 1.0767674446105957\n",
      "Iteration 35027 Loss: 0.9954213500022888\n",
      "Iteration 35028 Loss: 1.065966248512268\n",
      "Iteration 35029 Loss: 0.7754247188568115\n",
      "Iteration 35029 Loss: 1.007812738418579\n",
      "Iteration 35030 Loss: 1.0288039445877075\n",
      "Iteration 35031 Loss: 1.028472900390625\n",
      "Iteration 35032 Loss: 0.9938098192214966\n",
      "Iteration 35033 Loss: 1.4211137294769287\n",
      "Iteration 35034 Loss: 0.9028916954994202\n",
      "Iteration 35035 Loss: 0.8887370228767395\n",
      "Iteration 35036 Loss: 0.9824106097221375\n",
      "Iteration 35037 Loss: 1.2344634532928467\n",
      "Iteration 35038 Loss: 0.6835361123085022\n",
      "Iteration 35039 Loss: 0.9842085242271423\n",
      "Iteration 35039 Loss: 1.0148447751998901\n",
      "Iteration 35040 Loss: 0.6673368215560913\n",
      "Iteration 35041 Loss: 0.8744289875030518\n",
      "Iteration 35042 Loss: 1.251019835472107\n",
      "Iteration 35043 Loss: 0.7292760610580444\n",
      "Iteration 35044 Loss: 0.789678156375885\n",
      "Iteration 35045 Loss: 1.2109315395355225\n",
      "Iteration 35046 Loss: 0.9230825304985046\n",
      "Iteration 35047 Loss: 1.0573034286499023\n",
      "Iteration 35048 Loss: 0.9862791895866394\n",
      "Iteration 35049 Loss: 0.9894663691520691\n",
      "Iteration 35049 Loss: 0.9478802680969238\n",
      "Iteration 35050 Loss: 1.0016881227493286\n",
      "Iteration 35051 Loss: 1.0592767000198364\n",
      "Iteration 35052 Loss: 0.8171042799949646\n",
      "Iteration 35053 Loss: 0.9907530546188354\n",
      "Iteration 35054 Loss: 1.0289571285247803\n",
      "Iteration 35055 Loss: 1.060471773147583\n",
      "Iteration 35056 Loss: 1.0854198932647705\n",
      "Iteration 35057 Loss: 0.9618885517120361\n",
      "Iteration 35058 Loss: 1.0258779525756836\n",
      "Iteration 35059 Loss: 0.9863027930259705\n",
      "Iteration 35059 Loss: 1.0017739534378052\n",
      "Iteration 35060 Loss: 0.8130063414573669\n",
      "Iteration 35061 Loss: 1.0633411407470703\n",
      "Iteration 35062 Loss: 0.7779580950737\n",
      "Iteration 35063 Loss: 1.0352627038955688\n",
      "Iteration 35064 Loss: 1.3600348234176636\n",
      "Iteration 35065 Loss: 0.9769248366355896\n",
      "Iteration 35066 Loss: 1.1012685298919678\n",
      "Iteration 35067 Loss: 1.1121410131454468\n",
      "Iteration 35068 Loss: 0.5324631929397583\n",
      "Iteration 35069 Loss: 0.6291409730911255\n",
      "Iteration 35069 Loss: 0.9401541948318481\n",
      "Iteration 35070 Loss: 1.0807690620422363\n",
      "Iteration 35071 Loss: 0.8922339081764221\n",
      "Iteration 35072 Loss: 1.1858454942703247\n",
      "Iteration 35073 Loss: 1.063486099243164\n",
      "Iteration 35074 Loss: 0.9951586723327637\n",
      "Iteration 35075 Loss: 1.3028011322021484\n",
      "Iteration 35076 Loss: 0.663022518157959\n",
      "Iteration 35077 Loss: 0.9391598105430603\n",
      "Iteration 35078 Loss: 0.9557384848594666\n",
      "Iteration 35079 Loss: 1.0327242612838745\n",
      "Iteration 35079 Loss: 1.0110938549041748\n",
      "Iteration 35080 Loss: 0.9737950563430786\n",
      "Iteration 35081 Loss: 1.3073348999023438\n",
      "Iteration 35082 Loss: 0.8621059656143188\n",
      "Iteration 35083 Loss: 0.9878396391868591\n",
      "Iteration 35084 Loss: 0.8692548871040344\n",
      "Iteration 35085 Loss: 0.9790081977844238\n",
      "Iteration 35086 Loss: 0.88233482837677\n",
      "Iteration 35087 Loss: 0.9066696166992188\n",
      "Iteration 35088 Loss: 0.922553539276123\n",
      "Iteration 35089 Loss: 1.0902595520019531\n",
      "Iteration 35089 Loss: 0.9781156778335571\n",
      "Iteration 35090 Loss: 1.0688762664794922\n",
      "Iteration 35091 Loss: 1.0035524368286133\n",
      "Iteration 35092 Loss: 0.7253305912017822\n",
      "Iteration 35093 Loss: 1.0913617610931396\n",
      "Iteration 35094 Loss: 1.1736162900924683\n",
      "Iteration 35095 Loss: 1.0542519092559814\n",
      "Iteration 35096 Loss: 1.1809675693511963\n",
      "Iteration 35097 Loss: 1.0664716958999634\n",
      "Iteration 35098 Loss: 1.1762162446975708\n",
      "Iteration 35099 Loss: 0.7733951210975647\n",
      "Iteration 35099 Loss: 1.0314040184020996\n",
      "Iteration 35100 Loss: 1.2838642597198486\n",
      "Iteration 35101 Loss: 0.8990928530693054\n",
      "Iteration 35102 Loss: 1.0924545526504517\n",
      "Iteration 35103 Loss: 0.9818066954612732\n",
      "Iteration 35104 Loss: 1.0676825046539307\n",
      "Iteration 35105 Loss: 0.9621238708496094\n",
      "Iteration 35106 Loss: 0.7631435990333557\n",
      "Iteration 35107 Loss: 1.4334990978240967\n",
      "Iteration 35108 Loss: 0.6863386631011963\n",
      "Iteration 35109 Loss: 0.969632089138031\n",
      "Iteration 35109 Loss: 1.0139639377593994\n",
      "Iteration 35110 Loss: 0.9105373620986938\n",
      "Iteration 35111 Loss: 0.9462217092514038\n",
      "Iteration 35112 Loss: 0.878226637840271\n",
      "Iteration 35113 Loss: 1.035942792892456\n",
      "Iteration 35114 Loss: 1.1285808086395264\n",
      "Iteration 35115 Loss: 1.0455857515335083\n",
      "Iteration 35116 Loss: 0.9294536709785461\n",
      "Iteration 35117 Loss: 1.0244057178497314\n",
      "Iteration 35118 Loss: 0.580234706401825\n",
      "Iteration 35119 Loss: 1.1966356039047241\n",
      "Iteration 35119 Loss: 0.9675825238227844\n",
      "Iteration 35120 Loss: 0.8710948824882507\n",
      "Iteration 35121 Loss: 1.1198959350585938\n",
      "Iteration 35122 Loss: 1.1708735227584839\n",
      "Iteration 35123 Loss: 1.0601589679718018\n",
      "Iteration 35124 Loss: 0.7552960515022278\n",
      "Iteration 35125 Loss: 1.2775135040283203\n",
      "Iteration 35126 Loss: 0.9819912910461426\n",
      "Iteration 35127 Loss: 1.1986584663391113\n",
      "Iteration 35128 Loss: 1.0253783464431763\n",
      "Iteration 35129 Loss: 0.8859020471572876\n",
      "Iteration 35129 Loss: 1.034676194190979\n",
      "Iteration 35130 Loss: 0.9500565528869629\n",
      "Iteration 35131 Loss: 1.259657621383667\n",
      "Iteration 35132 Loss: 0.9175372123718262\n",
      "Iteration 35133 Loss: 1.3624306917190552\n",
      "Iteration 35134 Loss: 1.041980266571045\n",
      "Iteration 35135 Loss: 1.0956110954284668\n",
      "Iteration 35136 Loss: 1.113715410232544\n",
      "Iteration 35137 Loss: 0.843616783618927\n",
      "Iteration 35138 Loss: 0.800423800945282\n",
      "Iteration 35139 Loss: 1.0070366859436035\n",
      "Iteration 35139 Loss: 1.039206624031067\n",
      "Iteration 35140 Loss: 0.925966739654541\n",
      "Iteration 35141 Loss: 1.5259004831314087\n",
      "Iteration 35142 Loss: 1.0741416215896606\n",
      "Iteration 35143 Loss: 1.1312485933303833\n",
      "Iteration 35144 Loss: 1.1805375814437866\n",
      "Iteration 35145 Loss: 1.1198527812957764\n",
      "Iteration 35146 Loss: 1.0031541585922241\n",
      "Iteration 35147 Loss: 0.6207968592643738\n",
      "Iteration 35148 Loss: 1.0470765829086304\n",
      "Iteration 35149 Loss: 1.1520062685012817\n",
      "Iteration 35149 Loss: 1.0780681371688843\n",
      "Iteration 35150 Loss: 1.0024365186691284\n",
      "Iteration 35151 Loss: 1.161281704902649\n",
      "Iteration 35152 Loss: 1.0716776847839355\n",
      "Iteration 35153 Loss: 0.8408315181732178\n",
      "Iteration 35154 Loss: 0.9248985648155212\n",
      "Iteration 35155 Loss: 0.9893229603767395\n",
      "Iteration 35156 Loss: 1.0210273265838623\n",
      "Iteration 35157 Loss: 0.9107263088226318\n",
      "Iteration 35158 Loss: 0.7128953337669373\n",
      "Iteration 35159 Loss: 0.7525145411491394\n",
      "Iteration 35159 Loss: 0.9387613534927368\n",
      "Iteration 35160 Loss: 1.0544581413269043\n",
      "Iteration 35161 Loss: 0.4844847619533539\n",
      "Iteration 35162 Loss: 1.2678426504135132\n",
      "Iteration 35163 Loss: 1.1437833309173584\n",
      "Iteration 35164 Loss: 1.1492866277694702\n",
      "Iteration 35165 Loss: 0.874543309211731\n",
      "Iteration 35166 Loss: 1.1623256206512451\n",
      "Iteration 35167 Loss: 1.2936776876449585\n",
      "Iteration 35168 Loss: 0.8105838298797607\n",
      "Iteration 35169 Loss: 1.087630033493042\n",
      "Iteration 35169 Loss: 1.0328614711761475\n",
      "Iteration 35170 Loss: 1.2569957971572876\n",
      "Iteration 35171 Loss: 0.8631277680397034\n",
      "Iteration 35172 Loss: 1.0452172756195068\n",
      "Iteration 35173 Loss: 1.013869047164917\n",
      "Iteration 35174 Loss: 0.8601012825965881\n",
      "Iteration 35175 Loss: 0.9380991458892822\n",
      "Iteration 35176 Loss: 1.0580443143844604\n",
      "Iteration 35177 Loss: 0.6936190724372864\n",
      "Iteration 35178 Loss: 0.6584941744804382\n",
      "Iteration 35179 Loss: 0.9186896681785583\n",
      "Iteration 35179 Loss: 0.9306257367134094\n",
      "Iteration 35180 Loss: 1.0112590789794922\n",
      "Iteration 35181 Loss: 1.0664260387420654\n",
      "Iteration 35182 Loss: 0.888857901096344\n",
      "Iteration 35183 Loss: 1.0916770696640015\n",
      "Iteration 35184 Loss: 0.9158172607421875\n",
      "Iteration 35185 Loss: 1.0570999383926392\n",
      "Iteration 35186 Loss: 0.8223191499710083\n",
      "Iteration 35187 Loss: 1.0347630977630615\n",
      "Iteration 35188 Loss: 0.9178582429885864\n",
      "Iteration 35189 Loss: 1.0083245038986206\n",
      "Iteration 35189 Loss: 0.9814402461051941\n",
      "Iteration 35190 Loss: 0.8198613524436951\n",
      "Iteration 35191 Loss: 0.7070976495742798\n",
      "Iteration 35192 Loss: 0.8257238864898682\n",
      "Iteration 35193 Loss: 1.0031684637069702\n",
      "Iteration 35194 Loss: 1.2825394868850708\n",
      "Iteration 35195 Loss: 1.0385558605194092\n",
      "Iteration 35196 Loss: 0.9089933633804321\n",
      "Iteration 35197 Loss: 1.499713659286499\n",
      "Iteration 35198 Loss: 1.0687084197998047\n",
      "Iteration 35199 Loss: 0.9828523993492126\n",
      "Iteration 35199 Loss: 1.0137214660644531\n",
      "Iteration 35200 Loss: 1.2180553674697876\n",
      "Iteration 35201 Loss: 1.062873363494873\n",
      "Iteration 35202 Loss: 0.9715158343315125\n",
      "Iteration 35203 Loss: 1.0535099506378174\n",
      "Iteration 35204 Loss: 0.9471596479415894\n",
      "Iteration 35205 Loss: 1.4279100894927979\n",
      "Iteration 35206 Loss: 0.8866366744041443\n",
      "Iteration 35207 Loss: 1.3491379022598267\n",
      "Iteration 35208 Loss: 0.8011099100112915\n",
      "Iteration 35209 Loss: 1.0498133897781372\n",
      "Iteration 35209 Loss: 1.0767722129821777\n",
      "Iteration 35210 Loss: 0.7894269227981567\n",
      "Iteration 35211 Loss: 0.8497307896614075\n",
      "Iteration 35212 Loss: 1.2305548191070557\n",
      "Iteration 35213 Loss: 1.0716803073883057\n",
      "Iteration 35214 Loss: 1.0482416152954102\n",
      "Iteration 35215 Loss: 0.7338882088661194\n",
      "Iteration 35216 Loss: 0.7769251465797424\n",
      "Iteration 35217 Loss: 0.8323025107383728\n",
      "Iteration 35218 Loss: 1.105810523033142\n",
      "Iteration 35219 Loss: 0.895863950252533\n",
      "Iteration 35219 Loss: 0.9334424734115601\n",
      "Iteration 35220 Loss: 0.7081080079078674\n",
      "Iteration 35221 Loss: 1.014387607574463\n",
      "Iteration 35222 Loss: 0.9740515351295471\n",
      "Iteration 35223 Loss: 1.020633578300476\n",
      "Iteration 35224 Loss: 0.9676957726478577\n",
      "Iteration 35225 Loss: 1.2547086477279663\n",
      "Iteration 35226 Loss: 0.9546796083450317\n",
      "Iteration 35227 Loss: 0.943085789680481\n",
      "Iteration 35228 Loss: 1.0408258438110352\n",
      "Iteration 35229 Loss: 0.9403753280639648\n",
      "Iteration 35229 Loss: 0.9818552136421204\n",
      "Iteration 35230 Loss: 1.0846079587936401\n",
      "Iteration 35231 Loss: 0.7014651894569397\n",
      "Iteration 35232 Loss: 0.8244629502296448\n",
      "Iteration 35233 Loss: 1.10171639919281\n",
      "Iteration 35234 Loss: 0.6613720059394836\n",
      "Iteration 35235 Loss: 0.7965137958526611\n",
      "Iteration 35236 Loss: 0.9580754637718201\n",
      "Iteration 35237 Loss: 0.9473592638969421\n",
      "Iteration 35238 Loss: 1.3088245391845703\n",
      "Iteration 35239 Loss: 0.9286229610443115\n",
      "Iteration 35239 Loss: 0.9313020706176758\n",
      "Iteration 35240 Loss: 0.7173495888710022\n",
      "Iteration 35241 Loss: 0.9534129500389099\n",
      "Iteration 35242 Loss: 0.957342803478241\n",
      "Iteration 35243 Loss: 1.0805200338363647\n",
      "Iteration 35244 Loss: 0.8300316333770752\n",
      "Iteration 35245 Loss: 0.6299343705177307\n",
      "Iteration 35246 Loss: 1.1390957832336426\n",
      "Iteration 35247 Loss: 1.1881518363952637\n",
      "Iteration 35248 Loss: 1.010290265083313\n",
      "Iteration 35249 Loss: 0.8489823341369629\n",
      "Iteration 35249 Loss: 0.9355112314224243\n",
      "Iteration 35250 Loss: 0.9019322395324707\n",
      "Iteration 35251 Loss: 1.0359729528427124\n",
      "Iteration 35252 Loss: 1.0744603872299194\n",
      "Iteration 35253 Loss: 0.7378078103065491\n",
      "Iteration 35254 Loss: 1.3126641511917114\n",
      "Iteration 35255 Loss: 0.8616176247596741\n",
      "Iteration 35256 Loss: 0.9308032989501953\n",
      "Iteration 35257 Loss: 0.741990327835083\n",
      "Iteration 35258 Loss: 0.7193257212638855\n",
      "Iteration 35259 Loss: 0.7149872183799744\n",
      "Iteration 35259 Loss: 0.9031561613082886\n",
      "Iteration 35260 Loss: 1.1370526552200317\n",
      "Iteration 35261 Loss: 1.0277451276779175\n",
      "Iteration 35262 Loss: 0.9881525635719299\n",
      "Iteration 35263 Loss: 0.7562815546989441\n",
      "Iteration 35264 Loss: 0.7663143873214722\n",
      "Iteration 35265 Loss: 0.8688275814056396\n",
      "Iteration 35266 Loss: 0.666500985622406\n",
      "Iteration 35267 Loss: 1.074081540107727\n",
      "Iteration 35268 Loss: 0.7600326538085938\n",
      "Iteration 35269 Loss: 0.9645757675170898\n",
      "Iteration 35269 Loss: 0.9009564518928528\n",
      "Iteration 35270 Loss: 0.9724159240722656\n",
      "Iteration 35271 Loss: 1.1006336212158203\n",
      "Iteration 35272 Loss: 1.0018595457077026\n",
      "Iteration 35273 Loss: 1.0987975597381592\n",
      "Iteration 35274 Loss: 1.1283605098724365\n",
      "Iteration 35275 Loss: 1.216101050376892\n",
      "Iteration 35276 Loss: 1.10088312625885\n",
      "Iteration 35277 Loss: 1.2312026023864746\n",
      "Iteration 35278 Loss: 0.9439723491668701\n",
      "Iteration 35279 Loss: 0.9116842150688171\n",
      "Iteration 35279 Loss: 1.0705912113189697\n",
      "Iteration 35280 Loss: 0.8759471774101257\n",
      "Iteration 35281 Loss: 0.9868846535682678\n",
      "Iteration 35282 Loss: 0.8945096731185913\n",
      "Iteration 35283 Loss: 1.3101829290390015\n",
      "Iteration 35284 Loss: 0.9247365593910217\n",
      "Iteration 35285 Loss: 1.1005905866622925\n",
      "Iteration 35286 Loss: 0.9175306558609009\n",
      "Iteration 35287 Loss: 0.8798658847808838\n",
      "Iteration 35288 Loss: 1.0602480173110962\n",
      "Iteration 35289 Loss: 1.08550226688385\n",
      "Iteration 35289 Loss: 1.0035998821258545\n",
      "Iteration 35290 Loss: 0.8341941833496094\n",
      "Iteration 35291 Loss: 0.7302162647247314\n",
      "Iteration 35292 Loss: 0.8334192037582397\n",
      "Iteration 35293 Loss: 1.1670572757720947\n",
      "Iteration 35294 Loss: 0.842846691608429\n",
      "Iteration 35295 Loss: 0.8916962742805481\n",
      "Iteration 35296 Loss: 0.9297515153884888\n",
      "Iteration 35297 Loss: 0.8527105450630188\n",
      "Iteration 35298 Loss: 1.2486932277679443\n",
      "Iteration 35299 Loss: 0.8009471893310547\n",
      "Iteration 35299 Loss: 0.913153350353241\n",
      "Iteration 35300 Loss: 1.0575259923934937\n",
      "Iteration 35301 Loss: 0.9463757276535034\n",
      "Iteration 35302 Loss: 1.116076946258545\n",
      "Iteration 35303 Loss: 0.8031929135322571\n",
      "Iteration 35304 Loss: 0.6499181985855103\n",
      "Iteration 35305 Loss: 0.7496821880340576\n",
      "Iteration 35306 Loss: 1.0271226167678833\n",
      "Iteration 35307 Loss: 0.7770756483078003\n",
      "Iteration 35308 Loss: 0.8906317353248596\n",
      "Iteration 35309 Loss: 0.48162415623664856\n",
      "Iteration 35309 Loss: 0.8499226570129395\n",
      "Iteration 35310 Loss: 1.0187824964523315\n",
      "Iteration 35311 Loss: 1.0552045106887817\n",
      "Iteration 35312 Loss: 1.4998847246170044\n",
      "Iteration 35313 Loss: 1.046902060508728\n",
      "Iteration 35314 Loss: 1.093645453453064\n",
      "Iteration 35315 Loss: 1.0428111553192139\n",
      "Iteration 35316 Loss: 1.0735975503921509\n",
      "Iteration 35317 Loss: 1.1215705871582031\n",
      "Iteration 35318 Loss: 1.0489274263381958\n",
      "Iteration 35319 Loss: 0.5690499544143677\n",
      "Iteration 35319 Loss: 1.057037591934204\n",
      "Iteration 35320 Loss: 0.9692297577857971\n",
      "Iteration 35321 Loss: 0.8585925698280334\n",
      "Iteration 35322 Loss: 0.9645807147026062\n",
      "Iteration 35323 Loss: 0.7746661901473999\n",
      "Iteration 35324 Loss: 1.1123778820037842\n",
      "Iteration 35325 Loss: 1.0901895761489868\n",
      "Iteration 35326 Loss: 1.3142355680465698\n",
      "Iteration 35327 Loss: 1.1267911195755005\n",
      "Iteration 35328 Loss: 0.7420838475227356\n",
      "Iteration 35329 Loss: 0.8178253769874573\n",
      "Iteration 35329 Loss: 0.977057158946991\n",
      "Iteration 35330 Loss: 1.199559211730957\n",
      "Iteration 35331 Loss: 0.7233408093452454\n",
      "Iteration 35332 Loss: 0.8857383728027344\n",
      "Iteration 35333 Loss: 1.3493074178695679\n",
      "Iteration 35334 Loss: 1.209884762763977\n",
      "Iteration 35335 Loss: 0.8706521987915039\n",
      "Iteration 35336 Loss: 0.8530784249305725\n",
      "Iteration 35337 Loss: 0.6060818433761597\n",
      "Iteration 35338 Loss: 1.1560412645339966\n",
      "Iteration 35339 Loss: 0.6495620608329773\n",
      "Iteration 35339 Loss: 0.9503247141838074\n",
      "Iteration 35340 Loss: 1.1184505224227905\n",
      "Iteration 35341 Loss: 0.9115622639656067\n",
      "Iteration 35342 Loss: 0.8723726868629456\n",
      "Iteration 35343 Loss: 0.7208225131034851\n",
      "Iteration 35344 Loss: 1.0761263370513916\n",
      "Iteration 35345 Loss: 0.7844708561897278\n",
      "Iteration 35346 Loss: 1.1647446155548096\n",
      "Iteration 35347 Loss: 1.230135202407837\n",
      "Iteration 35348 Loss: 1.1841403245925903\n",
      "Iteration 35349 Loss: 0.9424875378608704\n",
      "Iteration 35349 Loss: 1.0005313158035278\n",
      "Iteration 35350 Loss: 1.010446310043335\n",
      "Iteration 35351 Loss: 1.3192883729934692\n",
      "Iteration 35352 Loss: 0.9615486264228821\n",
      "Iteration 35353 Loss: 0.9338703751564026\n",
      "Iteration 35354 Loss: 0.8887096047401428\n",
      "Iteration 35355 Loss: 0.885214626789093\n",
      "Iteration 35356 Loss: 0.6573562622070312\n",
      "Iteration 35357 Loss: 0.8309423923492432\n",
      "Iteration 35358 Loss: 1.2742881774902344\n",
      "Iteration 35359 Loss: 0.9809461832046509\n",
      "Iteration 35359 Loss: 0.9742611050605774\n",
      "Iteration 35360 Loss: 0.9971790313720703\n",
      "Iteration 35361 Loss: 0.9717654585838318\n",
      "Iteration 35362 Loss: 0.9677284955978394\n",
      "Iteration 35363 Loss: 0.9930918216705322\n",
      "Iteration 35364 Loss: 0.8349683284759521\n",
      "Iteration 35365 Loss: 0.995872974395752\n",
      "Iteration 35366 Loss: 1.2101292610168457\n",
      "Iteration 35367 Loss: 1.1132800579071045\n",
      "Iteration 35368 Loss: 1.0511733293533325\n",
      "Iteration 35369 Loss: 1.0407534837722778\n",
      "Iteration 35369 Loss: 1.0175942182540894\n",
      "Iteration 35370 Loss: 0.9149326086044312\n",
      "Iteration 35371 Loss: 1.1539031267166138\n",
      "Iteration 35372 Loss: 0.645436704158783\n",
      "Iteration 35373 Loss: 0.9096793532371521\n",
      "Iteration 35374 Loss: 1.160119652748108\n",
      "Iteration 35375 Loss: 1.5196598768234253\n",
      "Iteration 35376 Loss: 0.8277469873428345\n",
      "Iteration 35377 Loss: 0.9932849407196045\n",
      "Iteration 35378 Loss: 1.3764302730560303\n",
      "Iteration 35379 Loss: 1.0114959478378296\n",
      "Iteration 35379 Loss: 1.0512689352035522\n",
      "Iteration 35380 Loss: 1.0064619779586792\n",
      "Iteration 35381 Loss: 0.8522339463233948\n",
      "Iteration 35382 Loss: 0.966998279094696\n",
      "Iteration 35383 Loss: 0.8095399141311646\n",
      "Iteration 35384 Loss: 0.7522697448730469\n",
      "Iteration 35385 Loss: 0.8745272159576416\n",
      "Iteration 35386 Loss: 1.0308183431625366\n",
      "Iteration 35387 Loss: 0.9401065111160278\n",
      "Iteration 35388 Loss: 0.8898358941078186\n",
      "Iteration 35389 Loss: 0.6282016634941101\n",
      "Iteration 35389 Loss: 0.8750993609428406\n",
      "Iteration 35390 Loss: 1.3864864110946655\n",
      "Iteration 35391 Loss: 0.8170875906944275\n",
      "Iteration 35392 Loss: 1.0560473203659058\n",
      "Iteration 35393 Loss: 0.7812183499336243\n",
      "Iteration 35394 Loss: 0.8686872720718384\n",
      "Iteration 35395 Loss: 1.1528912782669067\n",
      "Iteration 35396 Loss: 1.0203415155410767\n",
      "Iteration 35397 Loss: 1.4297279119491577\n",
      "Iteration 35398 Loss: 1.0163893699645996\n",
      "Iteration 35399 Loss: 0.9735400676727295\n",
      "Iteration 35399 Loss: 1.0502417087554932\n",
      "Iteration 35400 Loss: 0.9177735447883606\n",
      "Iteration 35401 Loss: 0.6829277873039246\n",
      "Iteration 35402 Loss: 0.8909592032432556\n",
      "Iteration 35403 Loss: 0.9721404314041138\n",
      "Iteration 35404 Loss: 1.0748668909072876\n",
      "Iteration 35405 Loss: 1.0240280628204346\n",
      "Iteration 35406 Loss: 0.9487277865409851\n",
      "Iteration 35407 Loss: 0.8973900675773621\n",
      "Iteration 35408 Loss: 1.3416364192962646\n",
      "Iteration 35409 Loss: 0.9695026874542236\n",
      "Iteration 35409 Loss: 0.9719952344894409\n",
      "Iteration 35410 Loss: 0.7966448068618774\n",
      "Iteration 35411 Loss: 0.6685640215873718\n",
      "Iteration 35412 Loss: 1.3232206106185913\n",
      "Iteration 35413 Loss: 0.9643927812576294\n",
      "Iteration 35414 Loss: 0.8315719366073608\n",
      "Iteration 35415 Loss: 1.065096378326416\n",
      "Iteration 35416 Loss: 0.7867453694343567\n",
      "Iteration 35417 Loss: 1.0572991371154785\n",
      "Iteration 35418 Loss: 1.2506991624832153\n",
      "Iteration 35419 Loss: 1.1848253011703491\n",
      "Iteration 35419 Loss: 0.9929057955741882\n",
      "Iteration 35420 Loss: 1.2767096757888794\n",
      "Iteration 35421 Loss: 0.9213318228721619\n",
      "Iteration 35422 Loss: 0.864244818687439\n",
      "Iteration 35423 Loss: 0.8279440999031067\n",
      "Iteration 35424 Loss: 0.6370301842689514\n",
      "Iteration 35425 Loss: 1.076074242591858\n",
      "Iteration 35426 Loss: 0.9725054502487183\n",
      "Iteration 35427 Loss: 1.251501441001892\n",
      "Iteration 35428 Loss: 1.1725866794586182\n",
      "Iteration 35429 Loss: 1.2009100914001465\n",
      "Iteration 35429 Loss: 1.0200837850570679\n",
      "Iteration 35430 Loss: 0.8481655716896057\n",
      "Iteration 35431 Loss: 0.9414265751838684\n",
      "Iteration 35432 Loss: 0.8360292911529541\n",
      "Iteration 35433 Loss: 0.9994298815727234\n",
      "Iteration 35434 Loss: 0.8104963302612305\n",
      "Iteration 35435 Loss: 1.2330825328826904\n",
      "Iteration 35436 Loss: 0.9808918833732605\n",
      "Iteration 35437 Loss: 0.8785534501075745\n",
      "Iteration 35438 Loss: 0.9477691054344177\n",
      "Iteration 35439 Loss: 1.1959667205810547\n",
      "Iteration 35439 Loss: 0.9671810865402222\n",
      "Iteration 35440 Loss: 0.9490151405334473\n",
      "Iteration 35441 Loss: 1.3033041954040527\n",
      "Iteration 35442 Loss: 0.9258695840835571\n",
      "Iteration 35443 Loss: 1.0948705673217773\n",
      "Iteration 35444 Loss: 0.6756711602210999\n",
      "Iteration 35445 Loss: 1.1874680519104004\n",
      "Iteration 35446 Loss: 0.5653505921363831\n",
      "Iteration 35447 Loss: 0.8881363272666931\n",
      "Iteration 35448 Loss: 1.300671935081482\n",
      "Iteration 35449 Loss: 0.9030933380126953\n",
      "Iteration 35449 Loss: 0.9793450236320496\n",
      "Iteration 35450 Loss: 0.8944643139839172\n",
      "Iteration 35451 Loss: 1.0623210668563843\n",
      "Iteration 35452 Loss: 1.2165820598602295\n",
      "Iteration 35453 Loss: 0.8399547338485718\n",
      "Iteration 35454 Loss: 0.9860166907310486\n",
      "Iteration 35455 Loss: 1.3013168573379517\n",
      "Iteration 35456 Loss: 1.0488148927688599\n",
      "Iteration 35457 Loss: 1.3456377983093262\n",
      "Iteration 35458 Loss: 1.1292088031768799\n",
      "Iteration 35459 Loss: 0.8327500820159912\n",
      "Iteration 35459 Loss: 1.0657068490982056\n",
      "Iteration 35460 Loss: 0.8098688125610352\n",
      "Iteration 35461 Loss: 1.0321251153945923\n",
      "Iteration 35462 Loss: 0.9197815656661987\n",
      "Iteration 35463 Loss: 1.0113848447799683\n",
      "Iteration 35464 Loss: 0.9762921333312988\n",
      "Iteration 35465 Loss: 0.7324696779251099\n",
      "Iteration 35466 Loss: 0.7009259462356567\n",
      "Iteration 35467 Loss: 0.8642861843109131\n",
      "Iteration 35468 Loss: 0.9783591628074646\n",
      "Iteration 35469 Loss: 0.9147160053253174\n",
      "Iteration 35469 Loss: 0.8940210342407227\n",
      "Iteration 35470 Loss: 0.9830483198165894\n",
      "Iteration 35471 Loss: 1.1515922546386719\n",
      "Iteration 35472 Loss: 1.0711777210235596\n",
      "Iteration 35473 Loss: 0.8415563106536865\n",
      "Iteration 35474 Loss: 1.0701919794082642\n",
      "Iteration 35475 Loss: 0.8783913254737854\n",
      "Iteration 35476 Loss: 0.7527065277099609\n",
      "Iteration 35477 Loss: 1.1223033666610718\n",
      "Iteration 35478 Loss: 0.9038123488426208\n",
      "Iteration 35479 Loss: 1.2045502662658691\n",
      "Iteration 35479 Loss: 0.9979330897331238\n",
      "Iteration 35480 Loss: 1.0579530000686646\n",
      "Iteration 35481 Loss: 1.2442878484725952\n",
      "Iteration 35482 Loss: 0.806994616985321\n",
      "Iteration 35483 Loss: 0.6255465149879456\n",
      "Iteration 35484 Loss: 1.2093793153762817\n",
      "Iteration 35485 Loss: 0.9489184021949768\n",
      "Iteration 35486 Loss: 1.1748936176300049\n",
      "Iteration 35487 Loss: 1.0497689247131348\n",
      "Iteration 35488 Loss: 1.1628073453903198\n",
      "Iteration 35489 Loss: 1.1666539907455444\n",
      "Iteration 35489 Loss: 1.0447202920913696\n",
      "Iteration 35490 Loss: 1.0569015741348267\n",
      "Iteration 35491 Loss: 1.2312191724777222\n",
      "Iteration 35492 Loss: 1.0606039762496948\n",
      "Iteration 35493 Loss: 0.9835707545280457\n",
      "Iteration 35494 Loss: 0.9543525576591492\n",
      "Iteration 35495 Loss: 0.7292066812515259\n",
      "Iteration 35496 Loss: 0.9736387729644775\n",
      "Iteration 35497 Loss: 0.6662973165512085\n",
      "Iteration 35498 Loss: 0.891872227191925\n",
      "Iteration 35499 Loss: 0.9323086142539978\n",
      "Iteration 35499 Loss: 0.9479970932006836\n",
      "Iteration 35500 Loss: 1.1815344095230103\n",
      "Iteration 35501 Loss: 0.8593349456787109\n",
      "Iteration 35502 Loss: 0.7196627855300903\n",
      "Iteration 35503 Loss: 1.1623190641403198\n",
      "Iteration 35504 Loss: 0.7505670189857483\n",
      "Iteration 35505 Loss: 0.5059762597084045\n",
      "Iteration 35506 Loss: 1.0590729713439941\n",
      "Iteration 35507 Loss: 0.8158789277076721\n",
      "Iteration 35508 Loss: 1.0040911436080933\n",
      "Iteration 35509 Loss: 0.9550581574440002\n",
      "Iteration 35509 Loss: 0.9013495445251465\n",
      "Iteration 35510 Loss: 0.7190287709236145\n",
      "Iteration 35511 Loss: 1.2138535976409912\n",
      "Iteration 35512 Loss: 1.1208159923553467\n",
      "Iteration 35513 Loss: 1.0819495916366577\n",
      "Iteration 35514 Loss: 0.971947193145752\n",
      "Iteration 35515 Loss: 0.6607438921928406\n",
      "Iteration 35516 Loss: 1.4616796970367432\n",
      "Iteration 35517 Loss: 1.0545904636383057\n",
      "Iteration 35518 Loss: 1.281992793083191\n",
      "Iteration 35519 Loss: 0.6838305592536926\n",
      "Iteration 35519 Loss: 1.025043249130249\n",
      "Iteration 35520 Loss: 0.8855103850364685\n",
      "Iteration 35521 Loss: 0.9824795126914978\n",
      "Iteration 35522 Loss: 1.036783218383789\n",
      "Iteration 35523 Loss: 1.0998679399490356\n",
      "Iteration 35524 Loss: 0.9152824282646179\n",
      "Iteration 35525 Loss: 1.025770664215088\n",
      "Iteration 35526 Loss: 1.1768665313720703\n",
      "Iteration 35527 Loss: 1.1754440069198608\n",
      "Iteration 35528 Loss: 1.5580458641052246\n",
      "Iteration 35529 Loss: 0.9723711013793945\n",
      "Iteration 35529 Loss: 1.0828421115875244\n",
      "Iteration 35530 Loss: 1.0002740621566772\n",
      "Iteration 35531 Loss: 1.2719591856002808\n",
      "Iteration 35532 Loss: 0.9792185425758362\n",
      "Iteration 35533 Loss: 0.6415297389030457\n",
      "Iteration 35534 Loss: 1.1647233963012695\n",
      "Iteration 35535 Loss: 1.1454700231552124\n",
      "Iteration 35536 Loss: 0.9067521691322327\n",
      "Iteration 35537 Loss: 0.8738189935684204\n",
      "Iteration 35538 Loss: 0.6602108478546143\n",
      "Iteration 35539 Loss: 0.8142423629760742\n",
      "Iteration 35539 Loss: 0.9458199739456177\n",
      "Iteration 35540 Loss: 1.2901755571365356\n",
      "Iteration 35541 Loss: 1.1649305820465088\n",
      "Iteration 35542 Loss: 1.1342614889144897\n",
      "Iteration 35543 Loss: 1.0028916597366333\n",
      "Iteration 35544 Loss: 1.0730258226394653\n",
      "Iteration 35545 Loss: 1.1056058406829834\n",
      "Iteration 35546 Loss: 1.0755072832107544\n",
      "Iteration 35547 Loss: 0.9318495988845825\n",
      "Iteration 35548 Loss: 0.9511600136756897\n",
      "Iteration 35549 Loss: 0.9616639614105225\n",
      "Iteration 35549 Loss: 1.069107174873352\n",
      "Iteration 35550 Loss: 1.1433641910552979\n",
      "Iteration 35551 Loss: 0.8062677979469299\n",
      "Iteration 35552 Loss: 0.7731443047523499\n",
      "Iteration 35553 Loss: 0.5268442034721375\n",
      "Iteration 35554 Loss: 0.8959515690803528\n",
      "Iteration 35555 Loss: 0.7687311768531799\n",
      "Iteration 35556 Loss: 0.9492039680480957\n",
      "Iteration 35557 Loss: 1.227699637413025\n",
      "Iteration 35558 Loss: 0.8323308229446411\n",
      "Iteration 35559 Loss: 1.1740044355392456\n",
      "Iteration 35559 Loss: 0.9097541570663452\n",
      "Iteration 35560 Loss: 1.2502562999725342\n",
      "Iteration 35561 Loss: 1.0689042806625366\n",
      "Iteration 35562 Loss: 0.8120615482330322\n",
      "Iteration 35563 Loss: 1.204576849937439\n",
      "Iteration 35564 Loss: 1.2071561813354492\n",
      "Iteration 35565 Loss: 1.0256880521774292\n",
      "Iteration 35566 Loss: 0.8435056209564209\n",
      "Iteration 35567 Loss: 1.112371563911438\n",
      "Iteration 35568 Loss: 0.8617337346076965\n",
      "Iteration 35569 Loss: 1.1889852285385132\n",
      "Iteration 35569 Loss: 1.0575239658355713\n",
      "Iteration 35570 Loss: 1.5152184963226318\n",
      "Iteration 35571 Loss: 0.97124183177948\n",
      "Iteration 35572 Loss: 0.8029083013534546\n",
      "Iteration 35573 Loss: 0.6160988807678223\n",
      "Iteration 35574 Loss: 1.076535701751709\n",
      "Iteration 35575 Loss: 1.0615779161453247\n",
      "Iteration 35576 Loss: 0.8340966105461121\n",
      "Iteration 35577 Loss: 1.1712621450424194\n",
      "Iteration 35578 Loss: 0.9193512201309204\n",
      "Iteration 35579 Loss: 1.1328204870224\n",
      "Iteration 35579 Loss: 1.0101110935211182\n",
      "Iteration 35580 Loss: 0.943653404712677\n",
      "Iteration 35581 Loss: 1.1212804317474365\n",
      "Iteration 35582 Loss: 0.9263531565666199\n",
      "Iteration 35583 Loss: 1.0602152347564697\n",
      "Iteration 35584 Loss: 0.9952229261398315\n",
      "Iteration 35585 Loss: 1.025532603263855\n",
      "Iteration 35586 Loss: 0.7597499489784241\n",
      "Iteration 35587 Loss: 1.1534205675125122\n",
      "Iteration 35588 Loss: 0.8556230664253235\n",
      "Iteration 35589 Loss: 1.0287691354751587\n",
      "Iteration 35589 Loss: 0.9869820475578308\n",
      "Iteration 35590 Loss: 1.2524229288101196\n",
      "Iteration 35591 Loss: 1.0255988836288452\n",
      "Iteration 35592 Loss: 0.8193550109863281\n",
      "Iteration 35593 Loss: 0.9789221286773682\n",
      "Iteration 35594 Loss: 0.9521548748016357\n",
      "Iteration 35595 Loss: 0.9414658546447754\n",
      "Iteration 35596 Loss: 0.7076907157897949\n",
      "Iteration 35597 Loss: 0.6994138360023499\n",
      "Iteration 35598 Loss: 1.2444545030593872\n",
      "Iteration 35599 Loss: 1.1731133460998535\n",
      "Iteration 35599 Loss: 0.979459285736084\n",
      "Iteration 35600 Loss: 0.8038654923439026\n",
      "Iteration 35601 Loss: 0.7059558033943176\n",
      "Iteration 35602 Loss: 1.0147539377212524\n",
      "Iteration 35603 Loss: 1.118354082107544\n",
      "Iteration 35604 Loss: 0.9708676934242249\n",
      "Iteration 35605 Loss: 1.0712913274765015\n",
      "Iteration 35606 Loss: 1.1132642030715942\n",
      "Iteration 35607 Loss: 0.9638750553131104\n",
      "Iteration 35608 Loss: 1.1483772993087769\n",
      "Iteration 35609 Loss: 0.7667074799537659\n",
      "Iteration 35609 Loss: 0.9677311778068542\n",
      "Iteration 35610 Loss: 1.170994520187378\n",
      "Iteration 35611 Loss: 0.9064563512802124\n",
      "Iteration 35612 Loss: 1.0783464908599854\n",
      "Iteration 35613 Loss: 0.9045650362968445\n",
      "Iteration 35614 Loss: 1.2255433797836304\n",
      "Iteration 35615 Loss: 0.9567353129386902\n",
      "Iteration 35616 Loss: 0.711311399936676\n",
      "Iteration 35617 Loss: 1.1173977851867676\n",
      "Iteration 35618 Loss: 0.9825980067253113\n",
      "Iteration 35619 Loss: 0.7214126586914062\n",
      "Iteration 35619 Loss: 0.9775360226631165\n",
      "Iteration 35620 Loss: 0.8884651064872742\n",
      "Iteration 35621 Loss: 0.8372689485549927\n",
      "Iteration 35622 Loss: 0.783230185508728\n",
      "Iteration 35623 Loss: 0.7965940237045288\n",
      "Iteration 35624 Loss: 1.1431008577346802\n",
      "Iteration 35625 Loss: 0.8321909308433533\n",
      "Iteration 35626 Loss: 0.7181947231292725\n",
      "Iteration 35627 Loss: 0.697584331035614\n",
      "Iteration 35628 Loss: 0.7891324758529663\n",
      "Iteration 35629 Loss: 1.0304876565933228\n",
      "Iteration 35629 Loss: 0.8516249656677246\n",
      "Iteration 35630 Loss: 1.248100996017456\n",
      "Iteration 35631 Loss: 1.248752474784851\n",
      "Iteration 35632 Loss: 0.7305063009262085\n",
      "Iteration 35633 Loss: 0.9230529069900513\n",
      "Iteration 35634 Loss: 0.8432288765907288\n",
      "Iteration 35635 Loss: 1.3762537240982056\n",
      "Iteration 35636 Loss: 1.0816118717193604\n",
      "Iteration 35637 Loss: 0.9441723823547363\n",
      "Iteration 35638 Loss: 1.0452631711959839\n",
      "Iteration 35639 Loss: 0.9437218308448792\n",
      "Iteration 35639 Loss: 1.038466453552246\n",
      "Iteration 35640 Loss: 1.0063073635101318\n",
      "Iteration 35641 Loss: 0.8937591314315796\n",
      "Iteration 35642 Loss: 0.9651539325714111\n",
      "Iteration 35643 Loss: 0.968782901763916\n",
      "Iteration 35644 Loss: 0.7438707947731018\n",
      "Iteration 35645 Loss: 0.936129629611969\n",
      "Iteration 35646 Loss: 1.2259349822998047\n",
      "Iteration 35647 Loss: 1.0303343534469604\n",
      "Iteration 35648 Loss: 1.1375919580459595\n",
      "Iteration 35649 Loss: 0.8422872424125671\n",
      "Iteration 35649 Loss: 0.9750152826309204\n",
      "Iteration 35650 Loss: 1.1933188438415527\n",
      "Iteration 35651 Loss: 1.0388034582138062\n",
      "Iteration 35652 Loss: 0.8615921139717102\n",
      "Iteration 35653 Loss: 1.23131263256073\n",
      "Iteration 35654 Loss: 1.140831470489502\n",
      "Iteration 35655 Loss: 1.0112038850784302\n",
      "Iteration 35656 Loss: 1.1148922443389893\n",
      "Iteration 35657 Loss: 1.0508759021759033\n",
      "Iteration 35658 Loss: 0.8322385549545288\n",
      "Iteration 35659 Loss: 1.1880698204040527\n",
      "Iteration 35659 Loss: 1.0663139820098877\n",
      "Iteration 35660 Loss: 0.9241659045219421\n",
      "Iteration 35661 Loss: 0.9299435019493103\n",
      "Iteration 35662 Loss: 1.0177552700042725\n",
      "Iteration 35663 Loss: 1.093829870223999\n",
      "Iteration 35664 Loss: 0.9229820966720581\n",
      "Iteration 35665 Loss: 0.8979912400245667\n",
      "Iteration 35666 Loss: 1.0867148637771606\n",
      "Iteration 35667 Loss: 0.7958801984786987\n",
      "Iteration 35668 Loss: 1.218578815460205\n",
      "Iteration 35669 Loss: 1.3311958312988281\n",
      "Iteration 35669 Loss: 1.0219037532806396\n",
      "Iteration 35670 Loss: 1.1316817998886108\n",
      "Iteration 35671 Loss: 1.1725914478302002\n",
      "Iteration 35672 Loss: 1.3179457187652588\n",
      "Iteration 35673 Loss: 1.2017494440078735\n",
      "Iteration 35674 Loss: 0.931638777256012\n",
      "Iteration 35675 Loss: 1.061700463294983\n",
      "Iteration 35676 Loss: 0.9043710827827454\n",
      "Iteration 35677 Loss: 0.9705562591552734\n",
      "Iteration 35678 Loss: 1.2061419486999512\n",
      "Iteration 35679 Loss: 1.2495938539505005\n",
      "Iteration 35679 Loss: 1.1147971153259277\n",
      "Iteration 35680 Loss: 1.0159605741500854\n",
      "Iteration 35681 Loss: 1.181174397468567\n",
      "Iteration 35682 Loss: 1.2013635635375977\n",
      "Iteration 35683 Loss: 0.9261813759803772\n",
      "Iteration 35684 Loss: 0.8048463463783264\n",
      "Iteration 35685 Loss: 1.0417348146438599\n",
      "Iteration 35686 Loss: 1.0294381380081177\n",
      "Iteration 35687 Loss: 1.0866938829421997\n",
      "Iteration 35688 Loss: 1.2821025848388672\n",
      "Iteration 35689 Loss: 0.9116910099983215\n",
      "Iteration 35689 Loss: 1.0481185913085938\n",
      "Iteration 35690 Loss: 0.7815168499946594\n",
      "Iteration 35691 Loss: 0.944297194480896\n",
      "Iteration 35692 Loss: 1.2664560079574585\n",
      "Iteration 35693 Loss: 1.068079948425293\n",
      "Iteration 35694 Loss: 1.1605827808380127\n",
      "Iteration 35695 Loss: 0.8703534603118896\n",
      "Iteration 35696 Loss: 1.0711711645126343\n",
      "Iteration 35697 Loss: 1.3473334312438965\n",
      "Iteration 35698 Loss: 0.8520262241363525\n",
      "Iteration 35699 Loss: 1.197168231010437\n",
      "Iteration 35699 Loss: 1.0558984279632568\n",
      "Iteration 35700 Loss: 0.7955297827720642\n",
      "Iteration 35701 Loss: 0.9416975378990173\n",
      "Iteration 35702 Loss: 0.6406304240226746\n",
      "Iteration 35703 Loss: 0.8105040192604065\n",
      "Iteration 35704 Loss: 1.1794664859771729\n",
      "Iteration 35705 Loss: 0.7510011196136475\n",
      "Iteration 35706 Loss: 0.7703869342803955\n",
      "Iteration 35707 Loss: 0.8587565422058105\n",
      "Iteration 35708 Loss: 1.1431180238723755\n",
      "Iteration 35709 Loss: 0.6546904444694519\n",
      "Iteration 35709 Loss: 0.8545781970024109\n",
      "Iteration 35710 Loss: 1.2383943796157837\n",
      "Iteration 35711 Loss: 1.0703493356704712\n",
      "Iteration 35712 Loss: 0.9849960803985596\n",
      "Iteration 35713 Loss: 0.7511433959007263\n",
      "Iteration 35714 Loss: 0.9185686111450195\n",
      "Iteration 35715 Loss: 1.1005754470825195\n",
      "Iteration 35716 Loss: 1.2832410335540771\n",
      "Iteration 35717 Loss: 1.0238056182861328\n",
      "Iteration 35718 Loss: 0.6786515116691589\n",
      "Iteration 35719 Loss: 1.182296872138977\n",
      "Iteration 35719 Loss: 1.0232021808624268\n",
      "Iteration 35720 Loss: 0.8854655623435974\n",
      "Iteration 35721 Loss: 0.8348599672317505\n",
      "Iteration 35722 Loss: 0.9684771299362183\n",
      "Iteration 35723 Loss: 0.836534857749939\n",
      "Iteration 35724 Loss: 0.8696970343589783\n",
      "Iteration 35725 Loss: 0.8018473386764526\n",
      "Iteration 35726 Loss: 0.891707181930542\n",
      "Iteration 35727 Loss: 0.9539564251899719\n",
      "Iteration 35728 Loss: 0.9182201623916626\n",
      "Iteration 35729 Loss: 0.9074127674102783\n",
      "Iteration 35729 Loss: 0.8868179321289062\n",
      "Iteration 35730 Loss: 0.9305818676948547\n",
      "Iteration 35731 Loss: 0.7752559185028076\n",
      "Iteration 35732 Loss: 1.0312901735305786\n",
      "Iteration 35733 Loss: 0.881996214389801\n",
      "Iteration 35734 Loss: 1.333165168762207\n",
      "Iteration 35735 Loss: 0.8038363456726074\n",
      "Iteration 35736 Loss: 0.9951459765434265\n",
      "Iteration 35737 Loss: 0.869341254234314\n",
      "Iteration 35738 Loss: 1.2059051990509033\n",
      "Iteration 35739 Loss: 1.0661262273788452\n",
      "Iteration 35739 Loss: 0.9892643094062805\n",
      "Iteration 35740 Loss: 0.8749673366546631\n",
      "Iteration 35741 Loss: 0.7072651982307434\n",
      "Iteration 35742 Loss: 0.9762085676193237\n",
      "Iteration 35743 Loss: 0.841317355632782\n",
      "Iteration 35744 Loss: 1.2326726913452148\n",
      "Iteration 35745 Loss: 0.8392335176467896\n",
      "Iteration 35746 Loss: 0.9320493936538696\n",
      "Iteration 35747 Loss: 1.0571976900100708\n",
      "Iteration 35748 Loss: 1.3112848997116089\n",
      "Iteration 35749 Loss: 0.9578698873519897\n",
      "Iteration 35749 Loss: 0.9730066061019897\n",
      "Iteration 35750 Loss: 1.127734899520874\n",
      "Iteration 35751 Loss: 1.0720007419586182\n",
      "Iteration 35752 Loss: 1.3063308000564575\n",
      "Iteration 35753 Loss: 0.7988136410713196\n",
      "Iteration 35754 Loss: 1.0076359510421753\n",
      "Iteration 35755 Loss: 0.7729228734970093\n",
      "Iteration 35756 Loss: 0.8848995566368103\n",
      "Iteration 35757 Loss: 1.1203281879425049\n",
      "Iteration 35758 Loss: 1.0628927946090698\n",
      "Iteration 35759 Loss: 1.0535122156143188\n",
      "Iteration 35759 Loss: 1.020707130432129\n",
      "Iteration 35760 Loss: 0.9267778396606445\n",
      "Iteration 35761 Loss: 0.8543129563331604\n",
      "Iteration 35762 Loss: 0.9603338837623596\n",
      "Iteration 35763 Loss: 0.9949274659156799\n",
      "Iteration 35764 Loss: 0.7207221984863281\n",
      "Iteration 35765 Loss: 0.8046389222145081\n",
      "Iteration 35766 Loss: 1.0394325256347656\n",
      "Iteration 35767 Loss: 1.3417797088623047\n",
      "Iteration 35768 Loss: 1.1321589946746826\n",
      "Iteration 35769 Loss: 1.0500562191009521\n",
      "Iteration 35769 Loss: 0.9825140237808228\n",
      "Iteration 35770 Loss: 1.1642366647720337\n",
      "Iteration 35771 Loss: 1.0499745607376099\n",
      "Iteration 35772 Loss: 1.21914541721344\n",
      "Iteration 35773 Loss: 0.865463376045227\n",
      "Iteration 35774 Loss: 1.1225533485412598\n",
      "Iteration 35775 Loss: 0.6645725965499878\n",
      "Iteration 35776 Loss: 0.7087059020996094\n",
      "Iteration 35777 Loss: 0.9724960923194885\n",
      "Iteration 35778 Loss: 1.0224980115890503\n",
      "Iteration 35779 Loss: 0.9652118682861328\n",
      "Iteration 35779 Loss: 0.9754856824874878\n",
      "Iteration 35780 Loss: 0.9618900418281555\n",
      "Iteration 35781 Loss: 0.9865321516990662\n",
      "Iteration 35782 Loss: 1.0001167058944702\n",
      "Iteration 35783 Loss: 1.048305869102478\n",
      "Iteration 35784 Loss: 1.4686639308929443\n",
      "Iteration 35785 Loss: 1.3312103748321533\n",
      "Iteration 35786 Loss: 0.781031608581543\n",
      "Iteration 35787 Loss: 1.0092124938964844\n",
      "Iteration 35788 Loss: 1.1043957471847534\n",
      "Iteration 35789 Loss: 0.9750601649284363\n",
      "Iteration 35789 Loss: 1.066641926765442\n",
      "Iteration 35790 Loss: 1.255050778388977\n",
      "Iteration 35791 Loss: 1.1030399799346924\n",
      "Iteration 35792 Loss: 1.2431182861328125\n",
      "Iteration 35793 Loss: 0.861673891544342\n",
      "Iteration 35794 Loss: 1.147376537322998\n",
      "Iteration 35795 Loss: 0.8401223421096802\n",
      "Iteration 35796 Loss: 1.0650420188903809\n",
      "Iteration 35797 Loss: 1.1686408519744873\n",
      "Iteration 35798 Loss: 1.229978322982788\n",
      "Iteration 35799 Loss: 1.2471909523010254\n",
      "Iteration 35799 Loss: 1.1161234378814697\n",
      "Iteration 35800 Loss: 1.000982403755188\n",
      "Iteration 35801 Loss: 0.9938175082206726\n",
      "Iteration 35802 Loss: 0.6471074223518372\n",
      "Iteration 35803 Loss: 1.1159828901290894\n",
      "Iteration 35804 Loss: 0.8534076809883118\n",
      "Iteration 35805 Loss: 1.0065419673919678\n",
      "Iteration 35806 Loss: 1.2055647373199463\n",
      "Iteration 35807 Loss: 0.939113438129425\n",
      "Iteration 35808 Loss: 1.0728623867034912\n",
      "Iteration 35809 Loss: 1.15757417678833\n",
      "Iteration 35809 Loss: 0.9992955327033997\n",
      "Iteration 35810 Loss: 0.8114259839057922\n",
      "Iteration 35811 Loss: 0.9223377108573914\n",
      "Iteration 35812 Loss: 1.2313247919082642\n",
      "Iteration 35813 Loss: 0.8345297574996948\n",
      "Iteration 35814 Loss: 0.8274041414260864\n",
      "Iteration 35815 Loss: 1.1209182739257812\n",
      "Iteration 35816 Loss: 0.9822786450386047\n",
      "Iteration 35817 Loss: 0.943199872970581\n",
      "Iteration 35818 Loss: 0.6483819484710693\n",
      "Iteration 35819 Loss: 0.8534679412841797\n",
      "Iteration 35819 Loss: 0.91752690076828\n",
      "Iteration 35820 Loss: 1.1046137809753418\n",
      "Iteration 35821 Loss: 0.8625068664550781\n",
      "Iteration 35822 Loss: 1.0066124200820923\n",
      "Iteration 35823 Loss: 0.8691416382789612\n",
      "Iteration 35824 Loss: 1.1673375368118286\n",
      "Iteration 35825 Loss: 0.933386504650116\n",
      "Iteration 35826 Loss: 1.0787650346755981\n",
      "Iteration 35827 Loss: 0.788749098777771\n",
      "Iteration 35828 Loss: 0.8133382797241211\n",
      "Iteration 35829 Loss: 1.0226629972457886\n",
      "Iteration 35829 Loss: 0.9647113680839539\n",
      "Iteration 35830 Loss: 0.8316524028778076\n",
      "Iteration 35831 Loss: 0.7281357049942017\n",
      "Iteration 35832 Loss: 1.232521891593933\n",
      "Iteration 35833 Loss: 0.9985361695289612\n",
      "Iteration 35834 Loss: 1.0620028972625732\n",
      "Iteration 35835 Loss: 1.093600869178772\n",
      "Iteration 35836 Loss: 0.9677416086196899\n",
      "Iteration 35837 Loss: 0.7751728296279907\n",
      "Iteration 35838 Loss: 1.016349196434021\n",
      "Iteration 35839 Loss: 1.3779358863830566\n",
      "Iteration 35839 Loss: 1.0083650350570679\n",
      "Iteration 35840 Loss: 0.7568019032478333\n",
      "Iteration 35841 Loss: 1.3326303958892822\n",
      "Iteration 35842 Loss: 1.0098499059677124\n",
      "Iteration 35843 Loss: 1.0804246664047241\n",
      "Iteration 35844 Loss: 1.227246880531311\n",
      "Iteration 35845 Loss: 0.8598322868347168\n",
      "Iteration 35846 Loss: 1.2081120014190674\n",
      "Iteration 35847 Loss: 0.696260392665863\n",
      "Iteration 35848 Loss: 0.8968753814697266\n",
      "Iteration 35849 Loss: 1.0690617561340332\n",
      "Iteration 35849 Loss: 1.013709545135498\n",
      "Iteration 35850 Loss: 1.3866420984268188\n",
      "Iteration 35851 Loss: 0.8434069156646729\n",
      "Iteration 35852 Loss: 1.4506028890609741\n",
      "Iteration 35853 Loss: 1.028546690940857\n",
      "Iteration 35854 Loss: 1.1160130500793457\n",
      "Iteration 35855 Loss: 0.8809105157852173\n",
      "Iteration 35856 Loss: 0.8287128210067749\n",
      "Iteration 35857 Loss: 0.9625568985939026\n",
      "Iteration 35858 Loss: 1.127716302871704\n",
      "Iteration 35859 Loss: 1.0251599550247192\n",
      "Iteration 35859 Loss: 1.0650267601013184\n",
      "Iteration 35860 Loss: 1.1104953289031982\n",
      "Iteration 35861 Loss: 1.0355887413024902\n",
      "Iteration 35862 Loss: 0.8706910014152527\n",
      "Iteration 35863 Loss: 0.8494141101837158\n",
      "Iteration 35864 Loss: 0.7739773988723755\n",
      "Iteration 35865 Loss: 1.2204228639602661\n",
      "Iteration 35866 Loss: 1.048875093460083\n",
      "Iteration 35867 Loss: 0.9106607437133789\n",
      "Iteration 35868 Loss: 1.1561585664749146\n",
      "Iteration 35869 Loss: 0.9550479650497437\n",
      "Iteration 35869 Loss: 0.9931331872940063\n",
      "Iteration 35870 Loss: 1.077844262123108\n",
      "Iteration 35871 Loss: 0.9945908784866333\n",
      "Iteration 35872 Loss: 0.9455922842025757\n",
      "Iteration 35873 Loss: 1.0288453102111816\n",
      "Iteration 35874 Loss: 0.91673344373703\n",
      "Iteration 35875 Loss: 0.8219391107559204\n",
      "Iteration 35876 Loss: 1.195249080657959\n",
      "Iteration 35877 Loss: 1.1851818561553955\n",
      "Iteration 35878 Loss: 0.8020746111869812\n",
      "Iteration 35879 Loss: 0.9301000833511353\n",
      "Iteration 35879 Loss: 0.9898150563240051\n",
      "Iteration 35880 Loss: 0.731597900390625\n",
      "Iteration 35881 Loss: 0.8162752985954285\n",
      "Iteration 35882 Loss: 1.0424796342849731\n",
      "Iteration 35883 Loss: 0.7897874116897583\n",
      "Iteration 35884 Loss: 0.5321204662322998\n",
      "Iteration 35885 Loss: 0.6920338273048401\n",
      "Iteration 35886 Loss: 0.8005648255348206\n",
      "Iteration 35887 Loss: 1.2978312969207764\n",
      "Iteration 35888 Loss: 0.9145689606666565\n",
      "Iteration 35889 Loss: 0.9570862054824829\n",
      "Iteration 35889 Loss: 0.8574345707893372\n",
      "Iteration 35890 Loss: 1.046314001083374\n",
      "Iteration 35891 Loss: 0.9619264602661133\n",
      "Iteration 35892 Loss: 0.784103274345398\n",
      "Iteration 35893 Loss: 1.1224311590194702\n",
      "Iteration 35894 Loss: 1.1013249158859253\n",
      "Iteration 35895 Loss: 0.6297966837882996\n",
      "Iteration 35896 Loss: 1.0768523216247559\n",
      "Iteration 35897 Loss: 0.8970332741737366\n",
      "Iteration 35898 Loss: 1.124725341796875\n",
      "Iteration 35899 Loss: 0.49214109778404236\n",
      "Iteration 35899 Loss: 0.9236648678779602\n",
      "Iteration 35900 Loss: 0.9275195598602295\n",
      "Iteration 35901 Loss: 0.7306023836135864\n",
      "Iteration 35902 Loss: 0.7143145799636841\n",
      "Iteration 35903 Loss: 0.5913376808166504\n",
      "Iteration 35904 Loss: 1.1677441596984863\n",
      "Iteration 35905 Loss: 1.0128657817840576\n",
      "Iteration 35906 Loss: 1.108319640159607\n",
      "Iteration 35907 Loss: 0.9502571225166321\n",
      "Iteration 35908 Loss: 0.9885026216506958\n",
      "Iteration 35909 Loss: 0.6684150099754333\n",
      "Iteration 35909 Loss: 0.8859878778457642\n",
      "Iteration 35910 Loss: 1.022294282913208\n",
      "Iteration 35911 Loss: 0.9604766368865967\n",
      "Iteration 35912 Loss: 0.9433844685554504\n",
      "Iteration 35913 Loss: 1.4906870126724243\n",
      "Iteration 35914 Loss: 1.1727523803710938\n",
      "Iteration 35915 Loss: 0.7367351055145264\n",
      "Iteration 35916 Loss: 1.0639182329177856\n",
      "Iteration 35917 Loss: 0.9029008746147156\n",
      "Iteration 35918 Loss: 0.9140464067459106\n",
      "Iteration 35919 Loss: 0.8843348026275635\n",
      "Iteration 35919 Loss: 1.0091530084609985\n",
      "Iteration 35920 Loss: 1.004455804824829\n",
      "Iteration 35921 Loss: 1.123870611190796\n",
      "Iteration 35922 Loss: 0.9035297632217407\n",
      "Iteration 35923 Loss: 1.2001060247421265\n",
      "Iteration 35924 Loss: 1.0020211935043335\n",
      "Iteration 35925 Loss: 1.0738153457641602\n",
      "Iteration 35926 Loss: 0.7071760296821594\n",
      "Iteration 35927 Loss: 0.9680749773979187\n",
      "Iteration 35928 Loss: 1.020522952079773\n",
      "Iteration 35929 Loss: 0.5972872376441956\n",
      "Iteration 35929 Loss: 0.9600859880447388\n",
      "Iteration 35930 Loss: 0.994295597076416\n",
      "Iteration 35931 Loss: 0.9544551968574524\n",
      "Iteration 35932 Loss: 1.2557265758514404\n",
      "Iteration 35933 Loss: 1.0449354648590088\n",
      "Iteration 35934 Loss: 1.2868798971176147\n",
      "Iteration 35935 Loss: 0.9071844816207886\n",
      "Iteration 35936 Loss: 0.8072686195373535\n",
      "Iteration 35937 Loss: 0.9250514507293701\n",
      "Iteration 35938 Loss: 0.915071427822113\n",
      "Iteration 35939 Loss: 1.1535539627075195\n",
      "Iteration 35939 Loss: 1.024442195892334\n",
      "Iteration 35940 Loss: 0.9714956283569336\n",
      "Iteration 35941 Loss: 0.6782644987106323\n",
      "Iteration 35942 Loss: 1.0976895093917847\n",
      "Iteration 35943 Loss: 1.1988564729690552\n",
      "Iteration 35944 Loss: 1.220034122467041\n",
      "Iteration 35945 Loss: 1.293914794921875\n",
      "Iteration 35946 Loss: 0.9240900874137878\n",
      "Iteration 35947 Loss: 0.8747890591621399\n",
      "Iteration 35948 Loss: 0.9321852326393127\n",
      "Iteration 35949 Loss: 0.8350806832313538\n",
      "Iteration 35949 Loss: 1.0026400089263916\n",
      "Iteration 35950 Loss: 0.8056712746620178\n",
      "Iteration 35951 Loss: 1.0415880680084229\n",
      "Iteration 35952 Loss: 0.8532679677009583\n",
      "Iteration 35953 Loss: 1.3237802982330322\n",
      "Iteration 35954 Loss: 1.0632671117782593\n",
      "Iteration 35955 Loss: 0.7716568112373352\n",
      "Iteration 35956 Loss: 1.0222574472427368\n",
      "Iteration 35957 Loss: 0.780563235282898\n",
      "Iteration 35958 Loss: 0.9155743718147278\n",
      "Iteration 35959 Loss: 1.0409801006317139\n",
      "Iteration 35959 Loss: 0.9618606567382812\n",
      "Iteration 35960 Loss: 0.9860431551933289\n",
      "Iteration 35961 Loss: 1.0826823711395264\n",
      "Iteration 35962 Loss: 1.0781227350234985\n",
      "Iteration 35963 Loss: 1.1583086252212524\n",
      "Iteration 35964 Loss: 0.8400049805641174\n",
      "Iteration 35965 Loss: 0.9108052253723145\n",
      "Iteration 35966 Loss: 1.1438837051391602\n",
      "Iteration 35967 Loss: 1.034222960472107\n",
      "Iteration 35968 Loss: 0.8573311567306519\n",
      "Iteration 35969 Loss: 0.8114243149757385\n",
      "Iteration 35969 Loss: 0.9902828931808472\n",
      "Iteration 35970 Loss: 0.7655110955238342\n",
      "Iteration 35971 Loss: 1.0945717096328735\n",
      "Iteration 35972 Loss: 0.5693902969360352\n",
      "Iteration 35973 Loss: 0.6407400965690613\n",
      "Iteration 35974 Loss: 0.8344579935073853\n",
      "Iteration 35975 Loss: 1.0025126934051514\n",
      "Iteration 35976 Loss: 0.966252326965332\n",
      "Iteration 35977 Loss: 0.903406023979187\n",
      "Iteration 35978 Loss: 1.0433216094970703\n",
      "Iteration 35979 Loss: 1.2075484991073608\n",
      "Iteration 35979 Loss: 0.9027711749076843\n",
      "Iteration 35980 Loss: 1.1536747217178345\n",
      "Iteration 35981 Loss: 1.253911018371582\n",
      "Iteration 35982 Loss: 1.3422553539276123\n",
      "Iteration 35983 Loss: 1.0104464292526245\n",
      "Iteration 35984 Loss: 1.0211968421936035\n",
      "Iteration 35985 Loss: 1.3507566452026367\n",
      "Iteration 35986 Loss: 0.9933862090110779\n",
      "Iteration 35987 Loss: 0.8442195057868958\n",
      "Iteration 35988 Loss: 1.471174955368042\n",
      "Iteration 35989 Loss: 0.9788568019866943\n",
      "Iteration 35989 Loss: 1.1419878005981445\n",
      "Iteration 35990 Loss: 0.8449870347976685\n",
      "Iteration 35991 Loss: 0.9280523061752319\n",
      "Iteration 35992 Loss: 1.1228832006454468\n",
      "Iteration 35993 Loss: 1.0758661031723022\n",
      "Iteration 35994 Loss: 1.0406148433685303\n",
      "Iteration 35995 Loss: 1.257291316986084\n",
      "Iteration 35996 Loss: 0.9821150302886963\n",
      "Iteration 35997 Loss: 0.9189581871032715\n",
      "Iteration 35998 Loss: 1.0172688961029053\n",
      "Iteration 35999 Loss: 0.9635198712348938\n",
      "Iteration 35999 Loss: 1.0151557922363281\n",
      "Iteration 36000 Loss: 0.9660583138465881\n",
      "Iteration 36001 Loss: 0.99411541223526\n",
      "Iteration 36002 Loss: 0.9296958446502686\n",
      "Iteration 36003 Loss: 0.8729845881462097\n",
      "Iteration 36004 Loss: 1.07759690284729\n",
      "Iteration 36005 Loss: 1.1617056131362915\n",
      "Iteration 36006 Loss: 1.0018633604049683\n",
      "Iteration 36007 Loss: 0.953744113445282\n",
      "Iteration 36008 Loss: 0.8695096373558044\n",
      "Iteration 36009 Loss: 0.8452415466308594\n",
      "Iteration 36009 Loss: 0.9672514796257019\n",
      "Iteration 36010 Loss: 0.9804814457893372\n",
      "Iteration 36011 Loss: 1.215996265411377\n",
      "Iteration 36012 Loss: 1.1289994716644287\n",
      "Iteration 36013 Loss: 1.1522345542907715\n",
      "Iteration 36014 Loss: 1.0471423864364624\n",
      "Iteration 36015 Loss: 1.0535273551940918\n",
      "Iteration 36016 Loss: 1.3136614561080933\n",
      "Iteration 36017 Loss: 1.269777774810791\n",
      "Iteration 36018 Loss: 1.1453572511672974\n",
      "Iteration 36019 Loss: 0.5931474566459656\n",
      "Iteration 36019 Loss: 1.0900325775146484\n",
      "Iteration 36020 Loss: 0.9571862816810608\n",
      "Iteration 36021 Loss: 0.9214043021202087\n",
      "Iteration 36022 Loss: 0.9399228096008301\n",
      "Iteration 36023 Loss: 0.957340657711029\n",
      "Iteration 36024 Loss: 1.2287927865982056\n",
      "Iteration 36025 Loss: 1.2084778547286987\n",
      "Iteration 36026 Loss: 0.9017119407653809\n",
      "Iteration 36027 Loss: 1.1101351976394653\n",
      "Iteration 36028 Loss: 1.185178279876709\n",
      "Iteration 36029 Loss: 0.8124573826789856\n",
      "Iteration 36029 Loss: 1.0222607851028442\n",
      "Iteration 36030 Loss: 0.9431194067001343\n",
      "Iteration 36031 Loss: 1.2895405292510986\n",
      "Iteration 36032 Loss: 0.8960292339324951\n",
      "Iteration 36033 Loss: 0.7824144959449768\n",
      "Iteration 36034 Loss: 0.7800090312957764\n",
      "Iteration 36035 Loss: 1.1266934871673584\n",
      "Iteration 36036 Loss: 1.0631327629089355\n",
      "Iteration 36037 Loss: 1.1391535997390747\n",
      "Iteration 36038 Loss: 1.2405540943145752\n",
      "Iteration 36039 Loss: 0.8610415458679199\n",
      "Iteration 36039 Loss: 1.0121687650680542\n",
      "Iteration 36040 Loss: 0.9791424870491028\n",
      "Iteration 36041 Loss: 0.8655350208282471\n",
      "Iteration 36042 Loss: 0.9169951677322388\n",
      "Iteration 36043 Loss: 1.127705454826355\n",
      "Iteration 36044 Loss: 0.9231094121932983\n",
      "Iteration 36045 Loss: 0.9178580641746521\n",
      "Iteration 36046 Loss: 0.6313574910163879\n",
      "Iteration 36047 Loss: 0.9122763276100159\n",
      "Iteration 36048 Loss: 1.2790086269378662\n",
      "Iteration 36049 Loss: 0.9226932525634766\n",
      "Iteration 36049 Loss: 0.9475681185722351\n",
      "Iteration 36050 Loss: 1.1654784679412842\n",
      "Iteration 36051 Loss: 0.8289538025856018\n",
      "Iteration 36052 Loss: 0.7504363656044006\n",
      "Iteration 36053 Loss: 0.9993628859519958\n",
      "Iteration 36054 Loss: 0.7250086069107056\n",
      "Iteration 36055 Loss: 0.8828834295272827\n",
      "Iteration 36056 Loss: 1.506514310836792\n",
      "Iteration 36057 Loss: 0.7891251444816589\n",
      "Iteration 36058 Loss: 1.157361388206482\n",
      "Iteration 36059 Loss: 1.121376633644104\n",
      "Iteration 36059 Loss: 0.9926501512527466\n",
      "Iteration 36060 Loss: 0.6655996441841125\n",
      "Iteration 36061 Loss: 1.4252409934997559\n",
      "Iteration 36062 Loss: 1.116580843925476\n",
      "Iteration 36063 Loss: 0.8894314765930176\n",
      "Iteration 36064 Loss: 0.9237973093986511\n",
      "Iteration 36065 Loss: 1.2740821838378906\n",
      "Iteration 36066 Loss: 1.027266263961792\n",
      "Iteration 36067 Loss: 1.0839580297470093\n",
      "Iteration 36068 Loss: 0.9011285901069641\n",
      "Iteration 36069 Loss: 1.1447980403900146\n",
      "Iteration 36069 Loss: 1.045188307762146\n",
      "Iteration 36070 Loss: 1.0677815675735474\n",
      "Iteration 36071 Loss: 0.6205024123191833\n",
      "Iteration 36072 Loss: 1.1406638622283936\n",
      "Iteration 36073 Loss: 0.8917316794395447\n",
      "Iteration 36074 Loss: 0.8763331174850464\n",
      "Iteration 36075 Loss: 0.7056242227554321\n",
      "Iteration 36076 Loss: 1.152327537536621\n",
      "Iteration 36077 Loss: 0.9779709577560425\n",
      "Iteration 36078 Loss: 1.2988611459732056\n",
      "Iteration 36079 Loss: 1.0607444047927856\n",
      "Iteration 36079 Loss: 0.9792541265487671\n",
      "Iteration 36080 Loss: 1.125241756439209\n",
      "Iteration 36081 Loss: 1.0531269311904907\n",
      "Iteration 36082 Loss: 0.8263313174247742\n",
      "Iteration 36083 Loss: 1.1927714347839355\n",
      "Iteration 36084 Loss: 0.9822980165481567\n",
      "Iteration 36085 Loss: 0.9805887937545776\n",
      "Iteration 36086 Loss: 1.2598756551742554\n",
      "Iteration 36087 Loss: 1.0351715087890625\n",
      "Iteration 36088 Loss: 1.0392158031463623\n",
      "Iteration 36089 Loss: 0.8635944128036499\n",
      "Iteration 36089 Loss: 1.0358214378356934\n",
      "Iteration 36090 Loss: 0.4762653112411499\n",
      "Iteration 36091 Loss: 0.9383665919303894\n",
      "Iteration 36092 Loss: 0.9096082448959351\n",
      "Iteration 36093 Loss: 1.0346200466156006\n",
      "Iteration 36094 Loss: 0.9634504914283752\n",
      "Iteration 36095 Loss: 1.1922411918640137\n",
      "Iteration 36096 Loss: 0.8799317479133606\n",
      "Iteration 36097 Loss: 0.7973847985267639\n",
      "Iteration 36098 Loss: 1.0620867013931274\n",
      "Iteration 36099 Loss: 0.7862049341201782\n",
      "Iteration 36099 Loss: 0.9040160179138184\n",
      "Iteration 36100 Loss: 0.5794211030006409\n",
      "Iteration 36101 Loss: 1.0309165716171265\n",
      "Iteration 36102 Loss: 1.0867290496826172\n",
      "Iteration 36103 Loss: 0.6508327722549438\n",
      "Iteration 36104 Loss: 1.0168553590774536\n",
      "Iteration 36105 Loss: 1.2377824783325195\n",
      "Iteration 36106 Loss: 0.9556833505630493\n",
      "Iteration 36107 Loss: 1.05776846408844\n",
      "Iteration 36108 Loss: 0.7949837446212769\n",
      "Iteration 36109 Loss: 1.0339674949645996\n",
      "Iteration 36109 Loss: 0.9444940686225891\n",
      "Iteration 36110 Loss: 1.0277550220489502\n",
      "Iteration 36111 Loss: 0.9157189130783081\n",
      "Iteration 36112 Loss: 0.9163779616355896\n",
      "Iteration 36113 Loss: 1.0099682807922363\n",
      "Iteration 36114 Loss: 1.050450086593628\n",
      "Iteration 36115 Loss: 1.170924425125122\n",
      "Iteration 36116 Loss: 1.108199954032898\n",
      "Iteration 36117 Loss: 1.2121076583862305\n",
      "Iteration 36118 Loss: 0.7475559115409851\n",
      "Iteration 36119 Loss: 1.0774189233779907\n",
      "Iteration 36119 Loss: 1.023647665977478\n",
      "Iteration 36120 Loss: 0.7646657228469849\n",
      "Iteration 36121 Loss: 0.9808545112609863\n",
      "Iteration 36122 Loss: 1.0612682104110718\n",
      "Iteration 36123 Loss: 0.8986601829528809\n",
      "Iteration 36124 Loss: 1.0393933057785034\n",
      "Iteration 36125 Loss: 0.5630367398262024\n",
      "Iteration 36126 Loss: 0.9156278967857361\n",
      "Iteration 36127 Loss: 1.122350811958313\n",
      "Iteration 36128 Loss: 0.9552834630012512\n",
      "Iteration 36129 Loss: 1.2539842128753662\n",
      "Iteration 36129 Loss: 0.955512523651123\n",
      "Iteration 36130 Loss: 0.9886179566383362\n",
      "Iteration 36131 Loss: 0.7589430212974548\n",
      "Iteration 36132 Loss: 0.9393123984336853\n",
      "Iteration 36133 Loss: 0.9629025459289551\n",
      "Iteration 36134 Loss: 1.1061501502990723\n",
      "Iteration 36135 Loss: 0.8204398155212402\n",
      "Iteration 36136 Loss: 1.1802866458892822\n",
      "Iteration 36137 Loss: 1.1012285947799683\n",
      "Iteration 36138 Loss: 1.0476009845733643\n",
      "Iteration 36139 Loss: 0.888268232345581\n",
      "Iteration 36139 Loss: 0.9793750643730164\n",
      "Iteration 36140 Loss: 0.9816434383392334\n",
      "Iteration 36141 Loss: 1.0832616090774536\n",
      "Iteration 36142 Loss: 1.151308298110962\n",
      "Iteration 36143 Loss: 0.7095648050308228\n",
      "Iteration 36144 Loss: 0.8311171531677246\n",
      "Iteration 36145 Loss: 1.288013219833374\n",
      "Iteration 36146 Loss: 0.7917636036872864\n",
      "Iteration 36147 Loss: 0.6829763650894165\n",
      "Iteration 36148 Loss: 1.196542739868164\n",
      "Iteration 36149 Loss: 0.8987550735473633\n",
      "Iteration 36149 Loss: 0.9614946246147156\n",
      "Iteration 36150 Loss: 1.1024945974349976\n",
      "Iteration 36151 Loss: 0.7372839450836182\n",
      "Iteration 36152 Loss: 0.9204596281051636\n",
      "Iteration 36153 Loss: 0.9132933020591736\n",
      "Iteration 36154 Loss: 1.0047260522842407\n",
      "Iteration 36155 Loss: 1.2626246213912964\n",
      "Iteration 36156 Loss: 0.9676709175109863\n",
      "Iteration 36157 Loss: 0.9596901535987854\n",
      "Iteration 36158 Loss: 0.7080453634262085\n",
      "Iteration 36159 Loss: 1.41176438331604\n",
      "Iteration 36159 Loss: 0.9988052248954773\n",
      "Iteration 36160 Loss: 1.082374095916748\n",
      "Iteration 36161 Loss: 1.2119660377502441\n",
      "Iteration 36162 Loss: 1.229079246520996\n",
      "Iteration 36163 Loss: 0.9395583868026733\n",
      "Iteration 36164 Loss: 1.1010148525238037\n",
      "Iteration 36165 Loss: 1.1017934083938599\n",
      "Iteration 36166 Loss: 1.0472582578659058\n",
      "Iteration 36167 Loss: 0.9078038334846497\n",
      "Iteration 36168 Loss: 0.6904164552688599\n",
      "Iteration 36169 Loss: 1.2808078527450562\n",
      "Iteration 36169 Loss: 1.0592072010040283\n",
      "Iteration 36170 Loss: 0.8661923408508301\n",
      "Iteration 36171 Loss: 1.216914176940918\n",
      "Iteration 36172 Loss: 0.9694060683250427\n",
      "Iteration 36173 Loss: 0.9521430134773254\n",
      "Iteration 36174 Loss: 1.4147372245788574\n",
      "Iteration 36175 Loss: 1.1988880634307861\n",
      "Iteration 36176 Loss: 0.8604870438575745\n",
      "Iteration 36177 Loss: 0.9097541570663452\n",
      "Iteration 36178 Loss: 0.8264836668968201\n",
      "Iteration 36179 Loss: 0.851381242275238\n",
      "Iteration 36179 Loss: 1.006638765335083\n",
      "Iteration 36180 Loss: 1.3959524631500244\n",
      "Iteration 36181 Loss: 1.2975683212280273\n",
      "Iteration 36182 Loss: 1.218173861503601\n",
      "Iteration 36183 Loss: 0.8310010433197021\n",
      "Iteration 36184 Loss: 0.8085908889770508\n",
      "Iteration 36185 Loss: 0.7552546262741089\n",
      "Iteration 36186 Loss: 1.0057628154754639\n",
      "Iteration 36187 Loss: 1.059874176979065\n",
      "Iteration 36188 Loss: 1.0285519361495972\n",
      "Iteration 36189 Loss: 0.917021632194519\n",
      "Iteration 36189 Loss: 1.0317752361297607\n",
      "Iteration 36190 Loss: 1.0915844440460205\n",
      "Iteration 36191 Loss: 0.88484126329422\n",
      "Iteration 36192 Loss: 0.8596665263175964\n",
      "Iteration 36193 Loss: 0.8591974377632141\n",
      "Iteration 36194 Loss: 1.1698075532913208\n",
      "Iteration 36195 Loss: 0.8868168592453003\n",
      "Iteration 36196 Loss: 0.9445776343345642\n",
      "Iteration 36197 Loss: 0.8260674476623535\n",
      "Iteration 36198 Loss: 0.867662250995636\n",
      "Iteration 36199 Loss: 1.1233816146850586\n",
      "Iteration 36199 Loss: 0.9513603448867798\n",
      "Iteration 36200 Loss: 1.1364953517913818\n",
      "Iteration 36201 Loss: 0.8793783187866211\n",
      "Iteration 36202 Loss: 1.091339349746704\n",
      "Iteration 36203 Loss: 1.1157474517822266\n",
      "Iteration 36204 Loss: 0.8248301148414612\n",
      "Iteration 36205 Loss: 0.9170486927032471\n",
      "Iteration 36206 Loss: 1.0040181875228882\n",
      "Iteration 36207 Loss: 1.1473909616470337\n",
      "Iteration 36208 Loss: 1.1994569301605225\n",
      "Iteration 36209 Loss: 0.9701007008552551\n",
      "Iteration 36209 Loss: 1.0285805463790894\n",
      "Iteration 36210 Loss: 0.979918360710144\n",
      "Iteration 36211 Loss: 1.0356024503707886\n",
      "Iteration 36212 Loss: 0.9259035587310791\n",
      "Iteration 36213 Loss: 0.7998027801513672\n",
      "Iteration 36214 Loss: 0.9210026860237122\n",
      "Iteration 36215 Loss: 1.0585988759994507\n",
      "Iteration 36216 Loss: 1.03284752368927\n",
      "Iteration 36217 Loss: 1.0235083103179932\n",
      "Iteration 36218 Loss: 1.1375707387924194\n",
      "Iteration 36219 Loss: 1.0943721532821655\n",
      "Iteration 36219 Loss: 1.0009127855300903\n",
      "Iteration 36220 Loss: 0.6005681753158569\n",
      "Iteration 36221 Loss: 1.0041148662567139\n",
      "Iteration 36222 Loss: 1.1228920221328735\n",
      "Iteration 36223 Loss: 1.1746573448181152\n",
      "Iteration 36224 Loss: 1.4759554862976074\n",
      "Iteration 36225 Loss: 1.0380091667175293\n",
      "Iteration 36226 Loss: 0.9559699892997742\n",
      "Iteration 36227 Loss: 1.1431727409362793\n",
      "Iteration 36228 Loss: 0.8470296859741211\n",
      "Iteration 36229 Loss: 1.0294790267944336\n",
      "Iteration 36229 Loss: 1.039184808731079\n",
      "Iteration 36230 Loss: 0.9216772317886353\n",
      "Iteration 36231 Loss: 0.9261043071746826\n",
      "Iteration 36232 Loss: 0.9322546720504761\n",
      "Iteration 36233 Loss: 0.8133970499038696\n",
      "Iteration 36234 Loss: 0.9259086847305298\n",
      "Iteration 36235 Loss: 0.8279198408126831\n",
      "Iteration 36236 Loss: 0.9383177161216736\n",
      "Iteration 36237 Loss: 0.7802281379699707\n",
      "Iteration 36238 Loss: 1.0487388372421265\n",
      "Iteration 36239 Loss: 0.9809920787811279\n",
      "Iteration 36239 Loss: 0.9095537066459656\n",
      "Iteration 36240 Loss: 1.0103825330734253\n",
      "Iteration 36241 Loss: 0.9919107556343079\n",
      "Iteration 36242 Loss: 0.8963793516159058\n",
      "Iteration 36243 Loss: 0.9186373949050903\n",
      "Iteration 36244 Loss: 0.857268214225769\n",
      "Iteration 36245 Loss: 0.795659065246582\n",
      "Iteration 36246 Loss: 1.2256298065185547\n",
      "Iteration 36247 Loss: 1.2250556945800781\n",
      "Iteration 36248 Loss: 1.1205809116363525\n",
      "Iteration 36249 Loss: 0.7656927108764648\n",
      "Iteration 36249 Loss: 0.9807195663452148\n",
      "Iteration 36250 Loss: 1.0835483074188232\n",
      "Iteration 36251 Loss: 0.9508091807365417\n",
      "Iteration 36252 Loss: 0.9667003154754639\n",
      "Iteration 36253 Loss: 0.8713477253913879\n",
      "Iteration 36254 Loss: 1.1277449131011963\n",
      "Iteration 36255 Loss: 0.9171264171600342\n",
      "Iteration 36256 Loss: 1.1815922260284424\n",
      "Iteration 36257 Loss: 0.7622311115264893\n",
      "Iteration 36258 Loss: 0.9527767300605774\n",
      "Iteration 36259 Loss: 1.4268420934677124\n",
      "Iteration 36259 Loss: 1.0240719318389893\n",
      "Iteration 36260 Loss: 1.0561081171035767\n",
      "Iteration 36261 Loss: 1.302569031715393\n",
      "Iteration 36262 Loss: 1.0485575199127197\n",
      "Iteration 36263 Loss: 0.6628471612930298\n",
      "Iteration 36264 Loss: 1.3396577835083008\n",
      "Iteration 36265 Loss: 0.8762525916099548\n",
      "Iteration 36266 Loss: 1.0771865844726562\n",
      "Iteration 36267 Loss: 1.0037637948989868\n",
      "Iteration 36268 Loss: 0.7988362908363342\n",
      "Iteration 36269 Loss: 0.7915933728218079\n",
      "Iteration 36269 Loss: 0.9957372546195984\n",
      "Iteration 36270 Loss: 1.2333868741989136\n",
      "Iteration 36271 Loss: 0.8403670191764832\n",
      "Iteration 36272 Loss: 0.8703203797340393\n",
      "Iteration 36273 Loss: 0.959190309047699\n",
      "Iteration 36274 Loss: 0.7943507432937622\n",
      "Iteration 36275 Loss: 1.0691921710968018\n",
      "Iteration 36276 Loss: 0.7148365378379822\n",
      "Iteration 36277 Loss: 0.9062656760215759\n",
      "Iteration 36278 Loss: 0.8153402805328369\n",
      "Iteration 36279 Loss: 1.0461294651031494\n",
      "Iteration 36279 Loss: 0.924937903881073\n",
      "Iteration 36280 Loss: 0.7653789520263672\n",
      "Iteration 36281 Loss: 1.2302554845809937\n",
      "Iteration 36282 Loss: 1.1869007349014282\n",
      "Iteration 36283 Loss: 1.1830413341522217\n",
      "Iteration 36284 Loss: 0.9250696897506714\n",
      "Iteration 36285 Loss: 0.9024826884269714\n",
      "Iteration 36286 Loss: 1.0830950736999512\n",
      "Iteration 36287 Loss: 1.2212735414505005\n",
      "Iteration 36288 Loss: 0.6692147850990295\n",
      "Iteration 36289 Loss: 0.6534863114356995\n",
      "Iteration 36289 Loss: 0.9820197820663452\n",
      "Iteration 36290 Loss: 0.818383514881134\n",
      "Iteration 36291 Loss: 0.8339968323707581\n",
      "Iteration 36292 Loss: 0.8266753554344177\n",
      "Iteration 36293 Loss: 0.9900572299957275\n",
      "Iteration 36294 Loss: 1.2817177772521973\n",
      "Iteration 36295 Loss: 0.8979963064193726\n",
      "Iteration 36296 Loss: 0.9366087317466736\n",
      "Iteration 36297 Loss: 1.1907124519348145\n",
      "Iteration 36298 Loss: 1.0324403047561646\n",
      "Iteration 36299 Loss: 0.7800494432449341\n",
      "Iteration 36299 Loss: 0.9588638544082642\n",
      "Iteration 36300 Loss: 1.1006911993026733\n",
      "Iteration 36301 Loss: 1.1552014350891113\n",
      "Iteration 36302 Loss: 1.308682918548584\n",
      "Iteration 36303 Loss: 1.000232219696045\n",
      "Iteration 36304 Loss: 0.5594956278800964\n",
      "Iteration 36305 Loss: 0.9794566035270691\n",
      "Iteration 36306 Loss: 1.0764491558074951\n",
      "Iteration 36307 Loss: 1.067675232887268\n",
      "Iteration 36308 Loss: 1.2234349250793457\n",
      "Iteration 36309 Loss: 1.2151575088500977\n",
      "Iteration 36309 Loss: 1.0686477422714233\n",
      "Iteration 36310 Loss: 1.1149028539657593\n",
      "Iteration 36311 Loss: 1.1422655582427979\n",
      "Iteration 36312 Loss: 0.9050853848457336\n",
      "Iteration 36313 Loss: 0.8092493414878845\n",
      "Iteration 36314 Loss: 1.1323010921478271\n",
      "Iteration 36315 Loss: 1.0247374773025513\n",
      "Iteration 36316 Loss: 1.0361754894256592\n",
      "Iteration 36317 Loss: 1.146262526512146\n",
      "Iteration 36318 Loss: 0.6409587860107422\n",
      "Iteration 36319 Loss: 1.3121241331100464\n",
      "Iteration 36319 Loss: 1.0264062881469727\n",
      "Iteration 36320 Loss: 0.9152248501777649\n",
      "Iteration 36321 Loss: 1.2111226320266724\n",
      "Iteration 36322 Loss: 0.9534661173820496\n",
      "Iteration 36323 Loss: 1.02414071559906\n",
      "Iteration 36324 Loss: 0.872297465801239\n",
      "Iteration 36325 Loss: 0.9024428725242615\n",
      "Iteration 36326 Loss: 1.0650439262390137\n",
      "Iteration 36327 Loss: 1.5547893047332764\n",
      "Iteration 36328 Loss: 1.0114355087280273\n",
      "Iteration 36329 Loss: 0.8904677033424377\n",
      "Iteration 36329 Loss: 1.0400431156158447\n",
      "Iteration 36330 Loss: 1.0441352128982544\n",
      "Iteration 36331 Loss: 0.8725194334983826\n",
      "Iteration 36332 Loss: 0.8887940645217896\n",
      "Iteration 36333 Loss: 0.8076127767562866\n",
      "Iteration 36334 Loss: 0.9959934949874878\n",
      "Iteration 36335 Loss: 1.1077983379364014\n",
      "Iteration 36336 Loss: 0.9544528126716614\n",
      "Iteration 36337 Loss: 1.3141405582427979\n",
      "Iteration 36338 Loss: 0.8415802717208862\n",
      "Iteration 36339 Loss: 1.075600266456604\n",
      "Iteration 36339 Loss: 0.9902626872062683\n",
      "Iteration 36340 Loss: 0.7448340058326721\n",
      "Iteration 36341 Loss: 0.9695577025413513\n",
      "Iteration 36342 Loss: 1.0343570709228516\n",
      "Iteration 36343 Loss: 1.4114539623260498\n",
      "Iteration 36344 Loss: 0.8206281065940857\n",
      "Iteration 36345 Loss: 0.9235049486160278\n",
      "Iteration 36346 Loss: 1.0017976760864258\n",
      "Iteration 36347 Loss: 0.7371347546577454\n",
      "Iteration 36348 Loss: 1.1835813522338867\n",
      "Iteration 36349 Loss: 1.0151249170303345\n",
      "Iteration 36349 Loss: 0.9841974377632141\n",
      "Iteration 36350 Loss: 0.8956969380378723\n",
      "Iteration 36351 Loss: 0.7771685719490051\n",
      "Iteration 36352 Loss: 0.8359062075614929\n",
      "Iteration 36353 Loss: 1.1264880895614624\n",
      "Iteration 36354 Loss: 1.27860426902771\n",
      "Iteration 36355 Loss: 0.6865790486335754\n",
      "Iteration 36356 Loss: 0.9408569931983948\n",
      "Iteration 36357 Loss: 0.753531813621521\n",
      "Iteration 36358 Loss: 1.3966834545135498\n",
      "Iteration 36359 Loss: 1.2095030546188354\n",
      "Iteration 36359 Loss: 0.99010169506073\n",
      "Iteration 36360 Loss: 0.8508242964744568\n",
      "Iteration 36361 Loss: 0.993337869644165\n",
      "Iteration 36362 Loss: 0.9713911414146423\n",
      "Iteration 36363 Loss: 0.7188650369644165\n",
      "Iteration 36364 Loss: 0.8129055500030518\n",
      "Iteration 36365 Loss: 1.0358524322509766\n",
      "Iteration 36366 Loss: 1.1465317010879517\n",
      "Iteration 36367 Loss: 1.5705897808074951\n",
      "Iteration 36368 Loss: 1.1056591272354126\n",
      "Iteration 36369 Loss: 1.0425816774368286\n",
      "Iteration 36369 Loss: 1.0248539447784424\n",
      "Iteration 36370 Loss: 0.9337059259414673\n",
      "Iteration 36371 Loss: 1.299964189529419\n",
      "Iteration 36372 Loss: 1.1001102924346924\n",
      "Iteration 36373 Loss: 0.7556352019309998\n",
      "Iteration 36374 Loss: 0.9135417938232422\n",
      "Iteration 36375 Loss: 1.1107946634292603\n",
      "Iteration 36376 Loss: 1.0145436525344849\n",
      "Iteration 36377 Loss: 0.9194241166114807\n",
      "Iteration 36378 Loss: 0.5659939050674438\n",
      "Iteration 36379 Loss: 0.9130027294158936\n",
      "Iteration 36379 Loss: 0.9526716470718384\n",
      "Iteration 36380 Loss: 1.0054466724395752\n",
      "Iteration 36381 Loss: 0.8594911098480225\n",
      "Iteration 36382 Loss: 0.9548113346099854\n",
      "Iteration 36383 Loss: 1.3676005601882935\n",
      "Iteration 36384 Loss: 1.1622426509857178\n",
      "Iteration 36385 Loss: 1.2804133892059326\n",
      "Iteration 36386 Loss: 0.9552841782569885\n",
      "Iteration 36387 Loss: 0.785256028175354\n",
      "Iteration 36388 Loss: 1.4271348714828491\n",
      "Iteration 36389 Loss: 0.9921628832817078\n",
      "Iteration 36389 Loss: 1.0789844989776611\n",
      "Iteration 36390 Loss: 1.0465855598449707\n",
      "Iteration 36391 Loss: 1.2564834356307983\n",
      "Iteration 36392 Loss: 0.941791296005249\n",
      "Iteration 36393 Loss: 0.8896702527999878\n",
      "Iteration 36394 Loss: 1.1358011960983276\n",
      "Iteration 36395 Loss: 0.8941192626953125\n",
      "Iteration 36396 Loss: 1.13785719871521\n",
      "Iteration 36397 Loss: 0.9996291995048523\n",
      "Iteration 36398 Loss: 1.2265923023223877\n",
      "Iteration 36399 Loss: 0.9439602494239807\n",
      "Iteration 36399 Loss: 1.0472490787506104\n",
      "Iteration 36400 Loss: 1.126451849937439\n",
      "Iteration 36401 Loss: 0.9521895051002502\n",
      "Iteration 36402 Loss: 1.212820053100586\n",
      "Iteration 36403 Loss: 1.2230260372161865\n",
      "Iteration 36404 Loss: 0.6044445633888245\n",
      "Iteration 36405 Loss: 0.9728578329086304\n",
      "Iteration 36406 Loss: 0.7865594029426575\n",
      "Iteration 36407 Loss: 1.129507303237915\n",
      "Iteration 36408 Loss: 1.1517465114593506\n",
      "Iteration 36409 Loss: 1.4420108795166016\n",
      "Iteration 36409 Loss: 1.0601613521575928\n",
      "Iteration 36410 Loss: 1.1202096939086914\n",
      "Iteration 36411 Loss: 0.7781373262405396\n",
      "Iteration 36412 Loss: 0.9780697226524353\n",
      "Iteration 36413 Loss: 0.9164329767227173\n",
      "Iteration 36414 Loss: 1.1276061534881592\n",
      "Iteration 36415 Loss: 1.1638554334640503\n",
      "Iteration 36416 Loss: 1.0581265687942505\n",
      "Iteration 36417 Loss: 1.0798625946044922\n",
      "Iteration 36418 Loss: 0.8201534152030945\n",
      "Iteration 36419 Loss: 0.8470603823661804\n",
      "Iteration 36419 Loss: 0.9889513850212097\n",
      "Iteration 36420 Loss: 1.02607262134552\n",
      "Iteration 36421 Loss: 1.1875121593475342\n",
      "Iteration 36422 Loss: 1.1410109996795654\n",
      "Iteration 36423 Loss: 1.1053597927093506\n",
      "Iteration 36424 Loss: 0.9396811723709106\n",
      "Iteration 36425 Loss: 0.7953190207481384\n",
      "Iteration 36426 Loss: 1.0343295335769653\n",
      "Iteration 36427 Loss: 1.0532240867614746\n",
      "Iteration 36428 Loss: 0.8690336346626282\n",
      "Iteration 36429 Loss: 0.9993172287940979\n",
      "Iteration 36429 Loss: 1.0150859355926514\n",
      "Iteration 36430 Loss: 0.7817752957344055\n",
      "Iteration 36431 Loss: 1.3188127279281616\n",
      "Iteration 36432 Loss: 0.794365644454956\n",
      "Iteration 36433 Loss: 1.0664604902267456\n",
      "Iteration 36434 Loss: 0.872308611869812\n",
      "Iteration 36435 Loss: 1.1144590377807617\n",
      "Iteration 36436 Loss: 1.137766718864441\n",
      "Iteration 36437 Loss: 0.9215021729469299\n",
      "Iteration 36438 Loss: 0.9354134798049927\n",
      "Iteration 36439 Loss: 0.9033647179603577\n",
      "Iteration 36439 Loss: 0.9846229553222656\n",
      "Iteration 36440 Loss: 0.8350725173950195\n",
      "Iteration 36441 Loss: 0.7018531560897827\n",
      "Iteration 36442 Loss: 0.7243276238441467\n",
      "Iteration 36443 Loss: 1.134744644165039\n",
      "Iteration 36444 Loss: 0.8621084690093994\n",
      "Iteration 36445 Loss: 0.9446858763694763\n",
      "Iteration 36446 Loss: 0.8501251339912415\n",
      "Iteration 36447 Loss: 0.9007778167724609\n",
      "Iteration 36448 Loss: 0.8069241046905518\n",
      "Iteration 36449 Loss: 1.0283554792404175\n",
      "Iteration 36449 Loss: 0.878897488117218\n",
      "Iteration 36450 Loss: 0.7353708148002625\n",
      "Iteration 36451 Loss: 0.6971176266670227\n",
      "Iteration 36452 Loss: 1.1676288843154907\n",
      "Iteration 36453 Loss: 1.115002155303955\n",
      "Iteration 36454 Loss: 0.7848891019821167\n",
      "Iteration 36455 Loss: 0.9705031514167786\n",
      "Iteration 36456 Loss: 0.6855406165122986\n",
      "Iteration 36457 Loss: 0.9287247657775879\n",
      "Iteration 36458 Loss: 1.3342686891555786\n",
      "Iteration 36459 Loss: 1.354256510734558\n",
      "Iteration 36459 Loss: 0.977330207824707\n",
      "Iteration 36460 Loss: 1.0105100870132446\n",
      "Iteration 36461 Loss: 0.9170022010803223\n",
      "Iteration 36462 Loss: 0.8836327195167542\n",
      "Iteration 36463 Loss: 0.7641613483428955\n",
      "Iteration 36464 Loss: 1.1546739339828491\n",
      "Iteration 36465 Loss: 0.8883382678031921\n",
      "Iteration 36466 Loss: 1.0356863737106323\n",
      "Iteration 36467 Loss: 0.8720868229866028\n",
      "Iteration 36468 Loss: 0.8007029294967651\n",
      "Iteration 36469 Loss: 0.7872962355613708\n",
      "Iteration 36469 Loss: 0.9114090204238892\n",
      "Iteration 36470 Loss: 0.7726293802261353\n",
      "Iteration 36471 Loss: 1.1120907068252563\n",
      "Iteration 36472 Loss: 0.9156168103218079\n",
      "Iteration 36473 Loss: 1.2817100286483765\n",
      "Iteration 36474 Loss: 0.9556589126586914\n",
      "Iteration 36475 Loss: 0.900519073009491\n",
      "Iteration 36476 Loss: 0.9705736041069031\n",
      "Iteration 36477 Loss: 1.1289055347442627\n",
      "Iteration 36478 Loss: 0.8202068209648132\n",
      "Iteration 36479 Loss: 0.8239739537239075\n",
      "Iteration 36479 Loss: 0.968188464641571\n",
      "Iteration 36480 Loss: 0.9098967909812927\n",
      "Iteration 36481 Loss: 1.3154469728469849\n",
      "Iteration 36482 Loss: 0.7420880198478699\n",
      "Iteration 36483 Loss: 1.1524779796600342\n",
      "Iteration 36484 Loss: 1.0615029335021973\n",
      "Iteration 36485 Loss: 0.9855749607086182\n",
      "Iteration 36486 Loss: 1.066599726676941\n",
      "Iteration 36487 Loss: 0.931746780872345\n",
      "Iteration 36488 Loss: 0.7402899861335754\n",
      "Iteration 36489 Loss: 1.0881330966949463\n",
      "Iteration 36489 Loss: 0.9993757009506226\n",
      "Iteration 36490 Loss: 0.7767980694770813\n",
      "Iteration 36491 Loss: 1.1289457082748413\n",
      "Iteration 36492 Loss: 1.17235267162323\n",
      "Iteration 36493 Loss: 1.0896375179290771\n",
      "Iteration 36494 Loss: 1.094651222229004\n",
      "Iteration 36495 Loss: 0.9814023375511169\n",
      "Iteration 36496 Loss: 0.9803131222724915\n",
      "Iteration 36497 Loss: 1.0296558141708374\n",
      "Iteration 36498 Loss: 1.1067579984664917\n",
      "Iteration 36499 Loss: 1.3545384407043457\n",
      "Iteration 36499 Loss: 1.0715053081512451\n",
      "Iteration 36500 Loss: 0.9202649593353271\n",
      "Iteration 36501 Loss: 0.6141592264175415\n",
      "Iteration 36502 Loss: 0.9720181822776794\n",
      "Iteration 36503 Loss: 0.7882871031761169\n",
      "Iteration 36504 Loss: 1.094007134437561\n",
      "Iteration 36505 Loss: 1.0134755373001099\n",
      "Iteration 36506 Loss: 1.0471552610397339\n",
      "Iteration 36507 Loss: 0.9475518465042114\n",
      "Iteration 36508 Loss: 0.9938221573829651\n",
      "Iteration 36509 Loss: 0.9563076496124268\n",
      "Iteration 36509 Loss: 0.9347048997879028\n",
      "Iteration 36510 Loss: 0.9548290371894836\n",
      "Iteration 36511 Loss: 1.1064516305923462\n",
      "Iteration 36512 Loss: 0.5933759212493896\n",
      "Iteration 36513 Loss: 0.8625974059104919\n",
      "Iteration 36514 Loss: 1.289692759513855\n",
      "Iteration 36515 Loss: 0.752010703086853\n",
      "Iteration 36516 Loss: 0.9222906827926636\n",
      "Iteration 36517 Loss: 1.0863516330718994\n",
      "Iteration 36518 Loss: 0.8959674835205078\n",
      "Iteration 36519 Loss: 0.9926169514656067\n",
      "Iteration 36519 Loss: 0.9456183314323425\n",
      "Iteration 36520 Loss: 0.9578613042831421\n",
      "Iteration 36521 Loss: 1.0495142936706543\n",
      "Iteration 36522 Loss: 0.9070884585380554\n",
      "Iteration 36523 Loss: 1.1940611600875854\n",
      "Iteration 36524 Loss: 1.030524492263794\n",
      "Iteration 36525 Loss: 1.0896188020706177\n",
      "Iteration 36526 Loss: 0.8145366311073303\n",
      "Iteration 36527 Loss: 0.7202742695808411\n",
      "Iteration 36528 Loss: 1.2001322507858276\n",
      "Iteration 36529 Loss: 1.2083956003189087\n",
      "Iteration 36529 Loss: 1.0172007083892822\n",
      "Iteration 36530 Loss: 0.7600192427635193\n",
      "Iteration 36531 Loss: 1.0196274518966675\n",
      "Iteration 36532 Loss: 1.098616123199463\n",
      "Iteration 36533 Loss: 0.7176950573921204\n",
      "Iteration 36534 Loss: 0.8182103037834167\n",
      "Iteration 36535 Loss: 0.8504278063774109\n",
      "Iteration 36536 Loss: 1.1115399599075317\n",
      "Iteration 36537 Loss: 1.3056529760360718\n",
      "Iteration 36538 Loss: 0.951129674911499\n",
      "Iteration 36539 Loss: 1.050445556640625\n",
      "Iteration 36539 Loss: 0.9683364033699036\n",
      "Iteration 36540 Loss: 0.7783414721488953\n",
      "Iteration 36541 Loss: 0.7830168008804321\n",
      "Iteration 36542 Loss: 0.8731058835983276\n",
      "Iteration 36543 Loss: 0.8945930004119873\n",
      "Iteration 36544 Loss: 1.1791280508041382\n",
      "Iteration 36545 Loss: 1.0162498950958252\n",
      "Iteration 36546 Loss: 0.940592885017395\n",
      "Iteration 36547 Loss: 1.2587803602218628\n",
      "Iteration 36548 Loss: 1.1032122373580933\n",
      "Iteration 36549 Loss: 0.8595826029777527\n",
      "Iteration 36549 Loss: 0.9686603546142578\n",
      "Iteration 36550 Loss: 1.117011547088623\n",
      "Iteration 36551 Loss: 1.3640096187591553\n",
      "Iteration 36552 Loss: 0.87972491979599\n",
      "Iteration 36553 Loss: 1.0255345106124878\n",
      "Iteration 36554 Loss: 1.2679554224014282\n",
      "Iteration 36555 Loss: 1.0238019227981567\n",
      "Iteration 36556 Loss: 1.1442950963974\n",
      "Iteration 36557 Loss: 1.0248091220855713\n",
      "Iteration 36558 Loss: 1.1304839849472046\n",
      "Iteration 36559 Loss: 1.0398887395858765\n",
      "Iteration 36559 Loss: 1.101751446723938\n",
      "Iteration 36560 Loss: 0.8343758583068848\n",
      "Iteration 36561 Loss: 0.9301846623420715\n",
      "Iteration 36562 Loss: 0.8278945088386536\n",
      "Iteration 36563 Loss: 1.2323453426361084\n",
      "Iteration 36564 Loss: 0.7174693942070007\n",
      "Iteration 36565 Loss: 0.7172371745109558\n",
      "Iteration 36566 Loss: 1.0588009357452393\n",
      "Iteration 36567 Loss: 1.1761269569396973\n",
      "Iteration 36568 Loss: 1.016380786895752\n",
      "Iteration 36569 Loss: 1.1716700792312622\n",
      "Iteration 36569 Loss: 0.9682485461235046\n",
      "Iteration 36570 Loss: 1.2211495637893677\n",
      "Iteration 36571 Loss: 1.2576338052749634\n",
      "Iteration 36572 Loss: 0.7940436005592346\n",
      "Iteration 36573 Loss: 1.0147416591644287\n",
      "Iteration 36574 Loss: 0.8228273987770081\n",
      "Iteration 36575 Loss: 0.7782744765281677\n",
      "Iteration 36576 Loss: 1.0069282054901123\n",
      "Iteration 36577 Loss: 1.3572700023651123\n",
      "Iteration 36578 Loss: 0.847361147403717\n",
      "Iteration 36579 Loss: 0.8504142761230469\n",
      "Iteration 36579 Loss: 0.9950644373893738\n",
      "Iteration 36580 Loss: 0.983379065990448\n",
      "Iteration 36581 Loss: 1.1745182275772095\n",
      "Iteration 36582 Loss: 1.1752526760101318\n",
      "Iteration 36583 Loss: 0.9462641477584839\n",
      "Iteration 36584 Loss: 0.7644705772399902\n",
      "Iteration 36585 Loss: 1.0893428325653076\n",
      "Iteration 36586 Loss: 1.0294996500015259\n",
      "Iteration 36587 Loss: 0.5136152505874634\n",
      "Iteration 36588 Loss: 1.2454109191894531\n",
      "Iteration 36589 Loss: 1.4434387683868408\n",
      "Iteration 36589 Loss: 1.0365192890167236\n",
      "Iteration 36590 Loss: 0.9374503493309021\n",
      "Iteration 36591 Loss: 1.2417865991592407\n",
      "Iteration 36592 Loss: 0.8168359994888306\n",
      "Iteration 36593 Loss: 0.9168428778648376\n",
      "Iteration 36594 Loss: 0.790244996547699\n",
      "Iteration 36595 Loss: 0.8281204700469971\n",
      "Iteration 36596 Loss: 0.7839086055755615\n",
      "Iteration 36597 Loss: 0.9077550172805786\n",
      "Iteration 36598 Loss: 0.7110570669174194\n",
      "Iteration 36599 Loss: 0.9408143162727356\n",
      "Iteration 36599 Loss: 0.887481689453125\n",
      "Iteration 36600 Loss: 0.4855225682258606\n",
      "Iteration 36601 Loss: 0.7620330452919006\n",
      "Iteration 36602 Loss: 1.1033769845962524\n",
      "Iteration 36603 Loss: 0.7034810781478882\n",
      "Iteration 36604 Loss: 0.9186704754829407\n",
      "Iteration 36605 Loss: 1.346633791923523\n",
      "Iteration 36606 Loss: 0.9938132762908936\n",
      "Iteration 36607 Loss: 0.9790907502174377\n",
      "Iteration 36608 Loss: 0.9121842980384827\n",
      "Iteration 36609 Loss: 0.988271951675415\n",
      "Iteration 36609 Loss: 0.9193078279495239\n",
      "Iteration 36610 Loss: 0.9582575559616089\n",
      "Iteration 36611 Loss: 0.7315096855163574\n",
      "Iteration 36612 Loss: 1.2746392488479614\n",
      "Iteration 36613 Loss: 1.0296930074691772\n",
      "Iteration 36614 Loss: 0.8838977813720703\n",
      "Iteration 36615 Loss: 0.9964430332183838\n",
      "Iteration 36616 Loss: 0.8254417181015015\n",
      "Iteration 36617 Loss: 1.3862254619598389\n",
      "Iteration 36618 Loss: 0.8243522047996521\n",
      "Iteration 36619 Loss: 1.2212083339691162\n",
      "Iteration 36619 Loss: 1.0131667852401733\n",
      "Iteration 36620 Loss: 0.8825086951255798\n",
      "Iteration 36621 Loss: 1.4147347211837769\n",
      "Iteration 36622 Loss: 0.8093666434288025\n",
      "Iteration 36623 Loss: 1.2068233489990234\n",
      "Iteration 36624 Loss: 1.0047883987426758\n",
      "Iteration 36625 Loss: 0.9618006944656372\n",
      "Iteration 36626 Loss: 0.9019231796264648\n",
      "Iteration 36627 Loss: 0.5632916688919067\n",
      "Iteration 36628 Loss: 0.954730212688446\n",
      "Iteration 36629 Loss: 0.7162470817565918\n",
      "Iteration 36629 Loss: 0.9416214227676392\n",
      "Iteration 36630 Loss: 0.538178026676178\n",
      "Iteration 36631 Loss: 1.0323435068130493\n",
      "Iteration 36632 Loss: 0.7803529500961304\n",
      "Iteration 36633 Loss: 0.6741260886192322\n",
      "Iteration 36634 Loss: 0.639432430267334\n",
      "Iteration 36635 Loss: 0.911185622215271\n",
      "Iteration 36636 Loss: 1.1806672811508179\n",
      "Iteration 36637 Loss: 1.0587544441223145\n",
      "Iteration 36638 Loss: 0.679510772228241\n",
      "Iteration 36639 Loss: 0.8763911724090576\n",
      "Iteration 36639 Loss: 0.8370943069458008\n",
      "Iteration 36640 Loss: 0.6175093054771423\n",
      "Iteration 36641 Loss: 1.094599723815918\n",
      "Iteration 36642 Loss: 0.9573123455047607\n",
      "Iteration 36643 Loss: 1.0293245315551758\n",
      "Iteration 36644 Loss: 0.9452937841415405\n",
      "Iteration 36645 Loss: 0.8349609971046448\n",
      "Iteration 36646 Loss: 0.9158404469490051\n",
      "Iteration 36647 Loss: 1.3393332958221436\n",
      "Iteration 36648 Loss: 1.1249419450759888\n",
      "Iteration 36649 Loss: 0.9138354659080505\n",
      "Iteration 36649 Loss: 0.9772952198982239\n",
      "Iteration 36650 Loss: 0.7722304463386536\n",
      "Iteration 36651 Loss: 0.858596920967102\n",
      "Iteration 36652 Loss: 1.3119335174560547\n",
      "Iteration 36653 Loss: 0.751892626285553\n",
      "Iteration 36654 Loss: 0.7740064859390259\n",
      "Iteration 36655 Loss: 1.0926415920257568\n",
      "Iteration 36656 Loss: 0.8008143305778503\n",
      "Iteration 36657 Loss: 0.9016210436820984\n",
      "Iteration 36658 Loss: 0.9835542440414429\n",
      "Iteration 36659 Loss: 1.0288347005844116\n",
      "Iteration 36659 Loss: 0.9276126027107239\n",
      "Iteration 36660 Loss: 1.1782557964324951\n",
      "Iteration 36661 Loss: 1.0664141178131104\n",
      "Iteration 36662 Loss: 0.9081512093544006\n",
      "Iteration 36663 Loss: 1.3248214721679688\n",
      "Iteration 36664 Loss: 0.9846790432929993\n",
      "Iteration 36665 Loss: 1.0605442523956299\n",
      "Iteration 36666 Loss: 1.081007957458496\n",
      "Iteration 36667 Loss: 1.1157230138778687\n",
      "Iteration 36668 Loss: 1.0540504455566406\n",
      "Iteration 36669 Loss: 1.1771067380905151\n",
      "Iteration 36669 Loss: 1.0950753688812256\n",
      "Iteration 36670 Loss: 1.1054648160934448\n",
      "Iteration 36671 Loss: 0.6718156337738037\n",
      "Iteration 36672 Loss: 1.1079457998275757\n",
      "Iteration 36673 Loss: 1.2790447473526\n",
      "Iteration 36674 Loss: 1.053320288658142\n",
      "Iteration 36675 Loss: 1.3611783981323242\n",
      "Iteration 36676 Loss: 1.0673201084136963\n",
      "Iteration 36677 Loss: 1.1779487133026123\n",
      "Iteration 36678 Loss: 0.7038790583610535\n",
      "Iteration 36679 Loss: 0.7411268949508667\n",
      "Iteration 36679 Loss: 1.0269043445587158\n",
      "Iteration 36680 Loss: 0.777717649936676\n",
      "Iteration 36681 Loss: 1.23819899559021\n",
      "Iteration 36682 Loss: 1.0171175003051758\n",
      "Iteration 36683 Loss: 1.0970113277435303\n",
      "Iteration 36684 Loss: 1.1519941091537476\n",
      "Iteration 36685 Loss: 0.7382100224494934\n",
      "Iteration 36686 Loss: 0.9042193293571472\n",
      "Iteration 36687 Loss: 1.2222537994384766\n",
      "Iteration 36688 Loss: 1.0904946327209473\n",
      "Iteration 36689 Loss: 1.3880844116210938\n",
      "Iteration 36689 Loss: 1.062530279159546\n",
      "Iteration 36690 Loss: 0.9374880790710449\n",
      "Iteration 36691 Loss: 0.7990853190422058\n",
      "Iteration 36692 Loss: 0.8655270338058472\n",
      "Iteration 36693 Loss: 0.9737565517425537\n",
      "Iteration 36694 Loss: 1.006905198097229\n",
      "Iteration 36695 Loss: 0.8717206716537476\n",
      "Iteration 36696 Loss: 1.2175030708312988\n",
      "Iteration 36697 Loss: 0.7777963876724243\n",
      "Iteration 36698 Loss: 0.7878167629241943\n",
      "Iteration 36699 Loss: 0.8052698373794556\n",
      "Iteration 36699 Loss: 0.9042868614196777\n",
      "Iteration 36700 Loss: 0.9470484256744385\n",
      "Iteration 36701 Loss: 1.1418384313583374\n",
      "Iteration 36702 Loss: 0.6523592472076416\n",
      "Iteration 36703 Loss: 1.1509127616882324\n",
      "Iteration 36704 Loss: 0.9788030385971069\n",
      "Iteration 36705 Loss: 1.1269876956939697\n",
      "Iteration 36706 Loss: 1.4569069147109985\n",
      "Iteration 36707 Loss: 1.1073812246322632\n",
      "Iteration 36708 Loss: 0.8888930082321167\n",
      "Iteration 36709 Loss: 1.077750563621521\n",
      "Iteration 36709 Loss: 1.0528881549835205\n",
      "Iteration 36710 Loss: 1.207159399986267\n",
      "Iteration 36711 Loss: 0.8881876468658447\n",
      "Iteration 36712 Loss: 0.8668662309646606\n",
      "Iteration 36713 Loss: 1.0160064697265625\n",
      "Iteration 36714 Loss: 0.9452911615371704\n",
      "Iteration 36715 Loss: 1.325539469718933\n",
      "Iteration 36716 Loss: 1.0314643383026123\n",
      "Iteration 36717 Loss: 0.7239825129508972\n",
      "Iteration 36718 Loss: 1.09638249874115\n",
      "Iteration 36719 Loss: 0.9307367205619812\n",
      "Iteration 36719 Loss: 1.0031616687774658\n",
      "Iteration 36720 Loss: 0.6102393269538879\n",
      "Iteration 36721 Loss: 0.6793833374977112\n",
      "Iteration 36722 Loss: 0.873440146446228\n",
      "Iteration 36723 Loss: 0.9034216403961182\n",
      "Iteration 36724 Loss: 0.9542639255523682\n",
      "Iteration 36725 Loss: 1.114643931388855\n",
      "Iteration 36726 Loss: 1.1028499603271484\n",
      "Iteration 36727 Loss: 0.9476218819618225\n",
      "Iteration 36728 Loss: 1.0044922828674316\n",
      "Iteration 36729 Loss: 1.239598274230957\n",
      "Iteration 36729 Loss: 0.9429954290390015\n",
      "Iteration 36730 Loss: 0.8231188654899597\n",
      "Iteration 36731 Loss: 1.0367790460586548\n",
      "Iteration 36732 Loss: 1.014168381690979\n",
      "Iteration 36733 Loss: 0.9079422354698181\n",
      "Iteration 36734 Loss: 0.9632865190505981\n",
      "Iteration 36735 Loss: 1.0281481742858887\n",
      "Iteration 36736 Loss: 0.7371664047241211\n",
      "Iteration 36737 Loss: 1.0886310338974\n",
      "Iteration 36738 Loss: 0.729513943195343\n",
      "Iteration 36739 Loss: 0.85704106092453\n",
      "Iteration 36739 Loss: 0.9185794591903687\n",
      "Iteration 36740 Loss: 1.0577243566513062\n",
      "Iteration 36741 Loss: 0.8128188848495483\n",
      "Iteration 36742 Loss: 1.3484210968017578\n",
      "Iteration 36743 Loss: 0.9596878886222839\n",
      "Iteration 36744 Loss: 1.238950490951538\n",
      "Iteration 36745 Loss: 0.9828448295593262\n",
      "Iteration 36746 Loss: 1.226064920425415\n",
      "Iteration 36747 Loss: 1.0580296516418457\n",
      "Iteration 36748 Loss: 0.8088033199310303\n",
      "Iteration 36749 Loss: 0.7822620868682861\n",
      "Iteration 36749 Loss: 1.027560830116272\n",
      "Iteration 36750 Loss: 1.0858991146087646\n",
      "Iteration 36751 Loss: 1.3345754146575928\n",
      "Iteration 36752 Loss: 0.7054848074913025\n",
      "Iteration 36753 Loss: 1.3682786226272583\n",
      "Iteration 36754 Loss: 1.117148756980896\n",
      "Iteration 36755 Loss: 0.7905850410461426\n",
      "Iteration 36756 Loss: 0.8801175951957703\n",
      "Iteration 36757 Loss: 0.9813910126686096\n",
      "Iteration 36758 Loss: 0.8905542492866516\n",
      "Iteration 36759 Loss: 1.3117833137512207\n",
      "Iteration 36759 Loss: 1.046581745147705\n",
      "Iteration 36760 Loss: 1.0002315044403076\n",
      "Iteration 36761 Loss: 0.7532185316085815\n",
      "Iteration 36762 Loss: 0.8569671511650085\n",
      "Iteration 36763 Loss: 1.2254234552383423\n",
      "Iteration 36764 Loss: 0.8206511735916138\n",
      "Iteration 36765 Loss: 0.8922184109687805\n",
      "Iteration 36766 Loss: 1.042157769203186\n",
      "Iteration 36767 Loss: 1.285757303237915\n",
      "Iteration 36768 Loss: 0.8900392055511475\n",
      "Iteration 36769 Loss: 1.0278120040893555\n",
      "Iteration 36769 Loss: 0.9794476628303528\n",
      "Iteration 36770 Loss: 1.0641283988952637\n",
      "Iteration 36771 Loss: 1.1502126455307007\n",
      "Iteration 36772 Loss: 0.8623552322387695\n",
      "Iteration 36773 Loss: 0.9089158773422241\n",
      "Iteration 36774 Loss: 0.9640658497810364\n",
      "Iteration 36775 Loss: 1.1531448364257812\n",
      "Iteration 36776 Loss: 1.1840492486953735\n",
      "Iteration 36777 Loss: 1.187559723854065\n",
      "Iteration 36778 Loss: 1.3395930528640747\n",
      "Iteration 36779 Loss: 0.9182052612304688\n",
      "Iteration 36779 Loss: 1.0732231140136719\n",
      "Iteration 36780 Loss: 1.1521462202072144\n",
      "Iteration 36781 Loss: 0.8615642786026001\n",
      "Iteration 36782 Loss: 1.1144068241119385\n",
      "Iteration 36783 Loss: 0.6517674326896667\n",
      "Iteration 36784 Loss: 1.2115474939346313\n",
      "Iteration 36785 Loss: 0.8202272057533264\n",
      "Iteration 36786 Loss: 0.8100070357322693\n",
      "Iteration 36787 Loss: 1.1568872928619385\n",
      "Iteration 36788 Loss: 0.9902962446212769\n",
      "Iteration 36789 Loss: 1.3346741199493408\n",
      "Iteration 36789 Loss: 1.010352373123169\n",
      "Iteration 36790 Loss: 0.7953293323516846\n",
      "Iteration 36791 Loss: 1.0615215301513672\n",
      "Iteration 36792 Loss: 1.1374977827072144\n",
      "Iteration 36793 Loss: 0.9631364345550537\n",
      "Iteration 36794 Loss: 0.852299690246582\n",
      "Iteration 36795 Loss: 0.8866347670555115\n",
      "Iteration 36796 Loss: 1.0782629251480103\n",
      "Iteration 36797 Loss: 0.9817814230918884\n",
      "Iteration 36798 Loss: 0.9980020523071289\n",
      "Iteration 36799 Loss: 1.0444767475128174\n",
      "Iteration 36799 Loss: 0.9798942804336548\n",
      "Iteration 36800 Loss: 0.627424955368042\n",
      "Iteration 36801 Loss: 0.920210063457489\n",
      "Iteration 36802 Loss: 1.0557663440704346\n",
      "Iteration 36803 Loss: 1.2596592903137207\n",
      "Iteration 36804 Loss: 0.7341282963752747\n",
      "Iteration 36805 Loss: 1.0855902433395386\n",
      "Iteration 36806 Loss: 1.3166882991790771\n",
      "Iteration 36807 Loss: 0.8556331396102905\n",
      "Iteration 36808 Loss: 0.7437368631362915\n",
      "Iteration 36809 Loss: 0.8837573528289795\n",
      "Iteration 36809 Loss: 0.9482594728469849\n",
      "Iteration 36810 Loss: 1.170533537864685\n",
      "Iteration 36811 Loss: 1.1088659763336182\n",
      "Iteration 36812 Loss: 0.9893825054168701\n",
      "Iteration 36813 Loss: 1.0449033975601196\n",
      "Iteration 36814 Loss: 1.0066760778427124\n",
      "Iteration 36815 Loss: 0.765501856803894\n",
      "Iteration 36816 Loss: 0.9796377420425415\n",
      "Iteration 36817 Loss: 0.7580534815788269\n",
      "Iteration 36818 Loss: 0.7990163564682007\n",
      "Iteration 36819 Loss: 0.9757441282272339\n",
      "Iteration 36819 Loss: 0.9598315954208374\n",
      "Iteration 36820 Loss: 0.786554753780365\n",
      "Iteration 36821 Loss: 0.9496650099754333\n",
      "Iteration 36822 Loss: 1.193560242652893\n",
      "Iteration 36823 Loss: 1.0787935256958008\n",
      "Iteration 36824 Loss: 0.9217633605003357\n",
      "Iteration 36825 Loss: 0.865740954875946\n",
      "Iteration 36826 Loss: 0.7904742360115051\n",
      "Iteration 36827 Loss: 1.1446915864944458\n",
      "Iteration 36828 Loss: 1.0450962781906128\n",
      "Iteration 36829 Loss: 1.0285658836364746\n",
      "Iteration 36829 Loss: 0.9804905652999878\n",
      "Iteration 36830 Loss: 0.5460218787193298\n",
      "Iteration 36831 Loss: 1.55955970287323\n",
      "Iteration 36832 Loss: 0.891179084777832\n",
      "Iteration 36833 Loss: 1.1404619216918945\n",
      "Iteration 36834 Loss: 0.9411437511444092\n",
      "Iteration 36835 Loss: 1.290266990661621\n",
      "Iteration 36836 Loss: 1.1674708127975464\n",
      "Iteration 36837 Loss: 0.9592838287353516\n",
      "Iteration 36838 Loss: 0.607938826084137\n",
      "Iteration 36839 Loss: 1.0588010549545288\n",
      "Iteration 36839 Loss: 1.016212821006775\n",
      "Iteration 36840 Loss: 1.63359534740448\n",
      "Iteration 36841 Loss: 0.9442676305770874\n",
      "Iteration 36842 Loss: 1.0504623651504517\n",
      "Iteration 36843 Loss: 0.7897067666053772\n",
      "Iteration 36844 Loss: 0.9475477337837219\n",
      "Iteration 36845 Loss: 0.9745638370513916\n",
      "Iteration 36846 Loss: 0.7578481435775757\n",
      "Iteration 36847 Loss: 0.9578043222427368\n",
      "Iteration 36848 Loss: 1.0394700765609741\n",
      "Iteration 36849 Loss: 0.8844227194786072\n",
      "Iteration 36849 Loss: 0.997968852519989\n",
      "Iteration 36850 Loss: 1.0258132219314575\n",
      "Iteration 36851 Loss: 1.0658584833145142\n",
      "Iteration 36852 Loss: 0.6459168195724487\n",
      "Iteration 36853 Loss: 1.2002127170562744\n",
      "Iteration 36854 Loss: 0.8095118403434753\n",
      "Iteration 36855 Loss: 0.7401300668716431\n",
      "Iteration 36856 Loss: 1.0692707300186157\n",
      "Iteration 36857 Loss: 1.2018544673919678\n",
      "Iteration 36858 Loss: 1.3370583057403564\n",
      "Iteration 36859 Loss: 0.8516837954521179\n",
      "Iteration 36859 Loss: 0.994731068611145\n",
      "Iteration 36860 Loss: 0.9180806279182434\n",
      "Iteration 36861 Loss: 1.017601728439331\n",
      "Iteration 36862 Loss: 1.2002114057540894\n",
      "Iteration 36863 Loss: 0.9749264717102051\n",
      "Iteration 36864 Loss: 0.9475879669189453\n",
      "Iteration 36865 Loss: 1.157968282699585\n",
      "Iteration 36866 Loss: 1.3697247505187988\n",
      "Iteration 36867 Loss: 1.126083493232727\n",
      "Iteration 36868 Loss: 1.1072455644607544\n",
      "Iteration 36869 Loss: 1.1033352613449097\n",
      "Iteration 36869 Loss: 1.0922765731811523\n",
      "Iteration 36870 Loss: 0.6283025741577148\n",
      "Iteration 36871 Loss: 1.0131371021270752\n",
      "Iteration 36872 Loss: 0.9484432935714722\n",
      "Iteration 36873 Loss: 0.7466521263122559\n",
      "Iteration 36874 Loss: 0.47147804498672485\n",
      "Iteration 36875 Loss: 0.8467649221420288\n",
      "Iteration 36876 Loss: 1.2060920000076294\n",
      "Iteration 36877 Loss: 1.0549660921096802\n",
      "Iteration 36878 Loss: 0.9624667167663574\n",
      "Iteration 36879 Loss: 1.0097062587738037\n",
      "Iteration 36879 Loss: 0.8888009190559387\n",
      "Iteration 36880 Loss: 0.9914224743843079\n",
      "Iteration 36881 Loss: 0.983208954334259\n",
      "Iteration 36882 Loss: 0.9521554112434387\n",
      "Iteration 36883 Loss: 0.9282011389732361\n",
      "Iteration 36884 Loss: 1.0078622102737427\n",
      "Iteration 36885 Loss: 1.1478452682495117\n",
      "Iteration 36886 Loss: 1.0928096771240234\n",
      "Iteration 36887 Loss: 0.9499271512031555\n",
      "Iteration 36888 Loss: 0.9895738363265991\n",
      "Iteration 36889 Loss: 1.1656990051269531\n",
      "Iteration 36889 Loss: 1.0208704471588135\n",
      "Iteration 36890 Loss: 0.7890942692756653\n",
      "Iteration 36891 Loss: 0.8905619382858276\n",
      "Iteration 36892 Loss: 0.8464072942733765\n",
      "Iteration 36893 Loss: 1.0345613956451416\n",
      "Iteration 36894 Loss: 1.2505264282226562\n",
      "Iteration 36895 Loss: 1.117875337600708\n",
      "Iteration 36896 Loss: 0.8239833116531372\n",
      "Iteration 36897 Loss: 1.072298526763916\n",
      "Iteration 36898 Loss: 1.1078287363052368\n",
      "Iteration 36899 Loss: 0.6249393820762634\n",
      "Iteration 36899 Loss: 0.9558076858520508\n",
      "Iteration 36900 Loss: 0.6559488773345947\n",
      "Iteration 36901 Loss: 1.0237009525299072\n",
      "Iteration 36902 Loss: 0.7190650701522827\n",
      "Iteration 36903 Loss: 0.9702819585800171\n",
      "Iteration 36904 Loss: 1.090299129486084\n",
      "Iteration 36905 Loss: 1.0587923526763916\n",
      "Iteration 36906 Loss: 1.0324890613555908\n",
      "Iteration 36907 Loss: 0.8365226984024048\n",
      "Iteration 36908 Loss: 0.912030816078186\n",
      "Iteration 36909 Loss: 1.2217180728912354\n",
      "Iteration 36909 Loss: 0.9520848989486694\n",
      "Iteration 36910 Loss: 1.0726985931396484\n",
      "Iteration 36911 Loss: 1.3139277696609497\n",
      "Iteration 36912 Loss: 0.6831746697425842\n",
      "Iteration 36913 Loss: 1.0947394371032715\n",
      "Iteration 36914 Loss: 0.8640286922454834\n",
      "Iteration 36915 Loss: 1.2758054733276367\n",
      "Iteration 36916 Loss: 0.9751371145248413\n",
      "Iteration 36917 Loss: 0.8538525104522705\n",
      "Iteration 36918 Loss: 0.930749773979187\n",
      "Iteration 36919 Loss: 0.9833399653434753\n",
      "Iteration 36919 Loss: 1.0047452449798584\n",
      "Iteration 36920 Loss: 0.9907355904579163\n",
      "Iteration 36921 Loss: 1.2466142177581787\n",
      "Iteration 36922 Loss: 1.3928401470184326\n",
      "Iteration 36923 Loss: 0.6404824256896973\n",
      "Iteration 36924 Loss: 1.1343891620635986\n",
      "Iteration 36925 Loss: 1.2246328592300415\n",
      "Iteration 36926 Loss: 0.956048846244812\n",
      "Iteration 36927 Loss: 0.847343385219574\n",
      "Iteration 36928 Loss: 1.0461516380310059\n",
      "Iteration 36929 Loss: 0.8706686496734619\n",
      "Iteration 36929 Loss: 1.0349907875061035\n",
      "Iteration 36930 Loss: 1.1437444686889648\n",
      "Iteration 36931 Loss: 1.058849573135376\n",
      "Iteration 36932 Loss: 0.9662975072860718\n",
      "Iteration 36933 Loss: 1.138695478439331\n",
      "Iteration 36934 Loss: 0.9966529011726379\n",
      "Iteration 36935 Loss: 1.1804215908050537\n",
      "Iteration 36936 Loss: 1.460754632949829\n",
      "Iteration 36937 Loss: 0.8175947070121765\n",
      "Iteration 36938 Loss: 1.2510895729064941\n",
      "Iteration 36939 Loss: 1.2360049486160278\n",
      "Iteration 36939 Loss: 1.1250104904174805\n",
      "Iteration 36940 Loss: 1.1031744480133057\n",
      "Iteration 36941 Loss: 1.2030047178268433\n",
      "Iteration 36942 Loss: 1.279822587966919\n",
      "Iteration 36943 Loss: 1.0647830963134766\n",
      "Iteration 36944 Loss: 0.9960067868232727\n",
      "Iteration 36945 Loss: 0.9189116954803467\n",
      "Iteration 36946 Loss: 0.9665685892105103\n",
      "Iteration 36947 Loss: 1.1542092561721802\n",
      "Iteration 36948 Loss: 1.2995655536651611\n",
      "Iteration 36949 Loss: 0.5799579620361328\n",
      "Iteration 36949 Loss: 1.056600570678711\n",
      "Iteration 36950 Loss: 0.8863971829414368\n",
      "Iteration 36951 Loss: 0.9689983129501343\n",
      "Iteration 36952 Loss: 0.9179631471633911\n",
      "Iteration 36953 Loss: 1.1348694562911987\n",
      "Iteration 36954 Loss: 0.901850163936615\n",
      "Iteration 36955 Loss: 0.9246223568916321\n",
      "Iteration 36956 Loss: 1.1333796977996826\n",
      "Iteration 36957 Loss: 1.2170348167419434\n",
      "Iteration 36958 Loss: 1.043243408203125\n",
      "Iteration 36959 Loss: 0.7876638770103455\n",
      "Iteration 36959 Loss: 0.9916023015975952\n",
      "Iteration 36960 Loss: 1.0866938829421997\n",
      "Iteration 36961 Loss: 1.014173150062561\n",
      "Iteration 36962 Loss: 0.8647140264511108\n",
      "Iteration 36963 Loss: 0.9005169868469238\n",
      "Iteration 36964 Loss: 0.6653940677642822\n",
      "Iteration 36965 Loss: 1.0141137838363647\n",
      "Iteration 36966 Loss: 1.2805352210998535\n",
      "Iteration 36967 Loss: 1.0729293823242188\n",
      "Iteration 36968 Loss: 0.8030252456665039\n",
      "Iteration 36969 Loss: 0.9161403179168701\n",
      "Iteration 36969 Loss: 0.9618236422538757\n",
      "Iteration 36970 Loss: 0.9028882384300232\n",
      "Iteration 36971 Loss: 1.0257765054702759\n",
      "Iteration 36972 Loss: 0.6891841292381287\n",
      "Iteration 36973 Loss: 0.9666134715080261\n",
      "Iteration 36974 Loss: 1.0140591859817505\n",
      "Iteration 36975 Loss: 0.8265209794044495\n",
      "Iteration 36976 Loss: 1.0015029907226562\n",
      "Iteration 36977 Loss: 1.3980499505996704\n",
      "Iteration 36978 Loss: 0.8119819164276123\n",
      "Iteration 36979 Loss: 0.8190931677818298\n",
      "Iteration 36979 Loss: 0.9455671310424805\n",
      "Iteration 36980 Loss: 1.0120599269866943\n",
      "Iteration 36981 Loss: 0.8244983553886414\n",
      "Iteration 36982 Loss: 0.7308988571166992\n",
      "Iteration 36983 Loss: 1.0189014673233032\n",
      "Iteration 36984 Loss: 0.9381837844848633\n",
      "Iteration 36985 Loss: 0.8261243104934692\n",
      "Iteration 36986 Loss: 1.194974660873413\n",
      "Iteration 36987 Loss: 0.933168888092041\n",
      "Iteration 36988 Loss: 0.9288849830627441\n",
      "Iteration 36989 Loss: 1.3249342441558838\n",
      "Iteration 36989 Loss: 0.9732629656791687\n",
      "Iteration 36990 Loss: 1.0206339359283447\n",
      "Iteration 36991 Loss: 1.042447805404663\n",
      "Iteration 36992 Loss: 0.9540890455245972\n",
      "Iteration 36993 Loss: 0.9698411226272583\n",
      "Iteration 36994 Loss: 0.8860549926757812\n",
      "Iteration 36995 Loss: 0.8507557511329651\n",
      "Iteration 36996 Loss: 1.0036675930023193\n",
      "Iteration 36997 Loss: 0.6020687222480774\n",
      "Iteration 36998 Loss: 1.2462716102600098\n",
      "Iteration 36999 Loss: 1.059147596359253\n",
      "Iteration 36999 Loss: 0.9634978175163269\n",
      "Iteration 37000 Loss: 0.95591801404953\n",
      "Iteration 37001 Loss: 0.8006608486175537\n",
      "Iteration 37002 Loss: 0.8741311430931091\n",
      "Iteration 37003 Loss: 1.3276737928390503\n",
      "Iteration 37004 Loss: 1.134153962135315\n",
      "Iteration 37005 Loss: 1.1475242376327515\n",
      "Iteration 37006 Loss: 0.8829804062843323\n",
      "Iteration 37007 Loss: 1.0517116785049438\n",
      "Iteration 37008 Loss: 0.84198397397995\n",
      "Iteration 37009 Loss: 1.0101208686828613\n",
      "Iteration 37009 Loss: 1.0026859045028687\n",
      "Iteration 37010 Loss: 0.9985423684120178\n",
      "Iteration 37011 Loss: 0.9671310186386108\n",
      "Iteration 37012 Loss: 0.8305630087852478\n",
      "Iteration 37013 Loss: 1.0217994451522827\n",
      "Iteration 37014 Loss: 0.7756795287132263\n",
      "Iteration 37015 Loss: 1.120267391204834\n",
      "Iteration 37016 Loss: 1.228079915046692\n",
      "Iteration 37017 Loss: 0.8145588636398315\n",
      "Iteration 37018 Loss: 0.8363945484161377\n",
      "Iteration 37019 Loss: 1.2157952785491943\n",
      "Iteration 37019 Loss: 0.9808812141418457\n",
      "Iteration 37020 Loss: 0.9353470802307129\n",
      "Iteration 37021 Loss: 0.9856678247451782\n",
      "Iteration 37022 Loss: 0.9424766302108765\n",
      "Iteration 37023 Loss: 0.878226637840271\n",
      "Iteration 37024 Loss: 0.9513829350471497\n",
      "Iteration 37025 Loss: 1.3948705196380615\n",
      "Iteration 37026 Loss: 0.8981798887252808\n",
      "Iteration 37027 Loss: 0.8768436312675476\n",
      "Iteration 37028 Loss: 0.8476991653442383\n",
      "Iteration 37029 Loss: 0.9569985866546631\n",
      "Iteration 37029 Loss: 0.9667693376541138\n",
      "Iteration 37030 Loss: 0.9399574995040894\n",
      "Iteration 37031 Loss: 1.0387992858886719\n",
      "Iteration 37032 Loss: 1.0815638303756714\n",
      "Iteration 37033 Loss: 1.1187407970428467\n",
      "Iteration 37034 Loss: 0.9420650005340576\n",
      "Iteration 37035 Loss: 1.1055814027786255\n",
      "Iteration 37036 Loss: 1.137144684791565\n",
      "Iteration 37037 Loss: 1.1970964670181274\n",
      "Iteration 37038 Loss: 0.8014656901359558\n",
      "Iteration 37039 Loss: 0.8200929760932922\n",
      "Iteration 37039 Loss: 1.018250823020935\n",
      "Iteration 37040 Loss: 0.8613262176513672\n",
      "Iteration 37041 Loss: 0.7930941581726074\n",
      "Iteration 37042 Loss: 0.8290764689445496\n",
      "Iteration 37043 Loss: 1.0514198541641235\n",
      "Iteration 37044 Loss: 1.2197068929672241\n",
      "Iteration 37045 Loss: 0.98615962266922\n",
      "Iteration 37046 Loss: 1.0366636514663696\n",
      "Iteration 37047 Loss: 0.8157467842102051\n",
      "Iteration 37048 Loss: 1.066245198249817\n",
      "Iteration 37049 Loss: 1.2496411800384521\n",
      "Iteration 37049 Loss: 0.9909080266952515\n",
      "Iteration 37050 Loss: 1.3127368688583374\n",
      "Iteration 37051 Loss: 0.9669884443283081\n",
      "Iteration 37052 Loss: 0.9937014579772949\n",
      "Iteration 37053 Loss: 1.1565775871276855\n",
      "Iteration 37054 Loss: 0.7420227527618408\n",
      "Iteration 37055 Loss: 0.7973763346672058\n",
      "Iteration 37056 Loss: 0.6990612149238586\n",
      "Iteration 37057 Loss: 0.7345012426376343\n",
      "Iteration 37058 Loss: 1.1080105304718018\n",
      "Iteration 37059 Loss: 0.8694099187850952\n",
      "Iteration 37059 Loss: 0.9380385279655457\n",
      "Iteration 37060 Loss: 1.0540709495544434\n",
      "Iteration 37061 Loss: 1.2550673484802246\n",
      "Iteration 37062 Loss: 1.1605318784713745\n",
      "Iteration 37063 Loss: 1.1338876485824585\n",
      "Iteration 37064 Loss: 1.2888002395629883\n",
      "Iteration 37065 Loss: 1.0398132801055908\n",
      "Iteration 37066 Loss: 1.0663353204727173\n",
      "Iteration 37067 Loss: 1.3294144868850708\n",
      "Iteration 37068 Loss: 1.1489582061767578\n",
      "Iteration 37069 Loss: 1.0580360889434814\n",
      "Iteration 37069 Loss: 1.153491497039795\n",
      "Iteration 37070 Loss: 1.1063190698623657\n",
      "Iteration 37071 Loss: 1.0753569602966309\n",
      "Iteration 37072 Loss: 0.9420028924942017\n",
      "Iteration 37073 Loss: 0.7281991839408875\n",
      "Iteration 37074 Loss: 0.9042338132858276\n",
      "Iteration 37075 Loss: 0.956543505191803\n",
      "Iteration 37076 Loss: 0.8316280245780945\n",
      "Iteration 37077 Loss: 0.6484177112579346\n",
      "Iteration 37078 Loss: 0.8915978670120239\n",
      "Iteration 37079 Loss: 0.6691164374351501\n",
      "Iteration 37079 Loss: 0.875341534614563\n",
      "Iteration 37080 Loss: 0.7345872521400452\n",
      "Iteration 37081 Loss: 0.8725348114967346\n",
      "Iteration 37082 Loss: 1.0334464311599731\n",
      "Iteration 37083 Loss: 1.1204537153244019\n",
      "Iteration 37084 Loss: 1.0190740823745728\n",
      "Iteration 37085 Loss: 0.9343015551567078\n",
      "Iteration 37086 Loss: 0.7626933455467224\n",
      "Iteration 37087 Loss: 0.9941232800483704\n",
      "Iteration 37088 Loss: 0.8493185043334961\n",
      "Iteration 37089 Loss: 0.8617850542068481\n",
      "Iteration 37089 Loss: 0.9182317852973938\n",
      "Iteration 37090 Loss: 0.908271312713623\n",
      "Iteration 37091 Loss: 1.1208888292312622\n",
      "Iteration 37092 Loss: 1.0391262769699097\n",
      "Iteration 37093 Loss: 0.8947610259056091\n",
      "Iteration 37094 Loss: 0.887427568435669\n",
      "Iteration 37095 Loss: 1.4980183839797974\n",
      "Iteration 37096 Loss: 0.7974132299423218\n",
      "Iteration 37097 Loss: 1.0183762311935425\n",
      "Iteration 37098 Loss: 0.9300198554992676\n",
      "Iteration 37099 Loss: 1.2676299810409546\n",
      "Iteration 37099 Loss: 1.0361932516098022\n",
      "Iteration 37100 Loss: 0.7059851884841919\n",
      "Iteration 37101 Loss: 1.1765713691711426\n",
      "Iteration 37102 Loss: 0.8752444982528687\n",
      "Iteration 37103 Loss: 0.9008439183235168\n",
      "Iteration 37104 Loss: 1.0341356992721558\n",
      "Iteration 37105 Loss: 0.7793087363243103\n",
      "Iteration 37106 Loss: 1.0371137857437134\n",
      "Iteration 37107 Loss: 0.860022783279419\n",
      "Iteration 37108 Loss: 1.0343902111053467\n",
      "Iteration 37109 Loss: 1.001889705657959\n",
      "Iteration 37109 Loss: 0.9405506253242493\n",
      "Iteration 37110 Loss: 0.7395734786987305\n",
      "Iteration 37111 Loss: 1.0161724090576172\n",
      "Iteration 37112 Loss: 1.09230637550354\n",
      "Iteration 37113 Loss: 1.0165578126907349\n",
      "Iteration 37114 Loss: 0.9912660717964172\n",
      "Iteration 37115 Loss: 1.0887882709503174\n",
      "Iteration 37116 Loss: 1.4008097648620605\n",
      "Iteration 37117 Loss: 1.1793835163116455\n",
      "Iteration 37118 Loss: 0.8340443968772888\n",
      "Iteration 37119 Loss: 1.164850115776062\n",
      "Iteration 37119 Loss: 1.052375078201294\n",
      "Iteration 37120 Loss: 0.9282737970352173\n",
      "Iteration 37121 Loss: 0.8294223546981812\n",
      "Iteration 37122 Loss: 0.6506396532058716\n",
      "Iteration 37123 Loss: 1.0712134838104248\n",
      "Iteration 37124 Loss: 1.1904058456420898\n",
      "Iteration 37125 Loss: 0.9081206321716309\n",
      "Iteration 37126 Loss: 0.8807096481323242\n",
      "Iteration 37127 Loss: 1.153643012046814\n",
      "Iteration 37128 Loss: 1.0301058292388916\n",
      "Iteration 37129 Loss: 0.8024755120277405\n",
      "Iteration 37129 Loss: 0.9445009231567383\n",
      "Iteration 37130 Loss: 0.9922276735305786\n",
      "Iteration 37131 Loss: 0.8245377540588379\n",
      "Iteration 37132 Loss: 0.9474119544029236\n",
      "Iteration 37133 Loss: 0.8831687569618225\n",
      "Iteration 37134 Loss: 1.2277597188949585\n",
      "Iteration 37135 Loss: 0.9501101970672607\n",
      "Iteration 37136 Loss: 0.9289361834526062\n",
      "Iteration 37137 Loss: 1.1801259517669678\n",
      "Iteration 37138 Loss: 0.9312933087348938\n",
      "Iteration 37139 Loss: 0.9589996337890625\n",
      "Iteration 37139 Loss: 0.982457160949707\n",
      "Iteration 37140 Loss: 0.9986714124679565\n",
      "Iteration 37141 Loss: 1.250736117362976\n",
      "Iteration 37142 Loss: 1.1349971294403076\n",
      "Iteration 37143 Loss: 1.199743628501892\n",
      "Iteration 37144 Loss: 0.986145555973053\n",
      "Iteration 37145 Loss: 0.8048956394195557\n",
      "Iteration 37146 Loss: 1.0381487607955933\n",
      "Iteration 37147 Loss: 1.040122151374817\n",
      "Iteration 37148 Loss: 1.088090419769287\n",
      "Iteration 37149 Loss: 0.7146848440170288\n",
      "Iteration 37149 Loss: 1.0256235599517822\n",
      "Iteration 37150 Loss: 0.8588739037513733\n",
      "Iteration 37151 Loss: 0.8889952898025513\n",
      "Iteration 37152 Loss: 1.452557921409607\n",
      "Iteration 37153 Loss: 0.9668425917625427\n",
      "Iteration 37154 Loss: 0.9872174859046936\n",
      "Iteration 37155 Loss: 1.047308087348938\n",
      "Iteration 37156 Loss: 0.8737015128135681\n",
      "Iteration 37157 Loss: 0.7799428701400757\n",
      "Iteration 37158 Loss: 0.9958125948905945\n",
      "Iteration 37159 Loss: 0.9335188269615173\n",
      "Iteration 37159 Loss: 0.9784771203994751\n",
      "Iteration 37160 Loss: 0.6969147324562073\n",
      "Iteration 37161 Loss: 0.9793627262115479\n",
      "Iteration 37162 Loss: 1.3525441884994507\n",
      "Iteration 37163 Loss: 1.0042256116867065\n",
      "Iteration 37164 Loss: 1.1596770286560059\n",
      "Iteration 37165 Loss: 0.996244490146637\n",
      "Iteration 37166 Loss: 0.6099945902824402\n",
      "Iteration 37167 Loss: 1.2248709201812744\n",
      "Iteration 37168 Loss: 1.0944150686264038\n",
      "Iteration 37169 Loss: 1.076855182647705\n",
      "Iteration 37169 Loss: 1.0195105075836182\n",
      "Iteration 37170 Loss: 0.8443669676780701\n",
      "Iteration 37171 Loss: 1.1883305311203003\n",
      "Iteration 37172 Loss: 0.6707727313041687\n",
      "Iteration 37173 Loss: 1.2461475133895874\n",
      "Iteration 37174 Loss: 0.8847973942756653\n",
      "Iteration 37175 Loss: 0.8824633955955505\n",
      "Iteration 37176 Loss: 0.9336690306663513\n",
      "Iteration 37177 Loss: 0.7272257804870605\n",
      "Iteration 37178 Loss: 0.96048903465271\n",
      "Iteration 37179 Loss: 0.9402990341186523\n",
      "Iteration 37179 Loss: 0.9278560876846313\n",
      "Iteration 37180 Loss: 1.094040036201477\n",
      "Iteration 37181 Loss: 1.071558952331543\n",
      "Iteration 37182 Loss: 0.507732629776001\n",
      "Iteration 37183 Loss: 0.8229369521141052\n",
      "Iteration 37184 Loss: 0.5090481042861938\n",
      "Iteration 37185 Loss: 0.7084311842918396\n",
      "Iteration 37186 Loss: 0.8793674111366272\n",
      "Iteration 37187 Loss: 0.8778635859489441\n",
      "Iteration 37188 Loss: 0.905267596244812\n",
      "Iteration 37189 Loss: 1.0552036762237549\n",
      "Iteration 37189 Loss: 0.8431450128555298\n",
      "Iteration 37190 Loss: 0.782072126865387\n",
      "Iteration 37191 Loss: 0.830041766166687\n",
      "Iteration 37192 Loss: 1.0762827396392822\n",
      "Iteration 37193 Loss: 1.1042323112487793\n",
      "Iteration 37194 Loss: 1.081099033355713\n",
      "Iteration 37195 Loss: 0.9577832818031311\n",
      "Iteration 37196 Loss: 0.7223026156425476\n",
      "Iteration 37197 Loss: 0.913727343082428\n",
      "Iteration 37198 Loss: 0.7198169827461243\n",
      "Iteration 37199 Loss: 0.6231939196586609\n",
      "Iteration 37199 Loss: 0.8810551762580872\n",
      "Iteration 37200 Loss: 1.0750373601913452\n",
      "Iteration 37201 Loss: 0.9487512707710266\n",
      "Iteration 37202 Loss: 0.9040672183036804\n",
      "Iteration 37203 Loss: 1.039624810218811\n",
      "Iteration 37204 Loss: 0.8742817044258118\n",
      "Iteration 37205 Loss: 1.1103324890136719\n",
      "Iteration 37206 Loss: 0.8875344395637512\n",
      "Iteration 37207 Loss: 1.0532739162445068\n",
      "Iteration 37208 Loss: 0.767170786857605\n",
      "Iteration 37209 Loss: 1.1727677583694458\n",
      "Iteration 37209 Loss: 0.9832841753959656\n",
      "Iteration 37210 Loss: 0.7641162872314453\n",
      "Iteration 37211 Loss: 1.0439177751541138\n",
      "Iteration 37212 Loss: 1.0632665157318115\n",
      "Iteration 37213 Loss: 1.0722287893295288\n",
      "Iteration 37214 Loss: 1.2836483716964722\n",
      "Iteration 37215 Loss: 1.0574984550476074\n",
      "Iteration 37216 Loss: 0.9627158641815186\n",
      "Iteration 37217 Loss: 0.9054751992225647\n",
      "Iteration 37218 Loss: 0.8055593371391296\n",
      "Iteration 37219 Loss: 0.9112715125083923\n",
      "Iteration 37219 Loss: 0.9869698286056519\n",
      "Iteration 37220 Loss: 1.0777109861373901\n",
      "Iteration 37221 Loss: 1.0084660053253174\n",
      "Iteration 37222 Loss: 0.7417818307876587\n",
      "Iteration 37223 Loss: 1.028659701347351\n",
      "Iteration 37224 Loss: 1.0509568452835083\n",
      "Iteration 37225 Loss: 0.8670718669891357\n",
      "Iteration 37226 Loss: 0.8624781966209412\n",
      "Iteration 37227 Loss: 0.8990225195884705\n",
      "Iteration 37228 Loss: 0.7810166478157043\n",
      "Iteration 37229 Loss: 0.8076202273368835\n",
      "Iteration 37229 Loss: 0.9124784469604492\n",
      "Iteration 37230 Loss: 1.1494499444961548\n",
      "Iteration 37231 Loss: 0.7662904262542725\n",
      "Iteration 37232 Loss: 1.2260289192199707\n",
      "Iteration 37233 Loss: 1.5350946187973022\n",
      "Iteration 37234 Loss: 0.8674197793006897\n",
      "Iteration 37235 Loss: 0.9762784242630005\n",
      "Iteration 37236 Loss: 0.9799319505691528\n",
      "Iteration 37237 Loss: 1.0590118169784546\n",
      "Iteration 37238 Loss: 0.9207934737205505\n",
      "Iteration 37239 Loss: 0.9585654139518738\n",
      "Iteration 37239 Loss: 1.043886423110962\n",
      "Iteration 37240 Loss: 1.1764625310897827\n",
      "Iteration 37241 Loss: 0.7812650203704834\n",
      "Iteration 37242 Loss: 1.0470609664916992\n",
      "Iteration 37243 Loss: 0.7086253762245178\n",
      "Iteration 37244 Loss: 0.7077787518501282\n",
      "Iteration 37245 Loss: 0.8326075077056885\n",
      "Iteration 37246 Loss: 1.1797351837158203\n",
      "Iteration 37247 Loss: 0.98138028383255\n",
      "Iteration 37248 Loss: 1.7024853229522705\n",
      "Iteration 37249 Loss: 1.0632588863372803\n",
      "Iteration 37249 Loss: 1.0180660486221313\n",
      "Iteration 37250 Loss: 1.0255920886993408\n",
      "Iteration 37251 Loss: 0.8977861404418945\n",
      "Iteration 37252 Loss: 0.784789502620697\n",
      "Iteration 37253 Loss: 1.2594759464263916\n",
      "Iteration 37254 Loss: 1.1623374223709106\n",
      "Iteration 37255 Loss: 1.3406882286071777\n",
      "Iteration 37256 Loss: 1.0774613618850708\n",
      "Iteration 37257 Loss: 0.8611133098602295\n",
      "Iteration 37258 Loss: 1.2240761518478394\n",
      "Iteration 37259 Loss: 0.9014784693717957\n",
      "Iteration 37259 Loss: 1.0534799098968506\n",
      "Iteration 37260 Loss: 1.148459792137146\n",
      "Iteration 37261 Loss: 0.864342451095581\n",
      "Iteration 37262 Loss: 0.9568583369255066\n",
      "Iteration 37263 Loss: 1.0210789442062378\n",
      "Iteration 37264 Loss: 0.7301552295684814\n",
      "Iteration 37265 Loss: 0.8694861531257629\n",
      "Iteration 37266 Loss: 0.9677302241325378\n",
      "Iteration 37267 Loss: 1.0746474266052246\n",
      "Iteration 37268 Loss: 1.0095409154891968\n",
      "Iteration 37269 Loss: 1.2016870975494385\n",
      "Iteration 37269 Loss: 0.9843986630439758\n",
      "Iteration 37270 Loss: 1.1408294439315796\n",
      "Iteration 37271 Loss: 0.986514687538147\n",
      "Iteration 37272 Loss: 1.249047875404358\n",
      "Iteration 37273 Loss: 1.0415046215057373\n",
      "Iteration 37274 Loss: 0.7157729268074036\n",
      "Iteration 37275 Loss: 0.9582004547119141\n",
      "Iteration 37276 Loss: 0.6902741193771362\n",
      "Iteration 37277 Loss: 1.0795027017593384\n",
      "Iteration 37278 Loss: 0.9313578605651855\n",
      "Iteration 37279 Loss: 1.0835624933242798\n",
      "Iteration 37279 Loss: 0.9876567721366882\n",
      "Iteration 37280 Loss: 0.812844455242157\n",
      "Iteration 37281 Loss: 0.9404668211936951\n",
      "Iteration 37282 Loss: 0.9810431003570557\n",
      "Iteration 37283 Loss: 0.96812903881073\n",
      "Iteration 37284 Loss: 1.1669397354125977\n",
      "Iteration 37285 Loss: 0.7689627408981323\n",
      "Iteration 37286 Loss: 1.067754864692688\n",
      "Iteration 37287 Loss: 1.0694561004638672\n",
      "Iteration 37288 Loss: 0.9582526087760925\n",
      "Iteration 37289 Loss: 1.0641567707061768\n",
      "Iteration 37289 Loss: 0.9798007011413574\n",
      "Iteration 37290 Loss: 1.1655446290969849\n",
      "Iteration 37291 Loss: 0.7982138395309448\n",
      "Iteration 37292 Loss: 1.1956950426101685\n",
      "Iteration 37293 Loss: 1.0599719285964966\n",
      "Iteration 37294 Loss: 1.0002931356430054\n",
      "Iteration 37295 Loss: 0.8896286487579346\n",
      "Iteration 37296 Loss: 0.9777657985687256\n",
      "Iteration 37297 Loss: 1.1758216619491577\n",
      "Iteration 37298 Loss: 0.8331497311592102\n",
      "Iteration 37299 Loss: 0.7459700107574463\n",
      "Iteration 37299 Loss: 0.984205424785614\n",
      "Iteration 37300 Loss: 1.1001390218734741\n",
      "Iteration 37301 Loss: 1.3238704204559326\n",
      "Iteration 37302 Loss: 0.6088769435882568\n",
      "Iteration 37303 Loss: 0.8025934100151062\n",
      "Iteration 37304 Loss: 0.9731386303901672\n",
      "Iteration 37305 Loss: 0.929639458656311\n",
      "Iteration 37306 Loss: 1.1979882717132568\n",
      "Iteration 37307 Loss: 1.0489277839660645\n",
      "Iteration 37308 Loss: 0.8482553362846375\n",
      "Iteration 37309 Loss: 0.936273992061615\n",
      "Iteration 37309 Loss: 0.9769703149795532\n",
      "Iteration 37310 Loss: 1.0888514518737793\n",
      "Iteration 37311 Loss: 0.9292391538619995\n",
      "Iteration 37312 Loss: 0.7181284427642822\n",
      "Iteration 37313 Loss: 1.1334031820297241\n",
      "Iteration 37314 Loss: 0.7244110703468323\n",
      "Iteration 37315 Loss: 0.9193944334983826\n",
      "Iteration 37316 Loss: 0.8659948110580444\n",
      "Iteration 37317 Loss: 1.2472600936889648\n",
      "Iteration 37318 Loss: 0.8772507309913635\n",
      "Iteration 37319 Loss: 0.6952677965164185\n",
      "Iteration 37319 Loss: 0.9199201464653015\n",
      "Iteration 37320 Loss: 0.6226868033409119\n",
      "Iteration 37321 Loss: 0.881513774394989\n",
      "Iteration 37322 Loss: 0.9602296948432922\n",
      "Iteration 37323 Loss: 0.953216552734375\n",
      "Iteration 37324 Loss: 1.0673249959945679\n",
      "Iteration 37325 Loss: 0.8718906044960022\n",
      "Iteration 37326 Loss: 0.902718186378479\n",
      "Iteration 37327 Loss: 0.8329210877418518\n",
      "Iteration 37328 Loss: 1.0032048225402832\n",
      "Iteration 37329 Loss: 0.9138869643211365\n",
      "Iteration 37329 Loss: 0.9009593725204468\n",
      "Iteration 37330 Loss: 1.0141191482543945\n",
      "Iteration 37331 Loss: 0.7900458574295044\n",
      "Iteration 37332 Loss: 1.057861566543579\n",
      "Iteration 37333 Loss: 0.9318374395370483\n",
      "Iteration 37334 Loss: 0.8725028038024902\n",
      "Iteration 37335 Loss: 0.9346944093704224\n",
      "Iteration 37336 Loss: 0.9684395790100098\n",
      "Iteration 37337 Loss: 1.323646903038025\n",
      "Iteration 37338 Loss: 0.7346121072769165\n",
      "Iteration 37339 Loss: 0.8070523142814636\n",
      "Iteration 37339 Loss: 0.9434812664985657\n",
      "Iteration 37340 Loss: 0.7769989371299744\n",
      "Iteration 37341 Loss: 1.161142110824585\n",
      "Iteration 37342 Loss: 0.784086287021637\n",
      "Iteration 37343 Loss: 0.9233586192131042\n",
      "Iteration 37344 Loss: 1.4027763605117798\n",
      "Iteration 37345 Loss: 0.9890075922012329\n",
      "Iteration 37346 Loss: 1.1739128828048706\n",
      "Iteration 37347 Loss: 0.6937926411628723\n",
      "Iteration 37348 Loss: 1.224263072013855\n",
      "Iteration 37349 Loss: 0.9811443090438843\n",
      "Iteration 37349 Loss: 1.0110481977462769\n",
      "Iteration 37350 Loss: 0.9893960952758789\n",
      "Iteration 37351 Loss: 1.0751953125\n",
      "Iteration 37352 Loss: 1.136850118637085\n",
      "Iteration 37353 Loss: 1.3787376880645752\n",
      "Iteration 37354 Loss: 1.0035288333892822\n",
      "Iteration 37355 Loss: 0.8992005586624146\n",
      "Iteration 37356 Loss: 1.3165608644485474\n",
      "Iteration 37357 Loss: 1.1136572360992432\n",
      "Iteration 37358 Loss: 1.058028221130371\n",
      "Iteration 37359 Loss: 0.9201945662498474\n",
      "Iteration 37359 Loss: 1.089134931564331\n",
      "Iteration 37360 Loss: 1.081690788269043\n",
      "Iteration 37361 Loss: 0.6896020174026489\n",
      "Iteration 37362 Loss: 0.9257950186729431\n",
      "Iteration 37363 Loss: 0.7201453447341919\n",
      "Iteration 37364 Loss: 1.1894423961639404\n",
      "Iteration 37365 Loss: 0.9652706980705261\n",
      "Iteration 37366 Loss: 0.7426047325134277\n",
      "Iteration 37367 Loss: 0.45951059460639954\n",
      "Iteration 37368 Loss: 0.8788468241691589\n",
      "Iteration 37369 Loss: 1.1842210292816162\n",
      "Iteration 37369 Loss: 0.8837129473686218\n",
      "Iteration 37370 Loss: 0.802808403968811\n",
      "Iteration 37371 Loss: 1.0315276384353638\n",
      "Iteration 37372 Loss: 0.8462230563163757\n",
      "Iteration 37373 Loss: 0.9776049852371216\n",
      "Iteration 37374 Loss: 1.1046353578567505\n",
      "Iteration 37375 Loss: 1.162563443183899\n",
      "Iteration 37376 Loss: 0.6760132312774658\n",
      "Iteration 37377 Loss: 0.9909672141075134\n",
      "Iteration 37378 Loss: 1.298929214477539\n",
      "Iteration 37379 Loss: 0.7611411809921265\n",
      "Iteration 37379 Loss: 0.9652412533760071\n",
      "Iteration 37380 Loss: 0.9348816871643066\n",
      "Iteration 37381 Loss: 1.072709560394287\n",
      "Iteration 37382 Loss: 0.9753459692001343\n",
      "Iteration 37383 Loss: 0.8721592426300049\n",
      "Iteration 37384 Loss: 1.2791081666946411\n",
      "Iteration 37385 Loss: 1.2078516483306885\n",
      "Iteration 37386 Loss: 0.7966936230659485\n",
      "Iteration 37387 Loss: 0.9089174866676331\n",
      "Iteration 37388 Loss: 1.2147833108901978\n",
      "Iteration 37389 Loss: 1.1709476709365845\n",
      "Iteration 37389 Loss: 1.0433398485183716\n",
      "Iteration 37390 Loss: 1.3946821689605713\n",
      "Iteration 37391 Loss: 0.8688884377479553\n",
      "Iteration 37392 Loss: 1.1351919174194336\n",
      "Iteration 37393 Loss: 0.8024733662605286\n",
      "Iteration 37394 Loss: 1.2326990365982056\n",
      "Iteration 37395 Loss: 0.9240150451660156\n",
      "Iteration 37396 Loss: 0.855164110660553\n",
      "Iteration 37397 Loss: 0.931829571723938\n",
      "Iteration 37398 Loss: 0.6414471864700317\n",
      "Iteration 37399 Loss: 0.9323071837425232\n",
      "Iteration 37399 Loss: 0.9718698263168335\n",
      "Iteration 37400 Loss: 0.8067290782928467\n",
      "Iteration 37401 Loss: 0.7165511250495911\n",
      "Iteration 37402 Loss: 0.8265413045883179\n",
      "Iteration 37403 Loss: 1.1478668451309204\n",
      "Iteration 37404 Loss: 1.1615240573883057\n",
      "Iteration 37405 Loss: 1.053052306175232\n",
      "Iteration 37406 Loss: 0.6866775155067444\n",
      "Iteration 37407 Loss: 1.0577759742736816\n",
      "Iteration 37408 Loss: 0.8529914021492004\n",
      "Iteration 37409 Loss: 0.7343423366546631\n",
      "Iteration 37409 Loss: 0.9044052362442017\n",
      "Iteration 37410 Loss: 0.7432904243469238\n",
      "Iteration 37411 Loss: 0.9727787971496582\n",
      "Iteration 37412 Loss: 0.9577245116233826\n",
      "Iteration 37413 Loss: 1.104782223701477\n",
      "Iteration 37414 Loss: 0.9119855165481567\n",
      "Iteration 37415 Loss: 0.8552260994911194\n",
      "Iteration 37416 Loss: 1.0917232036590576\n",
      "Iteration 37417 Loss: 1.0248949527740479\n",
      "Iteration 37418 Loss: 0.8415911197662354\n",
      "Iteration 37419 Loss: 0.9628657698631287\n",
      "Iteration 37419 Loss: 0.9466861486434937\n",
      "Iteration 37420 Loss: 0.9177311062812805\n",
      "Iteration 37421 Loss: 0.9575476050376892\n",
      "Iteration 37422 Loss: 0.5679256916046143\n",
      "Iteration 37423 Loss: 0.9637680053710938\n",
      "Iteration 37424 Loss: 1.0485589504241943\n",
      "Iteration 37425 Loss: 1.0048093795776367\n",
      "Iteration 37426 Loss: 0.7153612971305847\n",
      "Iteration 37427 Loss: 1.3477811813354492\n",
      "Iteration 37428 Loss: 1.1220812797546387\n",
      "Iteration 37429 Loss: 1.3317887783050537\n",
      "Iteration 37429 Loss: 0.9977353811264038\n",
      "Iteration 37430 Loss: 0.69076007604599\n",
      "Iteration 37431 Loss: 1.071533441543579\n",
      "Iteration 37432 Loss: 1.0412793159484863\n",
      "Iteration 37433 Loss: 0.9295786023139954\n",
      "Iteration 37434 Loss: 1.127097487449646\n",
      "Iteration 37435 Loss: 0.9306154847145081\n",
      "Iteration 37436 Loss: 1.0835585594177246\n",
      "Iteration 37437 Loss: 1.113416075706482\n",
      "Iteration 37438 Loss: 0.8780832886695862\n",
      "Iteration 37439 Loss: 1.083255410194397\n",
      "Iteration 37439 Loss: 0.9949177503585815\n",
      "Iteration 37440 Loss: 0.9016804695129395\n",
      "Iteration 37441 Loss: 0.9546750783920288\n",
      "Iteration 37442 Loss: 1.113563895225525\n",
      "Iteration 37443 Loss: 1.162295937538147\n",
      "Iteration 37444 Loss: 0.66704922914505\n",
      "Iteration 37445 Loss: 0.7382379174232483\n",
      "Iteration 37446 Loss: 0.6926438212394714\n",
      "Iteration 37447 Loss: 0.9651069045066833\n",
      "Iteration 37448 Loss: 0.8965252637863159\n",
      "Iteration 37449 Loss: 0.8077296018600464\n",
      "Iteration 37449 Loss: 0.8899507522583008\n",
      "Iteration 37450 Loss: 0.7453034520149231\n",
      "Iteration 37451 Loss: 0.8567214012145996\n",
      "Iteration 37452 Loss: 0.9338690042495728\n",
      "Iteration 37453 Loss: 1.198839545249939\n",
      "Iteration 37454 Loss: 0.6962631344795227\n",
      "Iteration 37455 Loss: 1.2800853252410889\n",
      "Iteration 37456 Loss: 0.8767863512039185\n",
      "Iteration 37457 Loss: 1.1470614671707153\n",
      "Iteration 37458 Loss: 1.2638790607452393\n",
      "Iteration 37459 Loss: 1.3759249448776245\n",
      "Iteration 37459 Loss: 1.037473440170288\n",
      "Iteration 37460 Loss: 1.124233603477478\n",
      "Iteration 37461 Loss: 1.1147617101669312\n",
      "Iteration 37462 Loss: 0.9213416576385498\n",
      "Iteration 37463 Loss: 1.1018755435943604\n",
      "Iteration 37464 Loss: 1.140621304512024\n",
      "Iteration 37465 Loss: 1.0261340141296387\n",
      "Iteration 37466 Loss: 0.9431175589561462\n",
      "Iteration 37467 Loss: 0.7490832805633545\n",
      "Iteration 37468 Loss: 1.1095398664474487\n",
      "Iteration 37469 Loss: 1.025990605354309\n",
      "Iteration 37469 Loss: 1.0256699323654175\n",
      "Iteration 37470 Loss: 1.0398198366165161\n",
      "Iteration 37471 Loss: 0.8568810820579529\n",
      "Iteration 37472 Loss: 1.034029245376587\n",
      "Iteration 37473 Loss: 1.2691417932510376\n",
      "Iteration 37474 Loss: 0.988823413848877\n",
      "Iteration 37475 Loss: 0.8461225032806396\n",
      "Iteration 37476 Loss: 1.1831378936767578\n",
      "Iteration 37477 Loss: 0.7240524291992188\n",
      "Iteration 37478 Loss: 1.1023017168045044\n",
      "Iteration 37479 Loss: 0.7352966070175171\n",
      "Iteration 37479 Loss: 0.9779607057571411\n",
      "Iteration 37480 Loss: 0.9804542660713196\n",
      "Iteration 37481 Loss: 1.1818749904632568\n",
      "Iteration 37482 Loss: 1.3020155429840088\n",
      "Iteration 37483 Loss: 1.0456392765045166\n",
      "Iteration 37484 Loss: 0.9554819464683533\n",
      "Iteration 37485 Loss: 1.4668877124786377\n",
      "Iteration 37486 Loss: 0.9574671387672424\n",
      "Iteration 37487 Loss: 0.9623973965644836\n",
      "Iteration 37488 Loss: 1.3306547403335571\n",
      "Iteration 37489 Loss: 1.0539977550506592\n",
      "Iteration 37489 Loss: 1.1236870288848877\n",
      "Iteration 37490 Loss: 0.889594316482544\n",
      "Iteration 37491 Loss: 1.3252828121185303\n",
      "Iteration 37492 Loss: 1.0594756603240967\n",
      "Iteration 37493 Loss: 0.7650285959243774\n",
      "Iteration 37494 Loss: 1.1563128232955933\n",
      "Iteration 37495 Loss: 0.6671249866485596\n",
      "Iteration 37496 Loss: 1.209273338317871\n",
      "Iteration 37497 Loss: 0.9707819223403931\n",
      "Iteration 37498 Loss: 0.8079733848571777\n",
      "Iteration 37499 Loss: 0.9529115557670593\n",
      "Iteration 37499 Loss: 0.9803759455680847\n",
      "Iteration 37500 Loss: 0.9015573263168335\n",
      "Iteration 37501 Loss: 0.8936936855316162\n",
      "Iteration 37502 Loss: 0.8210371732711792\n",
      "Iteration 37503 Loss: 0.9494858384132385\n",
      "Iteration 37504 Loss: 1.0888667106628418\n",
      "Iteration 37505 Loss: 0.9100116491317749\n",
      "Iteration 37506 Loss: 1.1971582174301147\n",
      "Iteration 37507 Loss: 0.9974769949913025\n",
      "Iteration 37508 Loss: 0.8400593400001526\n",
      "Iteration 37509 Loss: 0.8390424251556396\n",
      "Iteration 37509 Loss: 0.943838894367218\n",
      "Iteration 37510 Loss: 0.9860635995864868\n",
      "Iteration 37511 Loss: 0.8105111122131348\n",
      "Iteration 37512 Loss: 1.0719350576400757\n",
      "Iteration 37513 Loss: 1.2243280410766602\n",
      "Iteration 37514 Loss: 0.7115581035614014\n",
      "Iteration 37515 Loss: 1.076271414756775\n",
      "Iteration 37516 Loss: 0.7147292494773865\n",
      "Iteration 37517 Loss: 0.8755359053611755\n",
      "Iteration 37518 Loss: 0.749596357345581\n",
      "Iteration 37519 Loss: 1.3875446319580078\n",
      "Iteration 37519 Loss: 0.9608073234558105\n",
      "Iteration 37520 Loss: 0.9702560901641846\n",
      "Iteration 37521 Loss: 0.9355834126472473\n",
      "Iteration 37522 Loss: 0.6214221119880676\n",
      "Iteration 37523 Loss: 1.3336886167526245\n",
      "Iteration 37524 Loss: 0.931016743183136\n",
      "Iteration 37525 Loss: 1.1710004806518555\n",
      "Iteration 37526 Loss: 1.1272177696228027\n",
      "Iteration 37527 Loss: 0.8667798638343811\n",
      "Iteration 37528 Loss: 0.8263903260231018\n",
      "Iteration 37529 Loss: 1.0897407531738281\n",
      "Iteration 37529 Loss: 0.9873097538948059\n",
      "Iteration 37530 Loss: 0.9497838616371155\n",
      "Iteration 37531 Loss: 1.0606639385223389\n",
      "Iteration 37532 Loss: 0.9754763245582581\n",
      "Iteration 37533 Loss: 0.7447754740715027\n",
      "Iteration 37534 Loss: 0.8078356981277466\n",
      "Iteration 37535 Loss: 0.9384841918945312\n",
      "Iteration 37536 Loss: 0.9464027285575867\n",
      "Iteration 37537 Loss: 1.1317440271377563\n",
      "Iteration 37538 Loss: 0.8839561939239502\n",
      "Iteration 37539 Loss: 0.8855518698692322\n",
      "Iteration 37539 Loss: 0.9324674606323242\n",
      "Iteration 37540 Loss: 0.8764610886573792\n",
      "Iteration 37541 Loss: 1.0921391248703003\n",
      "Iteration 37542 Loss: 0.8489463925361633\n",
      "Iteration 37543 Loss: 0.9739851355552673\n",
      "Iteration 37544 Loss: 0.9034088850021362\n",
      "Iteration 37545 Loss: 1.2014063596725464\n",
      "Iteration 37546 Loss: 1.1200369596481323\n",
      "Iteration 37547 Loss: 1.4320076704025269\n",
      "Iteration 37548 Loss: 1.049259901046753\n",
      "Iteration 37549 Loss: 0.933199942111969\n",
      "Iteration 37549 Loss: 1.0430852174758911\n",
      "Iteration 37550 Loss: 0.8442348837852478\n",
      "Iteration 37551 Loss: 1.0919029712677002\n",
      "Iteration 37552 Loss: 0.8893795609474182\n",
      "Iteration 37553 Loss: 1.0711461305618286\n",
      "Iteration 37554 Loss: 0.6823833584785461\n",
      "Iteration 37555 Loss: 1.2463279962539673\n",
      "Iteration 37556 Loss: 0.9580077528953552\n",
      "Iteration 37557 Loss: 0.9260174036026001\n",
      "Iteration 37558 Loss: 0.8640845417976379\n",
      "Iteration 37559 Loss: 0.9402651786804199\n",
      "Iteration 37559 Loss: 0.9513750076293945\n",
      "Iteration 37560 Loss: 1.2923080921173096\n",
      "Iteration 37561 Loss: 1.5378520488739014\n",
      "Iteration 37562 Loss: 1.0086299180984497\n",
      "Iteration 37563 Loss: 0.6773969531059265\n",
      "Iteration 37564 Loss: 0.8488020896911621\n",
      "Iteration 37565 Loss: 1.017359733581543\n",
      "Iteration 37566 Loss: 0.9470831155776978\n",
      "Iteration 37567 Loss: 1.1218088865280151\n",
      "Iteration 37568 Loss: 0.782420814037323\n",
      "Iteration 37569 Loss: 0.8718039393424988\n",
      "Iteration 37569 Loss: 1.0105465650558472\n",
      "Iteration 37570 Loss: 1.262105941772461\n",
      "Iteration 37571 Loss: 0.7127406597137451\n",
      "Iteration 37572 Loss: 0.7907079458236694\n",
      "Iteration 37573 Loss: 1.0388829708099365\n",
      "Iteration 37574 Loss: 1.2107751369476318\n",
      "Iteration 37575 Loss: 0.9737245440483093\n",
      "Iteration 37576 Loss: 0.7659757733345032\n",
      "Iteration 37577 Loss: 1.2922741174697876\n",
      "Iteration 37578 Loss: 0.8630751371383667\n",
      "Iteration 37579 Loss: 1.2869880199432373\n",
      "Iteration 37579 Loss: 1.0197250843048096\n",
      "Iteration 37580 Loss: 0.551241934299469\n",
      "Iteration 37581 Loss: 0.933264970779419\n",
      "Iteration 37582 Loss: 0.8483536243438721\n",
      "Iteration 37583 Loss: 0.9539126753807068\n",
      "Iteration 37584 Loss: 1.2918574810028076\n",
      "Iteration 37585 Loss: 0.9982975125312805\n",
      "Iteration 37586 Loss: 0.9955237507820129\n",
      "Iteration 37587 Loss: 0.880344569683075\n",
      "Iteration 37588 Loss: 0.8130530714988708\n",
      "Iteration 37589 Loss: 0.997143030166626\n",
      "Iteration 37589 Loss: 0.9262992739677429\n",
      "Iteration 37590 Loss: 1.1102077960968018\n",
      "Iteration 37591 Loss: 0.8853192925453186\n",
      "Iteration 37592 Loss: 1.237155795097351\n",
      "Iteration 37593 Loss: 1.0207265615463257\n",
      "Iteration 37594 Loss: 0.94835364818573\n",
      "Iteration 37595 Loss: 1.0675681829452515\n",
      "Iteration 37596 Loss: 1.0309559106826782\n",
      "Iteration 37597 Loss: 1.1786988973617554\n",
      "Iteration 37598 Loss: 0.9930342435836792\n",
      "Iteration 37599 Loss: 1.1468751430511475\n",
      "Iteration 37599 Loss: 1.0618895292282104\n",
      "Iteration 37600 Loss: 1.192765712738037\n",
      "Iteration 37601 Loss: 1.020627737045288\n",
      "Iteration 37602 Loss: 0.9010993838310242\n",
      "Iteration 37603 Loss: 0.9779233336448669\n",
      "Iteration 37604 Loss: 1.1088380813598633\n",
      "Iteration 37605 Loss: 1.217930555343628\n",
      "Iteration 37606 Loss: 0.8551353812217712\n",
      "Iteration 37607 Loss: 1.1524361371994019\n",
      "Iteration 37608 Loss: 0.7655078172683716\n",
      "Iteration 37609 Loss: 1.105678677558899\n",
      "Iteration 37609 Loss: 1.0297942161560059\n",
      "Iteration 37610 Loss: 0.9349709153175354\n",
      "Iteration 37611 Loss: 1.0961964130401611\n",
      "Iteration 37612 Loss: 0.9853348135948181\n",
      "Iteration 37613 Loss: 1.3881996870040894\n",
      "Iteration 37614 Loss: 1.0706539154052734\n",
      "Iteration 37615 Loss: 0.9862490296363831\n",
      "Iteration 37616 Loss: 0.8262531757354736\n",
      "Iteration 37617 Loss: 0.8149465918540955\n",
      "Iteration 37618 Loss: 1.1788506507873535\n",
      "Iteration 37619 Loss: 0.972611129283905\n",
      "Iteration 37619 Loss: 1.0254266262054443\n",
      "Iteration 37620 Loss: 0.8260802626609802\n",
      "Iteration 37621 Loss: 0.9830672144889832\n",
      "Iteration 37622 Loss: 0.861655056476593\n",
      "Iteration 37623 Loss: 1.0682411193847656\n",
      "Iteration 37624 Loss: 0.8224260807037354\n",
      "Iteration 37625 Loss: 0.8533341288566589\n",
      "Iteration 37626 Loss: 1.0508277416229248\n",
      "Iteration 37627 Loss: 0.9033238291740417\n",
      "Iteration 37628 Loss: 1.1404685974121094\n",
      "Iteration 37629 Loss: 0.7432868480682373\n",
      "Iteration 37629 Loss: 0.9252711534500122\n",
      "Iteration 37630 Loss: 0.7449498176574707\n",
      "Iteration 37631 Loss: 0.9466843008995056\n",
      "Iteration 37632 Loss: 0.9523335099220276\n",
      "Iteration 37633 Loss: 0.8404139280319214\n",
      "Iteration 37634 Loss: 1.0731873512268066\n",
      "Iteration 37635 Loss: 1.055479645729065\n",
      "Iteration 37636 Loss: 1.0129355192184448\n",
      "Iteration 37637 Loss: 0.9694393873214722\n",
      "Iteration 37638 Loss: 1.0868579149246216\n",
      "Iteration 37639 Loss: 0.9722652435302734\n",
      "Iteration 37639 Loss: 0.9654546976089478\n",
      "Iteration 37640 Loss: 0.9745701551437378\n",
      "Iteration 37641 Loss: 0.9456146955490112\n",
      "Iteration 37642 Loss: 1.1476144790649414\n",
      "Iteration 37643 Loss: 0.7127015590667725\n",
      "Iteration 37644 Loss: 0.712668776512146\n",
      "Iteration 37645 Loss: 1.2341810464859009\n",
      "Iteration 37646 Loss: 0.6012236475944519\n",
      "Iteration 37647 Loss: 1.036942958831787\n",
      "Iteration 37648 Loss: 0.9447005391120911\n",
      "Iteration 37649 Loss: 1.1224510669708252\n",
      "Iteration 37649 Loss: 0.9432668685913086\n",
      "Iteration 37650 Loss: 0.8768171072006226\n",
      "Iteration 37651 Loss: 1.0738009214401245\n",
      "Iteration 37652 Loss: 0.7165022492408752\n",
      "Iteration 37653 Loss: 0.9194639325141907\n",
      "Iteration 37654 Loss: 0.6574499011039734\n",
      "Iteration 37655 Loss: 1.08357834815979\n",
      "Iteration 37656 Loss: 0.7643755078315735\n",
      "Iteration 37657 Loss: 0.8818755745887756\n",
      "Iteration 37658 Loss: 0.6709361672401428\n",
      "Iteration 37659 Loss: 1.2670643329620361\n",
      "Iteration 37659 Loss: 0.8911864161491394\n",
      "Iteration 37660 Loss: 0.8461291790008545\n",
      "Iteration 37661 Loss: 0.9053520560264587\n",
      "Iteration 37662 Loss: 0.6965038776397705\n",
      "Iteration 37663 Loss: 1.058807373046875\n",
      "Iteration 37664 Loss: 0.6916435956954956\n",
      "Iteration 37665 Loss: 0.8702884316444397\n",
      "Iteration 37666 Loss: 0.8824877142906189\n",
      "Iteration 37667 Loss: 1.1432217359542847\n",
      "Iteration 37668 Loss: 1.109962821006775\n",
      "Iteration 37669 Loss: 1.1934294700622559\n",
      "Iteration 37669 Loss: 0.9397826194763184\n",
      "Iteration 37670 Loss: 1.18656587600708\n",
      "Iteration 37671 Loss: 0.8608989119529724\n",
      "Iteration 37672 Loss: 1.1049529314041138\n",
      "Iteration 37673 Loss: 0.9810853004455566\n",
      "Iteration 37674 Loss: 1.100432276725769\n",
      "Iteration 37675 Loss: 0.8606984615325928\n",
      "Iteration 37676 Loss: 1.5281375646591187\n",
      "Iteration 37677 Loss: 0.9030469655990601\n",
      "Iteration 37678 Loss: 0.9324827790260315\n",
      "Iteration 37679 Loss: 1.0192182064056396\n",
      "Iteration 37679 Loss: 1.0477519035339355\n",
      "Iteration 37680 Loss: 0.8458731174468994\n",
      "Iteration 37681 Loss: 1.007840633392334\n",
      "Iteration 37682 Loss: 0.9405133724212646\n",
      "Iteration 37683 Loss: 0.915782630443573\n",
      "Iteration 37684 Loss: 0.907412588596344\n",
      "Iteration 37685 Loss: 1.0162590742111206\n",
      "Iteration 37686 Loss: 1.126582145690918\n",
      "Iteration 37687 Loss: 1.17293119430542\n",
      "Iteration 37688 Loss: 0.47801080346107483\n",
      "Iteration 37689 Loss: 1.1899683475494385\n",
      "Iteration 37689 Loss: 0.9601173400878906\n",
      "Iteration 37690 Loss: 0.7743869423866272\n",
      "Iteration 37691 Loss: 0.8521835207939148\n",
      "Iteration 37692 Loss: 1.4468944072723389\n",
      "Iteration 37693 Loss: 1.3089417219161987\n",
      "Iteration 37694 Loss: 0.6840910911560059\n",
      "Iteration 37695 Loss: 1.372092366218567\n",
      "Iteration 37696 Loss: 0.9497947692871094\n",
      "Iteration 37697 Loss: 0.8398370146751404\n",
      "Iteration 37698 Loss: 0.8560416102409363\n",
      "Iteration 37699 Loss: 1.1786259412765503\n",
      "Iteration 37699 Loss: 1.0262889862060547\n",
      "Iteration 37700 Loss: 1.2636538743972778\n",
      "Iteration 37701 Loss: 1.10370934009552\n",
      "Iteration 37702 Loss: 0.8254901766777039\n",
      "Iteration 37703 Loss: 0.8903944492340088\n",
      "Iteration 37704 Loss: 0.9344309568405151\n",
      "Iteration 37705 Loss: 0.7403531670570374\n",
      "Iteration 37706 Loss: 0.8262414932250977\n",
      "Iteration 37707 Loss: 0.702633261680603\n",
      "Iteration 37708 Loss: 0.9999170899391174\n",
      "Iteration 37709 Loss: 0.5458061695098877\n",
      "Iteration 37709 Loss: 0.8832629919052124\n",
      "Iteration 37710 Loss: 0.9336360096931458\n",
      "Iteration 37711 Loss: 0.8557285070419312\n",
      "Iteration 37712 Loss: 1.4274117946624756\n",
      "Iteration 37713 Loss: 1.1080167293548584\n",
      "Iteration 37714 Loss: 1.383457899093628\n",
      "Iteration 37715 Loss: 0.8392161130905151\n",
      "Iteration 37716 Loss: 1.0588054656982422\n",
      "Iteration 37717 Loss: 0.9256993532180786\n",
      "Iteration 37718 Loss: 1.3162678480148315\n",
      "Iteration 37719 Loss: 1.0504851341247559\n",
      "Iteration 37719 Loss: 1.0898725986480713\n",
      "Iteration 37720 Loss: 0.7575266361236572\n",
      "Iteration 37721 Loss: 1.0560115575790405\n",
      "Iteration 37722 Loss: 1.2477549314498901\n",
      "Iteration 37723 Loss: 0.7354167103767395\n",
      "Iteration 37724 Loss: 0.7880347967147827\n",
      "Iteration 37725 Loss: 1.042708396911621\n",
      "Iteration 37726 Loss: 0.9929487705230713\n",
      "Iteration 37727 Loss: 1.0262820720672607\n",
      "Iteration 37728 Loss: 0.713376522064209\n",
      "Iteration 37729 Loss: 0.7915049195289612\n",
      "Iteration 37729 Loss: 0.9151565432548523\n",
      "Iteration 37730 Loss: 1.3466956615447998\n",
      "Iteration 37731 Loss: 0.9591207504272461\n",
      "Iteration 37732 Loss: 1.0175139904022217\n",
      "Iteration 37733 Loss: 0.7594172954559326\n",
      "Iteration 37734 Loss: 1.2057087421417236\n",
      "Iteration 37735 Loss: 0.8255293965339661\n",
      "Iteration 37736 Loss: 0.9768261909484863\n",
      "Iteration 37737 Loss: 0.9258045554161072\n",
      "Iteration 37738 Loss: 1.2448440790176392\n",
      "Iteration 37739 Loss: 1.2141722440719604\n",
      "Iteration 37739 Loss: 1.0475631952285767\n",
      "Iteration 37740 Loss: 0.7223649621009827\n",
      "Iteration 37741 Loss: 0.9699829816818237\n",
      "Iteration 37742 Loss: 1.2437924146652222\n",
      "Iteration 37743 Loss: 1.460837483406067\n",
      "Iteration 37744 Loss: 1.215041160583496\n",
      "Iteration 37745 Loss: 1.08077073097229\n",
      "Iteration 37746 Loss: 0.7588158249855042\n",
      "Iteration 37747 Loss: 0.9052895307540894\n",
      "Iteration 37748 Loss: 1.1496038436889648\n",
      "Iteration 37749 Loss: 1.0889414548873901\n",
      "Iteration 37749 Loss: 1.0595440864562988\n",
      "Iteration 37750 Loss: 1.128674030303955\n",
      "Iteration 37751 Loss: 1.1278095245361328\n",
      "Iteration 37752 Loss: 0.7087515592575073\n",
      "Iteration 37753 Loss: 0.9832075238227844\n",
      "Iteration 37754 Loss: 1.1460041999816895\n",
      "Iteration 37755 Loss: 1.2135180234909058\n",
      "Iteration 37756 Loss: 0.8964788317680359\n",
      "Iteration 37757 Loss: 0.8740576505661011\n",
      "Iteration 37758 Loss: 1.101569652557373\n",
      "Iteration 37759 Loss: 0.8823606967926025\n",
      "Iteration 37759 Loss: 1.006243109703064\n",
      "Iteration 37760 Loss: 0.9340673685073853\n",
      "Iteration 37761 Loss: 0.954315185546875\n",
      "Iteration 37762 Loss: 1.251772165298462\n",
      "Iteration 37763 Loss: 1.020666480064392\n",
      "Iteration 37764 Loss: 1.0356097221374512\n",
      "Iteration 37765 Loss: 0.7781159281730652\n",
      "Iteration 37766 Loss: 1.0196595191955566\n",
      "Iteration 37767 Loss: 1.0412591695785522\n",
      "Iteration 37768 Loss: 0.9438769221305847\n",
      "Iteration 37769 Loss: 1.148149847984314\n",
      "Iteration 37769 Loss: 1.012749195098877\n",
      "Iteration 37770 Loss: 1.2283387184143066\n",
      "Iteration 37771 Loss: 1.0546355247497559\n",
      "Iteration 37772 Loss: 0.7897992730140686\n",
      "Iteration 37773 Loss: 1.0863358974456787\n",
      "Iteration 37774 Loss: 0.8735069036483765\n",
      "Iteration 37775 Loss: 0.9230586886405945\n",
      "Iteration 37776 Loss: 0.8719610571861267\n",
      "Iteration 37777 Loss: 1.055403232574463\n",
      "Iteration 37778 Loss: 1.0175987482070923\n",
      "Iteration 37779 Loss: 0.8788074851036072\n",
      "Iteration 37779 Loss: 0.977944552898407\n",
      "Iteration 37780 Loss: 0.7265584468841553\n",
      "Iteration 37781 Loss: 0.9872947335243225\n",
      "Iteration 37782 Loss: 0.8664589524269104\n",
      "Iteration 37783 Loss: 1.0343856811523438\n",
      "Iteration 37784 Loss: 1.0645195245742798\n",
      "Iteration 37785 Loss: 1.0072883367538452\n",
      "Iteration 37786 Loss: 1.0987190008163452\n",
      "Iteration 37787 Loss: 1.577496886253357\n",
      "Iteration 37788 Loss: 1.2967725992202759\n",
      "Iteration 37789 Loss: 1.061444878578186\n",
      "Iteration 37789 Loss: 1.0720937252044678\n",
      "Iteration 37790 Loss: 0.9374217987060547\n",
      "Iteration 37791 Loss: 0.8628734350204468\n",
      "Iteration 37792 Loss: 0.5244659185409546\n",
      "Iteration 37793 Loss: 0.9769341349601746\n",
      "Iteration 37794 Loss: 0.8106003403663635\n",
      "Iteration 37795 Loss: 1.0425684452056885\n",
      "Iteration 37796 Loss: 0.7983701825141907\n",
      "Iteration 37797 Loss: 0.9226178526878357\n",
      "Iteration 37798 Loss: 0.7678037881851196\n",
      "Iteration 37799 Loss: 1.2335174083709717\n",
      "Iteration 37799 Loss: 0.8877173662185669\n",
      "Iteration 37800 Loss: 1.088000774383545\n",
      "Iteration 37801 Loss: 1.0533275604248047\n",
      "Iteration 37802 Loss: 0.8615211844444275\n",
      "Iteration 37803 Loss: 0.8877081871032715\n",
      "Iteration 37804 Loss: 0.9114816784858704\n",
      "Iteration 37805 Loss: 1.0091822147369385\n",
      "Iteration 37806 Loss: 0.6401246786117554\n",
      "Iteration 37807 Loss: 0.9925277829170227\n",
      "Iteration 37808 Loss: 1.1962357759475708\n",
      "Iteration 37809 Loss: 0.929702639579773\n",
      "Iteration 37809 Loss: 0.9569813013076782\n",
      "Iteration 37810 Loss: 0.5787081718444824\n",
      "Iteration 37811 Loss: 1.078093409538269\n",
      "Iteration 37812 Loss: 0.7847417593002319\n",
      "Iteration 37813 Loss: 0.880574643611908\n",
      "Iteration 37814 Loss: 1.3901771306991577\n",
      "Iteration 37815 Loss: 0.9595274925231934\n",
      "Iteration 37816 Loss: 1.0529532432556152\n",
      "Iteration 37817 Loss: 0.9088440537452698\n",
      "Iteration 37818 Loss: 0.738029956817627\n",
      "Iteration 37819 Loss: 0.8325006365776062\n",
      "Iteration 37819 Loss: 0.9204151034355164\n",
      "Iteration 37820 Loss: 1.123512864112854\n",
      "Iteration 37821 Loss: 1.3160737752914429\n",
      "Iteration 37822 Loss: 0.7711556553840637\n",
      "Iteration 37823 Loss: 1.0896967649459839\n",
      "Iteration 37824 Loss: 0.8634256720542908\n",
      "Iteration 37825 Loss: 1.4050484895706177\n",
      "Iteration 37826 Loss: 0.9244953989982605\n",
      "Iteration 37827 Loss: 1.1199662685394287\n",
      "Iteration 37828 Loss: 1.194725513458252\n",
      "Iteration 37829 Loss: 1.3779419660568237\n",
      "Iteration 37829 Loss: 1.1186044216156006\n",
      "Iteration 37830 Loss: 1.054344654083252\n",
      "Iteration 37831 Loss: 1.1969155073165894\n",
      "Iteration 37832 Loss: 0.8929107189178467\n",
      "Iteration 37833 Loss: 0.9549685120582581\n",
      "Iteration 37834 Loss: 0.799956202507019\n",
      "Iteration 37835 Loss: 1.0923060178756714\n",
      "Iteration 37836 Loss: 1.3349623680114746\n",
      "Iteration 37837 Loss: 0.8904302716255188\n",
      "Iteration 37838 Loss: 1.0432220697402954\n",
      "Iteration 37839 Loss: 0.9658069014549255\n",
      "Iteration 37839 Loss: 1.0225824117660522\n",
      "Iteration 37840 Loss: 0.875326931476593\n",
      "Iteration 37841 Loss: 1.2936903238296509\n",
      "Iteration 37842 Loss: 1.078536033630371\n",
      "Iteration 37843 Loss: 0.6754546165466309\n",
      "Iteration 37844 Loss: 0.9452254772186279\n",
      "Iteration 37845 Loss: 1.0211223363876343\n",
      "Iteration 37846 Loss: 1.2819057703018188\n",
      "Iteration 37847 Loss: 0.8835195899009705\n",
      "Iteration 37848 Loss: 0.7014317512512207\n",
      "Iteration 37849 Loss: 0.7382053732872009\n",
      "Iteration 37849 Loss: 0.9494417905807495\n",
      "Iteration 37850 Loss: 1.1847621202468872\n",
      "Iteration 37851 Loss: 1.0666667222976685\n",
      "Iteration 37852 Loss: 0.785039484500885\n",
      "Iteration 37853 Loss: 0.9205232858657837\n",
      "Iteration 37854 Loss: 1.0423072576522827\n",
      "Iteration 37855 Loss: 1.0491923093795776\n",
      "Iteration 37856 Loss: 1.0221027135849\n",
      "Iteration 37857 Loss: 0.7145046591758728\n",
      "Iteration 37858 Loss: 1.0192774534225464\n",
      "Iteration 37859 Loss: 0.9004560112953186\n",
      "Iteration 37859 Loss: 0.970483124256134\n",
      "Iteration 37860 Loss: 1.168247103691101\n",
      "Iteration 37861 Loss: 0.7878132462501526\n",
      "Iteration 37862 Loss: 1.3019073009490967\n",
      "Iteration 37863 Loss: 0.9835010170936584\n",
      "Iteration 37864 Loss: 0.9199942350387573\n",
      "Iteration 37865 Loss: 0.9775427579879761\n",
      "Iteration 37866 Loss: 1.0426857471466064\n",
      "Iteration 37867 Loss: 0.8294835090637207\n",
      "Iteration 37868 Loss: 0.7377366423606873\n",
      "Iteration 37869 Loss: 0.9047738313674927\n",
      "Iteration 37869 Loss: 0.9653686285018921\n",
      "Iteration 37870 Loss: 0.973949670791626\n",
      "Iteration 37871 Loss: 1.0637009143829346\n",
      "Iteration 37872 Loss: 0.7680658102035522\n",
      "Iteration 37873 Loss: 1.0978361368179321\n",
      "Iteration 37874 Loss: 0.4624079167842865\n",
      "Iteration 37875 Loss: 0.9790624380111694\n",
      "Iteration 37876 Loss: 0.9606961011886597\n",
      "Iteration 37877 Loss: 0.906148374080658\n",
      "Iteration 37878 Loss: 1.1855707168579102\n",
      "Iteration 37879 Loss: 1.1887781620025635\n",
      "Iteration 37879 Loss: 0.958621621131897\n",
      "Iteration 37880 Loss: 1.241276502609253\n",
      "Iteration 37881 Loss: 0.8264856934547424\n",
      "Iteration 37882 Loss: 1.2934879064559937\n",
      "Iteration 37883 Loss: 1.134831428527832\n",
      "Iteration 37884 Loss: 0.8045964241027832\n",
      "Iteration 37885 Loss: 1.0629240274429321\n",
      "Iteration 37886 Loss: 1.3524585962295532\n",
      "Iteration 37887 Loss: 1.1187325716018677\n",
      "Iteration 37888 Loss: 0.7371516227722168\n",
      "Iteration 37889 Loss: 0.8983595371246338\n",
      "Iteration 37889 Loss: 1.0470304489135742\n",
      "Iteration 37890 Loss: 0.968849778175354\n",
      "Iteration 37891 Loss: 1.0624487400054932\n",
      "Iteration 37892 Loss: 1.0042821168899536\n",
      "Iteration 37893 Loss: 1.1658318042755127\n",
      "Iteration 37894 Loss: 0.8024124503135681\n",
      "Iteration 37895 Loss: 1.0441324710845947\n",
      "Iteration 37896 Loss: 0.9717188477516174\n",
      "Iteration 37897 Loss: 0.866016149520874\n",
      "Iteration 37898 Loss: 0.8386996984481812\n",
      "Iteration 37899 Loss: 1.207475185394287\n",
      "Iteration 37899 Loss: 0.9931867718696594\n",
      "Iteration 37900 Loss: 0.905271589756012\n",
      "Iteration 37901 Loss: 0.9329902529716492\n",
      "Iteration 37902 Loss: 1.000162959098816\n",
      "Iteration 37903 Loss: 0.8623852729797363\n",
      "Iteration 37904 Loss: 1.180142879486084\n",
      "Iteration 37905 Loss: 1.0983848571777344\n",
      "Iteration 37906 Loss: 0.7757277488708496\n",
      "Iteration 37907 Loss: 1.0565394163131714\n",
      "Iteration 37908 Loss: 0.8096275925636292\n",
      "Iteration 37909 Loss: 1.0962928533554077\n",
      "Iteration 37909 Loss: 0.9717525243759155\n",
      "Iteration 37910 Loss: 0.875433087348938\n",
      "Iteration 37911 Loss: 1.0455622673034668\n",
      "Iteration 37912 Loss: 0.9285450577735901\n",
      "Iteration 37913 Loss: 0.922465443611145\n",
      "Iteration 37914 Loss: 0.934533417224884\n",
      "Iteration 37915 Loss: 0.9177319407463074\n",
      "Iteration 37916 Loss: 1.176943302154541\n",
      "Iteration 37917 Loss: 1.1537061929702759\n",
      "Iteration 37918 Loss: 0.8713814616203308\n",
      "Iteration 37919 Loss: 1.225114345550537\n",
      "Iteration 37919 Loss: 1.0051417350769043\n",
      "Iteration 37920 Loss: 0.7930998802185059\n",
      "Iteration 37921 Loss: 1.0754586458206177\n",
      "Iteration 37922 Loss: 0.9318150877952576\n",
      "Iteration 37923 Loss: 1.0030012130737305\n",
      "Iteration 37924 Loss: 0.9850687980651855\n",
      "Iteration 37925 Loss: 0.6608456969261169\n",
      "Iteration 37926 Loss: 1.2091476917266846\n",
      "Iteration 37927 Loss: 0.8875242471694946\n",
      "Iteration 37928 Loss: 1.031631588935852\n",
      "Iteration 37929 Loss: 0.7317396402359009\n",
      "Iteration 37929 Loss: 0.9309332966804504\n",
      "Iteration 37930 Loss: 0.9367419481277466\n",
      "Iteration 37931 Loss: 1.1194522380828857\n",
      "Iteration 37932 Loss: 1.219180941581726\n",
      "Iteration 37933 Loss: 1.3438879251480103\n",
      "Iteration 37934 Loss: 0.7114478945732117\n",
      "Iteration 37935 Loss: 0.8510134816169739\n",
      "Iteration 37936 Loss: 0.7489365935325623\n",
      "Iteration 37937 Loss: 0.6483860611915588\n",
      "Iteration 37938 Loss: 0.6720384955406189\n",
      "Iteration 37939 Loss: 1.3512767553329468\n",
      "Iteration 37939 Loss: 0.9602362513542175\n",
      "Iteration 37940 Loss: 0.8054571151733398\n",
      "Iteration 37941 Loss: 0.8273637890815735\n",
      "Iteration 37942 Loss: 0.477616548538208\n",
      "Iteration 37943 Loss: 1.31975519657135\n",
      "Iteration 37944 Loss: 1.3609390258789062\n",
      "Iteration 37945 Loss: 1.1236493587493896\n",
      "Iteration 37946 Loss: 0.8606100082397461\n",
      "Iteration 37947 Loss: 0.5282553434371948\n",
      "Iteration 37948 Loss: 0.9699347019195557\n",
      "Iteration 37949 Loss: 1.132699966430664\n",
      "Iteration 37949 Loss: 0.9406280517578125\n",
      "Iteration 37950 Loss: 0.8902167677879333\n",
      "Iteration 37951 Loss: 1.1854770183563232\n",
      "Iteration 37952 Loss: 0.9827851057052612\n",
      "Iteration 37953 Loss: 1.34440279006958\n",
      "Iteration 37954 Loss: 0.8565109968185425\n",
      "Iteration 37955 Loss: 1.0861163139343262\n",
      "Iteration 37956 Loss: 1.098068118095398\n",
      "Iteration 37957 Loss: 1.1101616621017456\n",
      "Iteration 37958 Loss: 0.8736271262168884\n",
      "Iteration 37959 Loss: 1.122955322265625\n",
      "Iteration 37959 Loss: 1.0550321340560913\n",
      "Iteration 37960 Loss: 1.6925225257873535\n",
      "Iteration 37961 Loss: 1.000653862953186\n",
      "Iteration 37962 Loss: 1.322230577468872\n",
      "Iteration 37963 Loss: 1.2597949504852295\n",
      "Iteration 37964 Loss: 1.030842661857605\n",
      "Iteration 37965 Loss: 0.8114516139030457\n",
      "Iteration 37966 Loss: 1.2477918863296509\n",
      "Iteration 37967 Loss: 1.104767918586731\n",
      "Iteration 37968 Loss: 1.2069075107574463\n",
      "Iteration 37969 Loss: 0.8601832389831543\n",
      "Iteration 37969 Loss: 1.1537147760391235\n",
      "Iteration 37970 Loss: 0.769556999206543\n",
      "Iteration 37971 Loss: 0.9474674463272095\n",
      "Iteration 37972 Loss: 0.6266777515411377\n",
      "Iteration 37973 Loss: 0.8771843314170837\n",
      "Iteration 37974 Loss: 0.8697980642318726\n",
      "Iteration 37975 Loss: 0.7878137230873108\n",
      "Iteration 37976 Loss: 1.019177794456482\n",
      "Iteration 37977 Loss: 1.0729055404663086\n",
      "Iteration 37978 Loss: 0.8841829299926758\n",
      "Iteration 37979 Loss: 0.9737087488174438\n",
      "Iteration 37979 Loss: 0.8828473091125488\n",
      "Iteration 37980 Loss: 1.0318330526351929\n",
      "Iteration 37981 Loss: 1.1977580785751343\n",
      "Iteration 37982 Loss: 0.9670785069465637\n",
      "Iteration 37983 Loss: 1.3515251874923706\n",
      "Iteration 37984 Loss: 0.6034520268440247\n",
      "Iteration 37985 Loss: 0.9819244146347046\n",
      "Iteration 37986 Loss: 1.0396853685379028\n",
      "Iteration 37987 Loss: 0.9289448261260986\n",
      "Iteration 37988 Loss: 0.7125062346458435\n",
      "Iteration 37989 Loss: 1.064728856086731\n",
      "Iteration 37989 Loss: 0.9879437685012817\n",
      "Iteration 37990 Loss: 0.7950587868690491\n",
      "Iteration 37991 Loss: 0.8978853225708008\n",
      "Iteration 37992 Loss: 1.2562142610549927\n",
      "Iteration 37993 Loss: 0.8954115509986877\n",
      "Iteration 37994 Loss: 1.2939974069595337\n",
      "Iteration 37995 Loss: 0.9092947244644165\n",
      "Iteration 37996 Loss: 0.8105288743972778\n",
      "Iteration 37997 Loss: 0.9220897555351257\n",
      "Iteration 37998 Loss: 0.8449554443359375\n",
      "Iteration 37999 Loss: 0.9195453524589539\n",
      "Iteration 37999 Loss: 0.9544981122016907\n",
      "Iteration 38000 Loss: 0.7396028637886047\n",
      "Iteration 38001 Loss: 0.8726147413253784\n",
      "Iteration 38002 Loss: 0.6612784266471863\n",
      "Iteration 38003 Loss: 1.1614103317260742\n",
      "Iteration 38004 Loss: 0.9369561672210693\n",
      "Iteration 38005 Loss: 1.0427201986312866\n",
      "Iteration 38006 Loss: 0.7496583461761475\n",
      "Iteration 38007 Loss: 1.02076256275177\n",
      "Iteration 38008 Loss: 0.733619213104248\n",
      "Iteration 38009 Loss: 0.8468972444534302\n",
      "Iteration 38009 Loss: 0.8765519857406616\n",
      "Iteration 38010 Loss: 0.810752809047699\n",
      "Iteration 38011 Loss: 1.1501879692077637\n",
      "Iteration 38012 Loss: 0.7330849170684814\n",
      "Iteration 38013 Loss: 1.100520133972168\n",
      "Iteration 38014 Loss: 0.9533964395523071\n",
      "Iteration 38015 Loss: 1.055311918258667\n",
      "Iteration 38016 Loss: 0.405642569065094\n",
      "Iteration 38017 Loss: 1.0837160348892212\n",
      "Iteration 38018 Loss: 1.1841670274734497\n",
      "Iteration 38019 Loss: 0.8729488253593445\n",
      "Iteration 38019 Loss: 0.934972882270813\n",
      "Iteration 38020 Loss: 1.0287059545516968\n",
      "Iteration 38021 Loss: 0.5860176086425781\n",
      "Iteration 38022 Loss: 0.9395318627357483\n",
      "Iteration 38023 Loss: 1.2757512331008911\n",
      "Iteration 38024 Loss: 1.3939849138259888\n",
      "Iteration 38025 Loss: 0.9755373001098633\n",
      "Iteration 38026 Loss: 0.910487711429596\n",
      "Iteration 38027 Loss: 0.792711615562439\n",
      "Iteration 38028 Loss: 0.8870987892150879\n",
      "Iteration 38029 Loss: 0.9436262249946594\n",
      "Iteration 38029 Loss: 0.9733452796936035\n",
      "Iteration 38030 Loss: 0.9994081258773804\n",
      "Iteration 38031 Loss: 1.0549519062042236\n",
      "Iteration 38032 Loss: 0.7416568994522095\n",
      "Iteration 38033 Loss: 0.440817654132843\n",
      "Iteration 38034 Loss: 0.8756501078605652\n",
      "Iteration 38035 Loss: 0.9568150043487549\n",
      "Iteration 38036 Loss: 0.969497799873352\n",
      "Iteration 38037 Loss: 1.1149605512619019\n",
      "Iteration 38038 Loss: 1.1931300163269043\n",
      "Iteration 38039 Loss: 1.0020734071731567\n",
      "Iteration 38039 Loss: 0.9348961114883423\n",
      "Iteration 38040 Loss: 0.9659460783004761\n",
      "Iteration 38041 Loss: 0.8618821501731873\n",
      "Iteration 38042 Loss: 1.1912199258804321\n",
      "Iteration 38043 Loss: 0.9155335426330566\n",
      "Iteration 38044 Loss: 0.8201747536659241\n",
      "Iteration 38045 Loss: 1.081992268562317\n",
      "Iteration 38046 Loss: 0.9978341460227966\n",
      "Iteration 38047 Loss: 0.684931755065918\n",
      "Iteration 38048 Loss: 1.0254980325698853\n",
      "Iteration 38049 Loss: 1.0373114347457886\n",
      "Iteration 38049 Loss: 0.9582324028015137\n",
      "Iteration 38050 Loss: 1.1066776514053345\n",
      "Iteration 38051 Loss: 0.7167317867279053\n",
      "Iteration 38052 Loss: 0.980618953704834\n",
      "Iteration 38053 Loss: 1.2258507013320923\n",
      "Iteration 38054 Loss: 0.6362291574478149\n",
      "Iteration 38055 Loss: 1.0017614364624023\n",
      "Iteration 38056 Loss: 1.1359837055206299\n",
      "Iteration 38057 Loss: 1.4751946926116943\n",
      "Iteration 38058 Loss: 1.0206326246261597\n",
      "Iteration 38059 Loss: 0.8378174901008606\n",
      "Iteration 38059 Loss: 1.0137498378753662\n",
      "Iteration 38060 Loss: 0.9181270599365234\n",
      "Iteration 38061 Loss: 1.2141344547271729\n",
      "Iteration 38062 Loss: 0.8137537240982056\n",
      "Iteration 38063 Loss: 1.0411357879638672\n",
      "Iteration 38064 Loss: 0.7959821224212646\n",
      "Iteration 38065 Loss: 1.0766979455947876\n",
      "Iteration 38066 Loss: 0.7196512222290039\n",
      "Iteration 38067 Loss: 1.1100009679794312\n",
      "Iteration 38068 Loss: 1.053590178489685\n",
      "Iteration 38069 Loss: 1.113877534866333\n",
      "Iteration 38069 Loss: 0.9856950640678406\n",
      "Iteration 38070 Loss: 1.1342169046401978\n",
      "Iteration 38071 Loss: 0.5713294148445129\n",
      "Iteration 38072 Loss: 1.0580869913101196\n",
      "Iteration 38073 Loss: 1.183750867843628\n",
      "Iteration 38074 Loss: 0.9108831882476807\n",
      "Iteration 38075 Loss: 1.1102714538574219\n",
      "Iteration 38076 Loss: 0.9129770994186401\n",
      "Iteration 38077 Loss: 1.2621026039123535\n",
      "Iteration 38078 Loss: 1.1345293521881104\n",
      "Iteration 38079 Loss: 1.1702117919921875\n",
      "Iteration 38079 Loss: 1.0448360443115234\n",
      "Iteration 38080 Loss: 1.3778480291366577\n",
      "Iteration 38081 Loss: 1.0871120691299438\n",
      "Iteration 38082 Loss: 0.6446301937103271\n",
      "Iteration 38083 Loss: 1.1963452100753784\n",
      "Iteration 38084 Loss: 1.0962212085723877\n",
      "Iteration 38085 Loss: 0.8226271271705627\n",
      "Iteration 38086 Loss: 1.288434386253357\n",
      "Iteration 38087 Loss: 0.9925289750099182\n",
      "Iteration 38088 Loss: 1.1828972101211548\n",
      "Iteration 38089 Loss: 0.9640220999717712\n",
      "Iteration 38089 Loss: 1.0652666091918945\n",
      "Iteration 38090 Loss: 1.1921272277832031\n",
      "Iteration 38091 Loss: 0.9632090330123901\n",
      "Iteration 38092 Loss: 0.8388863801956177\n",
      "Iteration 38093 Loss: 1.001624345779419\n",
      "Iteration 38094 Loss: 1.2166000604629517\n",
      "Iteration 38095 Loss: 1.0878851413726807\n",
      "Iteration 38096 Loss: 1.1724269390106201\n",
      "Iteration 38097 Loss: 1.1475008726119995\n",
      "Iteration 38098 Loss: 1.463467001914978\n",
      "Iteration 38099 Loss: 1.0583648681640625\n",
      "Iteration 38099 Loss: 1.1142092943191528\n",
      "Iteration 38100 Loss: 0.8811279535293579\n",
      "Iteration 38101 Loss: 0.8852487206459045\n",
      "Iteration 38102 Loss: 1.1962507963180542\n",
      "Iteration 38103 Loss: 1.0135821104049683\n",
      "Iteration 38104 Loss: 0.8451148867607117\n",
      "Iteration 38105 Loss: 1.0055181980133057\n",
      "Iteration 38106 Loss: 1.1044031381607056\n",
      "Iteration 38107 Loss: 0.6441545486450195\n",
      "Iteration 38108 Loss: 1.053707480430603\n",
      "Iteration 38109 Loss: 0.8181213140487671\n",
      "Iteration 38109 Loss: 0.9447229504585266\n",
      "Iteration 38110 Loss: 0.9666593074798584\n",
      "Iteration 38111 Loss: 0.7176440954208374\n",
      "Iteration 38112 Loss: 1.0882089138031006\n",
      "Iteration 38113 Loss: 0.8432601094245911\n",
      "Iteration 38114 Loss: 1.0944390296936035\n",
      "Iteration 38115 Loss: 0.8782162070274353\n",
      "Iteration 38116 Loss: 0.819949746131897\n",
      "Iteration 38117 Loss: 0.6823585033416748\n",
      "Iteration 38118 Loss: 1.0645407438278198\n",
      "Iteration 38119 Loss: 1.261535406112671\n",
      "Iteration 38119 Loss: 0.9416812062263489\n",
      "Iteration 38120 Loss: 0.8389469385147095\n",
      "Iteration 38121 Loss: 0.8636966943740845\n",
      "Iteration 38122 Loss: 1.1034832000732422\n",
      "Iteration 38123 Loss: 0.8880990147590637\n",
      "Iteration 38124 Loss: 1.39704167842865\n",
      "Iteration 38125 Loss: 0.9302682280540466\n",
      "Iteration 38126 Loss: 1.277267575263977\n",
      "Iteration 38127 Loss: 0.8737140893936157\n",
      "Iteration 38128 Loss: 1.051424264907837\n",
      "Iteration 38129 Loss: 0.9221628308296204\n",
      "Iteration 38129 Loss: 1.0146105289459229\n",
      "Iteration 38130 Loss: 1.224984049797058\n",
      "Iteration 38131 Loss: 1.0605396032333374\n",
      "Iteration 38132 Loss: 0.8214539885520935\n",
      "Iteration 38133 Loss: 0.9669662714004517\n",
      "Iteration 38134 Loss: 1.145158052444458\n",
      "Iteration 38135 Loss: 0.9902480840682983\n",
      "Iteration 38136 Loss: 0.9721627831459045\n",
      "Iteration 38137 Loss: 0.9204207062721252\n",
      "Iteration 38138 Loss: 1.1548750400543213\n",
      "Iteration 38139 Loss: 1.1052184104919434\n",
      "Iteration 38139 Loss: 1.0362026691436768\n",
      "Iteration 38140 Loss: 0.8410225510597229\n",
      "Iteration 38141 Loss: 0.9936984777450562\n",
      "Iteration 38142 Loss: 0.8070117235183716\n",
      "Iteration 38143 Loss: 0.708541214466095\n",
      "Iteration 38144 Loss: 1.0970425605773926\n",
      "Iteration 38145 Loss: 0.7827288508415222\n",
      "Iteration 38146 Loss: 0.5600577592849731\n",
      "Iteration 38147 Loss: 1.3187462091445923\n",
      "Iteration 38148 Loss: 0.7144904136657715\n",
      "Iteration 38149 Loss: 1.389275312423706\n",
      "Iteration 38149 Loss: 0.9212614893913269\n",
      "Iteration 38150 Loss: 0.8453882336616516\n",
      "Iteration 38151 Loss: 0.8384296298027039\n",
      "Iteration 38152 Loss: 0.9292547702789307\n",
      "Iteration 38153 Loss: 1.0353418588638306\n",
      "Iteration 38154 Loss: 1.0322445631027222\n",
      "Iteration 38155 Loss: 0.9631544351577759\n",
      "Iteration 38156 Loss: 0.814784824848175\n",
      "Iteration 38157 Loss: 1.187440037727356\n",
      "Iteration 38158 Loss: 0.7323487401008606\n",
      "Iteration 38159 Loss: 1.1378504037857056\n",
      "Iteration 38159 Loss: 0.9516237378120422\n",
      "Iteration 38160 Loss: 1.0397249460220337\n",
      "Iteration 38161 Loss: 1.3603020906448364\n",
      "Iteration 38162 Loss: 1.0955644845962524\n",
      "Iteration 38163 Loss: 1.0340845584869385\n",
      "Iteration 38164 Loss: 0.8880311846733093\n",
      "Iteration 38165 Loss: 0.8598242998123169\n",
      "Iteration 38166 Loss: 0.9880603551864624\n",
      "Iteration 38167 Loss: 0.7523025274276733\n",
      "Iteration 38168 Loss: 0.7487226128578186\n",
      "Iteration 38169 Loss: 1.301381230354309\n",
      "Iteration 38169 Loss: 1.0067996978759766\n",
      "Iteration 38170 Loss: 1.0583258867263794\n",
      "Iteration 38171 Loss: 1.085599660873413\n",
      "Iteration 38172 Loss: 0.9736368060112\n",
      "Iteration 38173 Loss: 0.8432831168174744\n",
      "Iteration 38174 Loss: 1.0548478364944458\n",
      "Iteration 38175 Loss: 0.846763551235199\n",
      "Iteration 38176 Loss: 1.1245964765548706\n",
      "Iteration 38177 Loss: 1.0365983247756958\n",
      "Iteration 38178 Loss: 0.975784420967102\n",
      "Iteration 38179 Loss: 1.2750887870788574\n",
      "Iteration 38179 Loss: 1.0274523496627808\n",
      "Iteration 38180 Loss: 1.02627694606781\n",
      "Iteration 38181 Loss: 0.8938316702842712\n",
      "Iteration 38182 Loss: 0.9338636994361877\n",
      "Iteration 38183 Loss: 0.828349769115448\n",
      "Iteration 38184 Loss: 0.8037800192832947\n",
      "Iteration 38185 Loss: 0.8546745181083679\n",
      "Iteration 38186 Loss: 0.8758141398429871\n",
      "Iteration 38187 Loss: 1.1047435998916626\n",
      "Iteration 38188 Loss: 0.9864380359649658\n",
      "Iteration 38189 Loss: 0.9151132702827454\n",
      "Iteration 38189 Loss: 0.9222885966300964\n",
      "Iteration 38190 Loss: 0.959136962890625\n",
      "Iteration 38191 Loss: 0.6589576005935669\n",
      "Iteration 38192 Loss: 0.991371750831604\n",
      "Iteration 38193 Loss: 1.0215320587158203\n",
      "Iteration 38194 Loss: 1.2063202857971191\n",
      "Iteration 38195 Loss: 1.0372755527496338\n",
      "Iteration 38196 Loss: 1.1689280271530151\n",
      "Iteration 38197 Loss: 1.0476951599121094\n",
      "Iteration 38198 Loss: 0.8772801756858826\n",
      "Iteration 38199 Loss: 0.8074095845222473\n",
      "Iteration 38199 Loss: 0.9775907397270203\n",
      "Iteration 38200 Loss: 0.988587498664856\n",
      "Iteration 38201 Loss: 0.7199682593345642\n",
      "Iteration 38202 Loss: 0.5659796595573425\n",
      "Iteration 38203 Loss: 1.1586753129959106\n",
      "Iteration 38204 Loss: 0.7783791422843933\n",
      "Iteration 38205 Loss: 1.2061660289764404\n",
      "Iteration 38206 Loss: 0.9359108209609985\n",
      "Iteration 38207 Loss: 0.8139337301254272\n",
      "Iteration 38208 Loss: 1.018096685409546\n",
      "Iteration 38209 Loss: 1.1404112577438354\n",
      "Iteration 38209 Loss: 0.932610809803009\n",
      "Iteration 38210 Loss: 1.21112859249115\n",
      "Iteration 38211 Loss: 0.9757933020591736\n",
      "Iteration 38212 Loss: 1.0230134725570679\n",
      "Iteration 38213 Loss: 1.3231580257415771\n",
      "Iteration 38214 Loss: 0.8910576701164246\n",
      "Iteration 38215 Loss: 0.812631368637085\n",
      "Iteration 38216 Loss: 0.8589377999305725\n",
      "Iteration 38217 Loss: 0.8650973439216614\n",
      "Iteration 38218 Loss: 1.0621192455291748\n",
      "Iteration 38219 Loss: 1.0076274871826172\n",
      "Iteration 38219 Loss: 1.0030564069747925\n",
      "Iteration 38220 Loss: 0.9443073272705078\n",
      "Iteration 38221 Loss: 1.4366157054901123\n",
      "Iteration 38222 Loss: 0.8755331635475159\n",
      "Iteration 38223 Loss: 0.7831100225448608\n",
      "Iteration 38224 Loss: 0.8437004089355469\n",
      "Iteration 38225 Loss: 1.1555296182632446\n",
      "Iteration 38226 Loss: 1.0577809810638428\n",
      "Iteration 38227 Loss: 0.9499813318252563\n",
      "Iteration 38228 Loss: 0.6372110843658447\n",
      "Iteration 38229 Loss: 0.8649144172668457\n",
      "Iteration 38229 Loss: 0.9548684358596802\n",
      "Iteration 38230 Loss: 0.9902253746986389\n",
      "Iteration 38231 Loss: 0.959425151348114\n",
      "Iteration 38232 Loss: 1.2193413972854614\n",
      "Iteration 38233 Loss: 1.0146440267562866\n",
      "Iteration 38234 Loss: 0.8734598159790039\n",
      "Iteration 38235 Loss: 0.9695615172386169\n",
      "Iteration 38236 Loss: 0.954224169254303\n",
      "Iteration 38237 Loss: 0.8805968165397644\n",
      "Iteration 38238 Loss: 1.1612801551818848\n",
      "Iteration 38239 Loss: 1.1692073345184326\n",
      "Iteration 38239 Loss: 1.019196629524231\n",
      "Iteration 38240 Loss: 1.0000172853469849\n",
      "Iteration 38241 Loss: 1.1238727569580078\n",
      "Iteration 38242 Loss: 0.7853548526763916\n",
      "Iteration 38243 Loss: 1.1547706127166748\n",
      "Iteration 38244 Loss: 1.2627679109573364\n",
      "Iteration 38245 Loss: 0.9808090329170227\n",
      "Iteration 38246 Loss: 1.0292954444885254\n",
      "Iteration 38247 Loss: 0.5836557745933533\n",
      "Iteration 38248 Loss: 1.3687585592269897\n",
      "Iteration 38249 Loss: 1.3535425662994385\n",
      "Iteration 38249 Loss: 1.0642844438552856\n",
      "Iteration 38250 Loss: 1.2291936874389648\n",
      "Iteration 38251 Loss: 1.1510262489318848\n",
      "Iteration 38252 Loss: 0.9980692267417908\n",
      "Iteration 38253 Loss: 0.8186060786247253\n",
      "Iteration 38254 Loss: 0.8530399203300476\n",
      "Iteration 38255 Loss: 1.0119825601577759\n",
      "Iteration 38256 Loss: 1.1609015464782715\n",
      "Iteration 38257 Loss: 0.7762550115585327\n",
      "Iteration 38258 Loss: 0.8404386043548584\n",
      "Iteration 38259 Loss: 1.0630379915237427\n",
      "Iteration 38259 Loss: 0.9902549982070923\n",
      "Iteration 38260 Loss: 0.9194015860557556\n",
      "Iteration 38261 Loss: 0.8867732882499695\n",
      "Iteration 38262 Loss: 0.7737957835197449\n",
      "Iteration 38263 Loss: 0.8617338538169861\n",
      "Iteration 38264 Loss: 1.0642048120498657\n",
      "Iteration 38265 Loss: 0.8327046036720276\n",
      "Iteration 38266 Loss: 0.9138792157173157\n",
      "Iteration 38267 Loss: 1.213581919670105\n",
      "Iteration 38268 Loss: 0.8293813467025757\n",
      "Iteration 38269 Loss: 1.2269247770309448\n",
      "Iteration 38269 Loss: 0.9522380828857422\n",
      "Iteration 38270 Loss: 1.0630888938903809\n",
      "Iteration 38271 Loss: 0.9545097947120667\n",
      "Iteration 38272 Loss: 0.8918266892433167\n",
      "Iteration 38273 Loss: 1.1202504634857178\n",
      "Iteration 38274 Loss: 0.5969414710998535\n",
      "Iteration 38275 Loss: 0.8832663893699646\n",
      "Iteration 38276 Loss: 0.8854062557220459\n",
      "Iteration 38277 Loss: 1.1382665634155273\n",
      "Iteration 38278 Loss: 1.3631176948547363\n",
      "Iteration 38279 Loss: 0.9820826053619385\n",
      "Iteration 38279 Loss: 0.9878756403923035\n",
      "Iteration 38280 Loss: 0.630970299243927\n",
      "Iteration 38281 Loss: 1.2319763898849487\n",
      "Iteration 38282 Loss: 0.8488621711730957\n",
      "Iteration 38283 Loss: 1.2514333724975586\n",
      "Iteration 38284 Loss: 1.0772359371185303\n",
      "Iteration 38285 Loss: 0.9535940885543823\n",
      "Iteration 38286 Loss: 1.1142634153366089\n",
      "Iteration 38287 Loss: 0.8366672992706299\n",
      "Iteration 38288 Loss: 0.7437587976455688\n",
      "Iteration 38289 Loss: 1.0769383907318115\n",
      "Iteration 38289 Loss: 0.9765700101852417\n",
      "Iteration 38290 Loss: 1.078170895576477\n",
      "Iteration 38291 Loss: 0.9963708519935608\n",
      "Iteration 38292 Loss: 1.083754062652588\n",
      "Iteration 38293 Loss: 0.9753751754760742\n",
      "Iteration 38294 Loss: 0.6266170740127563\n",
      "Iteration 38295 Loss: 0.7469351887702942\n",
      "Iteration 38296 Loss: 0.8073028922080994\n",
      "Iteration 38297 Loss: 0.662835955619812\n",
      "Iteration 38298 Loss: 0.8510967493057251\n",
      "Iteration 38299 Loss: 1.029986023902893\n",
      "Iteration 38299 Loss: 0.8858445286750793\n",
      "Iteration 38300 Loss: 0.9241313338279724\n",
      "Iteration 38301 Loss: 1.251657247543335\n",
      "Iteration 38302 Loss: 0.8100966811180115\n",
      "Iteration 38303 Loss: 1.0016096830368042\n",
      "Iteration 38304 Loss: 1.0261399745941162\n",
      "Iteration 38305 Loss: 1.0114763975143433\n",
      "Iteration 38306 Loss: 0.8057200312614441\n",
      "Iteration 38307 Loss: 0.754002571105957\n",
      "Iteration 38308 Loss: 0.9268612861633301\n",
      "Iteration 38309 Loss: 1.029523491859436\n",
      "Iteration 38309 Loss: 0.9541219472885132\n",
      "Iteration 38310 Loss: 0.8256180286407471\n",
      "Iteration 38311 Loss: 1.0384700298309326\n",
      "Iteration 38312 Loss: 1.2015230655670166\n",
      "Iteration 38313 Loss: 0.9541626572608948\n",
      "Iteration 38314 Loss: 1.0085748434066772\n",
      "Iteration 38315 Loss: 1.1714341640472412\n",
      "Iteration 38316 Loss: 0.6554388999938965\n",
      "Iteration 38317 Loss: 1.0057896375656128\n",
      "Iteration 38318 Loss: 0.9546970129013062\n",
      "Iteration 38319 Loss: 1.1001230478286743\n",
      "Iteration 38319 Loss: 0.9915831685066223\n",
      "Iteration 38320 Loss: 0.7805764079093933\n",
      "Iteration 38321 Loss: 0.9215895533561707\n",
      "Iteration 38322 Loss: 0.7004936933517456\n",
      "Iteration 38323 Loss: 0.8490135073661804\n",
      "Iteration 38324 Loss: 0.7753551602363586\n",
      "Iteration 38325 Loss: 1.1619054079055786\n",
      "Iteration 38326 Loss: 1.1892237663269043\n",
      "Iteration 38327 Loss: 0.979882538318634\n",
      "Iteration 38328 Loss: 1.0858075618743896\n",
      "Iteration 38329 Loss: 0.8848173022270203\n",
      "Iteration 38329 Loss: 0.9328664541244507\n",
      "Iteration 38330 Loss: 0.8440539836883545\n",
      "Iteration 38331 Loss: 1.2492209672927856\n",
      "Iteration 38332 Loss: 0.6641473174095154\n",
      "Iteration 38333 Loss: 0.7022337913513184\n",
      "Iteration 38334 Loss: 1.2422599792480469\n",
      "Iteration 38335 Loss: 1.0371025800704956\n",
      "Iteration 38336 Loss: 1.1802419424057007\n",
      "Iteration 38337 Loss: 1.4617081880569458\n",
      "Iteration 38338 Loss: 0.789466917514801\n",
      "Iteration 38339 Loss: 0.7552123069763184\n",
      "Iteration 38339 Loss: 0.9925647974014282\n",
      "Iteration 38340 Loss: 1.046913504600525\n",
      "Iteration 38341 Loss: 0.8528905510902405\n",
      "Iteration 38342 Loss: 1.0563474893569946\n",
      "Iteration 38343 Loss: 0.9891121983528137\n",
      "Iteration 38344 Loss: 1.1024489402770996\n",
      "Iteration 38345 Loss: 1.3045090436935425\n",
      "Iteration 38346 Loss: 1.0203734636306763\n",
      "Iteration 38347 Loss: 1.3117913007736206\n",
      "Iteration 38348 Loss: 0.7259812355041504\n",
      "Iteration 38349 Loss: 1.088444471359253\n",
      "Iteration 38349 Loss: 1.0498812198638916\n",
      "Iteration 38350 Loss: 1.2888861894607544\n",
      "Iteration 38351 Loss: 1.049688458442688\n",
      "Iteration 38352 Loss: 1.4140394926071167\n",
      "Iteration 38353 Loss: 0.9912140965461731\n",
      "Iteration 38354 Loss: 0.7200402021408081\n",
      "Iteration 38355 Loss: 0.8398512005805969\n",
      "Iteration 38356 Loss: 0.824962854385376\n",
      "Iteration 38357 Loss: 0.9359999895095825\n",
      "Iteration 38358 Loss: 0.8764529228210449\n",
      "Iteration 38359 Loss: 1.0559300184249878\n",
      "Iteration 38359 Loss: 0.9997065663337708\n",
      "Iteration 38360 Loss: 1.464263916015625\n",
      "Iteration 38361 Loss: 0.8271062970161438\n",
      "Iteration 38362 Loss: 1.1573010683059692\n",
      "Iteration 38363 Loss: 1.1940966844558716\n",
      "Iteration 38364 Loss: 0.6989010572433472\n",
      "Iteration 38365 Loss: 1.0514402389526367\n",
      "Iteration 38366 Loss: 0.94432133436203\n",
      "Iteration 38367 Loss: 0.6999799013137817\n",
      "Iteration 38368 Loss: 1.239287257194519\n",
      "Iteration 38369 Loss: 0.7248127460479736\n",
      "Iteration 38369 Loss: 1.0001510381698608\n",
      "Iteration 38370 Loss: 0.9237250685691833\n",
      "Iteration 38371 Loss: 0.9619816541671753\n",
      "Iteration 38372 Loss: 0.867951512336731\n",
      "Iteration 38373 Loss: 0.7431801557540894\n",
      "Iteration 38374 Loss: 1.0071673393249512\n",
      "Iteration 38375 Loss: 0.9062187671661377\n",
      "Iteration 38376 Loss: 1.1362282037734985\n",
      "Iteration 38377 Loss: 0.8818130493164062\n",
      "Iteration 38378 Loss: 0.936610758304596\n",
      "Iteration 38379 Loss: 1.2896357774734497\n",
      "Iteration 38379 Loss: 0.9654512405395508\n",
      "Iteration 38380 Loss: 1.115713119506836\n",
      "Iteration 38381 Loss: 0.8206930160522461\n",
      "Iteration 38382 Loss: 0.6920900344848633\n",
      "Iteration 38383 Loss: 0.7759572267532349\n",
      "Iteration 38384 Loss: 0.8216649293899536\n",
      "Iteration 38385 Loss: 0.964332640171051\n",
      "Iteration 38386 Loss: 0.7716783881187439\n",
      "Iteration 38387 Loss: 1.2406491041183472\n",
      "Iteration 38388 Loss: 1.0898114442825317\n",
      "Iteration 38389 Loss: 1.015866994857788\n",
      "Iteration 38389 Loss: 0.9308456182479858\n",
      "Iteration 38390 Loss: 0.9564827084541321\n",
      "Iteration 38391 Loss: 1.0506566762924194\n",
      "Iteration 38392 Loss: 1.1631118059158325\n",
      "Iteration 38393 Loss: 1.05568528175354\n",
      "Iteration 38394 Loss: 0.9681899547576904\n",
      "Iteration 38395 Loss: 0.3217751681804657\n",
      "Iteration 38396 Loss: 0.9787192344665527\n",
      "Iteration 38397 Loss: 1.191537857055664\n",
      "Iteration 38398 Loss: 1.1207870244979858\n",
      "Iteration 38399 Loss: 0.9757713079452515\n",
      "Iteration 38399 Loss: 0.9782716631889343\n",
      "Iteration 38400 Loss: 1.0161669254302979\n",
      "Iteration 38401 Loss: 1.1560797691345215\n",
      "Iteration 38402 Loss: 0.9524239897727966\n",
      "Iteration 38403 Loss: 0.8751486539840698\n",
      "Iteration 38404 Loss: 1.0257012844085693\n",
      "Iteration 38405 Loss: 0.8976054787635803\n",
      "Iteration 38406 Loss: 1.361235499382019\n",
      "Iteration 38407 Loss: 0.6731459498405457\n",
      "Iteration 38408 Loss: 0.9620484709739685\n",
      "Iteration 38409 Loss: 0.8850448131561279\n",
      "Iteration 38409 Loss: 0.9804600477218628\n",
      "Iteration 38410 Loss: 0.8629875183105469\n",
      "Iteration 38411 Loss: 0.8909984230995178\n",
      "Iteration 38412 Loss: 1.033089280128479\n",
      "Iteration 38413 Loss: 1.3689905405044556\n",
      "Iteration 38414 Loss: 0.9555000066757202\n",
      "Iteration 38415 Loss: 0.6403324604034424\n",
      "Iteration 38416 Loss: 0.996283233165741\n",
      "Iteration 38417 Loss: 0.9778208136558533\n",
      "Iteration 38418 Loss: 0.6212626695632935\n",
      "Iteration 38419 Loss: 0.8316479325294495\n",
      "Iteration 38419 Loss: 0.9178913235664368\n",
      "Iteration 38420 Loss: 1.0626229047775269\n",
      "Iteration 38421 Loss: 0.8355137705802917\n",
      "Iteration 38422 Loss: 0.6831355690956116\n",
      "Iteration 38423 Loss: 1.0320297479629517\n",
      "Iteration 38424 Loss: 1.0127936601638794\n",
      "Iteration 38425 Loss: 0.8874735236167908\n",
      "Iteration 38426 Loss: 0.9489884376525879\n",
      "Iteration 38427 Loss: 1.0567916631698608\n",
      "Iteration 38428 Loss: 0.6288647651672363\n",
      "Iteration 38429 Loss: 1.088490605354309\n",
      "Iteration 38429 Loss: 0.9236704111099243\n",
      "Iteration 38430 Loss: 0.9666348695755005\n",
      "Iteration 38431 Loss: 0.7863293886184692\n",
      "Iteration 38432 Loss: 0.8669837713241577\n",
      "Iteration 38433 Loss: 1.2971487045288086\n",
      "Iteration 38434 Loss: 1.2207281589508057\n",
      "Iteration 38435 Loss: 0.8702363967895508\n",
      "Iteration 38436 Loss: 0.9789901971817017\n",
      "Iteration 38437 Loss: 1.1515570878982544\n",
      "Iteration 38438 Loss: 0.8313596844673157\n",
      "Iteration 38439 Loss: 0.9616425037384033\n",
      "Iteration 38439 Loss: 0.9931610822677612\n",
      "Iteration 38440 Loss: 1.1326745748519897\n",
      "Iteration 38441 Loss: 0.9461805820465088\n",
      "Iteration 38442 Loss: 0.9100606441497803\n",
      "Iteration 38443 Loss: 1.1307084560394287\n",
      "Iteration 38444 Loss: 1.1441032886505127\n",
      "Iteration 38445 Loss: 1.054341435432434\n",
      "Iteration 38446 Loss: 0.9283664226531982\n",
      "Iteration 38447 Loss: 0.934623658657074\n",
      "Iteration 38448 Loss: 0.9486313462257385\n",
      "Iteration 38449 Loss: 0.8611822128295898\n",
      "Iteration 38449 Loss: 0.9990872144699097\n",
      "Iteration 38450 Loss: 0.5940631031990051\n",
      "Iteration 38451 Loss: 1.117633581161499\n",
      "Iteration 38452 Loss: 0.6839073896408081\n",
      "Iteration 38453 Loss: 0.7249638438224792\n",
      "Iteration 38454 Loss: 1.0176219940185547\n",
      "Iteration 38455 Loss: 0.579008936882019\n",
      "Iteration 38456 Loss: 0.9191651344299316\n",
      "Iteration 38457 Loss: 1.376758098602295\n",
      "Iteration 38458 Loss: 1.1772226095199585\n",
      "Iteration 38459 Loss: 1.245666742324829\n",
      "Iteration 38459 Loss: 0.9436012506484985\n",
      "Iteration 38460 Loss: 0.7347516417503357\n",
      "Iteration 38461 Loss: 0.8472469449043274\n",
      "Iteration 38462 Loss: 0.9175713658332825\n",
      "Iteration 38463 Loss: 1.1845885515213013\n",
      "Iteration 38464 Loss: 1.0824880599975586\n",
      "Iteration 38465 Loss: 0.8936975598335266\n",
      "Iteration 38466 Loss: 1.223170280456543\n",
      "Iteration 38467 Loss: 0.9991525411605835\n",
      "Iteration 38468 Loss: 0.9679578542709351\n",
      "Iteration 38469 Loss: 0.5250611901283264\n",
      "Iteration 38469 Loss: 0.9375685453414917\n",
      "Iteration 38470 Loss: 0.9186375141143799\n",
      "Iteration 38471 Loss: 0.9433474540710449\n",
      "Iteration 38472 Loss: 1.1517798900604248\n",
      "Iteration 38473 Loss: 0.7247648239135742\n",
      "Iteration 38474 Loss: 1.194319486618042\n",
      "Iteration 38475 Loss: 0.9185206294059753\n",
      "Iteration 38476 Loss: 0.8812809586524963\n",
      "Iteration 38477 Loss: 0.9343197345733643\n",
      "Iteration 38478 Loss: 1.0154988765716553\n",
      "Iteration 38479 Loss: 1.0284643173217773\n",
      "Iteration 38479 Loss: 0.9710933566093445\n",
      "Iteration 38480 Loss: 1.1243027448654175\n",
      "Iteration 38481 Loss: 0.9750683903694153\n",
      "Iteration 38482 Loss: 1.007811188697815\n",
      "Iteration 38483 Loss: 0.8269482254981995\n",
      "Iteration 38484 Loss: 0.8005548119544983\n",
      "Iteration 38485 Loss: 0.7112180590629578\n",
      "Iteration 38486 Loss: 0.7416808605194092\n",
      "Iteration 38487 Loss: 0.9781465530395508\n",
      "Iteration 38488 Loss: 0.9118713140487671\n",
      "Iteration 38489 Loss: 0.7444872856140137\n",
      "Iteration 38489 Loss: 0.8822089433670044\n",
      "Iteration 38490 Loss: 1.124295711517334\n",
      "Iteration 38491 Loss: 1.230405569076538\n",
      "Iteration 38492 Loss: 1.0751374959945679\n",
      "Iteration 38493 Loss: 1.0548350811004639\n",
      "Iteration 38494 Loss: 1.2094879150390625\n",
      "Iteration 38495 Loss: 1.140726923942566\n",
      "Iteration 38496 Loss: 1.193424940109253\n",
      "Iteration 38497 Loss: 0.997180700302124\n",
      "Iteration 38498 Loss: 0.7236452698707581\n",
      "Iteration 38499 Loss: 1.2238913774490356\n",
      "Iteration 38499 Loss: 1.0973031520843506\n",
      "Iteration 38500 Loss: 1.1486784219741821\n",
      "Iteration 38501 Loss: 0.6571323871612549\n",
      "Iteration 38502 Loss: 0.8052407503128052\n",
      "Iteration 38503 Loss: 1.2530690431594849\n",
      "Iteration 38504 Loss: 1.1345680952072144\n",
      "Iteration 38505 Loss: 1.0036386251449585\n",
      "Iteration 38506 Loss: 1.2976938486099243\n",
      "Iteration 38507 Loss: 0.9394674301147461\n",
      "Iteration 38508 Loss: 1.301571249961853\n",
      "Iteration 38509 Loss: 1.05999755859375\n",
      "Iteration 38509 Loss: 1.0601056814193726\n",
      "Iteration 38510 Loss: 0.8147562742233276\n",
      "Iteration 38511 Loss: 0.8379161953926086\n",
      "Iteration 38512 Loss: 0.8025457859039307\n",
      "Iteration 38513 Loss: 1.3070061206817627\n",
      "Iteration 38514 Loss: 1.1846764087677002\n",
      "Iteration 38515 Loss: 0.9958705306053162\n",
      "Iteration 38516 Loss: 1.0507938861846924\n",
      "Iteration 38517 Loss: 0.9385800957679749\n",
      "Iteration 38518 Loss: 1.1138752698898315\n",
      "Iteration 38519 Loss: 0.9523921608924866\n",
      "Iteration 38519 Loss: 0.9998413324356079\n",
      "Iteration 38520 Loss: 1.0704289674758911\n",
      "Iteration 38521 Loss: 1.0298042297363281\n",
      "Iteration 38522 Loss: 0.7125023007392883\n",
      "Iteration 38523 Loss: 0.5137608647346497\n",
      "Iteration 38524 Loss: 1.563082218170166\n",
      "Iteration 38525 Loss: 1.0189253091812134\n",
      "Iteration 38526 Loss: 1.3687025308609009\n",
      "Iteration 38527 Loss: 0.891501247882843\n",
      "Iteration 38528 Loss: 0.8814780712127686\n",
      "Iteration 38529 Loss: 1.310665488243103\n",
      "Iteration 38529 Loss: 1.0360852479934692\n",
      "Iteration 38530 Loss: 0.8431215882301331\n",
      "Iteration 38531 Loss: 0.9904863834381104\n",
      "Iteration 38532 Loss: 0.8991590738296509\n",
      "Iteration 38533 Loss: 1.219812273979187\n",
      "Iteration 38534 Loss: 1.03983473777771\n",
      "Iteration 38535 Loss: 1.0885074138641357\n",
      "Iteration 38536 Loss: 1.4108749628067017\n",
      "Iteration 38537 Loss: 0.47394871711730957\n",
      "Iteration 38538 Loss: 0.9989861845970154\n",
      "Iteration 38539 Loss: 0.9212273955345154\n",
      "Iteration 38539 Loss: 0.9885959625244141\n",
      "Iteration 38540 Loss: 0.7667702436447144\n",
      "Iteration 38541 Loss: 0.9684715270996094\n",
      "Iteration 38542 Loss: 0.8418506383895874\n",
      "Iteration 38543 Loss: 1.3557718992233276\n",
      "Iteration 38544 Loss: 1.1696298122406006\n",
      "Iteration 38545 Loss: 1.0286865234375\n",
      "Iteration 38546 Loss: 1.1005971431732178\n",
      "Iteration 38547 Loss: 1.140076994895935\n",
      "Iteration 38548 Loss: 1.0939053297042847\n",
      "Iteration 38549 Loss: 0.9915872812271118\n",
      "Iteration 38549 Loss: 1.0457347631454468\n",
      "Iteration 38550 Loss: 1.338158369064331\n",
      "Iteration 38551 Loss: 1.0833793878555298\n",
      "Iteration 38552 Loss: 1.2695647478103638\n",
      "Iteration 38553 Loss: 0.9212479591369629\n",
      "Iteration 38554 Loss: 1.1015671491622925\n",
      "Iteration 38555 Loss: 0.6971842050552368\n",
      "Iteration 38556 Loss: 1.2849516868591309\n",
      "Iteration 38557 Loss: 1.1475982666015625\n",
      "Iteration 38558 Loss: 0.7686955332756042\n",
      "Iteration 38559 Loss: 0.9922227263450623\n",
      "Iteration 38559 Loss: 1.0604569911956787\n",
      "Iteration 38560 Loss: 0.794446587562561\n",
      "Iteration 38561 Loss: 0.7325729727745056\n",
      "Iteration 38562 Loss: 1.21493399143219\n",
      "Iteration 38563 Loss: 1.1088247299194336\n",
      "Iteration 38564 Loss: 0.9616832733154297\n",
      "Iteration 38565 Loss: 1.2863428592681885\n",
      "Iteration 38566 Loss: 1.3291194438934326\n",
      "Iteration 38567 Loss: 1.1255463361740112\n",
      "Iteration 38568 Loss: 1.0039818286895752\n",
      "Iteration 38569 Loss: 0.6029485464096069\n",
      "Iteration 38569 Loss: 1.0160400867462158\n",
      "Iteration 38570 Loss: 0.9573019742965698\n",
      "Iteration 38571 Loss: 1.1533578634262085\n",
      "Iteration 38572 Loss: 1.0442376136779785\n",
      "Iteration 38573 Loss: 0.9918256998062134\n",
      "Iteration 38574 Loss: 0.9009370803833008\n",
      "Iteration 38575 Loss: 1.0299609899520874\n",
      "Iteration 38576 Loss: 0.9790536165237427\n",
      "Iteration 38577 Loss: 1.1919803619384766\n",
      "Iteration 38578 Loss: 1.1225452423095703\n",
      "Iteration 38579 Loss: 1.0935156345367432\n",
      "Iteration 38579 Loss: 1.0464715957641602\n",
      "Iteration 38580 Loss: 1.1581242084503174\n",
      "Iteration 38581 Loss: 1.3095264434814453\n",
      "Iteration 38582 Loss: 0.848351001739502\n",
      "Iteration 38583 Loss: 0.9389762282371521\n",
      "Iteration 38584 Loss: 0.8043628334999084\n",
      "Iteration 38585 Loss: 1.0113533735275269\n",
      "Iteration 38586 Loss: 1.2369693517684937\n",
      "Iteration 38587 Loss: 1.4434401988983154\n",
      "Iteration 38588 Loss: 0.9721262454986572\n",
      "Iteration 38589 Loss: 0.9769860506057739\n",
      "Iteration 38589 Loss: 1.0700215101242065\n",
      "Iteration 38590 Loss: 1.036564826965332\n",
      "Iteration 38591 Loss: 1.1481890678405762\n",
      "Iteration 38592 Loss: 0.901025116443634\n",
      "Iteration 38593 Loss: 0.9300578236579895\n",
      "Iteration 38594 Loss: 1.1740835905075073\n",
      "Iteration 38595 Loss: 0.7418641448020935\n",
      "Iteration 38596 Loss: 0.7558462023735046\n",
      "Iteration 38597 Loss: 0.9836698174476624\n",
      "Iteration 38598 Loss: 1.0456516742706299\n",
      "Iteration 38599 Loss: 0.9166687726974487\n",
      "Iteration 38599 Loss: 0.9633622169494629\n",
      "Iteration 38600 Loss: 0.8439764380455017\n",
      "Iteration 38601 Loss: 0.6852196455001831\n",
      "Iteration 38602 Loss: 0.9263430237770081\n",
      "Iteration 38603 Loss: 1.263791799545288\n",
      "Iteration 38604 Loss: 0.6818374991416931\n",
      "Iteration 38605 Loss: 1.3390321731567383\n",
      "Iteration 38606 Loss: 0.8248388767242432\n",
      "Iteration 38607 Loss: 1.3100578784942627\n",
      "Iteration 38608 Loss: 0.8506345152854919\n",
      "Iteration 38609 Loss: 1.1522846221923828\n",
      "Iteration 38609 Loss: 0.9878016710281372\n",
      "Iteration 38610 Loss: 0.850067675113678\n",
      "Iteration 38611 Loss: 1.0611075162887573\n",
      "Iteration 38612 Loss: 0.8391872644424438\n",
      "Iteration 38613 Loss: 0.8140051364898682\n",
      "Iteration 38614 Loss: 0.8008446097373962\n",
      "Iteration 38615 Loss: 0.7709357738494873\n",
      "Iteration 38616 Loss: 1.0053093433380127\n",
      "Iteration 38617 Loss: 1.108557939529419\n",
      "Iteration 38618 Loss: 1.0976533889770508\n",
      "Iteration 38619 Loss: 1.2199233770370483\n",
      "Iteration 38619 Loss: 0.9567591547966003\n",
      "Iteration 38620 Loss: 0.8544511795043945\n",
      "Iteration 38621 Loss: 1.037432312965393\n",
      "Iteration 38622 Loss: 0.9867076277732849\n",
      "Iteration 38623 Loss: 1.6055622100830078\n",
      "Iteration 38624 Loss: 0.8779776096343994\n",
      "Iteration 38625 Loss: 0.8492326140403748\n",
      "Iteration 38626 Loss: 0.7366337180137634\n",
      "Iteration 38627 Loss: 1.1271206140518188\n",
      "Iteration 38628 Loss: 0.9750756025314331\n",
      "Iteration 38629 Loss: 1.0542311668395996\n",
      "Iteration 38629 Loss: 1.0104424953460693\n",
      "Iteration 38630 Loss: 1.1387120485305786\n",
      "Iteration 38631 Loss: 1.0298755168914795\n",
      "Iteration 38632 Loss: 0.835457444190979\n",
      "Iteration 38633 Loss: 1.163966417312622\n",
      "Iteration 38634 Loss: 0.9712819457054138\n",
      "Iteration 38635 Loss: 1.0631226301193237\n",
      "Iteration 38636 Loss: 0.972572386264801\n",
      "Iteration 38637 Loss: 0.6444274187088013\n",
      "Iteration 38638 Loss: 1.0115749835968018\n",
      "Iteration 38639 Loss: 1.2526066303253174\n",
      "Iteration 38639 Loss: 1.008359670639038\n",
      "Iteration 38640 Loss: 1.0229651927947998\n",
      "Iteration 38641 Loss: 1.0237455368041992\n",
      "Iteration 38642 Loss: 1.2230273485183716\n",
      "Iteration 38643 Loss: 1.0968579053878784\n",
      "Iteration 38644 Loss: 1.2243974208831787\n",
      "Iteration 38645 Loss: 0.9763324856758118\n",
      "Iteration 38646 Loss: 1.2285720109939575\n",
      "Iteration 38647 Loss: 0.6807863712310791\n",
      "Iteration 38648 Loss: 0.9743154644966125\n",
      "Iteration 38649 Loss: 1.061776876449585\n",
      "Iteration 38649 Loss: 1.0512776374816895\n",
      "Iteration 38650 Loss: 0.8225852847099304\n",
      "Iteration 38651 Loss: 0.9294959306716919\n",
      "Iteration 38652 Loss: 1.049316644668579\n",
      "Iteration 38653 Loss: 0.6611294150352478\n",
      "Iteration 38654 Loss: 0.7415888905525208\n",
      "Iteration 38655 Loss: 0.9850560426712036\n",
      "Iteration 38656 Loss: 0.7818399667739868\n",
      "Iteration 38657 Loss: 1.2490650415420532\n",
      "Iteration 38658 Loss: 0.9021138548851013\n",
      "Iteration 38659 Loss: 1.16670823097229\n",
      "Iteration 38659 Loss: 0.92889004945755\n",
      "Iteration 38660 Loss: 0.802262008190155\n",
      "Iteration 38661 Loss: 0.828456461429596\n",
      "Iteration 38662 Loss: 1.165425181388855\n",
      "Iteration 38663 Loss: 1.2928725481033325\n",
      "Iteration 38664 Loss: 0.8212599158287048\n",
      "Iteration 38665 Loss: 1.1722824573516846\n",
      "Iteration 38666 Loss: 1.089316725730896\n",
      "Iteration 38667 Loss: 1.2607123851776123\n",
      "Iteration 38668 Loss: 0.9464681148529053\n",
      "Iteration 38669 Loss: 1.138195514678955\n",
      "Iteration 38669 Loss: 1.051725149154663\n",
      "Iteration 38670 Loss: 1.4544607400894165\n",
      "Iteration 38671 Loss: 0.9884282350540161\n",
      "Iteration 38672 Loss: 0.8595770001411438\n",
      "Iteration 38673 Loss: 0.8550288677215576\n",
      "Iteration 38674 Loss: 1.217000126838684\n",
      "Iteration 38675 Loss: 0.8759018778800964\n",
      "Iteration 38676 Loss: 0.8992190361022949\n",
      "Iteration 38677 Loss: 1.0965399742126465\n",
      "Iteration 38678 Loss: 1.0156623125076294\n",
      "Iteration 38679 Loss: 0.8538379669189453\n",
      "Iteration 38679 Loss: 1.0115658044815063\n",
      "Iteration 38680 Loss: 1.033165454864502\n",
      "Iteration 38681 Loss: 1.3601711988449097\n",
      "Iteration 38682 Loss: 0.8475738167762756\n",
      "Iteration 38683 Loss: 1.339558720588684\n",
      "Iteration 38684 Loss: 1.048871397972107\n",
      "Iteration 38685 Loss: 1.1916316747665405\n",
      "Iteration 38686 Loss: 0.9957890510559082\n",
      "Iteration 38687 Loss: 0.9374064207077026\n",
      "Iteration 38688 Loss: 1.2032783031463623\n",
      "Iteration 38689 Loss: 0.8803871273994446\n",
      "Iteration 38689 Loss: 1.0837833881378174\n",
      "Iteration 38690 Loss: 1.0894745588302612\n",
      "Iteration 38691 Loss: 1.2934614419937134\n",
      "Iteration 38692 Loss: 1.0318979024887085\n",
      "Iteration 38693 Loss: 0.818513035774231\n",
      "Iteration 38694 Loss: 1.2544878721237183\n",
      "Iteration 38695 Loss: 0.9711812734603882\n",
      "Iteration 38696 Loss: 0.7008192539215088\n",
      "Iteration 38697 Loss: 0.8974395990371704\n",
      "Iteration 38698 Loss: 0.938227117061615\n",
      "Iteration 38699 Loss: 1.4130719900131226\n",
      "Iteration 38699 Loss: 1.0408574342727661\n",
      "Iteration 38700 Loss: 1.0011088848114014\n",
      "Iteration 38701 Loss: 0.7529584169387817\n",
      "Iteration 38702 Loss: 1.001291036605835\n",
      "Iteration 38703 Loss: 0.7657294869422913\n",
      "Iteration 38704 Loss: 0.7955012321472168\n",
      "Iteration 38705 Loss: 0.9831333160400391\n",
      "Iteration 38706 Loss: 1.2115672826766968\n",
      "Iteration 38707 Loss: 0.9634715914726257\n",
      "Iteration 38708 Loss: 0.9834942817687988\n",
      "Iteration 38709 Loss: 0.7109841704368591\n",
      "Iteration 38709 Loss: 0.9169238805770874\n",
      "Iteration 38710 Loss: 1.115983247756958\n",
      "Iteration 38711 Loss: 0.8292140960693359\n",
      "Iteration 38712 Loss: 1.2726489305496216\n",
      "Iteration 38713 Loss: 1.2058435678482056\n",
      "Iteration 38714 Loss: 0.7267610430717468\n",
      "Iteration 38715 Loss: 0.949873685836792\n",
      "Iteration 38716 Loss: 1.3089474439620972\n",
      "Iteration 38717 Loss: 0.7772220969200134\n",
      "Iteration 38718 Loss: 0.6655698418617249\n",
      "Iteration 38719 Loss: 0.9676227569580078\n",
      "Iteration 38719 Loss: 0.9819685816764832\n",
      "Iteration 38720 Loss: 1.2552706003189087\n",
      "Iteration 38721 Loss: 1.1987289190292358\n",
      "Iteration 38722 Loss: 1.0349514484405518\n",
      "Iteration 38723 Loss: 0.8489569425582886\n",
      "Iteration 38724 Loss: 0.9186050891876221\n",
      "Iteration 38725 Loss: 0.8099023699760437\n",
      "Iteration 38726 Loss: 0.7405659556388855\n",
      "Iteration 38727 Loss: 0.7732846140861511\n",
      "Iteration 38728 Loss: 0.9267332553863525\n",
      "Iteration 38729 Loss: 1.164870023727417\n",
      "Iteration 38729 Loss: 0.9671869277954102\n",
      "Iteration 38730 Loss: 1.0363396406173706\n",
      "Iteration 38731 Loss: 1.1741373538970947\n",
      "Iteration 38732 Loss: 1.001253604888916\n",
      "Iteration 38733 Loss: 0.8959962129592896\n",
      "Iteration 38734 Loss: 1.0939849615097046\n",
      "Iteration 38735 Loss: 0.8017022609710693\n",
      "Iteration 38736 Loss: 1.180790662765503\n",
      "Iteration 38737 Loss: 1.125635027885437\n",
      "Iteration 38738 Loss: 0.9893251061439514\n",
      "Iteration 38739 Loss: 0.9524609446525574\n",
      "Iteration 38739 Loss: 1.0251625776290894\n",
      "Iteration 38740 Loss: 0.9902009963989258\n",
      "Iteration 38741 Loss: 0.9642098546028137\n",
      "Iteration 38742 Loss: 1.4130079746246338\n",
      "Iteration 38743 Loss: 0.909731388092041\n",
      "Iteration 38744 Loss: 0.9013063311576843\n",
      "Iteration 38745 Loss: 0.8768699765205383\n",
      "Iteration 38746 Loss: 1.1635820865631104\n",
      "Iteration 38747 Loss: 0.8830426931381226\n",
      "Iteration 38748 Loss: 0.9169890880584717\n",
      "Iteration 38749 Loss: 0.9757977724075317\n",
      "Iteration 38749 Loss: 0.9994737505912781\n",
      "Iteration 38750 Loss: 0.7177001237869263\n",
      "Iteration 38751 Loss: 1.2224820852279663\n",
      "Iteration 38752 Loss: 1.0077733993530273\n",
      "Iteration 38753 Loss: 0.99811190366745\n",
      "Iteration 38754 Loss: 1.0253195762634277\n",
      "Iteration 38755 Loss: 0.9810482859611511\n",
      "Iteration 38756 Loss: 1.3321677446365356\n",
      "Iteration 38757 Loss: 0.676182746887207\n",
      "Iteration 38758 Loss: 0.9776977896690369\n",
      "Iteration 38759 Loss: 1.0002659559249878\n",
      "Iteration 38759 Loss: 0.9938749074935913\n",
      "Iteration 38760 Loss: 0.9465228319168091\n",
      "Iteration 38761 Loss: 1.210649847984314\n",
      "Iteration 38762 Loss: 1.128028154373169\n",
      "Iteration 38763 Loss: 1.1021392345428467\n",
      "Iteration 38764 Loss: 0.794505774974823\n",
      "Iteration 38765 Loss: 1.1461567878723145\n",
      "Iteration 38766 Loss: 1.087230920791626\n",
      "Iteration 38767 Loss: 1.2459957599639893\n",
      "Iteration 38768 Loss: 0.7998097538948059\n",
      "Iteration 38769 Loss: 0.7215813398361206\n",
      "Iteration 38769 Loss: 1.0182620286941528\n",
      "Iteration 38770 Loss: 1.1591628789901733\n",
      "Iteration 38771 Loss: 1.2156490087509155\n",
      "Iteration 38772 Loss: 0.7463573813438416\n",
      "Iteration 38773 Loss: 1.1896694898605347\n",
      "Iteration 38774 Loss: 1.2390395402908325\n",
      "Iteration 38775 Loss: 1.1765040159225464\n",
      "Iteration 38776 Loss: 0.7692543864250183\n",
      "Iteration 38777 Loss: 0.9217438101768494\n",
      "Iteration 38778 Loss: 0.9205796122550964\n",
      "Iteration 38779 Loss: 1.0101593732833862\n",
      "Iteration 38779 Loss: 1.0348119735717773\n",
      "Iteration 38780 Loss: 0.9086076021194458\n",
      "Iteration 38781 Loss: 0.8156938552856445\n",
      "Iteration 38782 Loss: 0.7548007965087891\n",
      "Iteration 38783 Loss: 1.1444096565246582\n",
      "Iteration 38784 Loss: 0.976100504398346\n",
      "Iteration 38785 Loss: 0.7852961421012878\n",
      "Iteration 38786 Loss: 1.107930064201355\n",
      "Iteration 38787 Loss: 1.0571644306182861\n",
      "Iteration 38788 Loss: 1.0743868350982666\n",
      "Iteration 38789 Loss: 0.8283401131629944\n",
      "Iteration 38789 Loss: 0.9452729225158691\n",
      "Iteration 38790 Loss: 0.9564722180366516\n",
      "Iteration 38791 Loss: 0.9692167639732361\n",
      "Iteration 38792 Loss: 0.9129601120948792\n",
      "Iteration 38793 Loss: 1.02934730052948\n",
      "Iteration 38794 Loss: 0.868838369846344\n",
      "Iteration 38795 Loss: 0.9947453141212463\n",
      "Iteration 38796 Loss: 0.9050685167312622\n",
      "Iteration 38797 Loss: 0.9340823292732239\n",
      "Iteration 38798 Loss: 0.884535014629364\n",
      "Iteration 38799 Loss: 0.8192207217216492\n",
      "Iteration 38799 Loss: 0.9274486303329468\n",
      "Iteration 38800 Loss: 0.8724962472915649\n",
      "Iteration 38801 Loss: 0.7610342502593994\n",
      "Iteration 38802 Loss: 0.9951696395874023\n",
      "Iteration 38803 Loss: 1.1119674444198608\n",
      "Iteration 38804 Loss: 1.3206324577331543\n",
      "Iteration 38805 Loss: 1.2587029933929443\n",
      "Iteration 38806 Loss: 0.6473240852355957\n",
      "Iteration 38807 Loss: 0.8220683336257935\n",
      "Iteration 38808 Loss: 1.1042195558547974\n",
      "Iteration 38809 Loss: 1.2156600952148438\n",
      "Iteration 38809 Loss: 1.010927438735962\n",
      "Iteration 38810 Loss: 1.1001006364822388\n",
      "Iteration 38811 Loss: 0.7146848440170288\n",
      "Iteration 38812 Loss: 0.9334359765052795\n",
      "Iteration 38813 Loss: 0.8384729027748108\n",
      "Iteration 38814 Loss: 1.1578118801116943\n",
      "Iteration 38815 Loss: 0.8764284253120422\n",
      "Iteration 38816 Loss: 0.8430383801460266\n",
      "Iteration 38817 Loss: 1.1338480710983276\n",
      "Iteration 38818 Loss: 0.9008592963218689\n",
      "Iteration 38819 Loss: 0.822488009929657\n",
      "Iteration 38819 Loss: 0.9321168661117554\n",
      "Iteration 38820 Loss: 0.8305506110191345\n",
      "Iteration 38821 Loss: 1.2752827405929565\n",
      "Iteration 38822 Loss: 1.0054901838302612\n",
      "Iteration 38823 Loss: 0.950310468673706\n",
      "Iteration 38824 Loss: 1.22451651096344\n",
      "Iteration 38825 Loss: 0.7741169333457947\n",
      "Iteration 38826 Loss: 0.791902482509613\n",
      "Iteration 38827 Loss: 0.9536600112915039\n",
      "Iteration 38828 Loss: 0.8301882147789001\n",
      "Iteration 38829 Loss: 0.9784327149391174\n",
      "Iteration 38829 Loss: 0.961445152759552\n",
      "Iteration 38830 Loss: 0.9062537550926208\n",
      "Iteration 38831 Loss: 0.767325222492218\n",
      "Iteration 38832 Loss: 1.1731432676315308\n",
      "Iteration 38833 Loss: 1.0093730688095093\n",
      "Iteration 38834 Loss: 0.9026010632514954\n",
      "Iteration 38835 Loss: 0.6890860795974731\n",
      "Iteration 38836 Loss: 0.7505596876144409\n",
      "Iteration 38837 Loss: 0.5639422535896301\n",
      "Iteration 38838 Loss: 1.0232194662094116\n",
      "Iteration 38839 Loss: 0.9133002161979675\n",
      "Iteration 38839 Loss: 0.8698803782463074\n",
      "Iteration 38840 Loss: 0.9593183398246765\n",
      "Iteration 38841 Loss: 0.9100880026817322\n",
      "Iteration 38842 Loss: 1.2889820337295532\n",
      "Iteration 38843 Loss: 1.1098418235778809\n",
      "Iteration 38844 Loss: 1.0389972925186157\n",
      "Iteration 38845 Loss: 0.7844845652580261\n",
      "Iteration 38846 Loss: 1.0174744129180908\n",
      "Iteration 38847 Loss: 0.9893472790718079\n",
      "Iteration 38848 Loss: 0.9140819311141968\n",
      "Iteration 38849 Loss: 0.9447756409645081\n",
      "Iteration 38849 Loss: 0.9957391619682312\n",
      "Iteration 38850 Loss: 0.7135816216468811\n",
      "Iteration 38851 Loss: 1.3712279796600342\n",
      "Iteration 38852 Loss: 1.3826628923416138\n",
      "Iteration 38853 Loss: 1.0137237310409546\n",
      "Iteration 38854 Loss: 1.1356096267700195\n",
      "Iteration 38855 Loss: 1.1668943166732788\n",
      "Iteration 38856 Loss: 0.9465072154998779\n",
      "Iteration 38857 Loss: 0.7497434616088867\n",
      "Iteration 38858 Loss: 0.7108882665634155\n",
      "Iteration 38859 Loss: 0.9924590587615967\n",
      "Iteration 38859 Loss: 1.0183298587799072\n",
      "Iteration 38860 Loss: 1.134737253189087\n",
      "Iteration 38861 Loss: 0.9256314039230347\n",
      "Iteration 38862 Loss: 0.863241970539093\n",
      "Iteration 38863 Loss: 0.9291240572929382\n",
      "Iteration 38864 Loss: 1.0924888849258423\n",
      "Iteration 38865 Loss: 1.1991583108901978\n",
      "Iteration 38866 Loss: 0.8562697768211365\n",
      "Iteration 38867 Loss: 0.8084803819656372\n",
      "Iteration 38868 Loss: 0.9716572165489197\n",
      "Iteration 38869 Loss: 0.8035904169082642\n",
      "Iteration 38869 Loss: 0.9584379196166992\n",
      "Iteration 38870 Loss: 0.6469014286994934\n",
      "Iteration 38871 Loss: 1.059858798980713\n",
      "Iteration 38872 Loss: 0.8535761833190918\n",
      "Iteration 38873 Loss: 0.8778526782989502\n",
      "Iteration 38874 Loss: 0.7812888026237488\n",
      "Iteration 38875 Loss: 0.8516532182693481\n",
      "Iteration 38876 Loss: 0.691875159740448\n",
      "Iteration 38877 Loss: 0.7107658982276917\n",
      "Iteration 38878 Loss: 0.6895400285720825\n",
      "Iteration 38879 Loss: 0.9704357385635376\n",
      "Iteration 38879 Loss: 0.8133746981620789\n",
      "Iteration 38880 Loss: 0.6812201738357544\n",
      "Iteration 38881 Loss: 1.18670654296875\n",
      "Iteration 38882 Loss: 0.5661352872848511\n",
      "Iteration 38883 Loss: 0.960338294506073\n",
      "Iteration 38884 Loss: 1.3876466751098633\n",
      "Iteration 38885 Loss: 0.957139253616333\n",
      "Iteration 38886 Loss: 1.162882924079895\n",
      "Iteration 38887 Loss: 0.6429305672645569\n",
      "Iteration 38888 Loss: 1.0119779109954834\n",
      "Iteration 38889 Loss: 1.1579039096832275\n",
      "Iteration 38889 Loss: 0.9714881777763367\n",
      "Iteration 38890 Loss: 1.078918218612671\n",
      "Iteration 38891 Loss: 0.8577319979667664\n",
      "Iteration 38892 Loss: 0.9385484457015991\n",
      "Iteration 38893 Loss: 0.9431859254837036\n",
      "Iteration 38894 Loss: 0.7259381413459778\n",
      "Iteration 38895 Loss: 0.8461315035820007\n",
      "Iteration 38896 Loss: 1.1063820123672485\n",
      "Iteration 38897 Loss: 0.9786108136177063\n",
      "Iteration 38898 Loss: 0.7294892072677612\n",
      "Iteration 38899 Loss: 0.7674274444580078\n",
      "Iteration 38899 Loss: 0.8972363471984863\n",
      "Iteration 38900 Loss: 1.3167445659637451\n",
      "Iteration 38901 Loss: 0.7100116014480591\n",
      "Iteration 38902 Loss: 0.7517119646072388\n",
      "Iteration 38903 Loss: 0.9303672909736633\n",
      "Iteration 38904 Loss: 0.9967225790023804\n",
      "Iteration 38905 Loss: 1.2249778509140015\n",
      "Iteration 38906 Loss: 0.6966537833213806\n",
      "Iteration 38907 Loss: 1.1454427242279053\n",
      "Iteration 38908 Loss: 0.8079313039779663\n",
      "Iteration 38909 Loss: 0.7856236100196838\n",
      "Iteration 38909 Loss: 0.9366188049316406\n",
      "Iteration 38910 Loss: 0.8178898096084595\n",
      "Iteration 38911 Loss: 0.9862971305847168\n",
      "Iteration 38912 Loss: 1.2042288780212402\n",
      "Iteration 38913 Loss: 0.9055238366127014\n",
      "Iteration 38914 Loss: 0.7573801279067993\n",
      "Iteration 38915 Loss: 1.2528188228607178\n",
      "Iteration 38916 Loss: 0.8062697649002075\n",
      "Iteration 38917 Loss: 1.2248618602752686\n",
      "Iteration 38918 Loss: 0.8684493899345398\n",
      "Iteration 38919 Loss: 1.1482781171798706\n",
      "Iteration 38919 Loss: 0.9971997141838074\n",
      "Iteration 38920 Loss: 0.9059956073760986\n",
      "Iteration 38921 Loss: 0.8315839767456055\n",
      "Iteration 38922 Loss: 0.9097763299942017\n",
      "Iteration 38923 Loss: 0.8294315338134766\n",
      "Iteration 38924 Loss: 1.1637601852416992\n",
      "Iteration 38925 Loss: 1.3066588640213013\n",
      "Iteration 38926 Loss: 1.0356159210205078\n",
      "Iteration 38927 Loss: 1.1334766149520874\n",
      "Iteration 38928 Loss: 0.959792971611023\n",
      "Iteration 38929 Loss: 1.180972933769226\n",
      "Iteration 38929 Loss: 1.0257065296173096\n",
      "Iteration 38930 Loss: 0.8986043930053711\n",
      "Iteration 38931 Loss: 1.374259114265442\n",
      "Iteration 38932 Loss: 1.0758057832717896\n",
      "Iteration 38933 Loss: 1.1437840461730957\n",
      "Iteration 38934 Loss: 1.1973867416381836\n",
      "Iteration 38935 Loss: 0.9017959237098694\n",
      "Iteration 38936 Loss: 0.8559361100196838\n",
      "Iteration 38937 Loss: 0.9363308548927307\n",
      "Iteration 38938 Loss: 0.7134571075439453\n",
      "Iteration 38939 Loss: 0.6864968538284302\n",
      "Iteration 38939 Loss: 0.9783856272697449\n",
      "Iteration 38940 Loss: 0.8106171488761902\n",
      "Iteration 38941 Loss: 0.6381683349609375\n",
      "Iteration 38942 Loss: 0.9884582161903381\n",
      "Iteration 38943 Loss: 1.0163756608963013\n",
      "Iteration 38944 Loss: 1.2151269912719727\n",
      "Iteration 38945 Loss: 0.9021458625793457\n",
      "Iteration 38946 Loss: 1.0402867794036865\n",
      "Iteration 38947 Loss: 1.1263285875320435\n",
      "Iteration 38948 Loss: 1.0569896697998047\n",
      "Iteration 38949 Loss: 0.9613229036331177\n",
      "Iteration 38949 Loss: 0.9755820035934448\n",
      "Iteration 38950 Loss: 0.7150283455848694\n",
      "Iteration 38951 Loss: 0.7225937247276306\n",
      "Iteration 38952 Loss: 0.9585342407226562\n",
      "Iteration 38953 Loss: 1.1000218391418457\n",
      "Iteration 38954 Loss: 0.7865133881568909\n",
      "Iteration 38955 Loss: 1.1659355163574219\n",
      "Iteration 38956 Loss: 0.934640109539032\n",
      "Iteration 38957 Loss: 0.7782000303268433\n",
      "Iteration 38958 Loss: 0.8398540019989014\n",
      "Iteration 38959 Loss: 1.1321593523025513\n",
      "Iteration 38959 Loss: 0.9133480787277222\n",
      "Iteration 38960 Loss: 1.0672039985656738\n",
      "Iteration 38961 Loss: 1.0514554977416992\n",
      "Iteration 38962 Loss: 0.8606318831443787\n",
      "Iteration 38963 Loss: 1.1050715446472168\n",
      "Iteration 38964 Loss: 1.3185663223266602\n",
      "Iteration 38965 Loss: 1.1610194444656372\n",
      "Iteration 38966 Loss: 0.8447477221488953\n",
      "Iteration 38967 Loss: 0.941352903842926\n",
      "Iteration 38968 Loss: 1.1055119037628174\n",
      "Iteration 38969 Loss: 1.1281139850616455\n",
      "Iteration 38969 Loss: 1.0583674907684326\n",
      "Iteration 38970 Loss: 0.7326088547706604\n",
      "Iteration 38971 Loss: 0.7870527505874634\n",
      "Iteration 38972 Loss: 1.0098012685775757\n",
      "Iteration 38973 Loss: 1.1927175521850586\n",
      "Iteration 38974 Loss: 1.2922494411468506\n",
      "Iteration 38975 Loss: 1.0614120960235596\n",
      "Iteration 38976 Loss: 0.8965940475463867\n",
      "Iteration 38977 Loss: 0.7620911598205566\n",
      "Iteration 38978 Loss: 1.1687580347061157\n",
      "Iteration 38979 Loss: 0.9047973155975342\n",
      "Iteration 38979 Loss: 0.9808082580566406\n",
      "Iteration 38980 Loss: 1.1371887922286987\n",
      "Iteration 38981 Loss: 0.9768339395523071\n",
      "Iteration 38982 Loss: 1.0415951013565063\n",
      "Iteration 38983 Loss: 1.089118242263794\n",
      "Iteration 38984 Loss: 1.0644915103912354\n",
      "Iteration 38985 Loss: 0.8967713713645935\n",
      "Iteration 38986 Loss: 1.0666660070419312\n",
      "Iteration 38987 Loss: 0.9957910776138306\n",
      "Iteration 38988 Loss: 0.9947327971458435\n",
      "Iteration 38989 Loss: 1.4731309413909912\n",
      "Iteration 38989 Loss: 1.073632001876831\n",
      "Iteration 38990 Loss: 0.6318645477294922\n",
      "Iteration 38991 Loss: 0.9740681052207947\n",
      "Iteration 38992 Loss: 0.894335925579071\n",
      "Iteration 38993 Loss: 0.8945777416229248\n",
      "Iteration 38994 Loss: 0.7884132862091064\n",
      "Iteration 38995 Loss: 0.8229315876960754\n",
      "Iteration 38996 Loss: 0.7337614297866821\n",
      "Iteration 38997 Loss: 0.7742140293121338\n",
      "Iteration 38998 Loss: 0.8946557641029358\n",
      "Iteration 38999 Loss: 0.9793810248374939\n",
      "Iteration 38999 Loss: 0.8388202786445618\n",
      "Iteration 39000 Loss: 1.181246042251587\n",
      "Iteration 39001 Loss: 1.0828105211257935\n",
      "Iteration 39002 Loss: 1.058671236038208\n",
      "Iteration 39003 Loss: 1.021854281425476\n",
      "Iteration 39004 Loss: 0.7841254472732544\n",
      "Iteration 39005 Loss: 1.0632563829421997\n",
      "Iteration 39006 Loss: 1.0737287998199463\n",
      "Iteration 39007 Loss: 1.3301578760147095\n",
      "Iteration 39008 Loss: 1.2682832479476929\n",
      "Iteration 39009 Loss: 0.8889882564544678\n",
      "Iteration 39009 Loss: 1.0753122568130493\n",
      "Iteration 39010 Loss: 0.901607096195221\n",
      "Iteration 39011 Loss: 0.9331506490707397\n",
      "Iteration 39012 Loss: 1.0095738172531128\n",
      "Iteration 39013 Loss: 0.8803137540817261\n",
      "Iteration 39014 Loss: 0.7463191151618958\n",
      "Iteration 39015 Loss: 0.7491537928581238\n",
      "Iteration 39016 Loss: 0.8758149147033691\n",
      "Iteration 39017 Loss: 1.0882879495620728\n",
      "Iteration 39018 Loss: 1.125011920928955\n",
      "Iteration 39019 Loss: 0.9719840884208679\n",
      "Iteration 39019 Loss: 0.9281217455863953\n",
      "Iteration 39020 Loss: 1.313340425491333\n",
      "Iteration 39021 Loss: 1.1194708347320557\n",
      "Iteration 39022 Loss: 0.9080485105514526\n",
      "Iteration 39023 Loss: 1.3719253540039062\n",
      "Iteration 39024 Loss: 0.9894724488258362\n",
      "Iteration 39025 Loss: 0.9173857569694519\n",
      "Iteration 39026 Loss: 1.1012451648712158\n",
      "Iteration 39027 Loss: 1.2716585397720337\n",
      "Iteration 39028 Loss: 1.2429099082946777\n",
      "Iteration 39029 Loss: 0.8551408648490906\n",
      "Iteration 39029 Loss: 1.1090598106384277\n",
      "Iteration 39030 Loss: 1.1763323545455933\n",
      "Iteration 39031 Loss: 0.7795296311378479\n",
      "Iteration 39032 Loss: 0.825256884098053\n",
      "Iteration 39033 Loss: 0.9317337274551392\n",
      "Iteration 39034 Loss: 1.1549034118652344\n",
      "Iteration 39035 Loss: 0.688606858253479\n",
      "Iteration 39036 Loss: 1.234632968902588\n",
      "Iteration 39037 Loss: 0.9102016091346741\n",
      "Iteration 39038 Loss: 0.9220545291900635\n",
      "Iteration 39039 Loss: 1.1059139966964722\n",
      "Iteration 39039 Loss: 0.9729166030883789\n",
      "Iteration 39040 Loss: 0.5520206689834595\n",
      "Iteration 39041 Loss: 1.328070878982544\n",
      "Iteration 39042 Loss: 0.7682351469993591\n",
      "Iteration 39043 Loss: 1.1476887464523315\n",
      "Iteration 39044 Loss: 0.9730889201164246\n",
      "Iteration 39045 Loss: 0.9517109990119934\n",
      "Iteration 39046 Loss: 1.0653142929077148\n",
      "Iteration 39047 Loss: 1.2249826192855835\n",
      "Iteration 39048 Loss: 1.0783438682556152\n",
      "Iteration 39049 Loss: 0.9301494359970093\n",
      "Iteration 39049 Loss: 1.0019605159759521\n",
      "Iteration 39050 Loss: 0.890399158000946\n",
      "Iteration 39051 Loss: 1.0017110109329224\n",
      "Iteration 39052 Loss: 0.9486475586891174\n",
      "Iteration 39053 Loss: 0.8163148760795593\n",
      "Iteration 39054 Loss: 0.8696349263191223\n",
      "Iteration 39055 Loss: 1.0626194477081299\n",
      "Iteration 39056 Loss: 0.7968438267707825\n",
      "Iteration 39057 Loss: 0.7488085031509399\n",
      "Iteration 39058 Loss: 0.819370687007904\n",
      "Iteration 39059 Loss: 1.0144264698028564\n",
      "Iteration 39059 Loss: 0.896877646446228\n",
      "Iteration 39060 Loss: 1.1541110277175903\n",
      "Iteration 39061 Loss: 0.9431048035621643\n",
      "Iteration 39062 Loss: 1.07778799533844\n",
      "Iteration 39063 Loss: 0.785887598991394\n",
      "Iteration 39064 Loss: 0.9058825969696045\n",
      "Iteration 39065 Loss: 0.9721759557723999\n",
      "Iteration 39066 Loss: 0.9511698484420776\n",
      "Iteration 39067 Loss: 0.5910572409629822\n",
      "Iteration 39068 Loss: 1.2403028011322021\n",
      "Iteration 39069 Loss: 0.8793390989303589\n",
      "Iteration 39069 Loss: 0.9500818252563477\n",
      "Iteration 39070 Loss: 1.3133243322372437\n",
      "Iteration 39071 Loss: 1.0788477659225464\n",
      "Iteration 39072 Loss: 1.0819952487945557\n",
      "Iteration 39073 Loss: 1.1314631700515747\n",
      "Iteration 39074 Loss: 1.0304490327835083\n",
      "Iteration 39075 Loss: 1.3539230823516846\n",
      "Iteration 39076 Loss: 1.0131924152374268\n",
      "Iteration 39077 Loss: 0.8843119740486145\n",
      "Iteration 39078 Loss: 1.3181164264678955\n",
      "Iteration 39079 Loss: 1.0645978450775146\n",
      "Iteration 39079 Loss: 1.1270220279693604\n",
      "Iteration 39080 Loss: 0.9475818276405334\n",
      "Iteration 39081 Loss: 0.9300315976142883\n",
      "Iteration 39082 Loss: 0.9722619652748108\n",
      "Iteration 39083 Loss: 1.3119521141052246\n",
      "Iteration 39084 Loss: 0.9710609316825867\n",
      "Iteration 39085 Loss: 0.8236911296844482\n",
      "Iteration 39086 Loss: 0.9238636493682861\n",
      "Iteration 39087 Loss: 1.2188585996627808\n",
      "Iteration 39088 Loss: 0.9395669102668762\n",
      "Iteration 39089 Loss: 1.0491420030593872\n",
      "Iteration 39089 Loss: 1.0088011026382446\n",
      "Iteration 39090 Loss: 1.0715365409851074\n",
      "Iteration 39091 Loss: 0.9415289163589478\n",
      "Iteration 39092 Loss: 0.6599730849266052\n",
      "Iteration 39093 Loss: 0.7928804159164429\n",
      "Iteration 39094 Loss: 1.0288993120193481\n",
      "Iteration 39095 Loss: 0.8032721877098083\n",
      "Iteration 39096 Loss: 0.7629013657569885\n",
      "Iteration 39097 Loss: 0.9428622722625732\n",
      "Iteration 39098 Loss: 1.2125462293624878\n",
      "Iteration 39099 Loss: 1.1054043769836426\n",
      "Iteration 39099 Loss: 0.9321805238723755\n",
      "Iteration 39100 Loss: 1.0132901668548584\n",
      "Iteration 39101 Loss: 1.2038638591766357\n",
      "Iteration 39102 Loss: 0.595637321472168\n",
      "Iteration 39103 Loss: 1.3872445821762085\n",
      "Iteration 39104 Loss: 0.7713021039962769\n",
      "Iteration 39105 Loss: 1.0460411310195923\n",
      "Iteration 39106 Loss: 0.9642071723937988\n",
      "Iteration 39107 Loss: 0.9426558017730713\n",
      "Iteration 39108 Loss: 0.8112891316413879\n",
      "Iteration 39109 Loss: 1.0754231214523315\n",
      "Iteration 39109 Loss: 0.9810954928398132\n",
      "Iteration 39110 Loss: 0.9435709714889526\n",
      "Iteration 39111 Loss: 1.0710606575012207\n",
      "Iteration 39112 Loss: 1.0022145509719849\n",
      "Iteration 39113 Loss: 0.9269468784332275\n",
      "Iteration 39114 Loss: 0.973361611366272\n",
      "Iteration 39115 Loss: 0.8557021617889404\n",
      "Iteration 39116 Loss: 1.3870582580566406\n",
      "Iteration 39117 Loss: 1.1518288850784302\n",
      "Iteration 39118 Loss: 1.2830430269241333\n",
      "Iteration 39119 Loss: 0.5723741054534912\n",
      "Iteration 39119 Loss: 1.0167161226272583\n",
      "Iteration 39120 Loss: 0.8251356482505798\n",
      "Iteration 39121 Loss: 0.988239049911499\n",
      "Iteration 39122 Loss: 1.012848973274231\n",
      "Iteration 39123 Loss: 1.0095285177230835\n",
      "Iteration 39124 Loss: 0.7860380411148071\n",
      "Iteration 39125 Loss: 0.9238899946212769\n",
      "Iteration 39126 Loss: 1.141603946685791\n",
      "Iteration 39127 Loss: 0.6286443471908569\n",
      "Iteration 39128 Loss: 0.76817786693573\n",
      "Iteration 39129 Loss: 1.0216816663742065\n",
      "Iteration 39129 Loss: 0.9105788469314575\n",
      "Iteration 39130 Loss: 1.0653705596923828\n",
      "Iteration 39131 Loss: 0.8878109455108643\n",
      "Iteration 39132 Loss: 1.0179115533828735\n",
      "Iteration 39133 Loss: 1.2454183101654053\n",
      "Iteration 39134 Loss: 1.183477520942688\n",
      "Iteration 39135 Loss: 0.963691771030426\n",
      "Iteration 39136 Loss: 1.3290424346923828\n",
      "Iteration 39137 Loss: 1.0704820156097412\n",
      "Iteration 39138 Loss: 0.8947244882583618\n",
      "Iteration 39139 Loss: 0.945971667766571\n",
      "Iteration 39139 Loss: 1.0603901147842407\n",
      "Iteration 39140 Loss: 1.1803820133209229\n",
      "Iteration 39141 Loss: 0.8976036906242371\n",
      "Iteration 39142 Loss: 0.8325760960578918\n",
      "Iteration 39143 Loss: 0.8365072011947632\n",
      "Iteration 39144 Loss: 1.1446244716644287\n",
      "Iteration 39145 Loss: 0.9096484780311584\n",
      "Iteration 39146 Loss: 1.288791298866272\n",
      "Iteration 39147 Loss: 0.9096950888633728\n",
      "Iteration 39148 Loss: 1.015358328819275\n",
      "Iteration 39149 Loss: 1.12388014793396\n",
      "Iteration 39149 Loss: 1.013906717300415\n",
      "Iteration 39150 Loss: 1.030552864074707\n",
      "Iteration 39151 Loss: 0.6613194346427917\n",
      "Iteration 39152 Loss: 0.9869853258132935\n",
      "Iteration 39153 Loss: 0.9159711003303528\n",
      "Iteration 39154 Loss: 1.1242598295211792\n",
      "Iteration 39155 Loss: 1.2759517431259155\n",
      "Iteration 39156 Loss: 1.1174068450927734\n",
      "Iteration 39157 Loss: 0.9523043632507324\n",
      "Iteration 39158 Loss: 1.0168901681900024\n",
      "Iteration 39159 Loss: 1.0908621549606323\n",
      "Iteration 39159 Loss: 1.017250418663025\n",
      "Iteration 39160 Loss: 1.258015751838684\n",
      "Iteration 39161 Loss: 0.8102912306785583\n",
      "Iteration 39162 Loss: 0.8842898011207581\n",
      "Iteration 39163 Loss: 1.1143440008163452\n",
      "Iteration 39164 Loss: 0.7096396684646606\n",
      "Iteration 39165 Loss: 0.988889217376709\n",
      "Iteration 39166 Loss: 0.878238320350647\n",
      "Iteration 39167 Loss: 1.027998447418213\n",
      "Iteration 39168 Loss: 0.6827179193496704\n",
      "Iteration 39169 Loss: 1.0417479276657104\n",
      "Iteration 39169 Loss: 0.9396171569824219\n",
      "Iteration 39170 Loss: 1.0300934314727783\n",
      "Iteration 39171 Loss: 0.9390684962272644\n",
      "Iteration 39172 Loss: 0.797599196434021\n",
      "Iteration 39173 Loss: 0.6652989983558655\n",
      "Iteration 39174 Loss: 0.8655378222465515\n",
      "Iteration 39175 Loss: 0.7359691858291626\n",
      "Iteration 39176 Loss: 1.064956784248352\n",
      "Iteration 39177 Loss: 0.6207703948020935\n",
      "Iteration 39178 Loss: 0.9382108449935913\n",
      "Iteration 39179 Loss: 1.0089623928070068\n",
      "Iteration 39179 Loss: 0.8666467666625977\n",
      "Iteration 39180 Loss: 1.3201240301132202\n",
      "Iteration 39181 Loss: 0.8246915936470032\n",
      "Iteration 39182 Loss: 0.842070460319519\n",
      "Iteration 39183 Loss: 1.0641635656356812\n",
      "Iteration 39184 Loss: 1.0718655586242676\n",
      "Iteration 39185 Loss: 1.3419404029846191\n",
      "Iteration 39186 Loss: 0.7347109317779541\n",
      "Iteration 39187 Loss: 1.1132198572158813\n",
      "Iteration 39188 Loss: 1.4455163478851318\n",
      "Iteration 39189 Loss: 0.903282642364502\n",
      "Iteration 39189 Loss: 1.0661585330963135\n",
      "Iteration 39190 Loss: 1.0511488914489746\n",
      "Iteration 39191 Loss: 1.233863115310669\n",
      "Iteration 39192 Loss: 1.1642438173294067\n",
      "Iteration 39193 Loss: 0.9589457511901855\n",
      "Iteration 39194 Loss: 1.0998483896255493\n",
      "Iteration 39195 Loss: 0.7723938226699829\n",
      "Iteration 39196 Loss: 1.2123148441314697\n",
      "Iteration 39197 Loss: 0.9602049589157104\n",
      "Iteration 39198 Loss: 1.012418270111084\n",
      "Iteration 39199 Loss: 1.091429591178894\n",
      "Iteration 39199 Loss: 1.0556811094284058\n",
      "Iteration 39200 Loss: 0.894709050655365\n",
      "Iteration 39201 Loss: 0.6200631856918335\n",
      "Iteration 39202 Loss: 0.9462441802024841\n",
      "Iteration 39203 Loss: 0.8281720876693726\n",
      "Iteration 39204 Loss: 0.8497294187545776\n",
      "Iteration 39205 Loss: 0.9712265133857727\n",
      "Iteration 39206 Loss: 0.9420515298843384\n",
      "Iteration 39207 Loss: 1.206105351448059\n",
      "Iteration 39208 Loss: 0.9336176514625549\n",
      "Iteration 39209 Loss: 1.0221911668777466\n",
      "Iteration 39209 Loss: 0.9214110374450684\n",
      "Iteration 39210 Loss: 0.834945559501648\n",
      "Iteration 39211 Loss: 0.8543364405632019\n",
      "Iteration 39212 Loss: 1.5091001987457275\n",
      "Iteration 39213 Loss: 1.2202767133712769\n",
      "Iteration 39214 Loss: 0.9675462245941162\n",
      "Iteration 39215 Loss: 0.995394229888916\n",
      "Iteration 39216 Loss: 1.317510962486267\n",
      "Iteration 39217 Loss: 1.0630440711975098\n",
      "Iteration 39218 Loss: 0.783841073513031\n",
      "Iteration 39219 Loss: 1.2471632957458496\n",
      "Iteration 39219 Loss: 1.0793157815933228\n",
      "Iteration 39220 Loss: 1.178905725479126\n",
      "Iteration 39221 Loss: 1.2210304737091064\n",
      "Iteration 39222 Loss: 0.8636646866798401\n",
      "Iteration 39223 Loss: 0.8521531820297241\n",
      "Iteration 39224 Loss: 0.9134800434112549\n",
      "Iteration 39225 Loss: 0.796783447265625\n",
      "Iteration 39226 Loss: 0.8000645637512207\n",
      "Iteration 39227 Loss: 1.097541332244873\n",
      "Iteration 39228 Loss: 1.1154828071594238\n",
      "Iteration 39229 Loss: 0.9776504635810852\n",
      "Iteration 39229 Loss: 0.9816757440567017\n",
      "Iteration 39230 Loss: 1.063344955444336\n",
      "Iteration 39231 Loss: 0.9927282929420471\n",
      "Iteration 39232 Loss: 0.8237493634223938\n",
      "Iteration 39233 Loss: 0.9719161987304688\n",
      "Iteration 39234 Loss: 1.1616448163986206\n",
      "Iteration 39235 Loss: 0.9119577407836914\n",
      "Iteration 39236 Loss: 1.081225872039795\n",
      "Iteration 39237 Loss: 1.1431899070739746\n",
      "Iteration 39238 Loss: 0.9111301898956299\n",
      "Iteration 39239 Loss: 1.0031448602676392\n",
      "Iteration 39239 Loss: 1.0064033269882202\n",
      "Iteration 39240 Loss: 1.0967143774032593\n",
      "Iteration 39241 Loss: 0.9483523368835449\n",
      "Iteration 39242 Loss: 1.1605669260025024\n",
      "Iteration 39243 Loss: 0.7537899017333984\n",
      "Iteration 39244 Loss: 1.233252763748169\n",
      "Iteration 39245 Loss: 1.1342524290084839\n",
      "Iteration 39246 Loss: 1.0984656810760498\n",
      "Iteration 39247 Loss: 1.1411890983581543\n",
      "Iteration 39248 Loss: 1.0021060705184937\n",
      "Iteration 39249 Loss: 0.9458569288253784\n",
      "Iteration 39249 Loss: 1.051454782485962\n",
      "Iteration 39250 Loss: 1.0247758626937866\n",
      "Iteration 39251 Loss: 0.9645952582359314\n",
      "Iteration 39252 Loss: 0.9670389890670776\n",
      "Iteration 39253 Loss: 1.3512647151947021\n",
      "Iteration 39254 Loss: 1.2809404134750366\n",
      "Iteration 39255 Loss: 1.0074679851531982\n",
      "Iteration 39256 Loss: 1.0855622291564941\n",
      "Iteration 39257 Loss: 1.295236587524414\n",
      "Iteration 39258 Loss: 1.221882700920105\n",
      "Iteration 39259 Loss: 0.9720368385314941\n",
      "Iteration 39259 Loss: 1.1170800924301147\n",
      "Iteration 39260 Loss: 0.6719483733177185\n",
      "Iteration 39261 Loss: 1.0082846879959106\n",
      "Iteration 39262 Loss: 0.8237730860710144\n",
      "Iteration 39263 Loss: 0.9433383345603943\n",
      "Iteration 39264 Loss: 0.6367253065109253\n",
      "Iteration 39265 Loss: 1.085371732711792\n",
      "Iteration 39266 Loss: 0.9311100840568542\n",
      "Iteration 39267 Loss: 0.8054800033569336\n",
      "Iteration 39268 Loss: 0.8777652382850647\n",
      "Iteration 39269 Loss: 1.1815028190612793\n",
      "Iteration 39269 Loss: 0.8965299725532532\n",
      "Iteration 39270 Loss: 0.8927759528160095\n",
      "Iteration 39271 Loss: 1.0576658248901367\n",
      "Iteration 39272 Loss: 1.1300530433654785\n",
      "Iteration 39273 Loss: 0.8942635655403137\n",
      "Iteration 39274 Loss: 0.8140336871147156\n",
      "Iteration 39275 Loss: 0.897175669670105\n",
      "Iteration 39276 Loss: 1.1282660961151123\n",
      "Iteration 39277 Loss: 0.6818748712539673\n",
      "Iteration 39278 Loss: 1.0859276056289673\n",
      "Iteration 39279 Loss: 1.141428828239441\n",
      "Iteration 39279 Loss: 0.9723466038703918\n",
      "Iteration 39280 Loss: 0.9917402863502502\n",
      "Iteration 39281 Loss: 1.2495546340942383\n",
      "Iteration 39282 Loss: 0.9957410097122192\n",
      "Iteration 39283 Loss: 1.1774678230285645\n",
      "Iteration 39284 Loss: 1.1890722513198853\n",
      "Iteration 39285 Loss: 0.9316049814224243\n",
      "Iteration 39286 Loss: 1.4694188833236694\n",
      "Iteration 39287 Loss: 1.1431025266647339\n",
      "Iteration 39288 Loss: 1.1404653787612915\n",
      "Iteration 39289 Loss: 1.0563926696777344\n",
      "Iteration 39289 Loss: 1.134455919265747\n",
      "Iteration 39290 Loss: 1.171913743019104\n",
      "Iteration 39291 Loss: 1.195723056793213\n",
      "Iteration 39292 Loss: 1.1366780996322632\n",
      "Iteration 39293 Loss: 0.993690550327301\n",
      "Iteration 39294 Loss: 0.8002116680145264\n",
      "Iteration 39295 Loss: 1.143950343132019\n",
      "Iteration 39296 Loss: 1.142359733581543\n",
      "Iteration 39297 Loss: 1.1779948472976685\n",
      "Iteration 39298 Loss: 1.00459623336792\n",
      "Iteration 39299 Loss: 1.0496872663497925\n",
      "Iteration 39299 Loss: 1.0816805362701416\n",
      "Iteration 39300 Loss: 0.962201714515686\n",
      "Iteration 39301 Loss: 0.857758104801178\n",
      "Iteration 39302 Loss: 1.0857346057891846\n",
      "Iteration 39303 Loss: 1.1825512647628784\n",
      "Iteration 39304 Loss: 1.1417860984802246\n",
      "Iteration 39305 Loss: 1.1217467784881592\n",
      "Iteration 39306 Loss: 1.0447980165481567\n",
      "Iteration 39307 Loss: 0.6721948981285095\n",
      "Iteration 39308 Loss: 0.621538519859314\n",
      "Iteration 39309 Loss: 0.9087613224983215\n",
      "Iteration 39309 Loss: 0.959907054901123\n",
      "Iteration 39310 Loss: 0.6886180639266968\n",
      "Iteration 39311 Loss: 0.9544566869735718\n",
      "Iteration 39312 Loss: 1.07802414894104\n",
      "Iteration 39313 Loss: 1.1012167930603027\n",
      "Iteration 39314 Loss: 1.2872651815414429\n",
      "Iteration 39315 Loss: 0.9818387031555176\n",
      "Iteration 39316 Loss: 0.9416126608848572\n",
      "Iteration 39317 Loss: 1.2438503503799438\n",
      "Iteration 39318 Loss: 0.8729959726333618\n",
      "Iteration 39319 Loss: 1.2137376070022583\n",
      "Iteration 39319 Loss: 1.0363616943359375\n",
      "Iteration 39320 Loss: 1.1157327890396118\n",
      "Iteration 39321 Loss: 0.7826682329177856\n",
      "Iteration 39322 Loss: 1.0232621431350708\n",
      "Iteration 39323 Loss: 1.0113919973373413\n",
      "Iteration 39324 Loss: 1.0499303340911865\n",
      "Iteration 39325 Loss: 1.1958191394805908\n",
      "Iteration 39326 Loss: 1.1126853227615356\n",
      "Iteration 39327 Loss: 0.6961517333984375\n",
      "Iteration 39328 Loss: 1.354566216468811\n",
      "Iteration 39329 Loss: 0.8635891079902649\n",
      "Iteration 39329 Loss: 1.0205795764923096\n",
      "Iteration 39330 Loss: 0.7239773273468018\n",
      "Iteration 39331 Loss: 0.842302680015564\n",
      "Iteration 39332 Loss: 0.9501211643218994\n",
      "Iteration 39333 Loss: 0.88485187292099\n",
      "Iteration 39334 Loss: 0.6024258136749268\n",
      "Iteration 39335 Loss: 0.7860430479049683\n",
      "Iteration 39336 Loss: 1.213314175605774\n",
      "Iteration 39337 Loss: 1.2737715244293213\n",
      "Iteration 39338 Loss: 1.0657285451889038\n",
      "Iteration 39339 Loss: 0.7704747915267944\n",
      "Iteration 39339 Loss: 0.9113011360168457\n",
      "Iteration 39340 Loss: 0.9024561047554016\n",
      "Iteration 39341 Loss: 1.0362789630889893\n",
      "Iteration 39342 Loss: 0.977110743522644\n",
      "Iteration 39343 Loss: 0.7199178338050842\n",
      "Iteration 39344 Loss: 1.1113795042037964\n",
      "Iteration 39345 Loss: 0.7421684861183167\n",
      "Iteration 39346 Loss: 0.9551651477813721\n",
      "Iteration 39347 Loss: 0.7614709138870239\n",
      "Iteration 39348 Loss: 0.7240268588066101\n",
      "Iteration 39349 Loss: 0.7561220526695251\n",
      "Iteration 39349 Loss: 0.868609607219696\n",
      "Iteration 39350 Loss: 1.0298081636428833\n",
      "Iteration 39351 Loss: 1.0261393785476685\n",
      "Iteration 39352 Loss: 0.9537703394889832\n",
      "Iteration 39353 Loss: 1.3377957344055176\n",
      "Iteration 39354 Loss: 0.8935680389404297\n",
      "Iteration 39355 Loss: 0.8895422220230103\n",
      "Iteration 39356 Loss: 1.327923059463501\n",
      "Iteration 39357 Loss: 0.8174964785575867\n",
      "Iteration 39358 Loss: 0.7414394617080688\n",
      "Iteration 39359 Loss: 0.6401970386505127\n",
      "Iteration 39359 Loss: 0.9657679796218872\n",
      "Iteration 39360 Loss: 1.0769392251968384\n",
      "Iteration 39361 Loss: 0.8903878331184387\n",
      "Iteration 39362 Loss: 0.7984796166419983\n",
      "Iteration 39363 Loss: 0.8077199459075928\n",
      "Iteration 39364 Loss: 1.196502923965454\n",
      "Iteration 39365 Loss: 0.8973767161369324\n",
      "Iteration 39366 Loss: 1.090093970298767\n",
      "Iteration 39367 Loss: 1.204270362854004\n",
      "Iteration 39368 Loss: 1.1174625158309937\n",
      "Iteration 39369 Loss: 0.952773928642273\n",
      "Iteration 39369 Loss: 1.0032007694244385\n",
      "Iteration 39370 Loss: 1.187995195388794\n",
      "Iteration 39371 Loss: 1.1318341493606567\n",
      "Iteration 39372 Loss: 1.009401798248291\n",
      "Iteration 39373 Loss: 0.8464427590370178\n",
      "Iteration 39374 Loss: 1.038309097290039\n",
      "Iteration 39375 Loss: 1.0551000833511353\n",
      "Iteration 39376 Loss: 1.1259613037109375\n",
      "Iteration 39377 Loss: 0.8968791365623474\n",
      "Iteration 39378 Loss: 1.4327131509780884\n",
      "Iteration 39379 Loss: 1.0300405025482178\n",
      "Iteration 39379 Loss: 1.075467824935913\n",
      "Iteration 39380 Loss: 0.5824904441833496\n",
      "Iteration 39381 Loss: 1.0356597900390625\n",
      "Iteration 39382 Loss: 0.9485662579536438\n",
      "Iteration 39383 Loss: 0.7764326930046082\n",
      "Iteration 39384 Loss: 0.8359462022781372\n",
      "Iteration 39385 Loss: 1.3362895250320435\n",
      "Iteration 39386 Loss: 1.28281569480896\n",
      "Iteration 39387 Loss: 1.2273163795471191\n",
      "Iteration 39388 Loss: 0.663763701915741\n",
      "Iteration 39389 Loss: 1.0145514011383057\n",
      "Iteration 39389 Loss: 0.9703832864761353\n",
      "Iteration 39390 Loss: 0.7632158398628235\n",
      "Iteration 39391 Loss: 0.8455337882041931\n",
      "Iteration 39392 Loss: 0.9093693494796753\n",
      "Iteration 39393 Loss: 0.9697726964950562\n",
      "Iteration 39394 Loss: 0.7631423473358154\n",
      "Iteration 39395 Loss: 0.6279712319374084\n",
      "Iteration 39396 Loss: 1.1606305837631226\n",
      "Iteration 39397 Loss: 0.7369961142539978\n",
      "Iteration 39398 Loss: 0.9225015044212341\n",
      "Iteration 39399 Loss: 1.1383265256881714\n",
      "Iteration 39399 Loss: 0.8837459683418274\n",
      "Iteration 39400 Loss: 0.6335537433624268\n",
      "Iteration 39401 Loss: 1.0562573671340942\n",
      "Iteration 39402 Loss: 1.0944730043411255\n",
      "Iteration 39403 Loss: 0.9222287535667419\n",
      "Iteration 39404 Loss: 0.8970947265625\n",
      "Iteration 39405 Loss: 0.7449343204498291\n",
      "Iteration 39406 Loss: 0.8623905181884766\n",
      "Iteration 39407 Loss: 0.6511175036430359\n",
      "Iteration 39408 Loss: 0.9650477766990662\n",
      "Iteration 39409 Loss: 0.9250923991203308\n",
      "Iteration 39409 Loss: 0.8752189874649048\n",
      "Iteration 39410 Loss: 0.891889750957489\n",
      "Iteration 39411 Loss: 0.7329685688018799\n",
      "Iteration 39412 Loss: 0.9846286773681641\n",
      "Iteration 39413 Loss: 0.9336034655570984\n",
      "Iteration 39414 Loss: 0.881151556968689\n",
      "Iteration 39415 Loss: 0.9497032761573792\n",
      "Iteration 39416 Loss: 0.5971710681915283\n",
      "Iteration 39417 Loss: 0.9707961082458496\n",
      "Iteration 39418 Loss: 1.108935832977295\n",
      "Iteration 39419 Loss: 0.7206295728683472\n",
      "Iteration 39419 Loss: 0.8771478533744812\n",
      "Iteration 39420 Loss: 0.8771454095840454\n",
      "Iteration 39421 Loss: 0.9159408211708069\n",
      "Iteration 39422 Loss: 0.831039547920227\n",
      "Iteration 39423 Loss: 1.0091978311538696\n",
      "Iteration 39424 Loss: 0.9043451547622681\n",
      "Iteration 39425 Loss: 0.7667502164840698\n",
      "Iteration 39426 Loss: 0.8570181131362915\n",
      "Iteration 39427 Loss: 1.0475890636444092\n",
      "Iteration 39428 Loss: 1.0596184730529785\n",
      "Iteration 39429 Loss: 1.202020287513733\n",
      "Iteration 39429 Loss: 0.9470664858818054\n",
      "Iteration 39430 Loss: 0.8691036105155945\n",
      "Iteration 39431 Loss: 0.8870181441307068\n",
      "Iteration 39432 Loss: 0.7471514940261841\n",
      "Iteration 39433 Loss: 0.929082453250885\n",
      "Iteration 39434 Loss: 1.2630096673965454\n",
      "Iteration 39435 Loss: 0.902671217918396\n",
      "Iteration 39436 Loss: 1.162537932395935\n",
      "Iteration 39437 Loss: 0.9298862218856812\n",
      "Iteration 39438 Loss: 1.1290233135223389\n",
      "Iteration 39439 Loss: 1.1600866317749023\n",
      "Iteration 39439 Loss: 0.9979570508003235\n",
      "Iteration 39440 Loss: 1.1472153663635254\n",
      "Iteration 39441 Loss: 0.9035364389419556\n",
      "Iteration 39442 Loss: 0.5950906276702881\n",
      "Iteration 39443 Loss: 1.0430020093917847\n",
      "Iteration 39444 Loss: 0.6762396097183228\n",
      "Iteration 39445 Loss: 0.9708762764930725\n",
      "Iteration 39446 Loss: 0.9983683824539185\n",
      "Iteration 39447 Loss: 1.075911045074463\n",
      "Iteration 39448 Loss: 0.9459148645401001\n",
      "Iteration 39449 Loss: 1.134351372718811\n",
      "Iteration 39449 Loss: 0.9490505456924438\n",
      "Iteration 39450 Loss: 1.1361130475997925\n",
      "Iteration 39451 Loss: 1.1841702461242676\n",
      "Iteration 39452 Loss: 1.212511420249939\n",
      "Iteration 39453 Loss: 0.7836033701896667\n",
      "Iteration 39454 Loss: 1.0571507215499878\n",
      "Iteration 39455 Loss: 1.0476948022842407\n",
      "Iteration 39456 Loss: 1.270319938659668\n",
      "Iteration 39457 Loss: 0.9039846062660217\n",
      "Iteration 39458 Loss: 0.8554431796073914\n",
      "Iteration 39459 Loss: 0.9923075437545776\n",
      "Iteration 39459 Loss: 1.0443298816680908\n",
      "Iteration 39460 Loss: 0.9094918370246887\n",
      "Iteration 39461 Loss: 0.7679643630981445\n",
      "Iteration 39462 Loss: 1.199927806854248\n",
      "Iteration 39463 Loss: 1.1894433498382568\n",
      "Iteration 39464 Loss: 0.8075727820396423\n",
      "Iteration 39465 Loss: 0.9955047965049744\n",
      "Iteration 39466 Loss: 0.6414605379104614\n",
      "Iteration 39467 Loss: 0.9436085820198059\n",
      "Iteration 39468 Loss: 0.6584234237670898\n",
      "Iteration 39469 Loss: 0.8450681567192078\n",
      "Iteration 39469 Loss: 0.8958465456962585\n",
      "Iteration 39470 Loss: 0.8544503450393677\n",
      "Iteration 39471 Loss: 1.1407166719436646\n",
      "Iteration 39472 Loss: 1.2672669887542725\n",
      "Iteration 39473 Loss: 0.8977786898612976\n",
      "Iteration 39474 Loss: 0.64386385679245\n",
      "Iteration 39475 Loss: 0.8593807220458984\n",
      "Iteration 39476 Loss: 1.132080316543579\n",
      "Iteration 39477 Loss: 0.8855343461036682\n",
      "Iteration 39478 Loss: 0.9695382714271545\n",
      "Iteration 39479 Loss: 0.9669312238693237\n",
      "Iteration 39479 Loss: 0.9617540240287781\n",
      "Iteration 39480 Loss: 0.9250922203063965\n",
      "Iteration 39481 Loss: 0.9255896210670471\n",
      "Iteration 39482 Loss: 0.9671798348426819\n",
      "Iteration 39483 Loss: 1.0627208948135376\n",
      "Iteration 39484 Loss: 1.237716555595398\n",
      "Iteration 39485 Loss: 1.3235633373260498\n",
      "Iteration 39486 Loss: 0.8788483142852783\n",
      "Iteration 39487 Loss: 0.6408982276916504\n",
      "Iteration 39488 Loss: 1.3540291786193848\n",
      "Iteration 39489 Loss: 1.0311788320541382\n",
      "Iteration 39489 Loss: 1.0346816778182983\n",
      "Iteration 39490 Loss: 0.7648894786834717\n",
      "Iteration 39491 Loss: 0.7443996071815491\n",
      "Iteration 39492 Loss: 1.4273847341537476\n",
      "Iteration 39493 Loss: 1.1951273679733276\n",
      "Iteration 39494 Loss: 0.7654200196266174\n",
      "Iteration 39495 Loss: 1.1794341802597046\n",
      "Iteration 39496 Loss: 0.7994001507759094\n",
      "Iteration 39497 Loss: 0.9311978220939636\n",
      "Iteration 39498 Loss: 1.2648558616638184\n",
      "Iteration 39499 Loss: 0.9684814810752869\n",
      "Iteration 39499 Loss: 1.004059076309204\n",
      "Iteration 39500 Loss: 1.2831141948699951\n",
      "Iteration 39501 Loss: 0.9293894171714783\n",
      "Iteration 39502 Loss: 0.6761897206306458\n",
      "Iteration 39503 Loss: 1.0279942750930786\n",
      "Iteration 39504 Loss: 0.8362481594085693\n",
      "Iteration 39505 Loss: 0.7495021820068359\n",
      "Iteration 39506 Loss: 0.9566997289657593\n",
      "Iteration 39507 Loss: 0.9161694645881653\n",
      "Iteration 39508 Loss: 0.949787437915802\n",
      "Iteration 39509 Loss: 0.6258207559585571\n",
      "Iteration 39509 Loss: 0.8950914144515991\n",
      "Iteration 39510 Loss: 0.7671682834625244\n",
      "Iteration 39511 Loss: 1.137772560119629\n",
      "Iteration 39512 Loss: 1.0591034889221191\n",
      "Iteration 39513 Loss: 0.8806961178779602\n",
      "Iteration 39514 Loss: 0.8854261040687561\n",
      "Iteration 39515 Loss: 1.1804077625274658\n",
      "Iteration 39516 Loss: 0.6850613355636597\n",
      "Iteration 39517 Loss: 1.1228678226470947\n",
      "Iteration 39518 Loss: 1.0732485055923462\n",
      "Iteration 39519 Loss: 0.9456595778465271\n",
      "Iteration 39519 Loss: 0.9737411737442017\n",
      "Iteration 39520 Loss: 1.0086805820465088\n",
      "Iteration 39521 Loss: 0.9510102272033691\n",
      "Iteration 39522 Loss: 1.140884518623352\n",
      "Iteration 39523 Loss: 0.5794306993484497\n",
      "Iteration 39524 Loss: 0.9085691571235657\n",
      "Iteration 39525 Loss: 0.7954916954040527\n",
      "Iteration 39526 Loss: 1.0986926555633545\n",
      "Iteration 39527 Loss: 1.3107750415802002\n",
      "Iteration 39528 Loss: 0.9840494990348816\n",
      "Iteration 39529 Loss: 0.9491584897041321\n",
      "Iteration 39529 Loss: 0.9726742506027222\n",
      "Iteration 39530 Loss: 0.9153820276260376\n",
      "Iteration 39531 Loss: 1.140242338180542\n",
      "Iteration 39532 Loss: 1.0292339324951172\n",
      "Iteration 39533 Loss: 1.1385990381240845\n",
      "Iteration 39534 Loss: 0.9131747484207153\n",
      "Iteration 39535 Loss: 0.8353531360626221\n",
      "Iteration 39536 Loss: 0.9064756035804749\n",
      "Iteration 39537 Loss: 0.9497733116149902\n",
      "Iteration 39538 Loss: 0.805051326751709\n",
      "Iteration 39539 Loss: 1.0530387163162231\n",
      "Iteration 39539 Loss: 0.9686325192451477\n",
      "Iteration 39540 Loss: 0.8317410349845886\n",
      "Iteration 39541 Loss: 0.8383232951164246\n",
      "Iteration 39542 Loss: 1.0106933116912842\n",
      "Iteration 39543 Loss: 0.9829818606376648\n",
      "Iteration 39544 Loss: 0.8258246183395386\n",
      "Iteration 39545 Loss: 1.0683386325836182\n",
      "Iteration 39546 Loss: 1.0944234132766724\n",
      "Iteration 39547 Loss: 0.9268288016319275\n",
      "Iteration 39548 Loss: 1.3096851110458374\n",
      "Iteration 39549 Loss: 1.3466752767562866\n",
      "Iteration 39549 Loss: 1.0235514640808105\n",
      "Iteration 39550 Loss: 1.1389760971069336\n",
      "Iteration 39551 Loss: 0.9994203448295593\n",
      "Iteration 39552 Loss: 0.870438277721405\n",
      "Iteration 39553 Loss: 1.166581153869629\n",
      "Iteration 39554 Loss: 1.119760513305664\n",
      "Iteration 39555 Loss: 1.0528870820999146\n",
      "Iteration 39556 Loss: 1.1227160692214966\n",
      "Iteration 39557 Loss: 1.0319875478744507\n",
      "Iteration 39558 Loss: 0.9539987444877625\n",
      "Iteration 39559 Loss: 1.2244350910186768\n",
      "Iteration 39559 Loss: 1.068120002746582\n",
      "Iteration 39560 Loss: 1.0812965631484985\n",
      "Iteration 39561 Loss: 0.8902996778488159\n",
      "Iteration 39562 Loss: 1.0700370073318481\n",
      "Iteration 39563 Loss: 0.7333689332008362\n",
      "Iteration 39564 Loss: 0.8823479413986206\n",
      "Iteration 39565 Loss: 0.9671227335929871\n",
      "Iteration 39566 Loss: 0.8040913343429565\n",
      "Iteration 39567 Loss: 0.9014883637428284\n",
      "Iteration 39568 Loss: 1.069514274597168\n",
      "Iteration 39569 Loss: 1.2550020217895508\n",
      "Iteration 39569 Loss: 0.9654568433761597\n",
      "Iteration 39570 Loss: 1.0048418045043945\n",
      "Iteration 39571 Loss: 0.7935950756072998\n",
      "Iteration 39572 Loss: 1.1574440002441406\n",
      "Iteration 39573 Loss: 1.0458513498306274\n",
      "Iteration 39574 Loss: 0.8211830258369446\n",
      "Iteration 39575 Loss: 0.8423897624015808\n",
      "Iteration 39576 Loss: 1.2326520681381226\n",
      "Iteration 39577 Loss: 1.1069161891937256\n",
      "Iteration 39578 Loss: 1.1733521223068237\n",
      "Iteration 39579 Loss: 1.2067688703536987\n",
      "Iteration 39579 Loss: 1.0384994745254517\n",
      "Iteration 39580 Loss: 0.9582237005233765\n",
      "Iteration 39581 Loss: 0.789089024066925\n",
      "Iteration 39582 Loss: 0.9665135145187378\n",
      "Iteration 39583 Loss: 1.2591664791107178\n",
      "Iteration 39584 Loss: 1.0537339448928833\n",
      "Iteration 39585 Loss: 1.0581151247024536\n",
      "Iteration 39586 Loss: 0.6943958401679993\n",
      "Iteration 39587 Loss: 0.8627541661262512\n",
      "Iteration 39588 Loss: 0.6987413167953491\n",
      "Iteration 39589 Loss: 1.5142414569854736\n",
      "Iteration 39589 Loss: 0.9854973554611206\n",
      "Iteration 39590 Loss: 1.3618837594985962\n",
      "Iteration 39591 Loss: 0.9138171672821045\n",
      "Iteration 39592 Loss: 0.8667226433753967\n",
      "Iteration 39593 Loss: 1.0729374885559082\n",
      "Iteration 39594 Loss: 0.8928896188735962\n",
      "Iteration 39595 Loss: 1.0446680784225464\n",
      "Iteration 39596 Loss: 0.604904055595398\n",
      "Iteration 39597 Loss: 1.157880187034607\n",
      "Iteration 39598 Loss: 0.7390697598457336\n",
      "Iteration 39599 Loss: 0.9755694270133972\n",
      "Iteration 39599 Loss: 0.9630341529846191\n",
      "Iteration 39600 Loss: 0.9123574495315552\n",
      "Iteration 39601 Loss: 1.1037243604660034\n",
      "Iteration 39602 Loss: 1.3660552501678467\n",
      "Iteration 39603 Loss: 0.6924176812171936\n",
      "Iteration 39604 Loss: 0.4989235997200012\n",
      "Iteration 39605 Loss: 1.1972839832305908\n",
      "Iteration 39606 Loss: 0.6983209848403931\n",
      "Iteration 39607 Loss: 0.8156576156616211\n",
      "Iteration 39608 Loss: 1.2273571491241455\n",
      "Iteration 39609 Loss: 1.2084053754806519\n",
      "Iteration 39609 Loss: 0.9720503687858582\n",
      "Iteration 39610 Loss: 1.0927255153656006\n",
      "Iteration 39611 Loss: 0.8571329116821289\n",
      "Iteration 39612 Loss: 1.3214447498321533\n",
      "Iteration 39613 Loss: 0.8536815643310547\n",
      "Iteration 39614 Loss: 0.9779751300811768\n",
      "Iteration 39615 Loss: 1.0487661361694336\n",
      "Iteration 39616 Loss: 1.1047312021255493\n",
      "Iteration 39617 Loss: 0.6911096572875977\n",
      "Iteration 39618 Loss: 0.8618878722190857\n",
      "Iteration 39619 Loss: 1.1410212516784668\n",
      "Iteration 39619 Loss: 0.9950476884841919\n",
      "Iteration 39620 Loss: 0.761081337928772\n",
      "Iteration 39621 Loss: 1.1589921712875366\n",
      "Iteration 39622 Loss: 0.8664599061012268\n",
      "Iteration 39623 Loss: 0.9202941060066223\n",
      "Iteration 39624 Loss: 1.0364285707473755\n",
      "Iteration 39625 Loss: 1.087951421737671\n",
      "Iteration 39626 Loss: 1.163007378578186\n",
      "Iteration 39627 Loss: 0.7153448462486267\n",
      "Iteration 39628 Loss: 0.9242158532142639\n",
      "Iteration 39629 Loss: 0.8054884076118469\n",
      "Iteration 39629 Loss: 0.9439264535903931\n",
      "Iteration 39630 Loss: 1.0633937120437622\n",
      "Iteration 39631 Loss: 0.6867507100105286\n",
      "Iteration 39632 Loss: 1.154226303100586\n",
      "Iteration 39633 Loss: 0.9395214915275574\n",
      "Iteration 39634 Loss: 0.8200425505638123\n",
      "Iteration 39635 Loss: 0.626189649105072\n",
      "Iteration 39636 Loss: 0.8414853811264038\n",
      "Iteration 39637 Loss: 0.7910342812538147\n",
      "Iteration 39638 Loss: 0.9362314939498901\n",
      "Iteration 39639 Loss: 0.8111675381660461\n",
      "Iteration 39639 Loss: 0.8670042753219604\n",
      "Iteration 39640 Loss: 1.4653013944625854\n",
      "Iteration 39641 Loss: 0.8613304495811462\n",
      "Iteration 39642 Loss: 0.9761816263198853\n",
      "Iteration 39643 Loss: 0.9854289293289185\n",
      "Iteration 39644 Loss: 0.6474460363388062\n",
      "Iteration 39645 Loss: 0.7209746241569519\n",
      "Iteration 39646 Loss: 0.8020971417427063\n",
      "Iteration 39647 Loss: 0.9016852974891663\n",
      "Iteration 39648 Loss: 0.834758460521698\n",
      "Iteration 39649 Loss: 0.9148496389389038\n",
      "Iteration 39649 Loss: 0.9110053777694702\n",
      "Iteration 39650 Loss: 0.9363202452659607\n",
      "Iteration 39651 Loss: 0.7210007905960083\n",
      "Iteration 39652 Loss: 1.3427343368530273\n",
      "Iteration 39653 Loss: 0.656318724155426\n",
      "Iteration 39654 Loss: 1.283897876739502\n",
      "Iteration 39655 Loss: 0.9391160011291504\n",
      "Iteration 39656 Loss: 1.0580579042434692\n",
      "Iteration 39657 Loss: 1.2122552394866943\n",
      "Iteration 39658 Loss: 0.9648084044456482\n",
      "Iteration 39659 Loss: 1.1153913736343384\n",
      "Iteration 39659 Loss: 1.022990107536316\n",
      "Iteration 39660 Loss: 1.1502158641815186\n",
      "Iteration 39661 Loss: 0.9443920850753784\n",
      "Iteration 39662 Loss: 0.9431864023208618\n",
      "Iteration 39663 Loss: 0.9625794291496277\n",
      "Iteration 39664 Loss: 0.8573114275932312\n",
      "Iteration 39665 Loss: 1.2412084341049194\n",
      "Iteration 39666 Loss: 0.9668314456939697\n",
      "Iteration 39667 Loss: 1.1644870042800903\n",
      "Iteration 39668 Loss: 0.9271050691604614\n",
      "Iteration 39669 Loss: 0.9739968776702881\n",
      "Iteration 39669 Loss: 1.0131313800811768\n",
      "Iteration 39670 Loss: 1.0988385677337646\n",
      "Iteration 39671 Loss: 1.0836344957351685\n",
      "Iteration 39672 Loss: 0.9552844762802124\n",
      "Iteration 39673 Loss: 0.7204414010047913\n",
      "Iteration 39674 Loss: 0.8758420944213867\n",
      "Iteration 39675 Loss: 0.8524547219276428\n",
      "Iteration 39676 Loss: 1.4497661590576172\n",
      "Iteration 39677 Loss: 0.9674014449119568\n",
      "Iteration 39678 Loss: 1.4947667121887207\n",
      "Iteration 39679 Loss: 1.0410460233688354\n",
      "Iteration 39679 Loss: 1.0539476871490479\n",
      "Iteration 39680 Loss: 0.7303557395935059\n",
      "Iteration 39681 Loss: 0.97398841381073\n",
      "Iteration 39682 Loss: 1.1415284872055054\n",
      "Iteration 39683 Loss: 1.3748255968093872\n",
      "Iteration 39684 Loss: 1.2810659408569336\n",
      "Iteration 39685 Loss: 0.8667797446250916\n",
      "Iteration 39686 Loss: 0.6629514098167419\n",
      "Iteration 39687 Loss: 1.067033290863037\n",
      "Iteration 39688 Loss: 0.8695370554924011\n",
      "Iteration 39689 Loss: 0.8350542187690735\n",
      "Iteration 39689 Loss: 0.9803119897842407\n",
      "Iteration 39690 Loss: 0.994924783706665\n",
      "Iteration 39691 Loss: 0.9095432162284851\n",
      "Iteration 39692 Loss: 1.087558627128601\n",
      "Iteration 39693 Loss: 1.0319098234176636\n",
      "Iteration 39694 Loss: 1.1202304363250732\n",
      "Iteration 39695 Loss: 1.0921664237976074\n",
      "Iteration 39696 Loss: 0.9903549551963806\n",
      "Iteration 39697 Loss: 1.0191487073898315\n",
      "Iteration 39698 Loss: 0.8808839321136475\n",
      "Iteration 39699 Loss: 1.1110602617263794\n",
      "Iteration 39699 Loss: 1.0237780809402466\n",
      "Iteration 39700 Loss: 1.057672142982483\n",
      "Iteration 39701 Loss: 1.2176119089126587\n",
      "Iteration 39702 Loss: 0.8623313307762146\n",
      "Iteration 39703 Loss: 0.8950799107551575\n",
      "Iteration 39704 Loss: 0.8975341320037842\n",
      "Iteration 39705 Loss: 1.0135905742645264\n",
      "Iteration 39706 Loss: 0.7810431122779846\n",
      "Iteration 39707 Loss: 0.7809969186782837\n",
      "Iteration 39708 Loss: 0.8992846012115479\n",
      "Iteration 39709 Loss: 1.040210485458374\n",
      "Iteration 39709 Loss: 0.9445355534553528\n",
      "Iteration 39710 Loss: 1.1278085708618164\n",
      "Iteration 39711 Loss: 0.8582929968833923\n",
      "Iteration 39712 Loss: 1.087424635887146\n",
      "Iteration 39713 Loss: 0.9682064056396484\n",
      "Iteration 39714 Loss: 1.1028622388839722\n",
      "Iteration 39715 Loss: 0.7687547206878662\n",
      "Iteration 39716 Loss: 0.7078941464424133\n",
      "Iteration 39717 Loss: 0.905659556388855\n",
      "Iteration 39718 Loss: 0.7383916974067688\n",
      "Iteration 39719 Loss: 0.8580187559127808\n",
      "Iteration 39719 Loss: 0.9123314023017883\n",
      "Iteration 39720 Loss: 0.9358277320861816\n",
      "Iteration 39721 Loss: 0.843813419342041\n",
      "Iteration 39722 Loss: 0.7997773885726929\n",
      "Iteration 39723 Loss: 0.9564909338951111\n",
      "Iteration 39724 Loss: 1.139683723449707\n",
      "Iteration 39725 Loss: 1.3263717889785767\n",
      "Iteration 39726 Loss: 0.8247528672218323\n",
      "Iteration 39727 Loss: 1.1862704753875732\n",
      "Iteration 39728 Loss: 0.6584439277648926\n",
      "Iteration 39729 Loss: 0.9046900868415833\n",
      "Iteration 39729 Loss: 0.9576122164726257\n",
      "Iteration 39730 Loss: 1.1106699705123901\n",
      "Iteration 39731 Loss: 1.2545708417892456\n",
      "Iteration 39732 Loss: 1.1022529602050781\n",
      "Iteration 39733 Loss: 1.5178875923156738\n",
      "Iteration 39734 Loss: 1.0925853252410889\n",
      "Iteration 39735 Loss: 1.0317999124526978\n",
      "Iteration 39736 Loss: 0.7252761721611023\n",
      "Iteration 39737 Loss: 0.9437742829322815\n",
      "Iteration 39738 Loss: 1.2102855443954468\n",
      "Iteration 39739 Loss: 0.8698404431343079\n",
      "Iteration 39739 Loss: 1.0858943462371826\n",
      "Iteration 39740 Loss: 0.9929469227790833\n",
      "Iteration 39741 Loss: 0.9456018209457397\n",
      "Iteration 39742 Loss: 0.6282374858856201\n",
      "Iteration 39743 Loss: 0.6586536169052124\n",
      "Iteration 39744 Loss: 0.8960078358650208\n",
      "Iteration 39745 Loss: 0.6961970329284668\n",
      "Iteration 39746 Loss: 0.900593101978302\n",
      "Iteration 39747 Loss: 0.8366537690162659\n",
      "Iteration 39748 Loss: 1.0757166147232056\n",
      "Iteration 39749 Loss: 0.8466562628746033\n",
      "Iteration 39749 Loss: 0.8477264642715454\n",
      "Iteration 39750 Loss: 0.6009276509284973\n",
      "Iteration 39751 Loss: 1.2860136032104492\n",
      "Iteration 39752 Loss: 0.8602036237716675\n",
      "Iteration 39753 Loss: 0.7897786498069763\n",
      "Iteration 39754 Loss: 1.2798190116882324\n",
      "Iteration 39755 Loss: 0.6042677760124207\n",
      "Iteration 39756 Loss: 0.75110924243927\n",
      "Iteration 39757 Loss: 1.0117863416671753\n",
      "Iteration 39758 Loss: 1.0851483345031738\n",
      "Iteration 39759 Loss: 0.8566106557846069\n",
      "Iteration 39759 Loss: 0.9125664830207825\n",
      "Iteration 39760 Loss: 0.9483505487442017\n",
      "Iteration 39761 Loss: 0.9687159657478333\n",
      "Iteration 39762 Loss: 0.8612993955612183\n",
      "Iteration 39763 Loss: 0.6386744976043701\n",
      "Iteration 39764 Loss: 0.9707053303718567\n",
      "Iteration 39765 Loss: 0.6775195598602295\n",
      "Iteration 39766 Loss: 1.1013270616531372\n",
      "Iteration 39767 Loss: 0.8870142102241516\n",
      "Iteration 39768 Loss: 0.8823484182357788\n",
      "Iteration 39769 Loss: 1.0719960927963257\n",
      "Iteration 39769 Loss: 0.9007951021194458\n",
      "Iteration 39770 Loss: 0.7928069233894348\n",
      "Iteration 39771 Loss: 1.0750236511230469\n",
      "Iteration 39772 Loss: 0.7040834426879883\n",
      "Iteration 39773 Loss: 0.8827921748161316\n",
      "Iteration 39774 Loss: 1.1790199279785156\n",
      "Iteration 39775 Loss: 0.94609135389328\n",
      "Iteration 39776 Loss: 0.6289973855018616\n",
      "Iteration 39777 Loss: 0.6611294150352478\n",
      "Iteration 39778 Loss: 0.6579042673110962\n",
      "Iteration 39779 Loss: 0.629760205745697\n",
      "Iteration 39779 Loss: 0.8157607913017273\n",
      "Iteration 39780 Loss: 1.0761830806732178\n",
      "Iteration 39781 Loss: 1.2376160621643066\n",
      "Iteration 39782 Loss: 0.9427869319915771\n",
      "Iteration 39783 Loss: 1.0563430786132812\n",
      "Iteration 39784 Loss: 0.8650537133216858\n",
      "Iteration 39785 Loss: 0.9956791400909424\n",
      "Iteration 39786 Loss: 0.9438103437423706\n",
      "Iteration 39787 Loss: 1.2635161876678467\n",
      "Iteration 39788 Loss: 0.9953823089599609\n",
      "Iteration 39789 Loss: 0.9266897439956665\n",
      "Iteration 39789 Loss: 1.030306100845337\n",
      "Iteration 39790 Loss: 0.889822781085968\n",
      "Iteration 39791 Loss: 0.9156512022018433\n",
      "Iteration 39792 Loss: 0.8490379452705383\n",
      "Iteration 39793 Loss: 1.2824279069900513\n",
      "Iteration 39794 Loss: 0.8543421030044556\n",
      "Iteration 39795 Loss: 1.306298851966858\n",
      "Iteration 39796 Loss: 0.9703084230422974\n",
      "Iteration 39797 Loss: 1.0134769678115845\n",
      "Iteration 39798 Loss: 1.3124022483825684\n",
      "Iteration 39799 Loss: 0.7055759429931641\n",
      "Iteration 39799 Loss: 1.009934425354004\n",
      "Iteration 39800 Loss: 0.8093287944793701\n",
      "Iteration 39801 Loss: 0.8385273218154907\n",
      "Iteration 39802 Loss: 1.0112384557724\n",
      "Iteration 39803 Loss: 0.7178966403007507\n",
      "Iteration 39804 Loss: 1.0419671535491943\n",
      "Iteration 39805 Loss: 0.5830219388008118\n",
      "Iteration 39806 Loss: 1.2026386260986328\n",
      "Iteration 39807 Loss: 1.0756491422653198\n",
      "Iteration 39808 Loss: 0.7106471061706543\n",
      "Iteration 39809 Loss: 1.0000505447387695\n",
      "Iteration 39809 Loss: 0.8990966081619263\n",
      "Iteration 39810 Loss: 0.547944188117981\n",
      "Iteration 39811 Loss: 1.0403584241867065\n",
      "Iteration 39812 Loss: 1.0217052698135376\n",
      "Iteration 39813 Loss: 1.2515771389007568\n",
      "Iteration 39814 Loss: 0.8087555170059204\n",
      "Iteration 39815 Loss: 0.8932399153709412\n",
      "Iteration 39816 Loss: 1.3110020160675049\n",
      "Iteration 39817 Loss: 0.9729748368263245\n",
      "Iteration 39818 Loss: 1.0531905889511108\n",
      "Iteration 39819 Loss: 1.23027765750885\n",
      "Iteration 39819 Loss: 1.0131025314331055\n",
      "Iteration 39820 Loss: 0.9012144804000854\n",
      "Iteration 39821 Loss: 1.0972564220428467\n",
      "Iteration 39822 Loss: 0.7791857123374939\n",
      "Iteration 39823 Loss: 0.9901547431945801\n",
      "Iteration 39824 Loss: 0.6751902103424072\n",
      "Iteration 39825 Loss: 1.0683636665344238\n",
      "Iteration 39826 Loss: 0.9304723739624023\n",
      "Iteration 39827 Loss: 0.7551915049552917\n",
      "Iteration 39828 Loss: 1.2744179964065552\n",
      "Iteration 39829 Loss: 0.8705794215202332\n",
      "Iteration 39829 Loss: 0.9342025518417358\n",
      "Iteration 39830 Loss: 0.8206145167350769\n",
      "Iteration 39831 Loss: 0.6391779780387878\n",
      "Iteration 39832 Loss: 0.9011862277984619\n",
      "Iteration 39833 Loss: 0.8902528285980225\n",
      "Iteration 39834 Loss: 0.8642576932907104\n",
      "Iteration 39835 Loss: 0.8885617852210999\n",
      "Iteration 39836 Loss: 1.2103679180145264\n",
      "Iteration 39837 Loss: 1.1043620109558105\n",
      "Iteration 39838 Loss: 0.6440517902374268\n",
      "Iteration 39839 Loss: 0.9576236605644226\n",
      "Iteration 39839 Loss: 0.8920456171035767\n",
      "Iteration 39840 Loss: 0.9144174456596375\n",
      "Iteration 39841 Loss: 0.874129056930542\n",
      "Iteration 39842 Loss: 1.0305092334747314\n",
      "Iteration 39843 Loss: 1.0695720911026\n",
      "Iteration 39844 Loss: 1.2687231302261353\n",
      "Iteration 39845 Loss: 1.1027170419692993\n",
      "Iteration 39846 Loss: 0.8461553454399109\n",
      "Iteration 39847 Loss: 1.3641222715377808\n",
      "Iteration 39848 Loss: 1.0124696493148804\n",
      "Iteration 39849 Loss: 0.7418712377548218\n",
      "Iteration 39849 Loss: 1.0224686861038208\n",
      "Iteration 39850 Loss: 1.1144379377365112\n",
      "Iteration 39851 Loss: 1.0463484525680542\n",
      "Iteration 39852 Loss: 1.0810258388519287\n",
      "Iteration 39853 Loss: 0.7246990203857422\n",
      "Iteration 39854 Loss: 1.2258563041687012\n",
      "Iteration 39855 Loss: 0.7670125961303711\n",
      "Iteration 39856 Loss: 0.9247233867645264\n",
      "Iteration 39857 Loss: 1.0249825716018677\n",
      "Iteration 39858 Loss: 1.0051313638687134\n",
      "Iteration 39859 Loss: 1.0479410886764526\n",
      "Iteration 39859 Loss: 0.9962159395217896\n",
      "Iteration 39860 Loss: 0.8866409659385681\n",
      "Iteration 39861 Loss: 1.0273734331130981\n",
      "Iteration 39862 Loss: 0.8793572783470154\n",
      "Iteration 39863 Loss: 1.2395825386047363\n",
      "Iteration 39864 Loss: 1.1433340311050415\n",
      "Iteration 39865 Loss: 1.0021789073944092\n",
      "Iteration 39866 Loss: 0.8188909888267517\n",
      "Iteration 39867 Loss: 0.9231998324394226\n",
      "Iteration 39868 Loss: 0.7466474175453186\n",
      "Iteration 39869 Loss: 0.7580559253692627\n",
      "Iteration 39869 Loss: 0.9425260424613953\n",
      "Iteration 39870 Loss: 1.0383381843566895\n",
      "Iteration 39871 Loss: 1.165330410003662\n",
      "Iteration 39872 Loss: 1.1792550086975098\n",
      "Iteration 39873 Loss: 1.058904767036438\n",
      "Iteration 39874 Loss: 0.8028004765510559\n",
      "Iteration 39875 Loss: 1.5292242765426636\n",
      "Iteration 39876 Loss: 0.6113565564155579\n",
      "Iteration 39877 Loss: 0.8980559706687927\n",
      "Iteration 39878 Loss: 1.1711649894714355\n",
      "Iteration 39879 Loss: 0.601973831653595\n",
      "Iteration 39879 Loss: 1.0056403875350952\n",
      "Iteration 39880 Loss: 1.0012558698654175\n",
      "Iteration 39881 Loss: 1.1906347274780273\n",
      "Iteration 39882 Loss: 1.1177356243133545\n",
      "Iteration 39883 Loss: 1.146719217300415\n",
      "Iteration 39884 Loss: 1.170337438583374\n",
      "Iteration 39885 Loss: 1.1912144422531128\n",
      "Iteration 39886 Loss: 0.7500454783439636\n",
      "Iteration 39887 Loss: 1.1101243495941162\n",
      "Iteration 39888 Loss: 0.854492723941803\n",
      "Iteration 39889 Loss: 0.7671255469322205\n",
      "Iteration 39889 Loss: 1.0299686193466187\n",
      "Iteration 39890 Loss: 0.6142752766609192\n",
      "Iteration 39891 Loss: 1.0768500566482544\n",
      "Iteration 39892 Loss: 0.8246597647666931\n",
      "Iteration 39893 Loss: 1.4492100477218628\n",
      "Iteration 39894 Loss: 0.7358183860778809\n",
      "Iteration 39895 Loss: 1.0619157552719116\n",
      "Iteration 39896 Loss: 0.8150582313537598\n",
      "Iteration 39897 Loss: 1.2553725242614746\n",
      "Iteration 39898 Loss: 1.023008942604065\n",
      "Iteration 39899 Loss: 0.7720897197723389\n",
      "Iteration 39899 Loss: 0.9628259539604187\n",
      "Iteration 39900 Loss: 0.6341169476509094\n",
      "Iteration 39901 Loss: 1.0928007364273071\n",
      "Iteration 39902 Loss: 1.1654540300369263\n",
      "Iteration 39903 Loss: 0.8751664757728577\n",
      "Iteration 39904 Loss: 1.1144447326660156\n",
      "Iteration 39905 Loss: 0.7824206352233887\n",
      "Iteration 39906 Loss: 0.7465715408325195\n",
      "Iteration 39907 Loss: 1.1793514490127563\n",
      "Iteration 39908 Loss: 0.7219172716140747\n",
      "Iteration 39909 Loss: 0.8378739953041077\n",
      "Iteration 39909 Loss: 0.9150117635726929\n",
      "Iteration 39910 Loss: 0.7190472483634949\n",
      "Iteration 39911 Loss: 0.5707835555076599\n",
      "Iteration 39912 Loss: 0.8878127336502075\n",
      "Iteration 39913 Loss: 1.0634130239486694\n",
      "Iteration 39914 Loss: 1.0739333629608154\n",
      "Iteration 39915 Loss: 1.0261074304580688\n",
      "Iteration 39916 Loss: 1.074925422668457\n",
      "Iteration 39917 Loss: 1.224120855331421\n",
      "Iteration 39918 Loss: 1.0503325462341309\n",
      "Iteration 39919 Loss: 0.8548939228057861\n",
      "Iteration 39919 Loss: 0.954537034034729\n",
      "Iteration 39920 Loss: 0.8778538107872009\n",
      "Iteration 39921 Loss: 0.8985214233398438\n",
      "Iteration 39922 Loss: 1.0825624465942383\n",
      "Iteration 39923 Loss: 1.309753179550171\n",
      "Iteration 39924 Loss: 1.0876747369766235\n",
      "Iteration 39925 Loss: 0.8948595523834229\n",
      "Iteration 39926 Loss: 0.8525620698928833\n",
      "Iteration 39927 Loss: 0.9550390243530273\n",
      "Iteration 39928 Loss: 1.2453408241271973\n",
      "Iteration 39929 Loss: 0.7838267087936401\n",
      "Iteration 39929 Loss: 0.9987994432449341\n",
      "Iteration 39930 Loss: 0.7354957461357117\n",
      "Iteration 39931 Loss: 0.9393617510795593\n",
      "Iteration 39932 Loss: 0.8666833639144897\n",
      "Iteration 39933 Loss: 1.0707228183746338\n",
      "Iteration 39934 Loss: 0.769708514213562\n",
      "Iteration 39935 Loss: 0.922605037689209\n",
      "Iteration 39936 Loss: 1.375309705734253\n",
      "Iteration 39937 Loss: 1.2338474988937378\n",
      "Iteration 39938 Loss: 0.9708956480026245\n",
      "Iteration 39939 Loss: 1.0701543092727661\n",
      "Iteration 39939 Loss: 0.9954784512519836\n",
      "Iteration 39940 Loss: 0.8731071949005127\n",
      "Iteration 39941 Loss: 0.9132330417633057\n",
      "Iteration 39942 Loss: 0.9690702557563782\n",
      "Iteration 39943 Loss: 1.0313694477081299\n",
      "Iteration 39944 Loss: 0.8820973634719849\n",
      "Iteration 39945 Loss: 0.9534543752670288\n",
      "Iteration 39946 Loss: 0.8108056783676147\n",
      "Iteration 39947 Loss: 0.8885587453842163\n",
      "Iteration 39948 Loss: 0.9636664986610413\n",
      "Iteration 39949 Loss: 0.9849554300308228\n",
      "Iteration 39949 Loss: 0.9270318150520325\n",
      "Iteration 39950 Loss: 0.5223618745803833\n",
      "Iteration 39951 Loss: 0.8694548010826111\n",
      "Iteration 39952 Loss: 1.1136934757232666\n",
      "Iteration 39953 Loss: 0.8948979377746582\n",
      "Iteration 39954 Loss: 0.888891339302063\n",
      "Iteration 39955 Loss: 1.1048977375030518\n",
      "Iteration 39956 Loss: 0.9552167057991028\n",
      "Iteration 39957 Loss: 0.6011815071105957\n",
      "Iteration 39958 Loss: 0.8156867623329163\n",
      "Iteration 39959 Loss: 1.14176344871521\n",
      "Iteration 39959 Loss: 0.8908044695854187\n",
      "Iteration 39960 Loss: 0.9985610246658325\n",
      "Iteration 39961 Loss: 0.8832759857177734\n",
      "Iteration 39962 Loss: 0.8140824437141418\n",
      "Iteration 39963 Loss: 0.8168812990188599\n",
      "Iteration 39964 Loss: 0.6965180039405823\n",
      "Iteration 39965 Loss: 0.9644363522529602\n",
      "Iteration 39966 Loss: 1.0774579048156738\n",
      "Iteration 39967 Loss: 1.180177927017212\n",
      "Iteration 39968 Loss: 1.0367714166641235\n",
      "Iteration 39969 Loss: 1.3351889848709106\n",
      "Iteration 39969 Loss: 0.9803351163864136\n",
      "Iteration 39970 Loss: 0.8889115452766418\n",
      "Iteration 39971 Loss: 0.7035269737243652\n",
      "Iteration 39972 Loss: 0.8795297741889954\n",
      "Iteration 39973 Loss: 1.0240962505340576\n",
      "Iteration 39974 Loss: 1.2062286138534546\n",
      "Iteration 39975 Loss: 1.1810821294784546\n",
      "Iteration 39976 Loss: 0.762645959854126\n",
      "Iteration 39977 Loss: 0.9059762358665466\n",
      "Iteration 39978 Loss: 1.1904209852218628\n",
      "Iteration 39979 Loss: 1.011273741722107\n",
      "Iteration 39979 Loss: 0.9753692746162415\n",
      "Iteration 39980 Loss: 1.008729100227356\n",
      "Iteration 39981 Loss: 1.1936079263687134\n",
      "Iteration 39982 Loss: 1.1413962841033936\n",
      "Iteration 39983 Loss: 0.8276752829551697\n",
      "Iteration 39984 Loss: 0.7883116006851196\n",
      "Iteration 39985 Loss: 0.9021165370941162\n",
      "Iteration 39986 Loss: 1.2049572467803955\n",
      "Iteration 39987 Loss: 1.1526191234588623\n",
      "Iteration 39988 Loss: 1.1950358152389526\n",
      "Iteration 39989 Loss: 1.158058524131775\n",
      "Iteration 39989 Loss: 1.057250738143921\n",
      "Iteration 39990 Loss: 0.8259209394454956\n",
      "Iteration 39991 Loss: 1.4045405387878418\n",
      "Iteration 39992 Loss: 0.9045240879058838\n",
      "Iteration 39993 Loss: 1.0873427391052246\n",
      "Iteration 39994 Loss: 0.5316551923751831\n",
      "Iteration 39995 Loss: 0.977689266204834\n",
      "Iteration 39996 Loss: 1.0443609952926636\n",
      "Iteration 39997 Loss: 1.2477911710739136\n",
      "Iteration 39998 Loss: 0.7019608616828918\n",
      "Iteration 39999 Loss: 0.7464563846588135\n",
      "Iteration 39999 Loss: 0.9472242593765259\n",
      "Iteration 40000 Loss: 0.9191676378250122\n",
      "Iteration 40001 Loss: 0.8173746466636658\n",
      "Iteration 40002 Loss: 0.8146483898162842\n",
      "Iteration 40003 Loss: 0.6305029988288879\n",
      "Iteration 40004 Loss: 0.7750769257545471\n",
      "Iteration 40005 Loss: 1.021191120147705\n",
      "Iteration 40006 Loss: 1.0817203521728516\n",
      "Iteration 40007 Loss: 0.9515195488929749\n",
      "Iteration 40008 Loss: 1.095659852027893\n",
      "Iteration 40009 Loss: 0.9700331091880798\n",
      "Iteration 40009 Loss: 0.9076894521713257\n",
      "Iteration 40010 Loss: 1.5483214855194092\n",
      "Iteration 40011 Loss: 0.6638603210449219\n",
      "Iteration 40012 Loss: 1.055035948753357\n",
      "Iteration 40013 Loss: 0.845316469669342\n",
      "Iteration 40014 Loss: 1.1288304328918457\n",
      "Iteration 40015 Loss: 0.9706064462661743\n",
      "Iteration 40016 Loss: 0.7588125467300415\n",
      "Iteration 40017 Loss: 1.068364143371582\n",
      "Iteration 40018 Loss: 0.9142630100250244\n",
      "Iteration 40019 Loss: 1.101864218711853\n",
      "Iteration 40019 Loss: 1.0055274963378906\n",
      "Iteration 40020 Loss: 0.9247341752052307\n",
      "Iteration 40021 Loss: 0.8651617765426636\n",
      "Iteration 40022 Loss: 1.2371580600738525\n",
      "Iteration 40023 Loss: 0.965936005115509\n",
      "Iteration 40024 Loss: 0.9566946029663086\n",
      "Iteration 40025 Loss: 1.04237961769104\n",
      "Iteration 40026 Loss: 0.7884910106658936\n",
      "Iteration 40027 Loss: 1.182942509651184\n",
      "Iteration 40028 Loss: 1.5418782234191895\n",
      "Iteration 40029 Loss: 0.9765226244926453\n",
      "Iteration 40029 Loss: 1.0481898784637451\n",
      "Iteration 40030 Loss: 0.8623965382575989\n",
      "Iteration 40031 Loss: 1.0220568180084229\n",
      "Iteration 40032 Loss: 0.6337942481040955\n",
      "Iteration 40033 Loss: 1.0152353048324585\n",
      "Iteration 40034 Loss: 1.1899100542068481\n",
      "Iteration 40035 Loss: 0.9586876630783081\n",
      "Iteration 40036 Loss: 0.8542604446411133\n",
      "Iteration 40037 Loss: 1.039734959602356\n",
      "Iteration 40038 Loss: 1.1523748636245728\n",
      "Iteration 40039 Loss: 0.7949219942092896\n",
      "Iteration 40039 Loss: 0.9523372650146484\n",
      "Iteration 40040 Loss: 1.2266480922698975\n",
      "Iteration 40041 Loss: 1.084089994430542\n",
      "Iteration 40042 Loss: 0.8488720655441284\n",
      "Iteration 40043 Loss: 0.7460886836051941\n",
      "Iteration 40044 Loss: 0.7664910554885864\n",
      "Iteration 40045 Loss: 1.3348267078399658\n",
      "Iteration 40046 Loss: 0.9873878955841064\n",
      "Iteration 40047 Loss: 0.6689079403877258\n",
      "Iteration 40048 Loss: 0.8566145300865173\n",
      "Iteration 40049 Loss: 1.25679349899292\n",
      "Iteration 40049 Loss: 0.9776719808578491\n",
      "Iteration 40050 Loss: 0.8565616607666016\n",
      "Iteration 40051 Loss: 0.8168694376945496\n",
      "Iteration 40052 Loss: 0.6918764114379883\n",
      "Iteration 40053 Loss: 1.0636893510818481\n",
      "Iteration 40054 Loss: 1.0622293949127197\n",
      "Iteration 40055 Loss: 1.0335807800292969\n",
      "Iteration 40056 Loss: 1.3008596897125244\n",
      "Iteration 40057 Loss: 0.5167480111122131\n",
      "Iteration 40058 Loss: 0.8867946863174438\n",
      "Iteration 40059 Loss: 1.2201988697052002\n",
      "Iteration 40059 Loss: 0.9449408650398254\n",
      "Iteration 40060 Loss: 0.8090258240699768\n",
      "Iteration 40061 Loss: 0.9156893491744995\n",
      "Iteration 40062 Loss: 1.0002027750015259\n",
      "Iteration 40063 Loss: 0.7132521271705627\n",
      "Iteration 40064 Loss: 0.7320637106895447\n",
      "Iteration 40065 Loss: 0.7963138222694397\n",
      "Iteration 40066 Loss: 1.013488531112671\n",
      "Iteration 40067 Loss: 0.7691587805747986\n",
      "Iteration 40068 Loss: 0.9638137817382812\n",
      "Iteration 40069 Loss: 0.9746872186660767\n",
      "Iteration 40069 Loss: 0.8687695264816284\n",
      "Iteration 40070 Loss: 0.9859769344329834\n",
      "Iteration 40071 Loss: 1.0684970617294312\n",
      "Iteration 40072 Loss: 1.361478328704834\n",
      "Iteration 40073 Loss: 0.9719328880310059\n",
      "Iteration 40074 Loss: 1.0287529230117798\n",
      "Iteration 40075 Loss: 0.9454586505889893\n",
      "Iteration 40076 Loss: 0.848805844783783\n",
      "Iteration 40077 Loss: 0.9917392134666443\n",
      "Iteration 40078 Loss: 0.9375267028808594\n",
      "Iteration 40079 Loss: 1.0423529148101807\n",
      "Iteration 40079 Loss: 1.0182521343231201\n",
      "Iteration 40080 Loss: 0.720192015171051\n",
      "Iteration 40081 Loss: 0.9473758935928345\n",
      "Iteration 40082 Loss: 0.7967206239700317\n",
      "Iteration 40083 Loss: 0.9308859705924988\n",
      "Iteration 40084 Loss: 0.9488154053688049\n",
      "Iteration 40085 Loss: 1.0344760417938232\n",
      "Iteration 40086 Loss: 0.820067822933197\n",
      "Iteration 40087 Loss: 0.9221675992012024\n",
      "Iteration 40088 Loss: 0.9944851398468018\n",
      "Iteration 40089 Loss: 1.0295575857162476\n",
      "Iteration 40089 Loss: 0.9144743084907532\n",
      "Iteration 40090 Loss: 1.1111103296279907\n",
      "Iteration 40091 Loss: 1.2239700555801392\n",
      "Iteration 40092 Loss: 0.9000298380851746\n",
      "Iteration 40093 Loss: 1.1861727237701416\n",
      "Iteration 40094 Loss: 0.9266070127487183\n",
      "Iteration 40095 Loss: 0.7659556865692139\n",
      "Iteration 40096 Loss: 0.9248282313346863\n",
      "Iteration 40097 Loss: 1.0781718492507935\n",
      "Iteration 40098 Loss: 0.9770768880844116\n",
      "Iteration 40099 Loss: 1.022852897644043\n",
      "Iteration 40099 Loss: 1.0116775035858154\n",
      "Iteration 40100 Loss: 1.0921417474746704\n",
      "Iteration 40101 Loss: 1.0883396863937378\n",
      "Iteration 40102 Loss: 0.9994638562202454\n",
      "Iteration 40103 Loss: 0.7306028604507446\n",
      "Iteration 40104 Loss: 1.15549635887146\n",
      "Iteration 40105 Loss: 0.892601490020752\n",
      "Iteration 40106 Loss: 0.8765189051628113\n",
      "Iteration 40107 Loss: 0.7737376689910889\n",
      "Iteration 40108 Loss: 0.9415645599365234\n",
      "Iteration 40109 Loss: 0.9017563462257385\n",
      "Iteration 40109 Loss: 0.9452223777770996\n",
      "Iteration 40110 Loss: 0.7493427395820618\n",
      "Iteration 40111 Loss: 1.1529028415679932\n",
      "Iteration 40112 Loss: 0.8104987144470215\n",
      "Iteration 40113 Loss: 0.8821309804916382\n",
      "Iteration 40114 Loss: 0.8535211086273193\n",
      "Iteration 40115 Loss: 0.9359612464904785\n",
      "Iteration 40116 Loss: 0.9831908941268921\n",
      "Iteration 40117 Loss: 0.6761495471000671\n",
      "Iteration 40118 Loss: 1.1303396224975586\n",
      "Iteration 40119 Loss: 1.0313390493392944\n",
      "Iteration 40119 Loss: 0.9205376505851746\n",
      "Iteration 40120 Loss: 0.9624013304710388\n",
      "Iteration 40121 Loss: 1.1362508535385132\n",
      "Iteration 40122 Loss: 1.1139980554580688\n",
      "Iteration 40123 Loss: 0.8296398520469666\n",
      "Iteration 40124 Loss: 0.696205735206604\n",
      "Iteration 40125 Loss: 1.0964678525924683\n",
      "Iteration 40126 Loss: 1.1091911792755127\n",
      "Iteration 40127 Loss: 1.0018671751022339\n",
      "Iteration 40128 Loss: 1.0131741762161255\n",
      "Iteration 40129 Loss: 1.1279634237289429\n",
      "Iteration 40129 Loss: 1.0087159872055054\n",
      "Iteration 40130 Loss: 0.8584719300270081\n",
      "Iteration 40131 Loss: 0.8711804151535034\n",
      "Iteration 40132 Loss: 1.022311806678772\n",
      "Iteration 40133 Loss: 0.855866551399231\n",
      "Iteration 40134 Loss: 0.9970464706420898\n",
      "Iteration 40135 Loss: 0.9175770878791809\n",
      "Iteration 40136 Loss: 0.7752056121826172\n",
      "Iteration 40137 Loss: 1.0453485250473022\n",
      "Iteration 40138 Loss: 1.148706078529358\n",
      "Iteration 40139 Loss: 0.5883274078369141\n",
      "Iteration 40139 Loss: 0.908004105091095\n",
      "Iteration 40140 Loss: 0.9046003818511963\n",
      "Iteration 40141 Loss: 1.1531295776367188\n",
      "Iteration 40142 Loss: 0.9213761687278748\n",
      "Iteration 40143 Loss: 1.3280009031295776\n",
      "Iteration 40144 Loss: 1.0283660888671875\n",
      "Iteration 40145 Loss: 1.1440796852111816\n",
      "Iteration 40146 Loss: 0.6874374151229858\n",
      "Iteration 40147 Loss: 1.1687688827514648\n",
      "Iteration 40148 Loss: 1.036370038986206\n",
      "Iteration 40149 Loss: 0.8936383128166199\n",
      "Iteration 40149 Loss: 1.0265767574310303\n",
      "Iteration 40150 Loss: 0.9264267683029175\n",
      "Iteration 40151 Loss: 0.9606857895851135\n",
      "Iteration 40152 Loss: 1.0016549825668335\n",
      "Iteration 40153 Loss: 0.8285659551620483\n",
      "Iteration 40154 Loss: 0.7508196234703064\n",
      "Iteration 40155 Loss: 0.8906757235527039\n",
      "Iteration 40156 Loss: 0.7689912915229797\n",
      "Iteration 40157 Loss: 0.7269328832626343\n",
      "Iteration 40158 Loss: 0.8144245743751526\n",
      "Iteration 40159 Loss: 0.8882148265838623\n",
      "Iteration 40159 Loss: 0.8557392358779907\n",
      "Iteration 40160 Loss: 1.0979682207107544\n",
      "Iteration 40161 Loss: 0.6607630848884583\n",
      "Iteration 40162 Loss: 1.151175856590271\n",
      "Iteration 40163 Loss: 1.2104820013046265\n",
      "Iteration 40164 Loss: 0.9695090651512146\n",
      "Iteration 40165 Loss: 1.196960210800171\n",
      "Iteration 40166 Loss: 0.9876178503036499\n",
      "Iteration 40167 Loss: 0.8131506443023682\n",
      "Iteration 40168 Loss: 0.8655222654342651\n",
      "Iteration 40169 Loss: 1.1610218286514282\n",
      "Iteration 40169 Loss: 1.011417031288147\n",
      "Iteration 40170 Loss: 0.7982969880104065\n",
      "Iteration 40171 Loss: 1.1255377531051636\n",
      "Iteration 40172 Loss: 1.0481939315795898\n",
      "Iteration 40173 Loss: 1.0671522617340088\n",
      "Iteration 40174 Loss: 0.9799001812934875\n",
      "Iteration 40175 Loss: 1.1856586933135986\n",
      "Iteration 40176 Loss: 0.9760237336158752\n",
      "Iteration 40177 Loss: 1.0550631284713745\n",
      "Iteration 40178 Loss: 0.9602292776107788\n",
      "Iteration 40179 Loss: 0.9450556635856628\n",
      "Iteration 40179 Loss: 1.0141111612319946\n",
      "Iteration 40180 Loss: 1.0432416200637817\n",
      "Iteration 40181 Loss: 1.1013044118881226\n",
      "Iteration 40182 Loss: 1.0371919870376587\n",
      "Iteration 40183 Loss: 0.6555551886558533\n",
      "Iteration 40184 Loss: 0.7588400840759277\n",
      "Iteration 40185 Loss: 1.1334174871444702\n",
      "Iteration 40186 Loss: 0.9187332987785339\n",
      "Iteration 40187 Loss: 0.9265521764755249\n",
      "Iteration 40188 Loss: 0.9566702842712402\n",
      "Iteration 40189 Loss: 0.7815616130828857\n",
      "Iteration 40189 Loss: 0.9313068389892578\n",
      "Iteration 40190 Loss: 0.7157139778137207\n",
      "Iteration 40191 Loss: 0.8792746067047119\n",
      "Iteration 40192 Loss: 1.059126377105713\n",
      "Iteration 40193 Loss: 0.9782094359397888\n",
      "Iteration 40194 Loss: 0.7307640314102173\n",
      "Iteration 40195 Loss: 1.3328133821487427\n",
      "Iteration 40196 Loss: 0.9569533467292786\n",
      "Iteration 40197 Loss: 1.071745753288269\n",
      "Iteration 40198 Loss: 0.7756255865097046\n",
      "Iteration 40199 Loss: 0.9565362334251404\n",
      "Iteration 40199 Loss: 0.9456762075424194\n",
      "Iteration 40200 Loss: 0.8006194233894348\n",
      "Iteration 40201 Loss: 0.7493399381637573\n",
      "Iteration 40202 Loss: 1.0507532358169556\n",
      "Iteration 40203 Loss: 0.9457537531852722\n",
      "Iteration 40204 Loss: 0.7853977084159851\n",
      "Iteration 40205 Loss: 1.1813875436782837\n",
      "Iteration 40206 Loss: 0.879572331905365\n",
      "Iteration 40207 Loss: 0.9892049431800842\n",
      "Iteration 40208 Loss: 0.80556720495224\n",
      "Iteration 40209 Loss: 0.9791632890701294\n",
      "Iteration 40209 Loss: 0.916675865650177\n",
      "Iteration 40210 Loss: 1.110569953918457\n",
      "Iteration 40211 Loss: 0.8813796639442444\n",
      "Iteration 40212 Loss: 0.8733546137809753\n",
      "Iteration 40213 Loss: 0.88582444190979\n",
      "Iteration 40214 Loss: 1.2807092666625977\n",
      "Iteration 40215 Loss: 0.745952844619751\n",
      "Iteration 40216 Loss: 0.770447313785553\n",
      "Iteration 40217 Loss: 1.1562104225158691\n",
      "Iteration 40218 Loss: 1.0226635932922363\n",
      "Iteration 40219 Loss: 1.1970137357711792\n",
      "Iteration 40219 Loss: 0.9924125671386719\n",
      "Iteration 40220 Loss: 1.4109203815460205\n",
      "Iteration 40221 Loss: 1.4289065599441528\n",
      "Iteration 40222 Loss: 0.6693582534790039\n",
      "Iteration 40223 Loss: 0.7788209915161133\n",
      "Iteration 40224 Loss: 0.6994597911834717\n",
      "Iteration 40225 Loss: 0.8552246689796448\n",
      "Iteration 40226 Loss: 1.057507038116455\n",
      "Iteration 40227 Loss: 1.1044577360153198\n",
      "Iteration 40228 Loss: 1.0172083377838135\n",
      "Iteration 40229 Loss: 0.7863333821296692\n",
      "Iteration 40229 Loss: 0.9808197021484375\n",
      "Iteration 40230 Loss: 1.2217763662338257\n",
      "Iteration 40231 Loss: 0.8996292948722839\n",
      "Iteration 40232 Loss: 1.1909980773925781\n",
      "Iteration 40233 Loss: 1.0983918905258179\n",
      "Iteration 40234 Loss: 0.9143049120903015\n",
      "Iteration 40235 Loss: 0.6982302069664001\n",
      "Iteration 40236 Loss: 0.9354735016822815\n",
      "Iteration 40237 Loss: 0.830572783946991\n",
      "Iteration 40238 Loss: 1.2114108800888062\n",
      "Iteration 40239 Loss: 0.9479378461837769\n",
      "Iteration 40239 Loss: 0.9948725700378418\n",
      "Iteration 40240 Loss: 1.0419504642486572\n",
      "Iteration 40241 Loss: 1.1097182035446167\n",
      "Iteration 40242 Loss: 0.9060821533203125\n",
      "Iteration 40243 Loss: 1.2240086793899536\n",
      "Iteration 40244 Loss: 0.6090853214263916\n",
      "Iteration 40245 Loss: 1.0483391284942627\n",
      "Iteration 40246 Loss: 0.7141429781913757\n",
      "Iteration 40247 Loss: 1.0907217264175415\n",
      "Iteration 40248 Loss: 1.0206013917922974\n",
      "Iteration 40249 Loss: 0.9903190732002258\n",
      "Iteration 40249 Loss: 0.9754968881607056\n",
      "Iteration 40250 Loss: 0.7719349265098572\n",
      "Iteration 40251 Loss: 1.0180611610412598\n",
      "Iteration 40252 Loss: 1.2628226280212402\n",
      "Iteration 40253 Loss: 0.8677798509597778\n",
      "Iteration 40254 Loss: 1.0558834075927734\n",
      "Iteration 40255 Loss: 1.2748327255249023\n",
      "Iteration 40256 Loss: 0.6015260219573975\n",
      "Iteration 40257 Loss: 0.9502468109130859\n",
      "Iteration 40258 Loss: 0.9717971086502075\n",
      "Iteration 40259 Loss: 1.0087647438049316\n",
      "Iteration 40259 Loss: 0.9783649444580078\n",
      "Iteration 40260 Loss: 1.0206027030944824\n",
      "Iteration 40261 Loss: 0.9711946845054626\n",
      "Iteration 40262 Loss: 1.027294397354126\n",
      "Iteration 40263 Loss: 1.3010072708129883\n",
      "Iteration 40264 Loss: 0.8132120966911316\n",
      "Iteration 40265 Loss: 1.0214276313781738\n",
      "Iteration 40266 Loss: 0.8345503807067871\n",
      "Iteration 40267 Loss: 1.0735522508621216\n",
      "Iteration 40268 Loss: 0.903898298740387\n",
      "Iteration 40269 Loss: 1.0448424816131592\n",
      "Iteration 40269 Loss: 1.0011582374572754\n",
      "Iteration 40270 Loss: 0.8414971232414246\n",
      "Iteration 40271 Loss: 1.2961593866348267\n",
      "Iteration 40272 Loss: 0.9304392337799072\n",
      "Iteration 40273 Loss: 0.8990838527679443\n",
      "Iteration 40274 Loss: 0.8515245318412781\n",
      "Iteration 40275 Loss: 0.7669804692268372\n",
      "Iteration 40276 Loss: 1.3078579902648926\n",
      "Iteration 40277 Loss: 1.004062533378601\n",
      "Iteration 40278 Loss: 1.427774429321289\n",
      "Iteration 40279 Loss: 0.9610993266105652\n",
      "Iteration 40279 Loss: 1.0286478996276855\n",
      "Iteration 40280 Loss: 0.9914049506187439\n",
      "Iteration 40281 Loss: 1.2136987447738647\n",
      "Iteration 40282 Loss: 1.0743387937545776\n",
      "Iteration 40283 Loss: 1.0608714818954468\n",
      "Iteration 40284 Loss: 0.7908092737197876\n",
      "Iteration 40285 Loss: 1.0052461624145508\n",
      "Iteration 40286 Loss: 0.7749443650245667\n",
      "Iteration 40287 Loss: 1.161359429359436\n",
      "Iteration 40288 Loss: 0.8472334742546082\n",
      "Iteration 40289 Loss: 0.876585066318512\n",
      "Iteration 40289 Loss: 0.9796492457389832\n",
      "Iteration 40290 Loss: 0.9763374328613281\n",
      "Iteration 40291 Loss: 0.7271583676338196\n",
      "Iteration 40292 Loss: 1.2302325963974\n",
      "Iteration 40293 Loss: 0.96719890832901\n",
      "Iteration 40294 Loss: 0.97231125831604\n",
      "Iteration 40295 Loss: 1.1948944330215454\n",
      "Iteration 40296 Loss: 1.3304270505905151\n",
      "Iteration 40297 Loss: 0.41777503490448\n",
      "Iteration 40298 Loss: 0.8843650817871094\n",
      "Iteration 40299 Loss: 1.0131546258926392\n",
      "Iteration 40299 Loss: 0.9713853597640991\n",
      "Iteration 40300 Loss: 0.5939308404922485\n",
      "Iteration 40301 Loss: 0.9516795873641968\n",
      "Iteration 40302 Loss: 0.9351937174797058\n",
      "Iteration 40303 Loss: 0.942345380783081\n",
      "Iteration 40304 Loss: 1.1398329734802246\n",
      "Iteration 40305 Loss: 0.8273240923881531\n",
      "Iteration 40306 Loss: 0.9069054126739502\n",
      "Iteration 40307 Loss: 1.0392906665802002\n",
      "Iteration 40308 Loss: 1.2029292583465576\n",
      "Iteration 40309 Loss: 1.0535178184509277\n",
      "Iteration 40309 Loss: 0.9592949748039246\n",
      "Iteration 40310 Loss: 1.3100978136062622\n",
      "Iteration 40311 Loss: 1.15439772605896\n",
      "Iteration 40312 Loss: 0.6796932220458984\n",
      "Iteration 40313 Loss: 1.0549622774124146\n",
      "Iteration 40314 Loss: 0.7918588519096375\n",
      "Iteration 40315 Loss: 1.2181721925735474\n",
      "Iteration 40316 Loss: 1.273510456085205\n",
      "Iteration 40317 Loss: 0.8849920630455017\n",
      "Iteration 40318 Loss: 0.8618946075439453\n",
      "Iteration 40319 Loss: 1.0375704765319824\n",
      "Iteration 40319 Loss: 1.02671480178833\n",
      "Iteration 40320 Loss: 1.0916576385498047\n",
      "Iteration 40321 Loss: 0.8351976871490479\n",
      "Iteration 40322 Loss: 0.7846289873123169\n",
      "Iteration 40323 Loss: 1.1553759574890137\n",
      "Iteration 40324 Loss: 0.7200056314468384\n",
      "Iteration 40325 Loss: 0.5810063481330872\n",
      "Iteration 40326 Loss: 0.9962548017501831\n",
      "Iteration 40327 Loss: 1.1224241256713867\n",
      "Iteration 40328 Loss: 0.6740723848342896\n",
      "Iteration 40329 Loss: 1.1066969633102417\n",
      "Iteration 40329 Loss: 0.9067320823669434\n",
      "Iteration 40330 Loss: 0.8656885623931885\n",
      "Iteration 40331 Loss: 0.8154971599578857\n",
      "Iteration 40332 Loss: 1.072773814201355\n",
      "Iteration 40333 Loss: 1.1783158779144287\n",
      "Iteration 40334 Loss: 1.1861356496810913\n",
      "Iteration 40335 Loss: 1.290776252746582\n",
      "Iteration 40336 Loss: 1.0835832357406616\n",
      "Iteration 40337 Loss: 0.9911422729492188\n",
      "Iteration 40338 Loss: 1.228800654411316\n",
      "Iteration 40339 Loss: 0.8478752374649048\n",
      "Iteration 40339 Loss: 1.0560587644577026\n",
      "Iteration 40340 Loss: 1.1974358558654785\n",
      "Iteration 40341 Loss: 1.1777634620666504\n",
      "Iteration 40342 Loss: 1.1583919525146484\n",
      "Iteration 40343 Loss: 0.8114161491394043\n",
      "Iteration 40344 Loss: 1.1678696870803833\n",
      "Iteration 40345 Loss: 0.8479286432266235\n",
      "Iteration 40346 Loss: 0.7693788409233093\n",
      "Iteration 40347 Loss: 0.9441559314727783\n",
      "Iteration 40348 Loss: 0.9746844172477722\n",
      "Iteration 40349 Loss: 0.8990756869316101\n",
      "Iteration 40349 Loss: 0.9948099851608276\n",
      "Iteration 40350 Loss: 0.701509952545166\n",
      "Iteration 40351 Loss: 0.7737541198730469\n",
      "Iteration 40352 Loss: 0.9618721008300781\n",
      "Iteration 40353 Loss: 1.166968584060669\n",
      "Iteration 40354 Loss: 0.9161736965179443\n",
      "Iteration 40355 Loss: 1.2463479042053223\n",
      "Iteration 40356 Loss: 1.1557661294937134\n",
      "Iteration 40357 Loss: 0.9848882555961609\n",
      "Iteration 40358 Loss: 1.17631196975708\n",
      "Iteration 40359 Loss: 0.7670899629592896\n",
      "Iteration 40359 Loss: 0.9850682020187378\n",
      "Iteration 40360 Loss: 0.7803844809532166\n",
      "Iteration 40361 Loss: 0.8938489556312561\n",
      "Iteration 40362 Loss: 0.7040071487426758\n",
      "Iteration 40363 Loss: 0.7915195226669312\n",
      "Iteration 40364 Loss: 0.8653271794319153\n",
      "Iteration 40365 Loss: 1.2084152698516846\n",
      "Iteration 40366 Loss: 1.1284419298171997\n",
      "Iteration 40367 Loss: 0.9626248478889465\n",
      "Iteration 40368 Loss: 0.9743055701255798\n",
      "Iteration 40369 Loss: 0.6784788966178894\n",
      "Iteration 40369 Loss: 0.8987353444099426\n",
      "Iteration 40370 Loss: 1.1441426277160645\n",
      "Iteration 40371 Loss: 0.9926835298538208\n",
      "Iteration 40372 Loss: 1.1959452629089355\n",
      "Iteration 40373 Loss: 1.0311399698257446\n",
      "Iteration 40374 Loss: 1.1359964609146118\n",
      "Iteration 40375 Loss: 0.8692801594734192\n",
      "Iteration 40376 Loss: 1.0316431522369385\n",
      "Iteration 40377 Loss: 1.3023536205291748\n",
      "Iteration 40378 Loss: 1.4686959981918335\n",
      "Iteration 40379 Loss: 1.0704402923583984\n",
      "Iteration 40379 Loss: 1.1242320537567139\n",
      "Iteration 40380 Loss: 0.9689983129501343\n",
      "Iteration 40381 Loss: 0.8486946225166321\n",
      "Iteration 40382 Loss: 0.7154094576835632\n",
      "Iteration 40383 Loss: 0.5724484920501709\n",
      "Iteration 40384 Loss: 1.4161102771759033\n",
      "Iteration 40385 Loss: 1.0646328926086426\n",
      "Iteration 40386 Loss: 1.1296299695968628\n",
      "Iteration 40387 Loss: 0.8766291737556458\n",
      "Iteration 40388 Loss: 1.0827219486236572\n",
      "Iteration 40389 Loss: 1.1370086669921875\n",
      "Iteration 40389 Loss: 0.981228232383728\n",
      "Iteration 40390 Loss: 0.9897617101669312\n",
      "Iteration 40391 Loss: 0.8621163368225098\n",
      "Iteration 40392 Loss: 0.8756481409072876\n",
      "Iteration 40393 Loss: 1.2964022159576416\n",
      "Iteration 40394 Loss: 0.8959426879882812\n",
      "Iteration 40395 Loss: 0.8531904816627502\n",
      "Iteration 40396 Loss: 1.2489017248153687\n",
      "Iteration 40397 Loss: 0.7315234541893005\n",
      "Iteration 40398 Loss: 0.968477725982666\n",
      "Iteration 40399 Loss: 0.9706462621688843\n",
      "Iteration 40399 Loss: 0.9692610502243042\n",
      "Iteration 40400 Loss: 0.9926834106445312\n",
      "Iteration 40401 Loss: 0.9777348041534424\n",
      "Iteration 40402 Loss: 0.8734925389289856\n",
      "Iteration 40403 Loss: 0.855530321598053\n",
      "Iteration 40404 Loss: 0.9775588512420654\n",
      "Iteration 40405 Loss: 0.831101655960083\n",
      "Iteration 40406 Loss: 0.9604648947715759\n",
      "Iteration 40407 Loss: 0.6785057187080383\n",
      "Iteration 40408 Loss: 0.6633345484733582\n",
      "Iteration 40409 Loss: 0.9339068531990051\n",
      "Iteration 40409 Loss: 0.874431312084198\n",
      "Iteration 40410 Loss: 0.98707515001297\n",
      "Iteration 40411 Loss: 0.7108672261238098\n",
      "Iteration 40412 Loss: 0.6855197548866272\n",
      "Iteration 40413 Loss: 1.1207095384597778\n",
      "Iteration 40414 Loss: 0.9967138171195984\n",
      "Iteration 40415 Loss: 0.7561724781990051\n",
      "Iteration 40416 Loss: 1.2933090925216675\n",
      "Iteration 40417 Loss: 0.7436561584472656\n",
      "Iteration 40418 Loss: 0.8762298226356506\n",
      "Iteration 40419 Loss: 0.9266596436500549\n",
      "Iteration 40419 Loss: 0.9096912145614624\n",
      "Iteration 40420 Loss: 1.0660704374313354\n",
      "Iteration 40421 Loss: 1.3902695178985596\n",
      "Iteration 40422 Loss: 1.1860865354537964\n",
      "Iteration 40423 Loss: 0.9950549006462097\n",
      "Iteration 40424 Loss: 0.8356436491012573\n",
      "Iteration 40425 Loss: 0.8345023393630981\n",
      "Iteration 40426 Loss: 1.029084324836731\n",
      "Iteration 40427 Loss: 0.9740156531333923\n",
      "Iteration 40428 Loss: 0.9984484314918518\n",
      "Iteration 40429 Loss: 0.8504051566123962\n",
      "Iteration 40429 Loss: 1.0159580707550049\n",
      "Iteration 40430 Loss: 0.6851793527603149\n",
      "Iteration 40431 Loss: 1.0206997394561768\n",
      "Iteration 40432 Loss: 0.9807835817337036\n",
      "Iteration 40433 Loss: 1.0308104753494263\n",
      "Iteration 40434 Loss: 1.1558133363723755\n",
      "Iteration 40435 Loss: 0.8339540362358093\n",
      "Iteration 40436 Loss: 1.0476927757263184\n",
      "Iteration 40437 Loss: 1.2303869724273682\n",
      "Iteration 40438 Loss: 1.0898441076278687\n",
      "Iteration 40439 Loss: 0.9381731748580933\n",
      "Iteration 40439 Loss: 1.0013337135314941\n",
      "Iteration 40440 Loss: 1.3777867555618286\n",
      "Iteration 40441 Loss: 1.0155246257781982\n",
      "Iteration 40442 Loss: 0.942277193069458\n",
      "Iteration 40443 Loss: 0.6399960517883301\n",
      "Iteration 40444 Loss: 0.845534086227417\n",
      "Iteration 40445 Loss: 0.8695346713066101\n",
      "Iteration 40446 Loss: 1.0907912254333496\n",
      "Iteration 40447 Loss: 0.9019997119903564\n",
      "Iteration 40448 Loss: 1.1564277410507202\n",
      "Iteration 40449 Loss: 1.4038480520248413\n",
      "Iteration 40449 Loss: 1.0243719816207886\n",
      "Iteration 40450 Loss: 1.2772525548934937\n",
      "Iteration 40451 Loss: 1.197671890258789\n",
      "Iteration 40452 Loss: 0.8974423408508301\n",
      "Iteration 40453 Loss: 0.8709274530410767\n",
      "Iteration 40454 Loss: 0.8285385370254517\n",
      "Iteration 40455 Loss: 1.0038211345672607\n",
      "Iteration 40456 Loss: 1.0164721012115479\n",
      "Iteration 40457 Loss: 0.9885048866271973\n",
      "Iteration 40458 Loss: 0.7786626815795898\n",
      "Iteration 40459 Loss: 0.7343462109565735\n",
      "Iteration 40459 Loss: 0.9593639373779297\n",
      "Iteration 40460 Loss: 0.7722333073616028\n",
      "Iteration 40461 Loss: 1.0276156663894653\n",
      "Iteration 40462 Loss: 0.6308100819587708\n",
      "Iteration 40463 Loss: 1.0142288208007812\n",
      "Iteration 40464 Loss: 1.0143771171569824\n",
      "Iteration 40465 Loss: 1.3216809034347534\n",
      "Iteration 40466 Loss: 0.9638771414756775\n",
      "Iteration 40467 Loss: 0.8224334716796875\n",
      "Iteration 40468 Loss: 0.9083427786827087\n",
      "Iteration 40469 Loss: 0.9034982919692993\n",
      "Iteration 40469 Loss: 0.9379097819328308\n",
      "Iteration 40470 Loss: 0.9735106229782104\n",
      "Iteration 40471 Loss: 0.9987471699714661\n",
      "Iteration 40472 Loss: 0.8381686210632324\n",
      "Iteration 40473 Loss: 1.1689951419830322\n",
      "Iteration 40474 Loss: 1.370161533355713\n",
      "Iteration 40475 Loss: 0.8373820781707764\n",
      "Iteration 40476 Loss: 0.8493264317512512\n",
      "Iteration 40477 Loss: 0.9004506468772888\n",
      "Iteration 40478 Loss: 0.9911876916885376\n",
      "Iteration 40479 Loss: 0.7628183364868164\n",
      "Iteration 40479 Loss: 0.9690747261047363\n",
      "Iteration 40480 Loss: 0.8767604231834412\n",
      "Iteration 40481 Loss: 0.873942494392395\n",
      "Iteration 40482 Loss: 0.9127441048622131\n",
      "Iteration 40483 Loss: 0.9566628336906433\n",
      "Iteration 40484 Loss: 0.8070576190948486\n",
      "Iteration 40485 Loss: 0.9105138182640076\n",
      "Iteration 40486 Loss: 0.840050220489502\n",
      "Iteration 40487 Loss: 1.3719624280929565\n",
      "Iteration 40488 Loss: 0.6848872900009155\n",
      "Iteration 40489 Loss: 0.8968319296836853\n",
      "Iteration 40489 Loss: 0.9131412506103516\n",
      "Iteration 40490 Loss: 1.115036129951477\n",
      "Iteration 40491 Loss: 0.9769793152809143\n",
      "Iteration 40492 Loss: 0.5628936290740967\n",
      "Iteration 40493 Loss: 1.1764453649520874\n",
      "Iteration 40494 Loss: 0.8993521928787231\n",
      "Iteration 40495 Loss: 1.2125517129898071\n",
      "Iteration 40496 Loss: 1.1067185401916504\n",
      "Iteration 40497 Loss: 0.988058865070343\n",
      "Iteration 40498 Loss: 0.9017017483711243\n",
      "Iteration 40499 Loss: 1.0951659679412842\n",
      "Iteration 40499 Loss: 1.0034903287887573\n",
      "Iteration 40500 Loss: 1.0584509372711182\n",
      "Iteration 40501 Loss: 0.9479408860206604\n",
      "Iteration 40502 Loss: 1.0259625911712646\n",
      "Iteration 40503 Loss: 0.9402116537094116\n",
      "Iteration 40504 Loss: 1.0713468790054321\n",
      "Iteration 40505 Loss: 0.8025135397911072\n",
      "Iteration 40506 Loss: 0.8806471228599548\n",
      "Iteration 40507 Loss: 0.5602153539657593\n",
      "Iteration 40508 Loss: 1.1286749839782715\n",
      "Iteration 40509 Loss: 1.011203646659851\n",
      "Iteration 40509 Loss: 0.9427167773246765\n",
      "Iteration 40510 Loss: 0.7491957545280457\n",
      "Iteration 40511 Loss: 0.795087993144989\n",
      "Iteration 40512 Loss: 1.078560471534729\n",
      "Iteration 40513 Loss: 0.8601184487342834\n",
      "Iteration 40514 Loss: 1.1447499990463257\n",
      "Iteration 40515 Loss: 1.0399805307388306\n",
      "Iteration 40516 Loss: 0.7937799096107483\n",
      "Iteration 40517 Loss: 1.2713013887405396\n",
      "Iteration 40518 Loss: 1.0201600790023804\n",
      "Iteration 40519 Loss: 1.276296854019165\n",
      "Iteration 40519 Loss: 1.0029231309890747\n",
      "Iteration 40520 Loss: 1.2645132541656494\n",
      "Iteration 40521 Loss: 0.8258026838302612\n",
      "Iteration 40522 Loss: 0.8273342847824097\n",
      "Iteration 40523 Loss: 0.7882400155067444\n",
      "Iteration 40524 Loss: 0.8882796764373779\n",
      "Iteration 40525 Loss: 0.8102384209632874\n",
      "Iteration 40526 Loss: 1.295351266860962\n",
      "Iteration 40527 Loss: 0.6808121800422668\n",
      "Iteration 40528 Loss: 0.7861607074737549\n",
      "Iteration 40529 Loss: 1.0541743040084839\n",
      "Iteration 40529 Loss: 0.9220907092094421\n",
      "Iteration 40530 Loss: 0.8943151235580444\n",
      "Iteration 40531 Loss: 0.9342001676559448\n",
      "Iteration 40532 Loss: 0.9625898003578186\n",
      "Iteration 40533 Loss: 0.7457568645477295\n",
      "Iteration 40534 Loss: 0.9244171977043152\n",
      "Iteration 40535 Loss: 1.1871542930603027\n",
      "Iteration 40536 Loss: 0.9170001149177551\n",
      "Iteration 40537 Loss: 0.9915139079093933\n",
      "Iteration 40538 Loss: 0.6384664177894592\n",
      "Iteration 40539 Loss: 1.1390161514282227\n",
      "Iteration 40539 Loss: 0.9334429502487183\n",
      "Iteration 40540 Loss: 1.1647112369537354\n",
      "Iteration 40541 Loss: 0.8903989791870117\n",
      "Iteration 40542 Loss: 1.3853362798690796\n",
      "Iteration 40543 Loss: 0.9304498434066772\n",
      "Iteration 40544 Loss: 1.0823187828063965\n",
      "Iteration 40545 Loss: 0.8439453840255737\n",
      "Iteration 40546 Loss: 1.01405930519104\n",
      "Iteration 40547 Loss: 0.7740563750267029\n",
      "Iteration 40548 Loss: 0.9756557941436768\n",
      "Iteration 40549 Loss: 0.8201560378074646\n",
      "Iteration 40549 Loss: 0.9881088137626648\n",
      "Iteration 40550 Loss: 0.7822337746620178\n",
      "Iteration 40551 Loss: 0.7262744903564453\n",
      "Iteration 40552 Loss: 1.0731680393218994\n",
      "Iteration 40553 Loss: 0.62163245677948\n",
      "Iteration 40554 Loss: 0.9290142059326172\n",
      "Iteration 40555 Loss: 0.8857641816139221\n",
      "Iteration 40556 Loss: 0.965586245059967\n",
      "Iteration 40557 Loss: 1.08891761302948\n",
      "Iteration 40558 Loss: 0.8968758583068848\n",
      "Iteration 40559 Loss: 1.1064505577087402\n",
      "Iteration 40559 Loss: 0.907591700553894\n",
      "Iteration 40560 Loss: 0.7713006734848022\n",
      "Iteration 40561 Loss: 1.2992079257965088\n",
      "Iteration 40562 Loss: 1.2596873044967651\n",
      "Iteration 40563 Loss: 1.171140193939209\n",
      "Iteration 40564 Loss: 0.9033086895942688\n",
      "Iteration 40565 Loss: 1.2946829795837402\n",
      "Iteration 40566 Loss: 1.3724806308746338\n",
      "Iteration 40567 Loss: 0.8515274524688721\n",
      "Iteration 40568 Loss: 0.9204445481300354\n",
      "Iteration 40569 Loss: 1.1396379470825195\n",
      "Iteration 40569 Loss: 1.0983418226242065\n",
      "Iteration 40570 Loss: 1.1282100677490234\n",
      "Iteration 40571 Loss: 1.0793282985687256\n",
      "Iteration 40572 Loss: 0.7917882800102234\n",
      "Iteration 40573 Loss: 0.7955572009086609\n",
      "Iteration 40574 Loss: 0.998978316783905\n",
      "Iteration 40575 Loss: 0.7557917833328247\n",
      "Iteration 40576 Loss: 1.1303448677062988\n",
      "Iteration 40577 Loss: 1.0516867637634277\n",
      "Iteration 40578 Loss: 0.74102783203125\n",
      "Iteration 40579 Loss: 1.0014028549194336\n",
      "Iteration 40579 Loss: 0.9474115371704102\n",
      "Iteration 40580 Loss: 0.7013779282569885\n",
      "Iteration 40581 Loss: 0.97874516248703\n",
      "Iteration 40582 Loss: 0.8808996677398682\n",
      "Iteration 40583 Loss: 1.3196043968200684\n",
      "Iteration 40584 Loss: 1.0997294187545776\n",
      "Iteration 40585 Loss: 0.810003399848938\n",
      "Iteration 40586 Loss: 0.8708526492118835\n",
      "Iteration 40587 Loss: 0.8902229070663452\n",
      "Iteration 40588 Loss: 1.0553631782531738\n",
      "Iteration 40589 Loss: 1.4878064393997192\n",
      "Iteration 40589 Loss: 1.00946044921875\n",
      "Iteration 40590 Loss: 0.9644986987113953\n",
      "Iteration 40591 Loss: 0.734307050704956\n",
      "Iteration 40592 Loss: 0.9171149730682373\n",
      "Iteration 40593 Loss: 0.8617607951164246\n",
      "Iteration 40594 Loss: 1.1020972728729248\n",
      "Iteration 40595 Loss: 1.3988138437271118\n",
      "Iteration 40596 Loss: 1.1044821739196777\n",
      "Iteration 40597 Loss: 0.670699954032898\n",
      "Iteration 40598 Loss: 0.7302877306938171\n",
      "Iteration 40599 Loss: 0.8455902338027954\n",
      "Iteration 40599 Loss: 0.9329652786254883\n",
      "Iteration 40600 Loss: 0.6159875988960266\n",
      "Iteration 40601 Loss: 0.8578178286552429\n",
      "Iteration 40602 Loss: 1.038219690322876\n",
      "Iteration 40603 Loss: 0.8575152158737183\n",
      "Iteration 40604 Loss: 0.8814764618873596\n",
      "Iteration 40605 Loss: 0.827523946762085\n",
      "Iteration 40606 Loss: 1.03132963180542\n",
      "Iteration 40607 Loss: 1.1247938871383667\n",
      "Iteration 40608 Loss: 1.0186563730239868\n",
      "Iteration 40609 Loss: 1.1890681982040405\n",
      "Iteration 40609 Loss: 0.9442388415336609\n",
      "Iteration 40610 Loss: 1.182708740234375\n",
      "Iteration 40611 Loss: 1.1919127702713013\n",
      "Iteration 40612 Loss: 0.7373003363609314\n",
      "Iteration 40613 Loss: 1.1296495199203491\n",
      "Iteration 40614 Loss: 0.9427905678749084\n",
      "Iteration 40615 Loss: 1.0322847366333008\n",
      "Iteration 40616 Loss: 0.9539330005645752\n",
      "Iteration 40617 Loss: 1.0629326105117798\n",
      "Iteration 40618 Loss: 1.0877580642700195\n",
      "Iteration 40619 Loss: 1.204390287399292\n",
      "Iteration 40619 Loss: 1.0525660514831543\n",
      "Iteration 40620 Loss: 1.1553149223327637\n",
      "Iteration 40621 Loss: 0.6737872958183289\n",
      "Iteration 40622 Loss: 0.764316201210022\n",
      "Iteration 40623 Loss: 0.7649996280670166\n",
      "Iteration 40624 Loss: 0.883091926574707\n",
      "Iteration 40625 Loss: 1.1216795444488525\n",
      "Iteration 40626 Loss: 1.0336259603500366\n",
      "Iteration 40627 Loss: 1.0725175142288208\n",
      "Iteration 40628 Loss: 0.723827064037323\n",
      "Iteration 40629 Loss: 1.0711549520492554\n",
      "Iteration 40629 Loss: 0.9264314770698547\n",
      "Iteration 40630 Loss: 0.9651956558227539\n",
      "Iteration 40631 Loss: 0.785811185836792\n",
      "Iteration 40632 Loss: 1.2450835704803467\n",
      "Iteration 40633 Loss: 1.1403123140335083\n",
      "Iteration 40634 Loss: 0.9171575903892517\n",
      "Iteration 40635 Loss: 0.8846067786216736\n",
      "Iteration 40636 Loss: 0.8904340863227844\n",
      "Iteration 40637 Loss: 0.8553542494773865\n",
      "Iteration 40638 Loss: 0.9363796710968018\n",
      "Iteration 40639 Loss: 1.0731756687164307\n",
      "Iteration 40639 Loss: 0.9693511128425598\n",
      "Iteration 40640 Loss: 0.9094198942184448\n",
      "Iteration 40641 Loss: 1.2761355638504028\n",
      "Iteration 40642 Loss: 1.2630703449249268\n",
      "Iteration 40643 Loss: 1.1636388301849365\n",
      "Iteration 40644 Loss: 1.0619760751724243\n",
      "Iteration 40645 Loss: 0.8591058850288391\n",
      "Iteration 40646 Loss: 1.1197140216827393\n",
      "Iteration 40647 Loss: 0.7187524437904358\n",
      "Iteration 40648 Loss: 1.1438333988189697\n",
      "Iteration 40649 Loss: 0.8989543914794922\n",
      "Iteration 40649 Loss: 1.0414600372314453\n",
      "Iteration 40650 Loss: 0.8602878451347351\n",
      "Iteration 40651 Loss: 0.7414793968200684\n",
      "Iteration 40652 Loss: 0.7260494232177734\n",
      "Iteration 40653 Loss: 1.1612821817398071\n",
      "Iteration 40654 Loss: 1.3739038705825806\n",
      "Iteration 40655 Loss: 0.8110454678535461\n",
      "Iteration 40656 Loss: 0.6901790499687195\n",
      "Iteration 40657 Loss: 1.114516258239746\n",
      "Iteration 40658 Loss: 0.6806713938713074\n",
      "Iteration 40659 Loss: 1.1501461267471313\n",
      "Iteration 40659 Loss: 0.9309560656547546\n",
      "Iteration 40660 Loss: 0.8718862533569336\n",
      "Iteration 40661 Loss: 1.207716941833496\n",
      "Iteration 40662 Loss: 0.6951198577880859\n",
      "Iteration 40663 Loss: 1.291046380996704\n",
      "Iteration 40664 Loss: 1.1473134756088257\n",
      "Iteration 40665 Loss: 0.8999568223953247\n",
      "Iteration 40666 Loss: 1.0680574178695679\n",
      "Iteration 40667 Loss: 0.9534603953361511\n",
      "Iteration 40668 Loss: 0.8866782188415527\n",
      "Iteration 40669 Loss: 1.013655424118042\n",
      "Iteration 40669 Loss: 1.0034891366958618\n",
      "Iteration 40670 Loss: 0.7369811534881592\n",
      "Iteration 40671 Loss: 0.6471776962280273\n",
      "Iteration 40672 Loss: 0.827468752861023\n",
      "Iteration 40673 Loss: 1.1395893096923828\n",
      "Iteration 40674 Loss: 1.046918272972107\n",
      "Iteration 40675 Loss: 0.9527590870857239\n",
      "Iteration 40676 Loss: 1.1322224140167236\n",
      "Iteration 40677 Loss: 0.6075214743614197\n",
      "Iteration 40678 Loss: 1.1918509006500244\n",
      "Iteration 40679 Loss: 0.7916892766952515\n",
      "Iteration 40679 Loss: 0.907417893409729\n",
      "Iteration 40680 Loss: 0.9659899473190308\n",
      "Iteration 40681 Loss: 1.3822160959243774\n",
      "Iteration 40682 Loss: 0.9349208474159241\n",
      "Iteration 40683 Loss: 1.0194758176803589\n",
      "Iteration 40684 Loss: 1.0160343647003174\n",
      "Iteration 40685 Loss: 0.8015310168266296\n",
      "Iteration 40686 Loss: 1.0144535303115845\n",
      "Iteration 40687 Loss: 0.9764961004257202\n",
      "Iteration 40688 Loss: 0.9840304851531982\n",
      "Iteration 40689 Loss: 0.9323373436927795\n",
      "Iteration 40689 Loss: 1.0027484893798828\n",
      "Iteration 40690 Loss: 0.980974018573761\n",
      "Iteration 40691 Loss: 0.8866266012191772\n",
      "Iteration 40692 Loss: 0.9038432240486145\n",
      "Iteration 40693 Loss: 1.2526899576187134\n",
      "Iteration 40694 Loss: 0.8406195044517517\n",
      "Iteration 40695 Loss: 0.9381613731384277\n",
      "Iteration 40696 Loss: 0.7652299404144287\n",
      "Iteration 40697 Loss: 0.8948800563812256\n",
      "Iteration 40698 Loss: 0.809363842010498\n",
      "Iteration 40699 Loss: 0.9644594788551331\n",
      "Iteration 40699 Loss: 0.9236847758293152\n",
      "Iteration 40700 Loss: 1.560564398765564\n",
      "Iteration 40701 Loss: 0.9209033846855164\n",
      "Iteration 40702 Loss: 0.627400279045105\n",
      "Iteration 40703 Loss: 0.9385059475898743\n",
      "Iteration 40704 Loss: 1.0998402833938599\n",
      "Iteration 40705 Loss: 1.0234860181808472\n",
      "Iteration 40706 Loss: 1.2073513269424438\n",
      "Iteration 40707 Loss: 0.8085669279098511\n",
      "Iteration 40708 Loss: 1.1760179996490479\n",
      "Iteration 40709 Loss: 0.9200416803359985\n",
      "Iteration 40709 Loss: 1.0282678604125977\n",
      "Iteration 40710 Loss: 0.8964290618896484\n",
      "Iteration 40711 Loss: 0.8874284625053406\n",
      "Iteration 40712 Loss: 0.8296400308609009\n",
      "Iteration 40713 Loss: 0.665004551410675\n",
      "Iteration 40714 Loss: 1.374535083770752\n",
      "Iteration 40715 Loss: 1.0623328685760498\n",
      "Iteration 40716 Loss: 0.9090860486030579\n",
      "Iteration 40717 Loss: 0.9614580869674683\n",
      "Iteration 40718 Loss: 0.8647333979606628\n",
      "Iteration 40719 Loss: 0.9249083399772644\n",
      "Iteration 40719 Loss: 0.9375556707382202\n",
      "Iteration 40720 Loss: 1.1559172868728638\n",
      "Iteration 40721 Loss: 0.8379318714141846\n",
      "Iteration 40722 Loss: 0.5991398096084595\n",
      "Iteration 40723 Loss: 0.8936254978179932\n",
      "Iteration 40724 Loss: 0.6009441614151001\n",
      "Iteration 40725 Loss: 1.1511622667312622\n",
      "Iteration 40726 Loss: 1.0090105533599854\n",
      "Iteration 40727 Loss: 0.7843073010444641\n",
      "Iteration 40728 Loss: 0.623867392539978\n",
      "Iteration 40729 Loss: 0.9463725090026855\n",
      "Iteration 40729 Loss: 0.8602278828620911\n",
      "Iteration 40730 Loss: 1.0334084033966064\n",
      "Iteration 40731 Loss: 0.9768044948577881\n",
      "Iteration 40732 Loss: 0.6946069002151489\n",
      "Iteration 40733 Loss: 1.1081714630126953\n",
      "Iteration 40734 Loss: 0.8213325142860413\n",
      "Iteration 40735 Loss: 0.702340841293335\n",
      "Iteration 40736 Loss: 1.212393045425415\n",
      "Iteration 40737 Loss: 0.9329255819320679\n",
      "Iteration 40738 Loss: 1.038852334022522\n",
      "Iteration 40739 Loss: 0.9821557998657227\n",
      "Iteration 40739 Loss: 0.950299084186554\n",
      "Iteration 40740 Loss: 1.0212416648864746\n",
      "Iteration 40741 Loss: 1.1569191217422485\n",
      "Iteration 40742 Loss: 1.0348563194274902\n",
      "Iteration 40743 Loss: 0.7729047536849976\n",
      "Iteration 40744 Loss: 1.0067224502563477\n",
      "Iteration 40745 Loss: 0.981642484664917\n",
      "Iteration 40746 Loss: 1.1969096660614014\n",
      "Iteration 40747 Loss: 0.6211163997650146\n",
      "Iteration 40748 Loss: 0.7414709329605103\n",
      "Iteration 40749 Loss: 0.6106225252151489\n",
      "Iteration 40749 Loss: 0.9144407510757446\n",
      "Iteration 40750 Loss: 1.1372973918914795\n",
      "Iteration 40751 Loss: 0.7501330971717834\n",
      "Iteration 40752 Loss: 0.9124144315719604\n",
      "Iteration 40753 Loss: 1.0627514123916626\n",
      "Iteration 40754 Loss: 0.6597801446914673\n",
      "Iteration 40755 Loss: 0.5682841539382935\n",
      "Iteration 40756 Loss: 1.2834277153015137\n",
      "Iteration 40757 Loss: 0.8390311002731323\n",
      "Iteration 40758 Loss: 0.925860583782196\n",
      "Iteration 40759 Loss: 1.1637518405914307\n",
      "Iteration 40759 Loss: 0.930273175239563\n",
      "Iteration 40760 Loss: 0.9083760380744934\n",
      "Iteration 40761 Loss: 0.7864560484886169\n",
      "Iteration 40762 Loss: 0.9049720764160156\n",
      "Iteration 40763 Loss: 0.49755409359931946\n",
      "Iteration 40764 Loss: 0.8996791839599609\n",
      "Iteration 40765 Loss: 0.9965895414352417\n",
      "Iteration 40766 Loss: 1.2210358381271362\n",
      "Iteration 40767 Loss: 0.9051321744918823\n",
      "Iteration 40768 Loss: 1.1124218702316284\n",
      "Iteration 40769 Loss: 0.8448755145072937\n",
      "Iteration 40769 Loss: 0.9077092409133911\n",
      "Iteration 40770 Loss: 1.0240440368652344\n",
      "Iteration 40771 Loss: 0.7364299297332764\n",
      "Iteration 40772 Loss: 1.1324137449264526\n",
      "Iteration 40773 Loss: 0.6490285992622375\n",
      "Iteration 40774 Loss: 0.7736175060272217\n",
      "Iteration 40775 Loss: 0.9646733999252319\n",
      "Iteration 40776 Loss: 0.7958253026008606\n",
      "Iteration 40777 Loss: 0.8373430967330933\n",
      "Iteration 40778 Loss: 1.0594303607940674\n",
      "Iteration 40779 Loss: 0.8573123216629028\n",
      "Iteration 40779 Loss: 0.8830119371414185\n",
      "Iteration 40780 Loss: 1.1174473762512207\n",
      "Iteration 40781 Loss: 1.2586207389831543\n",
      "Iteration 40782 Loss: 1.0060315132141113\n",
      "Iteration 40783 Loss: 1.0170429944992065\n",
      "Iteration 40784 Loss: 1.149195909500122\n",
      "Iteration 40785 Loss: 1.1479462385177612\n",
      "Iteration 40786 Loss: 1.1756491661071777\n",
      "Iteration 40787 Loss: 1.0135482549667358\n",
      "Iteration 40788 Loss: 1.0050113201141357\n",
      "Iteration 40789 Loss: 1.104038119316101\n",
      "Iteration 40789 Loss: 1.0994532108306885\n",
      "Iteration 40790 Loss: 1.0654529333114624\n",
      "Iteration 40791 Loss: 0.7004036903381348\n",
      "Iteration 40792 Loss: 0.6996464133262634\n",
      "Iteration 40793 Loss: 0.8181407451629639\n",
      "Iteration 40794 Loss: 1.1709251403808594\n",
      "Iteration 40795 Loss: 1.0286043882369995\n",
      "Iteration 40796 Loss: 1.4378166198730469\n",
      "Iteration 40797 Loss: 1.0050959587097168\n",
      "Iteration 40798 Loss: 0.8970147967338562\n",
      "Iteration 40799 Loss: 0.7150627970695496\n",
      "Iteration 40799 Loss: 0.9538164138793945\n",
      "Iteration 40800 Loss: 0.8048283457756042\n",
      "Iteration 40801 Loss: 1.1125760078430176\n",
      "Iteration 40802 Loss: 0.8577486276626587\n",
      "Iteration 40803 Loss: 1.1407006978988647\n",
      "Iteration 40804 Loss: 0.8716642260551453\n",
      "Iteration 40805 Loss: 1.0077614784240723\n",
      "Iteration 40806 Loss: 1.0352674722671509\n",
      "Iteration 40807 Loss: 1.112796664237976\n",
      "Iteration 40808 Loss: 0.9163864254951477\n",
      "Iteration 40809 Loss: 0.816502571105957\n",
      "Iteration 40809 Loss: 0.967623233795166\n",
      "Iteration 40810 Loss: 1.186124324798584\n",
      "Iteration 40811 Loss: 0.8894425630569458\n",
      "Iteration 40812 Loss: 0.8803836703300476\n",
      "Iteration 40813 Loss: 0.7352163791656494\n",
      "Iteration 40814 Loss: 1.0940784215927124\n",
      "Iteration 40815 Loss: 0.7069634199142456\n",
      "Iteration 40816 Loss: 0.6938648223876953\n",
      "Iteration 40817 Loss: 1.0092660188674927\n",
      "Iteration 40818 Loss: 1.0821894407272339\n",
      "Iteration 40819 Loss: 0.534631073474884\n",
      "Iteration 40819 Loss: 0.8812160491943359\n",
      "Iteration 40820 Loss: 1.0375218391418457\n",
      "Iteration 40821 Loss: 0.7647925019264221\n",
      "Iteration 40822 Loss: 0.7637028694152832\n",
      "Iteration 40823 Loss: 1.1251708269119263\n",
      "Iteration 40824 Loss: 1.02238929271698\n",
      "Iteration 40825 Loss: 0.652467668056488\n",
      "Iteration 40826 Loss: 0.6069924235343933\n",
      "Iteration 40827 Loss: 1.0854909420013428\n",
      "Iteration 40828 Loss: 1.0579715967178345\n",
      "Iteration 40829 Loss: 1.4886258840560913\n",
      "Iteration 40829 Loss: 0.960512638092041\n",
      "Iteration 40830 Loss: 1.0934414863586426\n",
      "Iteration 40831 Loss: 1.3237674236297607\n",
      "Iteration 40832 Loss: 1.0123753547668457\n",
      "Iteration 40833 Loss: 1.0990713834762573\n",
      "Iteration 40834 Loss: 1.1699466705322266\n",
      "Iteration 40835 Loss: 1.1732600927352905\n",
      "Iteration 40836 Loss: 0.761906087398529\n",
      "Iteration 40837 Loss: 0.9183717966079712\n",
      "Iteration 40838 Loss: 0.9162418842315674\n",
      "Iteration 40839 Loss: 1.3464515209197998\n",
      "Iteration 40839 Loss: 1.0814833641052246\n",
      "Iteration 40840 Loss: 0.990633487701416\n",
      "Iteration 40841 Loss: 1.392285943031311\n",
      "Iteration 40842 Loss: 1.1437503099441528\n",
      "Iteration 40843 Loss: 0.7653949856758118\n",
      "Iteration 40844 Loss: 1.1555709838867188\n",
      "Iteration 40845 Loss: 0.9079211950302124\n",
      "Iteration 40846 Loss: 0.8879984617233276\n",
      "Iteration 40847 Loss: 0.7281957268714905\n",
      "Iteration 40848 Loss: 1.137962818145752\n",
      "Iteration 40849 Loss: 0.7263668179512024\n",
      "Iteration 40849 Loss: 0.9836081266403198\n",
      "Iteration 40850 Loss: 1.0547782182693481\n",
      "Iteration 40851 Loss: 1.0578221082687378\n",
      "Iteration 40852 Loss: 0.674969494342804\n",
      "Iteration 40853 Loss: 1.0894551277160645\n",
      "Iteration 40854 Loss: 0.9173988103866577\n",
      "Iteration 40855 Loss: 0.7704575657844543\n",
      "Iteration 40856 Loss: 0.9608362913131714\n",
      "Iteration 40857 Loss: 0.8530859351158142\n",
      "Iteration 40858 Loss: 0.8071934580802917\n",
      "Iteration 40859 Loss: 0.972771942615509\n",
      "Iteration 40859 Loss: 0.9158768653869629\n",
      "Iteration 40860 Loss: 0.8367615342140198\n",
      "Iteration 40861 Loss: 0.9783422946929932\n",
      "Iteration 40862 Loss: 1.3776826858520508\n",
      "Iteration 40863 Loss: 0.7539879083633423\n",
      "Iteration 40864 Loss: 1.1466991901397705\n",
      "Iteration 40865 Loss: 0.9973188638687134\n",
      "Iteration 40866 Loss: 0.9518976211547852\n",
      "Iteration 40867 Loss: 0.7228230834007263\n",
      "Iteration 40868 Loss: 0.8787199258804321\n",
      "Iteration 40869 Loss: 1.1097073554992676\n",
      "Iteration 40869 Loss: 0.975394070148468\n",
      "Iteration 40870 Loss: 0.9564000368118286\n",
      "Iteration 40871 Loss: 0.943808913230896\n",
      "Iteration 40872 Loss: 0.8010433316230774\n",
      "Iteration 40873 Loss: 0.9681780338287354\n",
      "Iteration 40874 Loss: 0.9677801132202148\n",
      "Iteration 40875 Loss: 1.1732888221740723\n",
      "Iteration 40876 Loss: 1.0365499258041382\n",
      "Iteration 40877 Loss: 0.8023879528045654\n",
      "Iteration 40878 Loss: 0.9382386207580566\n",
      "Iteration 40879 Loss: 1.0392992496490479\n",
      "Iteration 40879 Loss: 0.9626976251602173\n",
      "Iteration 40880 Loss: 1.1077048778533936\n",
      "Iteration 40881 Loss: 0.7620342373847961\n",
      "Iteration 40882 Loss: 1.1814579963684082\n",
      "Iteration 40883 Loss: 0.9464312195777893\n",
      "Iteration 40884 Loss: 1.0658715963363647\n",
      "Iteration 40885 Loss: 1.182639241218567\n",
      "Iteration 40886 Loss: 1.0204764604568481\n",
      "Iteration 40887 Loss: 1.1023378372192383\n",
      "Iteration 40888 Loss: 0.8879085183143616\n",
      "Iteration 40889 Loss: 1.247595191001892\n",
      "Iteration 40889 Loss: 1.050445795059204\n",
      "Iteration 40890 Loss: 1.0703355073928833\n",
      "Iteration 40891 Loss: 1.0556442737579346\n",
      "Iteration 40892 Loss: 1.1366972923278809\n",
      "Iteration 40893 Loss: 0.9595955014228821\n",
      "Iteration 40894 Loss: 0.9025359153747559\n",
      "Iteration 40895 Loss: 1.0608291625976562\n",
      "Iteration 40896 Loss: 0.7194780111312866\n",
      "Iteration 40897 Loss: 0.9525655508041382\n",
      "Iteration 40898 Loss: 0.7596856355667114\n",
      "Iteration 40899 Loss: 1.1196693181991577\n",
      "Iteration 40899 Loss: 0.9737035632133484\n",
      "Iteration 40900 Loss: 0.5674110054969788\n",
      "Iteration 40901 Loss: 1.0912600755691528\n",
      "Iteration 40902 Loss: 1.062994360923767\n",
      "Iteration 40903 Loss: 1.0730183124542236\n",
      "Iteration 40904 Loss: 0.8862452507019043\n",
      "Iteration 40905 Loss: 1.2013373374938965\n",
      "Iteration 40906 Loss: 0.7327615022659302\n",
      "Iteration 40907 Loss: 0.9159154891967773\n",
      "Iteration 40908 Loss: 1.1376862525939941\n",
      "Iteration 40909 Loss: 1.085984706878662\n",
      "Iteration 40909 Loss: 0.9754613637924194\n",
      "Iteration 40910 Loss: 0.6718335747718811\n",
      "Iteration 40911 Loss: 1.0198174715042114\n",
      "Iteration 40912 Loss: 1.0824620723724365\n",
      "Iteration 40913 Loss: 0.9798946976661682\n",
      "Iteration 40914 Loss: 1.0576533079147339\n",
      "Iteration 40915 Loss: 1.1259305477142334\n",
      "Iteration 40916 Loss: 0.6515417695045471\n",
      "Iteration 40917 Loss: 0.9005769491195679\n",
      "Iteration 40918 Loss: 0.8211857676506042\n",
      "Iteration 40919 Loss: 1.0632871389389038\n",
      "Iteration 40919 Loss: 0.9374182820320129\n",
      "Iteration 40920 Loss: 1.0733109712600708\n",
      "Iteration 40921 Loss: 1.2099254131317139\n",
      "Iteration 40922 Loss: 0.8023208975791931\n",
      "Iteration 40923 Loss: 1.2212797403335571\n",
      "Iteration 40924 Loss: 1.0888183116912842\n",
      "Iteration 40925 Loss: 0.6589339375495911\n",
      "Iteration 40926 Loss: 0.9756320714950562\n",
      "Iteration 40927 Loss: 0.8967733383178711\n",
      "Iteration 40928 Loss: 0.7311102747917175\n",
      "Iteration 40929 Loss: 1.0369876623153687\n",
      "Iteration 40929 Loss: 0.9695093035697937\n",
      "Iteration 40930 Loss: 0.8023333549499512\n",
      "Iteration 40931 Loss: 0.9794798493385315\n",
      "Iteration 40932 Loss: 1.024693250656128\n",
      "Iteration 40933 Loss: 0.6126512885093689\n",
      "Iteration 40934 Loss: 0.9869744777679443\n",
      "Iteration 40935 Loss: 0.6938799619674683\n",
      "Iteration 40936 Loss: 1.1626019477844238\n",
      "Iteration 40937 Loss: 1.2160108089447021\n",
      "Iteration 40938 Loss: 0.9966756105422974\n",
      "Iteration 40939 Loss: 1.009306788444519\n",
      "Iteration 40939 Loss: 0.9484607577323914\n",
      "Iteration 40940 Loss: 0.8687269687652588\n",
      "Iteration 40941 Loss: 1.1136646270751953\n",
      "Iteration 40942 Loss: 0.8719423413276672\n",
      "Iteration 40943 Loss: 1.0115069150924683\n",
      "Iteration 40944 Loss: 1.0316263437271118\n",
      "Iteration 40945 Loss: 0.9583762884140015\n",
      "Iteration 40946 Loss: 1.089999794960022\n",
      "Iteration 40947 Loss: 0.892902135848999\n",
      "Iteration 40948 Loss: 0.952456533908844\n",
      "Iteration 40949 Loss: 0.8422836661338806\n",
      "Iteration 40949 Loss: 0.9633485674858093\n",
      "Iteration 40950 Loss: 1.12047278881073\n",
      "Iteration 40951 Loss: 0.7332126498222351\n",
      "Iteration 40952 Loss: 0.9481998085975647\n",
      "Iteration 40953 Loss: 1.3409936428070068\n",
      "Iteration 40954 Loss: 1.1006470918655396\n",
      "Iteration 40955 Loss: 0.7159679532051086\n",
      "Iteration 40956 Loss: 0.9455962181091309\n",
      "Iteration 40957 Loss: 0.8353958129882812\n",
      "Iteration 40958 Loss: 1.044601321220398\n",
      "Iteration 40959 Loss: 0.8856068253517151\n",
      "Iteration 40959 Loss: 0.9670694470405579\n",
      "Iteration 40960 Loss: 1.1354554891586304\n",
      "Iteration 40961 Loss: 1.0695444345474243\n",
      "Iteration 40962 Loss: 1.0672940015792847\n",
      "Iteration 40963 Loss: 0.8305508494377136\n",
      "Iteration 40964 Loss: 1.2988756895065308\n",
      "Iteration 40965 Loss: 0.9971981644630432\n",
      "Iteration 40966 Loss: 1.1285278797149658\n",
      "Iteration 40967 Loss: 0.8420991897583008\n",
      "Iteration 40968 Loss: 0.7831920385360718\n",
      "Iteration 40969 Loss: 1.136530876159668\n",
      "Iteration 40969 Loss: 1.0289268493652344\n",
      "Iteration 40970 Loss: 1.109184980392456\n",
      "Iteration 40971 Loss: 0.9161192774772644\n",
      "Iteration 40972 Loss: 0.6941575407981873\n",
      "Iteration 40973 Loss: 0.8630788326263428\n",
      "Iteration 40974 Loss: 0.7025749087333679\n",
      "Iteration 40975 Loss: 0.8395277857780457\n",
      "Iteration 40976 Loss: 1.4586302042007446\n",
      "Iteration 40977 Loss: 1.1105214357376099\n",
      "Iteration 40978 Loss: 1.0731614828109741\n",
      "Iteration 40979 Loss: 1.0259336233139038\n",
      "Iteration 40979 Loss: 0.9792889356613159\n",
      "Iteration 40980 Loss: 0.8445688486099243\n",
      "Iteration 40981 Loss: 0.9857870936393738\n",
      "Iteration 40982 Loss: 1.0511143207550049\n",
      "Iteration 40983 Loss: 0.6302539110183716\n",
      "Iteration 40984 Loss: 1.0817387104034424\n",
      "Iteration 40985 Loss: 0.9294697642326355\n",
      "Iteration 40986 Loss: 0.8462520241737366\n",
      "Iteration 40987 Loss: 1.0018024444580078\n",
      "Iteration 40988 Loss: 1.1107889413833618\n",
      "Iteration 40989 Loss: 0.6681541800498962\n",
      "Iteration 40989 Loss: 0.9149929881095886\n",
      "Iteration 40990 Loss: 0.9274614453315735\n",
      "Iteration 40991 Loss: 0.9815373420715332\n",
      "Iteration 40992 Loss: 0.7462030053138733\n",
      "Iteration 40993 Loss: 0.7185349464416504\n",
      "Iteration 40994 Loss: 0.8361238837242126\n",
      "Iteration 40995 Loss: 0.9050695896148682\n",
      "Iteration 40996 Loss: 0.7708580493927002\n",
      "Iteration 40997 Loss: 0.9314125776290894\n",
      "Iteration 40998 Loss: 1.0527514219284058\n",
      "Iteration 40999 Loss: 0.9650058746337891\n",
      "Iteration 40999 Loss: 0.8834958076477051\n",
      "Iteration 41000 Loss: 0.6344954967498779\n",
      "Iteration 41001 Loss: 1.0797064304351807\n",
      "Iteration 41002 Loss: 0.8964129090309143\n",
      "Iteration 41003 Loss: 0.6708933115005493\n",
      "Iteration 41004 Loss: 1.0620381832122803\n",
      "Iteration 41005 Loss: 0.910929262638092\n",
      "Iteration 41006 Loss: 0.7653232216835022\n",
      "Iteration 41007 Loss: 1.0488386154174805\n",
      "Iteration 41008 Loss: 1.161400556564331\n",
      "Iteration 41009 Loss: 0.9367308020591736\n",
      "Iteration 41009 Loss: 0.9166768789291382\n",
      "Iteration 41010 Loss: 0.7473005652427673\n",
      "Iteration 41011 Loss: 0.7410442233085632\n",
      "Iteration 41012 Loss: 0.9177075624465942\n",
      "Iteration 41013 Loss: 0.9234709739685059\n",
      "Iteration 41014 Loss: 0.9278964400291443\n",
      "Iteration 41015 Loss: 1.113028883934021\n",
      "Iteration 41016 Loss: 0.9342291355133057\n",
      "Iteration 41017 Loss: 1.2169417142868042\n",
      "Iteration 41018 Loss: 0.9489514231681824\n",
      "Iteration 41019 Loss: 1.0746619701385498\n",
      "Iteration 41019 Loss: 0.9545233845710754\n",
      "Iteration 41020 Loss: 0.7491282224655151\n",
      "Iteration 41021 Loss: 0.7309222221374512\n",
      "Iteration 41022 Loss: 1.2391713857650757\n",
      "Iteration 41023 Loss: 0.8613146543502808\n",
      "Iteration 41024 Loss: 0.9226027131080627\n",
      "Iteration 41025 Loss: 0.47603949904441833\n",
      "Iteration 41026 Loss: 0.6999509334564209\n",
      "Iteration 41027 Loss: 0.9600403308868408\n",
      "Iteration 41028 Loss: 0.8733201622962952\n",
      "Iteration 41029 Loss: 0.7911397814750671\n",
      "Iteration 41029 Loss: 0.8303629755973816\n",
      "Iteration 41030 Loss: 1.285773754119873\n",
      "Iteration 41031 Loss: 1.0151618719100952\n",
      "Iteration 41032 Loss: 1.23307204246521\n",
      "Iteration 41033 Loss: 0.8615691661834717\n",
      "Iteration 41034 Loss: 0.8159723281860352\n",
      "Iteration 41035 Loss: 0.6812264919281006\n",
      "Iteration 41036 Loss: 0.7789594531059265\n",
      "Iteration 41037 Loss: 1.0530937910079956\n",
      "Iteration 41038 Loss: 0.8991148471832275\n",
      "Iteration 41039 Loss: 1.1860300302505493\n",
      "Iteration 41039 Loss: 0.9809974431991577\n",
      "Iteration 41040 Loss: 1.0654733180999756\n",
      "Iteration 41041 Loss: 0.9033700823783875\n",
      "Iteration 41042 Loss: 1.0420268774032593\n",
      "Iteration 41043 Loss: 1.1768676042556763\n",
      "Iteration 41044 Loss: 1.0601396560668945\n",
      "Iteration 41045 Loss: 1.1526414155960083\n",
      "Iteration 41046 Loss: 0.8164504170417786\n",
      "Iteration 41047 Loss: 1.2151751518249512\n",
      "Iteration 41048 Loss: 0.7610148787498474\n",
      "Iteration 41049 Loss: 1.0598427057266235\n",
      "Iteration 41049 Loss: 1.0253002643585205\n",
      "Iteration 41050 Loss: 0.7086247801780701\n",
      "Iteration 41051 Loss: 1.069866418838501\n",
      "Iteration 41052 Loss: 0.664248526096344\n",
      "Iteration 41053 Loss: 0.964843213558197\n",
      "Iteration 41054 Loss: 1.1496407985687256\n",
      "Iteration 41055 Loss: 0.9115915894508362\n",
      "Iteration 41056 Loss: 0.980687141418457\n",
      "Iteration 41057 Loss: 0.7088510990142822\n",
      "Iteration 41058 Loss: 1.1430035829544067\n",
      "Iteration 41059 Loss: 1.2016469240188599\n",
      "Iteration 41059 Loss: 0.950300395488739\n",
      "Iteration 41060 Loss: 1.233422040939331\n",
      "Iteration 41061 Loss: 0.9655766487121582\n",
      "Iteration 41062 Loss: 0.7586821913719177\n",
      "Iteration 41063 Loss: 0.7117523550987244\n",
      "Iteration 41064 Loss: 1.07673180103302\n",
      "Iteration 41065 Loss: 0.7394706606864929\n",
      "Iteration 41066 Loss: 0.8598918318748474\n",
      "Iteration 41067 Loss: 0.8593624830245972\n",
      "Iteration 41068 Loss: 0.9235990643501282\n",
      "Iteration 41069 Loss: 0.7341403365135193\n",
      "Iteration 41069 Loss: 0.8862628936767578\n",
      "Iteration 41070 Loss: 0.9198310375213623\n",
      "Iteration 41071 Loss: 1.0556156635284424\n",
      "Iteration 41072 Loss: 0.8628730177879333\n",
      "Iteration 41073 Loss: 1.0097110271453857\n",
      "Iteration 41074 Loss: 0.9699139595031738\n",
      "Iteration 41075 Loss: 1.149040699005127\n",
      "Iteration 41076 Loss: 0.7740572690963745\n",
      "Iteration 41077 Loss: 1.353935956954956\n",
      "Iteration 41078 Loss: 1.2380906343460083\n",
      "Iteration 41079 Loss: 1.1670218706130981\n",
      "Iteration 41079 Loss: 1.0500091314315796\n",
      "Iteration 41080 Loss: 1.0249699354171753\n",
      "Iteration 41081 Loss: 1.468326210975647\n",
      "Iteration 41082 Loss: 1.054561972618103\n",
      "Iteration 41083 Loss: 1.0899348258972168\n",
      "Iteration 41084 Loss: 0.9687408208847046\n",
      "Iteration 41085 Loss: 1.1565617322921753\n",
      "Iteration 41086 Loss: 1.241377592086792\n",
      "Iteration 41087 Loss: 1.1001181602478027\n",
      "Iteration 41088 Loss: 0.5529682636260986\n",
      "Iteration 41089 Loss: 0.8132723569869995\n",
      "Iteration 41089 Loss: 1.0470832586288452\n",
      "Iteration 41090 Loss: 0.7950082421302795\n",
      "Iteration 41091 Loss: 1.1537723541259766\n",
      "Iteration 41092 Loss: 1.0371334552764893\n",
      "Iteration 41093 Loss: 0.8220945000648499\n",
      "Iteration 41094 Loss: 1.0635162591934204\n",
      "Iteration 41095 Loss: 0.916825532913208\n",
      "Iteration 41096 Loss: 1.0116225481033325\n",
      "Iteration 41097 Loss: 1.0897129774093628\n",
      "Iteration 41098 Loss: 1.0191826820373535\n",
      "Iteration 41099 Loss: 1.3331607580184937\n",
      "Iteration 41099 Loss: 1.0242029428482056\n",
      "Iteration 41100 Loss: 0.8761887550354004\n",
      "Iteration 41101 Loss: 0.9627444744110107\n",
      "Iteration 41102 Loss: 0.7335993647575378\n",
      "Iteration 41103 Loss: 0.8121324181556702\n",
      "Iteration 41104 Loss: 0.9016844630241394\n",
      "Iteration 41105 Loss: 0.85734623670578\n",
      "Iteration 41106 Loss: 0.7591944932937622\n",
      "Iteration 41107 Loss: 1.1402850151062012\n",
      "Iteration 41108 Loss: 1.0688613653182983\n",
      "Iteration 41109 Loss: 0.8860431909561157\n",
      "Iteration 41109 Loss: 0.8998079299926758\n",
      "Iteration 41110 Loss: 0.8733152747154236\n",
      "Iteration 41111 Loss: 1.2493345737457275\n",
      "Iteration 41112 Loss: 1.104232907295227\n",
      "Iteration 41113 Loss: 1.1019165515899658\n",
      "Iteration 41114 Loss: 0.9739816188812256\n",
      "Iteration 41115 Loss: 0.8990723490715027\n",
      "Iteration 41116 Loss: 0.8215003609657288\n",
      "Iteration 41117 Loss: 1.2078688144683838\n",
      "Iteration 41118 Loss: 1.202998161315918\n",
      "Iteration 41119 Loss: 0.7233126163482666\n",
      "Iteration 41119 Loss: 1.0157533884048462\n",
      "Iteration 41120 Loss: 0.8137202858924866\n",
      "Iteration 41121 Loss: 0.7629197239875793\n",
      "Iteration 41122 Loss: 1.1413036584854126\n",
      "Iteration 41123 Loss: 0.824226438999176\n",
      "Iteration 41124 Loss: 0.9459140300750732\n",
      "Iteration 41125 Loss: 0.6824743747711182\n",
      "Iteration 41126 Loss: 0.7953795194625854\n",
      "Iteration 41127 Loss: 0.9181293845176697\n",
      "Iteration 41128 Loss: 1.0271251201629639\n",
      "Iteration 41129 Loss: 0.96855229139328\n",
      "Iteration 41129 Loss: 0.8879744410514832\n",
      "Iteration 41130 Loss: 0.8106003999710083\n",
      "Iteration 41131 Loss: 0.9447313547134399\n",
      "Iteration 41132 Loss: 0.5911588668823242\n",
      "Iteration 41133 Loss: 1.0757869482040405\n",
      "Iteration 41134 Loss: 1.1041460037231445\n",
      "Iteration 41135 Loss: 0.9325245022773743\n",
      "Iteration 41136 Loss: 1.0551223754882812\n",
      "Iteration 41137 Loss: 0.9022431969642639\n",
      "Iteration 41138 Loss: 0.9937759041786194\n",
      "Iteration 41139 Loss: 1.0009084939956665\n",
      "Iteration 41139 Loss: 0.9410998225212097\n",
      "Iteration 41140 Loss: 1.279756784439087\n",
      "Iteration 41141 Loss: 0.8014068603515625\n",
      "Iteration 41142 Loss: 1.0644160509109497\n",
      "Iteration 41143 Loss: 0.8676961660385132\n",
      "Iteration 41144 Loss: 1.0052001476287842\n",
      "Iteration 41145 Loss: 1.0433954000473022\n",
      "Iteration 41146 Loss: 1.219933271408081\n",
      "Iteration 41147 Loss: 0.9919333457946777\n",
      "Iteration 41148 Loss: 0.7004027366638184\n",
      "Iteration 41149 Loss: 0.6486170887947083\n",
      "Iteration 41149 Loss: 0.9622756838798523\n",
      "Iteration 41150 Loss: 0.7360427379608154\n",
      "Iteration 41151 Loss: 0.6858421564102173\n",
      "Iteration 41152 Loss: 0.907207727432251\n",
      "Iteration 41153 Loss: 0.7731679677963257\n",
      "Iteration 41154 Loss: 0.8378260731697083\n",
      "Iteration 41155 Loss: 0.8280820846557617\n",
      "Iteration 41156 Loss: 1.1940714120864868\n",
      "Iteration 41157 Loss: 0.8897899389266968\n",
      "Iteration 41158 Loss: 0.9569582343101501\n",
      "Iteration 41159 Loss: 0.8352550268173218\n",
      "Iteration 41159 Loss: 0.8644243478775024\n",
      "Iteration 41160 Loss: 1.1351492404937744\n",
      "Iteration 41161 Loss: 1.1178923845291138\n",
      "Iteration 41162 Loss: 1.1001477241516113\n",
      "Iteration 41163 Loss: 1.243147611618042\n",
      "Iteration 41164 Loss: 0.8645884990692139\n",
      "Iteration 41165 Loss: 0.7327746748924255\n",
      "Iteration 41166 Loss: 1.220076084136963\n",
      "Iteration 41167 Loss: 0.9098228812217712\n",
      "Iteration 41168 Loss: 1.1981145143508911\n",
      "Iteration 41169 Loss: 1.3391125202178955\n",
      "Iteration 41169 Loss: 1.0860826969146729\n",
      "Iteration 41170 Loss: 0.9876958727836609\n",
      "Iteration 41171 Loss: 0.8624826073646545\n",
      "Iteration 41172 Loss: 0.6713518500328064\n",
      "Iteration 41173 Loss: 0.9730255007743835\n",
      "Iteration 41174 Loss: 0.8385676741600037\n",
      "Iteration 41175 Loss: 0.9775046706199646\n",
      "Iteration 41176 Loss: 1.0245246887207031\n",
      "Iteration 41177 Loss: 0.6636253595352173\n",
      "Iteration 41178 Loss: 0.9519475102424622\n",
      "Iteration 41179 Loss: 0.6292924284934998\n",
      "Iteration 41179 Loss: 0.8580018281936646\n",
      "Iteration 41180 Loss: 0.7422997355461121\n",
      "Iteration 41181 Loss: 1.013824462890625\n",
      "Iteration 41182 Loss: 1.108276128768921\n",
      "Iteration 41183 Loss: 0.8094006180763245\n",
      "Iteration 41184 Loss: 1.0355677604675293\n",
      "Iteration 41185 Loss: 0.837620198726654\n",
      "Iteration 41186 Loss: 0.9545687437057495\n",
      "Iteration 41187 Loss: 0.8352393507957458\n",
      "Iteration 41188 Loss: 0.8737438321113586\n",
      "Iteration 41189 Loss: 0.8557655811309814\n",
      "Iteration 41189 Loss: 0.9066306352615356\n",
      "Iteration 41190 Loss: 1.22883939743042\n",
      "Iteration 41191 Loss: 0.7841347455978394\n",
      "Iteration 41192 Loss: 1.0775301456451416\n",
      "Iteration 41193 Loss: 1.0447255373001099\n",
      "Iteration 41194 Loss: 1.0943509340286255\n",
      "Iteration 41195 Loss: 1.1277539730072021\n",
      "Iteration 41196 Loss: 0.9311173558235168\n",
      "Iteration 41197 Loss: 1.0136715173721313\n",
      "Iteration 41198 Loss: 1.044569969177246\n",
      "Iteration 41199 Loss: 0.8901538848876953\n",
      "Iteration 41199 Loss: 1.0236847400665283\n",
      "Iteration 41200 Loss: 1.1835556030273438\n",
      "Iteration 41201 Loss: 1.363985538482666\n",
      "Iteration 41202 Loss: 1.1610708236694336\n",
      "Iteration 41203 Loss: 1.019649624824524\n",
      "Iteration 41204 Loss: 0.9531720876693726\n",
      "Iteration 41205 Loss: 0.8059297800064087\n",
      "Iteration 41206 Loss: 0.9265398383140564\n",
      "Iteration 41207 Loss: 1.0012707710266113\n",
      "Iteration 41208 Loss: 1.0047599077224731\n",
      "Iteration 41209 Loss: 0.904486894607544\n",
      "Iteration 41209 Loss: 1.0324420928955078\n",
      "Iteration 41210 Loss: 0.7156699895858765\n",
      "Iteration 41211 Loss: 0.9731486439704895\n",
      "Iteration 41212 Loss: 0.9853560328483582\n",
      "Iteration 41213 Loss: 1.0639054775238037\n",
      "Iteration 41214 Loss: 0.7808099389076233\n",
      "Iteration 41215 Loss: 1.1984782218933105\n",
      "Iteration 41216 Loss: 0.9571350812911987\n",
      "Iteration 41217 Loss: 0.7540823221206665\n",
      "Iteration 41218 Loss: 1.123404860496521\n",
      "Iteration 41219 Loss: 1.3451206684112549\n",
      "Iteration 41219 Loss: 0.9897111654281616\n",
      "Iteration 41220 Loss: 1.1106557846069336\n",
      "Iteration 41221 Loss: 0.8581810593605042\n",
      "Iteration 41222 Loss: 0.9024105072021484\n",
      "Iteration 41223 Loss: 0.7907308340072632\n",
      "Iteration 41224 Loss: 0.9094937443733215\n",
      "Iteration 41225 Loss: 0.7017897367477417\n",
      "Iteration 41226 Loss: 1.2151196002960205\n",
      "Iteration 41227 Loss: 0.8549591898918152\n",
      "Iteration 41228 Loss: 0.862076997756958\n",
      "Iteration 41229 Loss: 1.0479109287261963\n",
      "Iteration 41229 Loss: 0.9253329038619995\n",
      "Iteration 41230 Loss: 0.7471404671669006\n",
      "Iteration 41231 Loss: 1.1469719409942627\n",
      "Iteration 41232 Loss: 0.9179584383964539\n",
      "Iteration 41233 Loss: 0.5843674540519714\n",
      "Iteration 41234 Loss: 0.7785603404045105\n",
      "Iteration 41235 Loss: 0.7698031067848206\n",
      "Iteration 41236 Loss: 1.034919023513794\n",
      "Iteration 41237 Loss: 1.0299663543701172\n",
      "Iteration 41238 Loss: 1.2314561605453491\n",
      "Iteration 41239 Loss: 0.7108147144317627\n",
      "Iteration 41239 Loss: 0.8951957821846008\n",
      "Iteration 41240 Loss: 1.052951693534851\n",
      "Iteration 41241 Loss: 1.008165955543518\n",
      "Iteration 41242 Loss: 0.8534743785858154\n",
      "Iteration 41243 Loss: 0.8876330256462097\n",
      "Iteration 41244 Loss: 1.0379667282104492\n",
      "Iteration 41245 Loss: 1.471633791923523\n",
      "Iteration 41246 Loss: 0.8801367282867432\n",
      "Iteration 41247 Loss: 1.0038225650787354\n",
      "Iteration 41248 Loss: 0.7189779877662659\n",
      "Iteration 41249 Loss: 0.8811716437339783\n",
      "Iteration 41249 Loss: 0.9795934557914734\n",
      "Iteration 41250 Loss: 1.0477386713027954\n",
      "Iteration 41251 Loss: 1.1720654964447021\n",
      "Iteration 41252 Loss: 1.273490309715271\n",
      "Iteration 41253 Loss: 1.2095494270324707\n",
      "Iteration 41254 Loss: 1.106029748916626\n",
      "Iteration 41255 Loss: 1.0425482988357544\n",
      "Iteration 41256 Loss: 1.1401430368423462\n",
      "Iteration 41257 Loss: 0.9039613604545593\n",
      "Iteration 41258 Loss: 0.8756833672523499\n",
      "Iteration 41259 Loss: 1.3631870746612549\n",
      "Iteration 41259 Loss: 1.113439679145813\n",
      "Iteration 41260 Loss: 0.8678871989250183\n",
      "Iteration 41261 Loss: 1.124497413635254\n",
      "Iteration 41262 Loss: 1.025935173034668\n",
      "Iteration 41263 Loss: 0.8517946600914001\n",
      "Iteration 41264 Loss: 0.9729242324829102\n",
      "Iteration 41265 Loss: 0.8219414353370667\n",
      "Iteration 41266 Loss: 0.9247966408729553\n",
      "Iteration 41267 Loss: 1.0348037481307983\n",
      "Iteration 41268 Loss: 1.0307945013046265\n",
      "Iteration 41269 Loss: 0.7069876790046692\n",
      "Iteration 41269 Loss: 0.9362362623214722\n",
      "Iteration 41270 Loss: 0.8475340604782104\n",
      "Iteration 41271 Loss: 0.8147982358932495\n",
      "Iteration 41272 Loss: 0.8992236256599426\n",
      "Iteration 41273 Loss: 0.8722088932991028\n",
      "Iteration 41274 Loss: 0.4737061858177185\n",
      "Iteration 41275 Loss: 0.7745105624198914\n",
      "Iteration 41276 Loss: 0.6516647934913635\n",
      "Iteration 41277 Loss: 0.7314338684082031\n",
      "Iteration 41278 Loss: 1.032820463180542\n",
      "Iteration 41279 Loss: 0.7947816848754883\n",
      "Iteration 41279 Loss: 0.7892682552337646\n",
      "Iteration 41280 Loss: 0.9517205357551575\n",
      "Iteration 41281 Loss: 0.7962343096733093\n",
      "Iteration 41282 Loss: 1.2121449708938599\n",
      "Iteration 41283 Loss: 0.821458637714386\n",
      "Iteration 41284 Loss: 0.9410195350646973\n",
      "Iteration 41285 Loss: 0.8441905379295349\n",
      "Iteration 41286 Loss: 1.023909330368042\n",
      "Iteration 41287 Loss: 0.8538718819618225\n",
      "Iteration 41288 Loss: 1.0649991035461426\n",
      "Iteration 41289 Loss: 0.9964374899864197\n",
      "Iteration 41289 Loss: 0.9505987167358398\n",
      "Iteration 41290 Loss: 1.1385505199432373\n",
      "Iteration 41291 Loss: 0.7207198739051819\n",
      "Iteration 41292 Loss: 0.8738272786140442\n",
      "Iteration 41293 Loss: 1.2939683198928833\n",
      "Iteration 41294 Loss: 0.7352738380432129\n",
      "Iteration 41295 Loss: 1.1107829809188843\n",
      "Iteration 41296 Loss: 0.9521111249923706\n",
      "Iteration 41297 Loss: 1.2724239826202393\n",
      "Iteration 41298 Loss: 0.878420889377594\n",
      "Iteration 41299 Loss: 0.9130497574806213\n",
      "Iteration 41299 Loss: 0.9889128804206848\n",
      "Iteration 41300 Loss: 0.8371593356132507\n",
      "Iteration 41301 Loss: 1.0368813276290894\n",
      "Iteration 41302 Loss: 1.2314151525497437\n",
      "Iteration 41303 Loss: 1.088340401649475\n",
      "Iteration 41304 Loss: 1.2697498798370361\n",
      "Iteration 41305 Loss: 0.8924009203910828\n",
      "Iteration 41306 Loss: 1.246795415878296\n",
      "Iteration 41307 Loss: 1.1524205207824707\n",
      "Iteration 41308 Loss: 1.0742876529693604\n",
      "Iteration 41309 Loss: 1.056115984916687\n",
      "Iteration 41309 Loss: 1.0885566473007202\n",
      "Iteration 41310 Loss: 1.0309603214263916\n",
      "Iteration 41311 Loss: 0.7389621138572693\n",
      "Iteration 41312 Loss: 1.0566279888153076\n",
      "Iteration 41313 Loss: 0.9227570295333862\n",
      "Iteration 41314 Loss: 1.0910725593566895\n",
      "Iteration 41315 Loss: 0.8922701478004456\n",
      "Iteration 41316 Loss: 0.9212263226509094\n",
      "Iteration 41317 Loss: 1.0059863328933716\n",
      "Iteration 41318 Loss: 0.9163634777069092\n",
      "Iteration 41319 Loss: 0.9182066321372986\n",
      "Iteration 41319 Loss: 0.9494432210922241\n",
      "Iteration 41320 Loss: 1.1100229024887085\n",
      "Iteration 41321 Loss: 1.1091949939727783\n",
      "Iteration 41322 Loss: 1.0148582458496094\n",
      "Iteration 41323 Loss: 0.7601884603500366\n",
      "Iteration 41324 Loss: 1.0392959117889404\n",
      "Iteration 41325 Loss: 1.1123794317245483\n",
      "Iteration 41326 Loss: 1.0403326749801636\n",
      "Iteration 41327 Loss: 1.1539208889007568\n",
      "Iteration 41328 Loss: 1.1738839149475098\n",
      "Iteration 41329 Loss: 1.0099074840545654\n",
      "Iteration 41329 Loss: 1.052398443222046\n",
      "Iteration 41330 Loss: 0.9322489500045776\n",
      "Iteration 41331 Loss: 1.3383225202560425\n",
      "Iteration 41332 Loss: 0.8721008896827698\n",
      "Iteration 41333 Loss: 0.7869082689285278\n",
      "Iteration 41334 Loss: 0.9524354338645935\n",
      "Iteration 41335 Loss: 0.8204836249351501\n",
      "Iteration 41336 Loss: 0.9596984386444092\n",
      "Iteration 41337 Loss: 0.7892205715179443\n",
      "Iteration 41338 Loss: 0.9586018323898315\n",
      "Iteration 41339 Loss: 1.5422041416168213\n",
      "Iteration 41339 Loss: 0.9952224493026733\n",
      "Iteration 41340 Loss: 0.6607888340950012\n",
      "Iteration 41341 Loss: 0.9437963962554932\n",
      "Iteration 41342 Loss: 1.2644009590148926\n",
      "Iteration 41343 Loss: 1.2977968454360962\n",
      "Iteration 41344 Loss: 0.8896109461784363\n",
      "Iteration 41345 Loss: 1.1796027421951294\n",
      "Iteration 41346 Loss: 1.2595254182815552\n",
      "Iteration 41347 Loss: 0.9677006602287292\n",
      "Iteration 41348 Loss: 0.8325759768486023\n",
      "Iteration 41349 Loss: 1.170529842376709\n",
      "Iteration 41349 Loss: 1.0466328859329224\n",
      "Iteration 41350 Loss: 0.9487267136573792\n",
      "Iteration 41351 Loss: 1.2697850465774536\n",
      "Iteration 41352 Loss: 0.6722991466522217\n",
      "Iteration 41353 Loss: 1.028260588645935\n",
      "Iteration 41354 Loss: 1.050117015838623\n",
      "Iteration 41355 Loss: 0.9076647162437439\n",
      "Iteration 41356 Loss: 1.1288845539093018\n",
      "Iteration 41357 Loss: 1.1140480041503906\n",
      "Iteration 41358 Loss: 1.2681552171707153\n",
      "Iteration 41359 Loss: 1.054368257522583\n",
      "Iteration 41359 Loss: 1.0442308187484741\n",
      "Iteration 41360 Loss: 1.2778598070144653\n",
      "Iteration 41361 Loss: 1.2487562894821167\n",
      "Iteration 41362 Loss: 1.0751811265945435\n",
      "Iteration 41363 Loss: 1.2895506620407104\n",
      "Iteration 41364 Loss: 1.1666995286941528\n",
      "Iteration 41365 Loss: 0.7913376092910767\n",
      "Iteration 41366 Loss: 1.2769880294799805\n",
      "Iteration 41367 Loss: 0.9458202719688416\n",
      "Iteration 41368 Loss: 1.2871371507644653\n",
      "Iteration 41369 Loss: 1.0384438037872314\n",
      "Iteration 41369 Loss: 1.139777421951294\n",
      "Iteration 41370 Loss: 1.17887544631958\n",
      "Iteration 41371 Loss: 1.086870551109314\n",
      "Iteration 41372 Loss: 0.9950651526451111\n",
      "Iteration 41373 Loss: 0.846781849861145\n",
      "Iteration 41374 Loss: 0.8147081732749939\n",
      "Iteration 41375 Loss: 0.8612145185470581\n",
      "Iteration 41376 Loss: 0.7957044243812561\n",
      "Iteration 41377 Loss: 1.1210105419158936\n",
      "Iteration 41378 Loss: 0.8252255320549011\n",
      "Iteration 41379 Loss: 0.9893655180931091\n",
      "Iteration 41379 Loss: 0.9514821767807007\n",
      "Iteration 41380 Loss: 0.9498447775840759\n",
      "Iteration 41381 Loss: 0.9716687798500061\n",
      "Iteration 41382 Loss: 1.114148497581482\n",
      "Iteration 41383 Loss: 1.0369449853897095\n",
      "Iteration 41384 Loss: 0.9492236971855164\n",
      "Iteration 41385 Loss: 0.7492421269416809\n",
      "Iteration 41386 Loss: 0.9020412564277649\n",
      "Iteration 41387 Loss: 0.9296820759773254\n",
      "Iteration 41388 Loss: 0.6120061874389648\n",
      "Iteration 41389 Loss: 0.9603945016860962\n",
      "Iteration 41389 Loss: 0.9175196886062622\n",
      "Iteration 41390 Loss: 0.974556028842926\n",
      "Iteration 41391 Loss: 1.0659594535827637\n",
      "Iteration 41392 Loss: 0.8179625272750854\n",
      "Iteration 41393 Loss: 0.9157882332801819\n",
      "Iteration 41394 Loss: 0.9470292329788208\n",
      "Iteration 41395 Loss: 0.8913126587867737\n",
      "Iteration 41396 Loss: 0.8418824076652527\n",
      "Iteration 41397 Loss: 0.865473210811615\n",
      "Iteration 41398 Loss: 1.1503790616989136\n",
      "Iteration 41399 Loss: 0.852197527885437\n",
      "Iteration 41399 Loss: 0.9322539567947388\n",
      "Iteration 41400 Loss: 0.8769643902778625\n",
      "Iteration 41401 Loss: 1.2618348598480225\n",
      "Iteration 41402 Loss: 0.8800357580184937\n",
      "Iteration 41403 Loss: 1.0662373304367065\n",
      "Iteration 41404 Loss: 0.8223971128463745\n",
      "Iteration 41405 Loss: 0.8288809061050415\n",
      "Iteration 41406 Loss: 1.1079097986221313\n",
      "Iteration 41407 Loss: 0.8440578579902649\n",
      "Iteration 41408 Loss: 1.1779969930648804\n",
      "Iteration 41409 Loss: 0.9869110584259033\n",
      "Iteration 41409 Loss: 0.9853226542472839\n",
      "Iteration 41410 Loss: 0.9371424317359924\n",
      "Iteration 41411 Loss: 0.9309029579162598\n",
      "Iteration 41412 Loss: 1.1439179182052612\n",
      "Iteration 41413 Loss: 0.8127942681312561\n",
      "Iteration 41414 Loss: 1.001419186592102\n",
      "Iteration 41415 Loss: 0.9489623308181763\n",
      "Iteration 41416 Loss: 0.8135372400283813\n",
      "Iteration 41417 Loss: 0.8081592917442322\n",
      "Iteration 41418 Loss: 0.8863969445228577\n",
      "Iteration 41419 Loss: 1.1004711389541626\n",
      "Iteration 41419 Loss: 0.9383703470230103\n",
      "Iteration 41420 Loss: 1.069012999534607\n",
      "Iteration 41421 Loss: 0.8031827807426453\n",
      "Iteration 41422 Loss: 0.8633209466934204\n",
      "Iteration 41423 Loss: 0.7981778979301453\n",
      "Iteration 41424 Loss: 0.8018634915351868\n",
      "Iteration 41425 Loss: 0.9559102654457092\n",
      "Iteration 41426 Loss: 1.1521296501159668\n",
      "Iteration 41427 Loss: 0.5878817439079285\n",
      "Iteration 41428 Loss: 0.8405545949935913\n",
      "Iteration 41429 Loss: 1.0476782321929932\n",
      "Iteration 41429 Loss: 0.8919712901115417\n",
      "Iteration 41430 Loss: 0.8824465274810791\n",
      "Iteration 41431 Loss: 0.9197873473167419\n",
      "Iteration 41432 Loss: 1.1429510116577148\n",
      "Iteration 41433 Loss: 1.0340776443481445\n",
      "Iteration 41434 Loss: 0.9700134992599487\n",
      "Iteration 41435 Loss: 0.9620975255966187\n",
      "Iteration 41436 Loss: 1.1940747499465942\n",
      "Iteration 41437 Loss: 1.2413676977157593\n",
      "Iteration 41438 Loss: 0.8480202555656433\n",
      "Iteration 41439 Loss: 0.896298885345459\n",
      "Iteration 41439 Loss: 1.0091135501861572\n",
      "Iteration 41440 Loss: 0.8762367963790894\n",
      "Iteration 41441 Loss: 0.8817495703697205\n",
      "Iteration 41442 Loss: 0.6743531823158264\n",
      "Iteration 41443 Loss: 1.0553386211395264\n",
      "Iteration 41444 Loss: 1.0072088241577148\n",
      "Iteration 41445 Loss: 0.803335428237915\n",
      "Iteration 41446 Loss: 1.2629494667053223\n",
      "Iteration 41447 Loss: 0.78171306848526\n",
      "Iteration 41448 Loss: 0.9349808692932129\n",
      "Iteration 41449 Loss: 1.016589879989624\n",
      "Iteration 41449 Loss: 0.9294456243515015\n",
      "Iteration 41450 Loss: 1.129031777381897\n",
      "Iteration 41451 Loss: 1.1966633796691895\n",
      "Iteration 41452 Loss: 0.9711626768112183\n",
      "Iteration 41453 Loss: 0.9961079955101013\n",
      "Iteration 41454 Loss: 0.9662086963653564\n",
      "Iteration 41455 Loss: 0.7316544055938721\n",
      "Iteration 41456 Loss: 1.1733506917953491\n",
      "Iteration 41457 Loss: 0.919105589389801\n",
      "Iteration 41458 Loss: 0.9828985929489136\n",
      "Iteration 41459 Loss: 0.992914617061615\n",
      "Iteration 41459 Loss: 1.00590980052948\n",
      "Iteration 41460 Loss: 1.024433970451355\n",
      "Iteration 41461 Loss: 1.04874849319458\n",
      "Iteration 41462 Loss: 0.8824800252914429\n",
      "Iteration 41463 Loss: 0.7026984095573425\n",
      "Iteration 41464 Loss: 1.1871984004974365\n",
      "Iteration 41465 Loss: 0.9618207812309265\n",
      "Iteration 41466 Loss: 1.2959673404693604\n",
      "Iteration 41467 Loss: 0.8152617812156677\n",
      "Iteration 41468 Loss: 0.9529701471328735\n",
      "Iteration 41469 Loss: 0.7878599762916565\n",
      "Iteration 41469 Loss: 0.9659439325332642\n",
      "Iteration 41470 Loss: 0.9661346077919006\n",
      "Iteration 41471 Loss: 0.9238783121109009\n",
      "Iteration 41472 Loss: 0.7988945245742798\n",
      "Iteration 41473 Loss: 1.0554718971252441\n",
      "Iteration 41474 Loss: 1.0027297735214233\n",
      "Iteration 41475 Loss: 0.566324770450592\n",
      "Iteration 41476 Loss: 0.9453643560409546\n",
      "Iteration 41477 Loss: 1.3274787664413452\n",
      "Iteration 41478 Loss: 0.8240621089935303\n",
      "Iteration 41479 Loss: 1.0981159210205078\n",
      "Iteration 41479 Loss: 0.9508454203605652\n",
      "Iteration 41480 Loss: 0.8850682973861694\n",
      "Iteration 41481 Loss: 1.1406211853027344\n",
      "Iteration 41482 Loss: 0.9987323880195618\n",
      "Iteration 41483 Loss: 0.8331162333488464\n",
      "Iteration 41484 Loss: 1.037899136543274\n",
      "Iteration 41485 Loss: 1.0207833051681519\n",
      "Iteration 41486 Loss: 0.9949766993522644\n",
      "Iteration 41487 Loss: 1.0867605209350586\n",
      "Iteration 41488 Loss: 0.8772731423377991\n",
      "Iteration 41489 Loss: 1.0406608581542969\n",
      "Iteration 41489 Loss: 0.9915891885757446\n",
      "Iteration 41490 Loss: 0.7594372630119324\n",
      "Iteration 41491 Loss: 0.9956727027893066\n",
      "Iteration 41492 Loss: 0.9043961763381958\n",
      "Iteration 41493 Loss: 1.4982259273529053\n",
      "Iteration 41494 Loss: 0.9360516667366028\n",
      "Iteration 41495 Loss: 0.7340824604034424\n",
      "Iteration 41496 Loss: 0.8431776165962219\n",
      "Iteration 41497 Loss: 0.8879809975624084\n",
      "Iteration 41498 Loss: 1.2196038961410522\n",
      "Iteration 41499 Loss: 0.8546754717826843\n",
      "Iteration 41499 Loss: 0.9633305668830872\n",
      "Iteration 41500 Loss: 0.6898280382156372\n",
      "Iteration 41501 Loss: 0.9955265522003174\n",
      "Iteration 41502 Loss: 1.2281581163406372\n",
      "Iteration 41503 Loss: 1.231751799583435\n",
      "Iteration 41504 Loss: 0.7464160919189453\n",
      "Iteration 41505 Loss: 0.7410774230957031\n",
      "Iteration 41506 Loss: 1.0879497528076172\n",
      "Iteration 41507 Loss: 0.8032630085945129\n",
      "Iteration 41508 Loss: 0.9408977627754211\n",
      "Iteration 41509 Loss: 1.1158381700515747\n",
      "Iteration 41509 Loss: 0.9580706357955933\n",
      "Iteration 41510 Loss: 1.042625904083252\n",
      "Iteration 41511 Loss: 0.6723965406417847\n",
      "Iteration 41512 Loss: 1.2534959316253662\n",
      "Iteration 41513 Loss: 0.8801194429397583\n",
      "Iteration 41514 Loss: 0.8628808259963989\n",
      "Iteration 41515 Loss: 1.3478455543518066\n",
      "Iteration 41516 Loss: 0.8787731528282166\n",
      "Iteration 41517 Loss: 1.169358491897583\n",
      "Iteration 41518 Loss: 1.0633530616760254\n",
      "Iteration 41519 Loss: 1.0136692523956299\n",
      "Iteration 41519 Loss: 1.0184518098831177\n",
      "Iteration 41520 Loss: 0.8798971772193909\n",
      "Iteration 41521 Loss: 1.405510425567627\n",
      "Iteration 41522 Loss: 0.727174699306488\n",
      "Iteration 41523 Loss: 1.1845366954803467\n",
      "Iteration 41524 Loss: 0.9724389910697937\n",
      "Iteration 41525 Loss: 1.006583333015442\n",
      "Iteration 41526 Loss: 0.671192467212677\n",
      "Iteration 41527 Loss: 0.7810263633728027\n",
      "Iteration 41528 Loss: 0.8172475695610046\n",
      "Iteration 41529 Loss: 0.7012870907783508\n",
      "Iteration 41529 Loss: 0.9146894216537476\n",
      "Iteration 41530 Loss: 0.804416298866272\n",
      "Iteration 41531 Loss: 0.9365662336349487\n",
      "Iteration 41532 Loss: 0.9551112055778503\n",
      "Iteration 41533 Loss: 0.8159680366516113\n",
      "Iteration 41534 Loss: 0.828901469707489\n",
      "Iteration 41535 Loss: 0.8727327585220337\n",
      "Iteration 41536 Loss: 0.8363207578659058\n",
      "Iteration 41537 Loss: 0.8426185846328735\n",
      "Iteration 41538 Loss: 0.8394172787666321\n",
      "Iteration 41539 Loss: 1.087949275970459\n",
      "Iteration 41539 Loss: 0.8820001482963562\n",
      "Iteration 41540 Loss: 0.9538834095001221\n",
      "Iteration 41541 Loss: 1.3427375555038452\n",
      "Iteration 41542 Loss: 0.9074419736862183\n",
      "Iteration 41543 Loss: 1.3207247257232666\n",
      "Iteration 41544 Loss: 0.973040759563446\n",
      "Iteration 41545 Loss: 0.9049797654151917\n",
      "Iteration 41546 Loss: 1.0285776853561401\n",
      "Iteration 41547 Loss: 0.796186089515686\n",
      "Iteration 41548 Loss: 0.9729017615318298\n",
      "Iteration 41549 Loss: 0.9386581182479858\n",
      "Iteration 41549 Loss: 1.0139132738113403\n",
      "Iteration 41550 Loss: 1.006752371788025\n",
      "Iteration 41551 Loss: 0.9936745762825012\n",
      "Iteration 41552 Loss: 0.7003204822540283\n",
      "Iteration 41553 Loss: 0.9976660013198853\n",
      "Iteration 41554 Loss: 1.0113215446472168\n",
      "Iteration 41555 Loss: 0.900475025177002\n",
      "Iteration 41556 Loss: 1.0531220436096191\n",
      "Iteration 41557 Loss: 0.8485264778137207\n",
      "Iteration 41558 Loss: 1.327338457107544\n",
      "Iteration 41559 Loss: 0.5445376038551331\n",
      "Iteration 41559 Loss: 0.9383733868598938\n",
      "Iteration 41560 Loss: 0.9257590770721436\n",
      "Iteration 41561 Loss: 0.6738110184669495\n",
      "Iteration 41562 Loss: 1.0080705881118774\n",
      "Iteration 41563 Loss: 0.8924317359924316\n",
      "Iteration 41564 Loss: 0.6257843971252441\n",
      "Iteration 41565 Loss: 0.7923331260681152\n",
      "Iteration 41566 Loss: 1.0394022464752197\n",
      "Iteration 41567 Loss: 0.9965779185295105\n",
      "Iteration 41568 Loss: 1.094148874282837\n",
      "Iteration 41569 Loss: 0.9149372577667236\n",
      "Iteration 41569 Loss: 0.8963257074356079\n",
      "Iteration 41570 Loss: 1.0533556938171387\n",
      "Iteration 41571 Loss: 1.1368236541748047\n",
      "Iteration 41572 Loss: 0.8571406602859497\n",
      "Iteration 41573 Loss: 0.8468595147132874\n",
      "Iteration 41574 Loss: 0.9106706380844116\n",
      "Iteration 41575 Loss: 0.8486345410346985\n",
      "Iteration 41576 Loss: 0.8078251481056213\n",
      "Iteration 41577 Loss: 1.1007741689682007\n",
      "Iteration 41578 Loss: 1.2422515153884888\n",
      "Iteration 41579 Loss: 0.806908369064331\n",
      "Iteration 41579 Loss: 0.9611244201660156\n",
      "Iteration 41580 Loss: 1.2178205251693726\n",
      "Iteration 41581 Loss: 0.9458349347114563\n",
      "Iteration 41582 Loss: 1.0259428024291992\n",
      "Iteration 41583 Loss: 1.1782665252685547\n",
      "Iteration 41584 Loss: 0.9684208631515503\n",
      "Iteration 41585 Loss: 0.9032496809959412\n",
      "Iteration 41586 Loss: 0.5825230479240417\n",
      "Iteration 41587 Loss: 0.612701952457428\n",
      "Iteration 41588 Loss: 0.9445561170578003\n",
      "Iteration 41589 Loss: 1.0459637641906738\n",
      "Iteration 41589 Loss: 0.9425280690193176\n",
      "Iteration 41590 Loss: 0.8914312124252319\n",
      "Iteration 41591 Loss: 0.6028677821159363\n",
      "Iteration 41592 Loss: 0.9423297643661499\n",
      "Iteration 41593 Loss: 1.326643705368042\n",
      "Iteration 41594 Loss: 0.8992270231246948\n",
      "Iteration 41595 Loss: 1.0991581678390503\n",
      "Iteration 41596 Loss: 0.8183759450912476\n",
      "Iteration 41597 Loss: 0.9902685880661011\n",
      "Iteration 41598 Loss: 0.9108762741088867\n",
      "Iteration 41599 Loss: 0.939802885055542\n",
      "Iteration 41599 Loss: 0.9420981407165527\n",
      "Iteration 41600 Loss: 0.8801708221435547\n",
      "Iteration 41601 Loss: 1.1815450191497803\n",
      "Iteration 41602 Loss: 1.0612013339996338\n",
      "Iteration 41603 Loss: 1.1568081378936768\n",
      "Iteration 41604 Loss: 0.9630988240242004\n",
      "Iteration 41605 Loss: 0.7970302700996399\n",
      "Iteration 41606 Loss: 0.8474317193031311\n",
      "Iteration 41607 Loss: 1.0249028205871582\n",
      "Iteration 41608 Loss: 0.8953520655632019\n",
      "Iteration 41609 Loss: 0.8479341268539429\n",
      "Iteration 41609 Loss: 0.9655475616455078\n",
      "Iteration 41610 Loss: 1.3355247974395752\n",
      "Iteration 41611 Loss: 0.7939668297767639\n",
      "Iteration 41612 Loss: 0.7505002617835999\n",
      "Iteration 41613 Loss: 0.8640435338020325\n",
      "Iteration 41614 Loss: 0.9867780804634094\n",
      "Iteration 41615 Loss: 0.913223385810852\n",
      "Iteration 41616 Loss: 0.9288290143013\n",
      "Iteration 41617 Loss: 0.8143979907035828\n",
      "Iteration 41618 Loss: 0.9165654182434082\n",
      "Iteration 41619 Loss: 0.8352870345115662\n",
      "Iteration 41619 Loss: 0.9139116406440735\n",
      "Iteration 41620 Loss: 0.8943909406661987\n",
      "Iteration 41621 Loss: 0.8624178767204285\n",
      "Iteration 41622 Loss: 1.049232006072998\n",
      "Iteration 41623 Loss: 0.9123589396476746\n",
      "Iteration 41624 Loss: 0.9351962208747864\n",
      "Iteration 41625 Loss: 0.8573560118675232\n",
      "Iteration 41626 Loss: 1.218183994293213\n",
      "Iteration 41627 Loss: 0.7811055183410645\n",
      "Iteration 41628 Loss: 0.6252214908599854\n",
      "Iteration 41629 Loss: 0.8216344714164734\n",
      "Iteration 41629 Loss: 0.8957098126411438\n",
      "Iteration 41630 Loss: 0.9692667126655579\n",
      "Iteration 41631 Loss: 0.812699019908905\n",
      "Iteration 41632 Loss: 0.9975186586380005\n",
      "Iteration 41633 Loss: 0.971564531326294\n",
      "Iteration 41634 Loss: 0.7653924226760864\n",
      "Iteration 41635 Loss: 0.97831130027771\n",
      "Iteration 41636 Loss: 0.9329090714454651\n",
      "Iteration 41637 Loss: 0.853701651096344\n",
      "Iteration 41638 Loss: 0.987113893032074\n",
      "Iteration 41639 Loss: 0.8884175419807434\n",
      "Iteration 41639 Loss: 0.9156894683837891\n",
      "Iteration 41640 Loss: 0.996338963508606\n",
      "Iteration 41641 Loss: 0.6863928437232971\n",
      "Iteration 41642 Loss: 0.9711832404136658\n",
      "Iteration 41643 Loss: 1.038002371788025\n",
      "Iteration 41644 Loss: 0.7802231907844543\n",
      "Iteration 41645 Loss: 0.7698628902435303\n",
      "Iteration 41646 Loss: 0.5929522514343262\n",
      "Iteration 41647 Loss: 1.278092861175537\n",
      "Iteration 41648 Loss: 1.2966886758804321\n",
      "Iteration 41649 Loss: 1.0749902725219727\n",
      "Iteration 41649 Loss: 0.948472797870636\n",
      "Iteration 41650 Loss: 0.9239470362663269\n",
      "Iteration 41651 Loss: 0.9433674812316895\n",
      "Iteration 41652 Loss: 0.9267351031303406\n",
      "Iteration 41653 Loss: 0.881580650806427\n",
      "Iteration 41654 Loss: 0.8471215963363647\n",
      "Iteration 41655 Loss: 0.7679089307785034\n",
      "Iteration 41656 Loss: 0.8718248605728149\n",
      "Iteration 41657 Loss: 1.2037427425384521\n",
      "Iteration 41658 Loss: 0.932218074798584\n",
      "Iteration 41659 Loss: 0.8747957944869995\n",
      "Iteration 41659 Loss: 0.9173242449760437\n",
      "Iteration 41660 Loss: 1.025580644607544\n",
      "Iteration 41661 Loss: 0.8193833231925964\n",
      "Iteration 41662 Loss: 0.9100956916809082\n",
      "Iteration 41663 Loss: 1.2145094871520996\n",
      "Iteration 41664 Loss: 1.3904120922088623\n",
      "Iteration 41665 Loss: 0.8638529777526855\n",
      "Iteration 41666 Loss: 0.9623555541038513\n",
      "Iteration 41667 Loss: 0.8921599984169006\n",
      "Iteration 41668 Loss: 1.0310285091400146\n",
      "Iteration 41669 Loss: 1.0234249830245972\n",
      "Iteration 41669 Loss: 1.0132803916931152\n",
      "Iteration 41670 Loss: 0.9948540329933167\n",
      "Iteration 41671 Loss: 0.9116291403770447\n",
      "Iteration 41672 Loss: 1.2166516780853271\n",
      "Iteration 41673 Loss: 0.9476280808448792\n",
      "Iteration 41674 Loss: 1.173698902130127\n",
      "Iteration 41675 Loss: 1.1487373113632202\n",
      "Iteration 41676 Loss: 0.9514082074165344\n",
      "Iteration 41677 Loss: 0.9369285106658936\n",
      "Iteration 41678 Loss: 1.0470975637435913\n",
      "Iteration 41679 Loss: 0.8400922417640686\n",
      "Iteration 41679 Loss: 1.0168726444244385\n",
      "Iteration 41680 Loss: 0.8097543716430664\n",
      "Iteration 41681 Loss: 1.009233832359314\n",
      "Iteration 41682 Loss: 0.882249116897583\n",
      "Iteration 41683 Loss: 0.7744934558868408\n",
      "Iteration 41684 Loss: 1.032364845275879\n",
      "Iteration 41685 Loss: 0.8435964584350586\n",
      "Iteration 41686 Loss: 0.8332259654998779\n",
      "Iteration 41687 Loss: 1.0861427783966064\n",
      "Iteration 41688 Loss: 0.8635949492454529\n",
      "Iteration 41689 Loss: 0.9702670574188232\n",
      "Iteration 41689 Loss: 0.9104922413825989\n",
      "Iteration 41690 Loss: 1.2456568479537964\n",
      "Iteration 41691 Loss: 1.1013054847717285\n",
      "Iteration 41692 Loss: 0.8575834035873413\n",
      "Iteration 41693 Loss: 0.8861582279205322\n",
      "Iteration 41694 Loss: 0.9372785091400146\n",
      "Iteration 41695 Loss: 0.627112627029419\n",
      "Iteration 41696 Loss: 0.9705789089202881\n",
      "Iteration 41697 Loss: 1.0309087038040161\n",
      "Iteration 41698 Loss: 0.8535420298576355\n",
      "Iteration 41699 Loss: 0.8531551361083984\n",
      "Iteration 41699 Loss: 0.9363280534744263\n",
      "Iteration 41700 Loss: 1.09955894947052\n",
      "Iteration 41701 Loss: 0.6896679401397705\n",
      "Iteration 41702 Loss: 1.0588902235031128\n",
      "Iteration 41703 Loss: 0.9030541181564331\n",
      "Iteration 41704 Loss: 1.1295084953308105\n",
      "Iteration 41705 Loss: 0.7255229353904724\n",
      "Iteration 41706 Loss: 1.1682302951812744\n",
      "Iteration 41707 Loss: 1.0553579330444336\n",
      "Iteration 41708 Loss: 0.8735866546630859\n",
      "Iteration 41709 Loss: 0.9468607306480408\n",
      "Iteration 41709 Loss: 0.9650238156318665\n",
      "Iteration 41710 Loss: 1.1042848825454712\n",
      "Iteration 41711 Loss: 1.0513867139816284\n",
      "Iteration 41712 Loss: 1.0545551776885986\n",
      "Iteration 41713 Loss: 1.3134050369262695\n",
      "Iteration 41714 Loss: 1.0982427597045898\n",
      "Iteration 41715 Loss: 0.938622236251831\n",
      "Iteration 41716 Loss: 1.0654927492141724\n",
      "Iteration 41717 Loss: 1.0208581686019897\n",
      "Iteration 41718 Loss: 0.9043896198272705\n",
      "Iteration 41719 Loss: 1.2140005826950073\n",
      "Iteration 41719 Loss: 1.076523780822754\n",
      "Iteration 41720 Loss: 0.9154412150382996\n",
      "Iteration 41721 Loss: 0.9705503582954407\n",
      "Iteration 41722 Loss: 1.1591721773147583\n",
      "Iteration 41723 Loss: 0.8049480319023132\n",
      "Iteration 41724 Loss: 1.044560432434082\n",
      "Iteration 41725 Loss: 1.1507772207260132\n",
      "Iteration 41726 Loss: 0.9877853393554688\n",
      "Iteration 41727 Loss: 1.1285042762756348\n",
      "Iteration 41728 Loss: 1.1530166864395142\n",
      "Iteration 41729 Loss: 0.8762272596359253\n",
      "Iteration 41729 Loss: 1.0190982818603516\n",
      "Iteration 41730 Loss: 0.8599019050598145\n",
      "Iteration 41731 Loss: 0.9362912774085999\n",
      "Iteration 41732 Loss: 1.1381924152374268\n",
      "Iteration 41733 Loss: 0.8501777648925781\n",
      "Iteration 41734 Loss: 0.9395745396614075\n",
      "Iteration 41735 Loss: 1.0446652173995972\n",
      "Iteration 41736 Loss: 0.7541298270225525\n",
      "Iteration 41737 Loss: 0.8535587787628174\n",
      "Iteration 41738 Loss: 1.0657261610031128\n",
      "Iteration 41739 Loss: 0.6661763787269592\n",
      "Iteration 41739 Loss: 0.9108394384384155\n",
      "Iteration 41740 Loss: 0.7624035477638245\n",
      "Iteration 41741 Loss: 1.0615593194961548\n",
      "Iteration 41742 Loss: 1.1641541719436646\n",
      "Iteration 41743 Loss: 1.1700485944747925\n",
      "Iteration 41744 Loss: 0.92432701587677\n",
      "Iteration 41745 Loss: 0.8385398387908936\n",
      "Iteration 41746 Loss: 1.0708428621292114\n",
      "Iteration 41747 Loss: 0.8914180397987366\n",
      "Iteration 41748 Loss: 0.9171139001846313\n",
      "Iteration 41749 Loss: 0.9018761515617371\n",
      "Iteration 41749 Loss: 0.970228374004364\n",
      "Iteration 41750 Loss: 0.982416570186615\n",
      "Iteration 41751 Loss: 0.8672425746917725\n",
      "Iteration 41752 Loss: 1.1930304765701294\n",
      "Iteration 41753 Loss: 0.8886483907699585\n",
      "Iteration 41754 Loss: 1.322817087173462\n",
      "Iteration 41755 Loss: 1.300594449043274\n",
      "Iteration 41756 Loss: 0.8333563208580017\n",
      "Iteration 41757 Loss: 1.0360920429229736\n",
      "Iteration 41758 Loss: 1.011808156967163\n",
      "Iteration 41759 Loss: 0.9680735468864441\n",
      "Iteration 41759 Loss: 1.0404078960418701\n",
      "Iteration 41760 Loss: 1.2048708200454712\n",
      "Iteration 41761 Loss: 0.9086405634880066\n",
      "Iteration 41762 Loss: 0.7099529504776001\n",
      "Iteration 41763 Loss: 1.1973434686660767\n",
      "Iteration 41764 Loss: 0.9567715525627136\n",
      "Iteration 41765 Loss: 1.0080207586288452\n",
      "Iteration 41766 Loss: 1.0772936344146729\n",
      "Iteration 41767 Loss: 0.8630629777908325\n",
      "Iteration 41768 Loss: 1.097269058227539\n",
      "Iteration 41769 Loss: 1.1281830072402954\n",
      "Iteration 41769 Loss: 1.0151407718658447\n",
      "Iteration 41770 Loss: 0.5378488302230835\n",
      "Iteration 41771 Loss: 1.079882025718689\n",
      "Iteration 41772 Loss: 0.7660331130027771\n",
      "Iteration 41773 Loss: 0.9133500456809998\n",
      "Iteration 41774 Loss: 0.9307656288146973\n",
      "Iteration 41775 Loss: 1.2385526895523071\n",
      "Iteration 41776 Loss: 0.9363831877708435\n",
      "Iteration 41777 Loss: 0.749157726764679\n",
      "Iteration 41778 Loss: 0.8960240483283997\n",
      "Iteration 41779 Loss: 0.9353805780410767\n",
      "Iteration 41779 Loss: 0.8983378410339355\n",
      "Iteration 41780 Loss: 1.0980018377304077\n",
      "Iteration 41781 Loss: 1.2324527502059937\n",
      "Iteration 41782 Loss: 0.9231735467910767\n",
      "Iteration 41783 Loss: 1.3716760873794556\n",
      "Iteration 41784 Loss: 0.9368071556091309\n",
      "Iteration 41785 Loss: 0.6408558487892151\n",
      "Iteration 41786 Loss: 0.9897152185440063\n",
      "Iteration 41787 Loss: 0.8070368766784668\n",
      "Iteration 41788 Loss: 1.050186276435852\n",
      "Iteration 41789 Loss: 0.7272835969924927\n",
      "Iteration 41789 Loss: 0.9777189493179321\n",
      "Iteration 41790 Loss: 1.1333004236221313\n",
      "Iteration 41791 Loss: 1.0793960094451904\n",
      "Iteration 41792 Loss: 1.0556657314300537\n",
      "Iteration 41793 Loss: 0.6861390471458435\n",
      "Iteration 41794 Loss: 0.8310636281967163\n",
      "Iteration 41795 Loss: 0.805634081363678\n",
      "Iteration 41796 Loss: 0.8426664471626282\n",
      "Iteration 41797 Loss: 1.0034596920013428\n",
      "Iteration 41798 Loss: 0.9475128650665283\n",
      "Iteration 41799 Loss: 1.291620135307312\n",
      "Iteration 41799 Loss: 0.9676458239555359\n",
      "Iteration 41800 Loss: 0.9457512497901917\n",
      "Iteration 41801 Loss: 0.9061499238014221\n",
      "Iteration 41802 Loss: 0.6904688477516174\n",
      "Iteration 41803 Loss: 0.9453549385070801\n",
      "Iteration 41804 Loss: 1.0188038349151611\n",
      "Iteration 41805 Loss: 0.8444805145263672\n",
      "Iteration 41806 Loss: 0.8262511491775513\n",
      "Iteration 41807 Loss: 1.0033575296401978\n",
      "Iteration 41808 Loss: 1.2163019180297852\n",
      "Iteration 41809 Loss: 0.9709357023239136\n",
      "Iteration 41809 Loss: 0.9367855787277222\n",
      "Iteration 41810 Loss: 0.7511605620384216\n",
      "Iteration 41811 Loss: 1.0349469184875488\n",
      "Iteration 41812 Loss: 1.1803518533706665\n",
      "Iteration 41813 Loss: 0.882488489151001\n",
      "Iteration 41814 Loss: 1.1147836446762085\n",
      "Iteration 41815 Loss: 0.7975248098373413\n",
      "Iteration 41816 Loss: 0.8676602840423584\n",
      "Iteration 41817 Loss: 1.0500141382217407\n",
      "Iteration 41818 Loss: 0.9288208484649658\n",
      "Iteration 41819 Loss: 1.0857176780700684\n",
      "Iteration 41819 Loss: 0.9693470001220703\n",
      "Iteration 41820 Loss: 0.7575932145118713\n",
      "Iteration 41821 Loss: 0.7472555041313171\n",
      "Iteration 41822 Loss: 1.0806329250335693\n",
      "Iteration 41823 Loss: 0.9285547137260437\n",
      "Iteration 41824 Loss: 0.9063789248466492\n",
      "Iteration 41825 Loss: 1.0856841802597046\n",
      "Iteration 41826 Loss: 0.8549863696098328\n",
      "Iteration 41827 Loss: 0.6209053993225098\n",
      "Iteration 41828 Loss: 1.1781808137893677\n",
      "Iteration 41829 Loss: 1.0383877754211426\n",
      "Iteration 41829 Loss: 0.9198558926582336\n",
      "Iteration 41830 Loss: 1.3302295207977295\n",
      "Iteration 41831 Loss: 1.0009279251098633\n",
      "Iteration 41832 Loss: 0.9384598731994629\n",
      "Iteration 41833 Loss: 1.0432252883911133\n",
      "Iteration 41834 Loss: 1.2865525484085083\n",
      "Iteration 41835 Loss: 0.6603795289993286\n",
      "Iteration 41836 Loss: 1.0428681373596191\n",
      "Iteration 41837 Loss: 1.0827386379241943\n",
      "Iteration 41838 Loss: 0.9218676090240479\n",
      "Iteration 41839 Loss: 0.8205897212028503\n",
      "Iteration 41839 Loss: 1.0127840042114258\n",
      "Iteration 41840 Loss: 0.7256776094436646\n",
      "Iteration 41841 Loss: 1.0545706748962402\n",
      "Iteration 41842 Loss: 0.9311602711677551\n",
      "Iteration 41843 Loss: 0.9793345332145691\n",
      "Iteration 41844 Loss: 0.9101307392120361\n",
      "Iteration 41845 Loss: 0.8473368883132935\n",
      "Iteration 41846 Loss: 1.1556675434112549\n",
      "Iteration 41847 Loss: 0.8139603137969971\n",
      "Iteration 41848 Loss: 0.9720476865768433\n",
      "Iteration 41849 Loss: 0.9715389609336853\n",
      "Iteration 41849 Loss: 0.9361424446105957\n",
      "Iteration 41850 Loss: 1.0868422985076904\n",
      "Iteration 41851 Loss: 0.8529430031776428\n",
      "Iteration 41852 Loss: 0.6105426549911499\n",
      "Iteration 41853 Loss: 0.6432515978813171\n",
      "Iteration 41854 Loss: 0.9418239593505859\n",
      "Iteration 41855 Loss: 0.9746324419975281\n",
      "Iteration 41856 Loss: 0.8067140579223633\n",
      "Iteration 41857 Loss: 0.7953481674194336\n",
      "Iteration 41858 Loss: 0.9138101935386658\n",
      "Iteration 41859 Loss: 1.0645557641983032\n",
      "Iteration 41859 Loss: 0.8690463900566101\n",
      "Iteration 41860 Loss: 1.2907097339630127\n",
      "Iteration 41861 Loss: 0.7796498537063599\n",
      "Iteration 41862 Loss: 1.0700780153274536\n",
      "Iteration 41863 Loss: 1.0169943571090698\n",
      "Iteration 41864 Loss: 1.2778111696243286\n",
      "Iteration 41865 Loss: 1.1962916851043701\n",
      "Iteration 41866 Loss: 1.0737876892089844\n",
      "Iteration 41867 Loss: 1.1166889667510986\n",
      "Iteration 41868 Loss: 0.8832218647003174\n",
      "Iteration 41869 Loss: 0.9607239365577698\n",
      "Iteration 41869 Loss: 1.0665956735610962\n",
      "Iteration 41870 Loss: 0.7896071076393127\n",
      "Iteration 41871 Loss: 1.264490008354187\n",
      "Iteration 41872 Loss: 1.0530959367752075\n",
      "Iteration 41873 Loss: 0.9345564842224121\n",
      "Iteration 41874 Loss: 0.9573933482170105\n",
      "Iteration 41875 Loss: 1.0415867567062378\n",
      "Iteration 41876 Loss: 1.1020426750183105\n",
      "Iteration 41877 Loss: 0.729496419429779\n",
      "Iteration 41878 Loss: 1.1594957113265991\n",
      "Iteration 41879 Loss: 0.9708748459815979\n",
      "Iteration 41879 Loss: 1.0002638101577759\n",
      "Iteration 41880 Loss: 1.308542013168335\n",
      "Iteration 41881 Loss: 0.8070395588874817\n",
      "Iteration 41882 Loss: 0.8526476621627808\n",
      "Iteration 41883 Loss: 0.8766442537307739\n",
      "Iteration 41884 Loss: 0.9060088396072388\n",
      "Iteration 41885 Loss: 0.8040304183959961\n",
      "Iteration 41886 Loss: 0.6643676161766052\n",
      "Iteration 41887 Loss: 0.9838335514068604\n",
      "Iteration 41888 Loss: 0.9546443819999695\n",
      "Iteration 41889 Loss: 0.9988773465156555\n",
      "Iteration 41889 Loss: 0.9156635403633118\n",
      "Iteration 41890 Loss: 0.999721348285675\n",
      "Iteration 41891 Loss: 0.7678275108337402\n",
      "Iteration 41892 Loss: 1.1773744821548462\n",
      "Iteration 41893 Loss: 1.2078735828399658\n",
      "Iteration 41894 Loss: 0.9091847538948059\n",
      "Iteration 41895 Loss: 1.0854108333587646\n",
      "Iteration 41896 Loss: 0.942942202091217\n",
      "Iteration 41897 Loss: 0.8690511584281921\n",
      "Iteration 41898 Loss: 0.8154616951942444\n",
      "Iteration 41899 Loss: 0.6956923604011536\n",
      "Iteration 41899 Loss: 0.9470540285110474\n",
      "Iteration 41900 Loss: 0.8702788949012756\n",
      "Iteration 41901 Loss: 1.0262178182601929\n",
      "Iteration 41902 Loss: 1.0508439540863037\n",
      "Iteration 41903 Loss: 1.4522709846496582\n",
      "Iteration 41904 Loss: 1.1662328243255615\n",
      "Iteration 41905 Loss: 1.0963650941848755\n",
      "Iteration 41906 Loss: 1.1739805936813354\n",
      "Iteration 41907 Loss: 0.9188477993011475\n",
      "Iteration 41908 Loss: 0.8583734035491943\n",
      "Iteration 41909 Loss: 0.9177122116088867\n",
      "Iteration 41909 Loss: 1.053112268447876\n",
      "Iteration 41910 Loss: 0.9384660720825195\n",
      "Iteration 41911 Loss: 1.0032883882522583\n",
      "Iteration 41912 Loss: 0.9288909435272217\n",
      "Iteration 41913 Loss: 1.079534649848938\n",
      "Iteration 41914 Loss: 0.6770625114440918\n",
      "Iteration 41915 Loss: 0.7692118883132935\n",
      "Iteration 41916 Loss: 0.9510359764099121\n",
      "Iteration 41917 Loss: 0.8802617788314819\n",
      "Iteration 41918 Loss: 0.8233306407928467\n",
      "Iteration 41919 Loss: 1.0358302593231201\n",
      "Iteration 41919 Loss: 0.9086912274360657\n",
      "Iteration 41920 Loss: 1.141837477684021\n",
      "Iteration 41921 Loss: 0.7152146100997925\n",
      "Iteration 41922 Loss: 1.0668038129806519\n",
      "Iteration 41923 Loss: 0.925984799861908\n",
      "Iteration 41924 Loss: 0.888123095035553\n",
      "Iteration 41925 Loss: 1.0859028100967407\n",
      "Iteration 41926 Loss: 0.8998706936836243\n",
      "Iteration 41927 Loss: 0.7831521034240723\n",
      "Iteration 41928 Loss: 0.9330731630325317\n",
      "Iteration 41929 Loss: 1.0631712675094604\n",
      "Iteration 41929 Loss: 0.9503133893013\n",
      "Iteration 41930 Loss: 0.8259273767471313\n",
      "Iteration 41931 Loss: 0.6716443300247192\n",
      "Iteration 41932 Loss: 0.7186828255653381\n",
      "Iteration 41933 Loss: 0.9980361461639404\n",
      "Iteration 41934 Loss: 1.1410188674926758\n",
      "Iteration 41935 Loss: 0.8138346076011658\n",
      "Iteration 41936 Loss: 1.252414584159851\n",
      "Iteration 41937 Loss: 0.6569491624832153\n",
      "Iteration 41938 Loss: 0.9936091899871826\n",
      "Iteration 41939 Loss: 0.8793362379074097\n",
      "Iteration 41939 Loss: 0.8951452970504761\n",
      "Iteration 41940 Loss: 1.0512161254882812\n",
      "Iteration 41941 Loss: 0.7358124256134033\n",
      "Iteration 41942 Loss: 1.0164225101470947\n",
      "Iteration 41943 Loss: 0.7827724814414978\n",
      "Iteration 41944 Loss: 0.5742567777633667\n",
      "Iteration 41945 Loss: 0.7051975727081299\n",
      "Iteration 41946 Loss: 0.8645731210708618\n",
      "Iteration 41947 Loss: 1.1283279657363892\n",
      "Iteration 41948 Loss: 1.0181365013122559\n",
      "Iteration 41949 Loss: 0.9597565531730652\n",
      "Iteration 41949 Loss: 0.8836472630500793\n",
      "Iteration 41950 Loss: 1.0369542837142944\n",
      "Iteration 41951 Loss: 0.9252405762672424\n",
      "Iteration 41952 Loss: 1.0354846715927124\n",
      "Iteration 41953 Loss: 0.9301613569259644\n",
      "Iteration 41954 Loss: 0.6024495959281921\n",
      "Iteration 41955 Loss: 1.2004081010818481\n",
      "Iteration 41956 Loss: 0.9444677829742432\n",
      "Iteration 41957 Loss: 0.9060693979263306\n",
      "Iteration 41958 Loss: 0.7474518418312073\n",
      "Iteration 41959 Loss: 0.749463677406311\n",
      "Iteration 41959 Loss: 0.907815158367157\n",
      "Iteration 41960 Loss: 0.7245188355445862\n",
      "Iteration 41961 Loss: 0.6098603010177612\n",
      "Iteration 41962 Loss: 0.9498968124389648\n",
      "Iteration 41963 Loss: 0.8796742558479309\n",
      "Iteration 41964 Loss: 0.8265863060951233\n",
      "Iteration 41965 Loss: 0.663470983505249\n",
      "Iteration 41966 Loss: 0.8467390537261963\n",
      "Iteration 41967 Loss: 1.0853655338287354\n",
      "Iteration 41968 Loss: 0.8089426159858704\n",
      "Iteration 41969 Loss: 0.5954409241676331\n",
      "Iteration 41969 Loss: 0.7990495562553406\n",
      "Iteration 41970 Loss: 1.2820320129394531\n",
      "Iteration 41971 Loss: 1.3263283967971802\n",
      "Iteration 41972 Loss: 0.9735535383224487\n",
      "Iteration 41973 Loss: 0.8481644988059998\n",
      "Iteration 41974 Loss: 0.819280207157135\n",
      "Iteration 41975 Loss: 0.8191845417022705\n",
      "Iteration 41976 Loss: 1.085590124130249\n",
      "Iteration 41977 Loss: 0.7603138089179993\n",
      "Iteration 41978 Loss: 0.9618846774101257\n",
      "Iteration 41979 Loss: 0.7191576957702637\n",
      "Iteration 41979 Loss: 0.959549069404602\n",
      "Iteration 41980 Loss: 1.027143955230713\n",
      "Iteration 41981 Loss: 0.8260129690170288\n",
      "Iteration 41982 Loss: 0.6432018876075745\n",
      "Iteration 41983 Loss: 1.0702342987060547\n",
      "Iteration 41984 Loss: 1.1262816190719604\n",
      "Iteration 41985 Loss: 0.9303072094917297\n",
      "Iteration 41986 Loss: 1.3832966089248657\n",
      "Iteration 41987 Loss: 1.1549301147460938\n",
      "Iteration 41988 Loss: 1.005334734916687\n",
      "Iteration 41989 Loss: 0.9213578104972839\n",
      "Iteration 41989 Loss: 1.0088101625442505\n",
      "Iteration 41990 Loss: 0.9161648750305176\n",
      "Iteration 41991 Loss: 1.030843734741211\n",
      "Iteration 41992 Loss: 0.9230174422264099\n",
      "Iteration 41993 Loss: 1.1994335651397705\n",
      "Iteration 41994 Loss: 0.7660340070724487\n",
      "Iteration 41995 Loss: 0.7154445648193359\n",
      "Iteration 41996 Loss: 1.0445148944854736\n",
      "Iteration 41997 Loss: 1.197388768196106\n",
      "Iteration 41998 Loss: 1.102081298828125\n",
      "Iteration 41999 Loss: 1.1822317838668823\n",
      "Iteration 41999 Loss: 1.0077154636383057\n",
      "Iteration 42000 Loss: 1.2721368074417114\n",
      "Iteration 42001 Loss: 1.1007936000823975\n",
      "Iteration 42002 Loss: 1.2310247421264648\n",
      "Iteration 42003 Loss: 0.8747789859771729\n",
      "Iteration 42004 Loss: 1.0876325368881226\n",
      "Iteration 42005 Loss: 0.7728842496871948\n",
      "Iteration 42006 Loss: 0.9975898861885071\n",
      "Iteration 42007 Loss: 0.7981506586074829\n",
      "Iteration 42008 Loss: 0.9546822309494019\n",
      "Iteration 42009 Loss: 1.023091197013855\n",
      "Iteration 42009 Loss: 1.0112764835357666\n",
      "Iteration 42010 Loss: 1.060610055923462\n",
      "Iteration 42011 Loss: 1.2843890190124512\n",
      "Iteration 42012 Loss: 0.5341647863388062\n",
      "Iteration 42013 Loss: 1.1654002666473389\n",
      "Iteration 42014 Loss: 1.472854495048523\n",
      "Iteration 42015 Loss: 0.926339864730835\n",
      "Iteration 42016 Loss: 0.8025107383728027\n",
      "Iteration 42017 Loss: 0.8305574059486389\n",
      "Iteration 42018 Loss: 1.2972698211669922\n",
      "Iteration 42019 Loss: 1.3221501111984253\n",
      "Iteration 42019 Loss: 1.0696247816085815\n",
      "Iteration 42020 Loss: 1.1391360759735107\n",
      "Iteration 42021 Loss: 0.9339730143547058\n",
      "Iteration 42022 Loss: 0.8395199179649353\n",
      "Iteration 42023 Loss: 0.8812692761421204\n",
      "Iteration 42024 Loss: 0.7957577705383301\n",
      "Iteration 42025 Loss: 0.9921233057975769\n",
      "Iteration 42026 Loss: 1.1298260688781738\n",
      "Iteration 42027 Loss: 1.0599112510681152\n",
      "Iteration 42028 Loss: 0.7550128698348999\n",
      "Iteration 42029 Loss: 1.07345712184906\n",
      "Iteration 42029 Loss: 0.9599987268447876\n",
      "Iteration 42030 Loss: 0.9643669128417969\n",
      "Iteration 42031 Loss: 0.7764748930931091\n",
      "Iteration 42032 Loss: 1.058660864830017\n",
      "Iteration 42033 Loss: 0.6591135859489441\n",
      "Iteration 42034 Loss: 0.8234532475471497\n",
      "Iteration 42035 Loss: 0.8253718018531799\n",
      "Iteration 42036 Loss: 1.342503547668457\n",
      "Iteration 42037 Loss: 0.9130778908729553\n",
      "Iteration 42038 Loss: 1.0612010955810547\n",
      "Iteration 42039 Loss: 1.017197847366333\n",
      "Iteration 42039 Loss: 0.9441421627998352\n",
      "Iteration 42040 Loss: 1.0029897689819336\n",
      "Iteration 42041 Loss: 0.9715948104858398\n",
      "Iteration 42042 Loss: 0.9275220036506653\n",
      "Iteration 42043 Loss: 1.0905592441558838\n",
      "Iteration 42044 Loss: 0.8190618753433228\n",
      "Iteration 42045 Loss: 0.8347303867340088\n",
      "Iteration 42046 Loss: 0.8510925769805908\n",
      "Iteration 42047 Loss: 1.1557806730270386\n",
      "Iteration 42048 Loss: 0.9427606463432312\n",
      "Iteration 42049 Loss: 1.0009081363677979\n",
      "Iteration 42049 Loss: 0.9596999883651733\n",
      "Iteration 42050 Loss: 1.057072639465332\n",
      "Iteration 42051 Loss: 0.5921290516853333\n",
      "Iteration 42052 Loss: 1.3770819902420044\n",
      "Iteration 42053 Loss: 0.8253300189971924\n",
      "Iteration 42054 Loss: 0.6873600482940674\n",
      "Iteration 42055 Loss: 0.9887875318527222\n",
      "Iteration 42056 Loss: 0.7770235538482666\n",
      "Iteration 42057 Loss: 0.548072874546051\n",
      "Iteration 42058 Loss: 1.2305399179458618\n",
      "Iteration 42059 Loss: 0.9479638338088989\n",
      "Iteration 42059 Loss: 0.9031360745429993\n",
      "Iteration 42060 Loss: 0.881149172782898\n",
      "Iteration 42061 Loss: 0.9985921382904053\n",
      "Iteration 42062 Loss: 1.3196347951889038\n",
      "Iteration 42063 Loss: 0.9022366404533386\n",
      "Iteration 42064 Loss: 0.9839513301849365\n",
      "Iteration 42065 Loss: 0.9255846738815308\n",
      "Iteration 42066 Loss: 1.2259255647659302\n",
      "Iteration 42067 Loss: 0.88139408826828\n",
      "Iteration 42068 Loss: 1.1588687896728516\n",
      "Iteration 42069 Loss: 1.3345597982406616\n",
      "Iteration 42069 Loss: 1.0611897706985474\n",
      "Iteration 42070 Loss: 0.8589646220207214\n",
      "Iteration 42071 Loss: 1.0905731916427612\n",
      "Iteration 42072 Loss: 1.0084654092788696\n",
      "Iteration 42073 Loss: 0.90874844789505\n",
      "Iteration 42074 Loss: 0.840783953666687\n",
      "Iteration 42075 Loss: 0.6905654668807983\n",
      "Iteration 42076 Loss: 0.7094481587409973\n",
      "Iteration 42077 Loss: 0.9191383123397827\n",
      "Iteration 42078 Loss: 0.9184919595718384\n",
      "Iteration 42079 Loss: 1.0035679340362549\n",
      "Iteration 42079 Loss: 0.8948747515678406\n",
      "Iteration 42080 Loss: 0.9421432018280029\n",
      "Iteration 42081 Loss: 0.6981173753738403\n",
      "Iteration 42082 Loss: 1.2619487047195435\n",
      "Iteration 42083 Loss: 1.13649320602417\n",
      "Iteration 42084 Loss: 1.342743158340454\n",
      "Iteration 42085 Loss: 0.9589800238609314\n",
      "Iteration 42086 Loss: 1.0409845113754272\n",
      "Iteration 42087 Loss: 0.8114510178565979\n",
      "Iteration 42088 Loss: 0.8048278093338013\n",
      "Iteration 42089 Loss: 0.972574770450592\n",
      "Iteration 42089 Loss: 0.997026264667511\n",
      "Iteration 42090 Loss: 1.1046082973480225\n",
      "Iteration 42091 Loss: 0.9192848205566406\n",
      "Iteration 42092 Loss: 0.9689456820487976\n",
      "Iteration 42093 Loss: 1.0972388982772827\n",
      "Iteration 42094 Loss: 0.911834716796875\n",
      "Iteration 42095 Loss: 1.2507760524749756\n",
      "Iteration 42096 Loss: 0.8634943962097168\n",
      "Iteration 42097 Loss: 1.1507948637008667\n",
      "Iteration 42098 Loss: 0.9024672508239746\n",
      "Iteration 42099 Loss: 1.0740909576416016\n",
      "Iteration 42099 Loss: 1.0243536233901978\n",
      "Iteration 42100 Loss: 0.9576601386070251\n",
      "Iteration 42101 Loss: 0.8369929194450378\n",
      "Iteration 42102 Loss: 0.593996524810791\n",
      "Iteration 42103 Loss: 1.0363295078277588\n",
      "Iteration 42104 Loss: 1.0779253244400024\n",
      "Iteration 42105 Loss: 0.5856705904006958\n",
      "Iteration 42106 Loss: 0.9884750843048096\n",
      "Iteration 42107 Loss: 0.7066786885261536\n",
      "Iteration 42108 Loss: 0.7135195732116699\n",
      "Iteration 42109 Loss: 0.7430511116981506\n",
      "Iteration 42109 Loss: 0.8240299224853516\n",
      "Iteration 42110 Loss: 1.0078727006912231\n",
      "Iteration 42111 Loss: 0.765613853931427\n",
      "Iteration 42112 Loss: 1.0813062191009521\n",
      "Iteration 42113 Loss: 1.1752458810806274\n",
      "Iteration 42114 Loss: 0.90934818983078\n",
      "Iteration 42115 Loss: 1.0650362968444824\n",
      "Iteration 42116 Loss: 1.0235477685928345\n",
      "Iteration 42117 Loss: 1.125025987625122\n",
      "Iteration 42118 Loss: 0.9119492769241333\n",
      "Iteration 42119 Loss: 1.1799695491790771\n",
      "Iteration 42119 Loss: 1.024491548538208\n",
      "Iteration 42120 Loss: 1.0378620624542236\n",
      "Iteration 42121 Loss: 1.085344910621643\n",
      "Iteration 42122 Loss: 0.8573252558708191\n",
      "Iteration 42123 Loss: 0.6944325566291809\n",
      "Iteration 42124 Loss: 0.9981944561004639\n",
      "Iteration 42125 Loss: 1.0152368545532227\n",
      "Iteration 42126 Loss: 1.1014946699142456\n",
      "Iteration 42127 Loss: 0.8344212174415588\n",
      "Iteration 42128 Loss: 1.2569211721420288\n",
      "Iteration 42129 Loss: 1.1610610485076904\n",
      "Iteration 42129 Loss: 1.0042294263839722\n",
      "Iteration 42130 Loss: 0.9024425745010376\n",
      "Iteration 42131 Loss: 1.220202922821045\n",
      "Iteration 42132 Loss: 0.7880831956863403\n",
      "Iteration 42133 Loss: 0.9703720808029175\n",
      "Iteration 42134 Loss: 1.0332647562026978\n",
      "Iteration 42135 Loss: 0.9453054070472717\n",
      "Iteration 42136 Loss: 0.7891873121261597\n",
      "Iteration 42137 Loss: 0.8528473377227783\n",
      "Iteration 42138 Loss: 0.7812305688858032\n",
      "Iteration 42139 Loss: 1.053705096244812\n",
      "Iteration 42139 Loss: 0.9336640238761902\n",
      "Iteration 42140 Loss: 0.8903546929359436\n",
      "Iteration 42141 Loss: 1.0807727575302124\n",
      "Iteration 42142 Loss: 1.2680273056030273\n",
      "Iteration 42143 Loss: 0.9476917386054993\n",
      "Iteration 42144 Loss: 1.0065693855285645\n",
      "Iteration 42145 Loss: 1.0651404857635498\n",
      "Iteration 42146 Loss: 0.6867726445198059\n",
      "Iteration 42147 Loss: 1.1893603801727295\n",
      "Iteration 42148 Loss: 1.0030008554458618\n",
      "Iteration 42149 Loss: 0.8217110633850098\n",
      "Iteration 42149 Loss: 0.995940089225769\n",
      "Iteration 42150 Loss: 1.1086821556091309\n",
      "Iteration 42151 Loss: 1.1749143600463867\n",
      "Iteration 42152 Loss: 1.0555288791656494\n",
      "Iteration 42153 Loss: 0.6952174305915833\n",
      "Iteration 42154 Loss: 0.5303612351417542\n",
      "Iteration 42155 Loss: 0.618012011051178\n",
      "Iteration 42156 Loss: 1.0874133110046387\n",
      "Iteration 42157 Loss: 1.0055421590805054\n",
      "Iteration 42158 Loss: 0.9742101430892944\n",
      "Iteration 42159 Loss: 1.4363651275634766\n",
      "Iteration 42159 Loss: 0.9686245918273926\n",
      "Iteration 42160 Loss: 1.0045526027679443\n",
      "Iteration 42161 Loss: 1.1616708040237427\n",
      "Iteration 42162 Loss: 0.836536169052124\n",
      "Iteration 42163 Loss: 1.014390468597412\n",
      "Iteration 42164 Loss: 1.080736517906189\n",
      "Iteration 42165 Loss: 0.9592538475990295\n",
      "Iteration 42166 Loss: 0.8161453008651733\n",
      "Iteration 42167 Loss: 1.0546784400939941\n",
      "Iteration 42168 Loss: 0.9762996435165405\n",
      "Iteration 42169 Loss: 0.8952325582504272\n",
      "Iteration 42169 Loss: 0.9799495935440063\n",
      "Iteration 42170 Loss: 0.9730733036994934\n",
      "Iteration 42171 Loss: 0.9351218342781067\n",
      "Iteration 42172 Loss: 1.2194868326187134\n",
      "Iteration 42173 Loss: 0.9836024045944214\n",
      "Iteration 42174 Loss: 1.2270491123199463\n",
      "Iteration 42175 Loss: 0.80660080909729\n",
      "Iteration 42176 Loss: 1.0390269756317139\n",
      "Iteration 42177 Loss: 1.0199931859970093\n",
      "Iteration 42178 Loss: 0.6839013695716858\n",
      "Iteration 42179 Loss: 0.9642855525016785\n",
      "Iteration 42179 Loss: 0.985214114189148\n",
      "Iteration 42180 Loss: 0.8804383873939514\n",
      "Iteration 42181 Loss: 0.8870983719825745\n",
      "Iteration 42182 Loss: 0.9208901524543762\n",
      "Iteration 42183 Loss: 0.7871824502944946\n",
      "Iteration 42184 Loss: 1.260401725769043\n",
      "Iteration 42185 Loss: 1.1281509399414062\n",
      "Iteration 42186 Loss: 0.6060341000556946\n",
      "Iteration 42187 Loss: 1.078630805015564\n",
      "Iteration 42188 Loss: 0.8002856373786926\n",
      "Iteration 42189 Loss: 0.8515028953552246\n",
      "Iteration 42189 Loss: 0.920061469078064\n",
      "Iteration 42190 Loss: 0.8518908619880676\n",
      "Iteration 42191 Loss: 1.0447101593017578\n",
      "Iteration 42192 Loss: 0.8963977098464966\n",
      "Iteration 42193 Loss: 1.192124605178833\n",
      "Iteration 42194 Loss: 0.897026777267456\n",
      "Iteration 42195 Loss: 0.718472957611084\n",
      "Iteration 42196 Loss: 1.15947425365448\n",
      "Iteration 42197 Loss: 0.9553435444831848\n",
      "Iteration 42198 Loss: 0.8339572548866272\n",
      "Iteration 42199 Loss: 1.0597783327102661\n",
      "Iteration 42199 Loss: 0.960917592048645\n",
      "Iteration 42200 Loss: 0.85530024766922\n",
      "Iteration 42201 Loss: 0.805147647857666\n",
      "Iteration 42202 Loss: 1.0842673778533936\n",
      "Iteration 42203 Loss: 0.6928841471672058\n",
      "Iteration 42204 Loss: 0.942517876625061\n",
      "Iteration 42205 Loss: 1.0104466676712036\n",
      "Iteration 42206 Loss: 0.8284561038017273\n",
      "Iteration 42207 Loss: 0.9775912761688232\n",
      "Iteration 42208 Loss: 1.11044442653656\n",
      "Iteration 42209 Loss: 0.5845356583595276\n",
      "Iteration 42209 Loss: 0.889159083366394\n",
      "Iteration 42210 Loss: 0.847370445728302\n",
      "Iteration 42211 Loss: 0.7151729464530945\n",
      "Iteration 42212 Loss: 1.3085145950317383\n",
      "Iteration 42213 Loss: 0.984859824180603\n",
      "Iteration 42214 Loss: 0.9244704246520996\n",
      "Iteration 42215 Loss: 1.0612552165985107\n",
      "Iteration 42216 Loss: 1.3224925994873047\n",
      "Iteration 42217 Loss: 0.7322929501533508\n",
      "Iteration 42218 Loss: 0.8216636776924133\n",
      "Iteration 42219 Loss: 0.9008123874664307\n",
      "Iteration 42219 Loss: 0.9618905186653137\n",
      "Iteration 42220 Loss: 0.9555238485336304\n",
      "Iteration 42221 Loss: 1.1591722965240479\n",
      "Iteration 42222 Loss: 0.6782824993133545\n",
      "Iteration 42223 Loss: 1.1715905666351318\n",
      "Iteration 42224 Loss: 1.2097275257110596\n",
      "Iteration 42225 Loss: 1.0205049514770508\n",
      "Iteration 42226 Loss: 0.7793430685997009\n",
      "Iteration 42227 Loss: 1.0052036046981812\n",
      "Iteration 42228 Loss: 0.9498432874679565\n",
      "Iteration 42229 Loss: 1.1146039962768555\n",
      "Iteration 42229 Loss: 1.0043795108795166\n",
      "Iteration 42230 Loss: 1.1969858407974243\n",
      "Iteration 42231 Loss: 0.7421995997428894\n",
      "Iteration 42232 Loss: 0.8643705248832703\n",
      "Iteration 42233 Loss: 0.8784539699554443\n",
      "Iteration 42234 Loss: 0.8332116007804871\n",
      "Iteration 42235 Loss: 0.639472246170044\n",
      "Iteration 42236 Loss: 0.9968574643135071\n",
      "Iteration 42237 Loss: 0.874157726764679\n",
      "Iteration 42238 Loss: 1.1188689470291138\n",
      "Iteration 42239 Loss: 1.141213297843933\n",
      "Iteration 42239 Loss: 0.9285791516304016\n",
      "Iteration 42240 Loss: 0.9557024240493774\n",
      "Iteration 42241 Loss: 1.4072844982147217\n",
      "Iteration 42242 Loss: 0.714951753616333\n",
      "Iteration 42243 Loss: 1.2453261613845825\n",
      "Iteration 42244 Loss: 1.0748591423034668\n",
      "Iteration 42245 Loss: 1.215760588645935\n",
      "Iteration 42246 Loss: 0.6325180530548096\n",
      "Iteration 42247 Loss: 0.7448076605796814\n",
      "Iteration 42248 Loss: 0.8324607014656067\n",
      "Iteration 42249 Loss: 0.6451245546340942\n",
      "Iteration 42249 Loss: 0.946879506111145\n",
      "Iteration 42250 Loss: 0.7233070135116577\n",
      "Iteration 42251 Loss: 0.9978713393211365\n",
      "Iteration 42252 Loss: 1.2412900924682617\n",
      "Iteration 42253 Loss: 0.8275713920593262\n",
      "Iteration 42254 Loss: 0.8572220206260681\n",
      "Iteration 42255 Loss: 0.8547287583351135\n",
      "Iteration 42256 Loss: 1.0942720174789429\n",
      "Iteration 42257 Loss: 0.9252805709838867\n",
      "Iteration 42258 Loss: 1.1296579837799072\n",
      "Iteration 42259 Loss: 0.7191532850265503\n",
      "Iteration 42259 Loss: 0.9370354413986206\n",
      "Iteration 42260 Loss: 1.0073004961013794\n",
      "Iteration 42261 Loss: 0.6135755777359009\n",
      "Iteration 42262 Loss: 0.6456064581871033\n",
      "Iteration 42263 Loss: 0.8790209293365479\n",
      "Iteration 42264 Loss: 0.5835227370262146\n",
      "Iteration 42265 Loss: 1.1970021724700928\n",
      "Iteration 42266 Loss: 1.0996215343475342\n",
      "Iteration 42267 Loss: 1.0579798221588135\n",
      "Iteration 42268 Loss: 0.8799866437911987\n",
      "Iteration 42269 Loss: 1.1029627323150635\n",
      "Iteration 42269 Loss: 0.9066579937934875\n",
      "Iteration 42270 Loss: 0.9858030676841736\n",
      "Iteration 42271 Loss: 1.0010876655578613\n",
      "Iteration 42272 Loss: 0.928866446018219\n",
      "Iteration 42273 Loss: 0.9958669543266296\n",
      "Iteration 42274 Loss: 1.0716073513031006\n",
      "Iteration 42275 Loss: 0.8899682760238647\n",
      "Iteration 42276 Loss: 0.7991159558296204\n",
      "Iteration 42277 Loss: 1.0327907800674438\n",
      "Iteration 42278 Loss: 0.8645570874214172\n",
      "Iteration 42279 Loss: 0.5608453750610352\n",
      "Iteration 42279 Loss: 0.9130509495735168\n",
      "Iteration 42280 Loss: 1.0152037143707275\n",
      "Iteration 42281 Loss: 1.042137861251831\n",
      "Iteration 42282 Loss: 1.093595266342163\n",
      "Iteration 42283 Loss: 0.7006215453147888\n",
      "Iteration 42284 Loss: 1.2054295539855957\n",
      "Iteration 42285 Loss: 1.1503386497497559\n",
      "Iteration 42286 Loss: 0.9140058755874634\n",
      "Iteration 42287 Loss: 1.014294147491455\n",
      "Iteration 42288 Loss: 0.969934344291687\n",
      "Iteration 42289 Loss: 0.7821463346481323\n",
      "Iteration 42289 Loss: 0.9887706637382507\n",
      "Iteration 42290 Loss: 0.895957350730896\n",
      "Iteration 42291 Loss: 0.738730788230896\n",
      "Iteration 42292 Loss: 0.8636079430580139\n",
      "Iteration 42293 Loss: 0.9020417928695679\n",
      "Iteration 42294 Loss: 0.8219913840293884\n",
      "Iteration 42295 Loss: 1.1419662237167358\n",
      "Iteration 42296 Loss: 1.0799686908721924\n",
      "Iteration 42297 Loss: 1.0342741012573242\n",
      "Iteration 42298 Loss: 1.0712833404541016\n",
      "Iteration 42299 Loss: 0.9806861281394958\n",
      "Iteration 42299 Loss: 0.9530507922172546\n",
      "Iteration 42300 Loss: 1.1500720977783203\n",
      "Iteration 42301 Loss: 0.8910134434700012\n",
      "Iteration 42302 Loss: 0.7886620163917542\n",
      "Iteration 42303 Loss: 0.9238039255142212\n",
      "Iteration 42304 Loss: 0.6546968817710876\n",
      "Iteration 42305 Loss: 1.277193546295166\n",
      "Iteration 42306 Loss: 1.2148655652999878\n",
      "Iteration 42307 Loss: 0.917492687702179\n",
      "Iteration 42308 Loss: 1.1832669973373413\n",
      "Iteration 42309 Loss: 1.3530769348144531\n",
      "Iteration 42309 Loss: 1.035414457321167\n",
      "Iteration 42310 Loss: 1.154079794883728\n",
      "Iteration 42311 Loss: 1.137571930885315\n",
      "Iteration 42312 Loss: 0.9030203223228455\n",
      "Iteration 42313 Loss: 0.9885150194168091\n",
      "Iteration 42314 Loss: 1.1941423416137695\n",
      "Iteration 42315 Loss: 0.9868736863136292\n",
      "Iteration 42316 Loss: 0.952555775642395\n",
      "Iteration 42317 Loss: 0.9457587599754333\n",
      "Iteration 42318 Loss: 1.1367008686065674\n",
      "Iteration 42319 Loss: 0.7252354621887207\n",
      "Iteration 42319 Loss: 1.012445330619812\n",
      "Iteration 42320 Loss: 1.1024259328842163\n",
      "Iteration 42321 Loss: 1.1524890661239624\n",
      "Iteration 42322 Loss: 0.5326393246650696\n",
      "Iteration 42323 Loss: 1.0359710454940796\n",
      "Iteration 42324 Loss: 0.8167138695716858\n",
      "Iteration 42325 Loss: 1.0222910642623901\n",
      "Iteration 42326 Loss: 1.1394047737121582\n",
      "Iteration 42327 Loss: 1.037663459777832\n",
      "Iteration 42328 Loss: 0.8387376666069031\n",
      "Iteration 42329 Loss: 1.3559973239898682\n",
      "Iteration 42329 Loss: 1.0034334659576416\n",
      "Iteration 42330 Loss: 0.8007310628890991\n",
      "Iteration 42331 Loss: 0.5949893593788147\n",
      "Iteration 42332 Loss: 1.0212321281433105\n",
      "Iteration 42333 Loss: 1.0369354486465454\n",
      "Iteration 42334 Loss: 0.7345967292785645\n",
      "Iteration 42335 Loss: 0.7329971194267273\n",
      "Iteration 42336 Loss: 0.860688328742981\n",
      "Iteration 42337 Loss: 0.823098361492157\n",
      "Iteration 42338 Loss: 1.1956921815872192\n",
      "Iteration 42339 Loss: 0.6566122770309448\n",
      "Iteration 42339 Loss: 0.8457573056221008\n",
      "Iteration 42340 Loss: 1.134469747543335\n",
      "Iteration 42341 Loss: 0.8459157943725586\n",
      "Iteration 42342 Loss: 0.872401237487793\n",
      "Iteration 42343 Loss: 0.7650079727172852\n",
      "Iteration 42344 Loss: 0.994159996509552\n",
      "Iteration 42345 Loss: 1.1925894021987915\n",
      "Iteration 42346 Loss: 0.785456657409668\n",
      "Iteration 42347 Loss: 0.8644511103630066\n",
      "Iteration 42348 Loss: 0.9056491851806641\n",
      "Iteration 42349 Loss: 0.8434819579124451\n",
      "Iteration 42349 Loss: 0.9203583598136902\n",
      "Iteration 42350 Loss: 0.898470938205719\n",
      "Iteration 42351 Loss: 0.6340963840484619\n",
      "Iteration 42352 Loss: 0.7334005236625671\n",
      "Iteration 42353 Loss: 0.7604041695594788\n",
      "Iteration 42354 Loss: 0.8252120614051819\n",
      "Iteration 42355 Loss: 0.8948710560798645\n",
      "Iteration 42356 Loss: 0.8894266486167908\n",
      "Iteration 42357 Loss: 0.831348180770874\n",
      "Iteration 42358 Loss: 0.8466669321060181\n",
      "Iteration 42359 Loss: 1.2019572257995605\n",
      "Iteration 42359 Loss: 0.8515853881835938\n",
      "Iteration 42360 Loss: 1.1709985733032227\n",
      "Iteration 42361 Loss: 0.9461548328399658\n",
      "Iteration 42362 Loss: 1.0115673542022705\n",
      "Iteration 42363 Loss: 1.1852174997329712\n",
      "Iteration 42364 Loss: 0.7081989645957947\n",
      "Iteration 42365 Loss: 0.8749003410339355\n",
      "Iteration 42366 Loss: 1.0851236581802368\n",
      "Iteration 42367 Loss: 1.1569207906723022\n",
      "Iteration 42368 Loss: 0.8204101920127869\n",
      "Iteration 42369 Loss: 1.054876685142517\n",
      "Iteration 42369 Loss: 1.0014368295669556\n",
      "Iteration 42370 Loss: 1.0118874311447144\n",
      "Iteration 42371 Loss: 1.2207111120224\n",
      "Iteration 42372 Loss: 0.7367291450500488\n",
      "Iteration 42373 Loss: 0.9657992124557495\n",
      "Iteration 42374 Loss: 1.0017585754394531\n",
      "Iteration 42375 Loss: 1.478357195854187\n",
      "Iteration 42376 Loss: 1.0895726680755615\n",
      "Iteration 42377 Loss: 0.9254917502403259\n",
      "Iteration 42378 Loss: 0.9666122794151306\n",
      "Iteration 42379 Loss: 1.3160908222198486\n",
      "Iteration 42379 Loss: 1.071300983428955\n",
      "Iteration 42380 Loss: 1.3044252395629883\n",
      "Iteration 42381 Loss: 1.212585210800171\n",
      "Iteration 42382 Loss: 1.0749965906143188\n",
      "Iteration 42383 Loss: 0.9901959300041199\n",
      "Iteration 42384 Loss: 1.1611371040344238\n",
      "Iteration 42385 Loss: 1.1816394329071045\n",
      "Iteration 42386 Loss: 0.9645581245422363\n",
      "Iteration 42387 Loss: 1.0448741912841797\n",
      "Iteration 42388 Loss: 0.8584235310554504\n",
      "Iteration 42389 Loss: 0.8575817942619324\n",
      "Iteration 42389 Loss: 1.0650417804718018\n",
      "Iteration 42390 Loss: 0.7673062086105347\n",
      "Iteration 42391 Loss: 1.1130883693695068\n",
      "Iteration 42392 Loss: 0.8505969047546387\n",
      "Iteration 42393 Loss: 0.9984963536262512\n",
      "Iteration 42394 Loss: 0.9904745221138\n",
      "Iteration 42395 Loss: 0.9422495365142822\n",
      "Iteration 42396 Loss: 1.0454429388046265\n",
      "Iteration 42397 Loss: 0.9261206388473511\n",
      "Iteration 42398 Loss: 0.8663716316223145\n",
      "Iteration 42399 Loss: 0.893383264541626\n",
      "Iteration 42399 Loss: 0.9393531084060669\n",
      "Iteration 42400 Loss: 0.7978293895721436\n",
      "Iteration 42401 Loss: 0.7657867074012756\n",
      "Iteration 42402 Loss: 0.873924195766449\n",
      "Iteration 42403 Loss: 1.066403865814209\n",
      "Iteration 42404 Loss: 1.107914924621582\n",
      "Iteration 42405 Loss: 1.1607322692871094\n",
      "Iteration 42406 Loss: 0.816957414150238\n",
      "Iteration 42407 Loss: 1.142927646636963\n",
      "Iteration 42408 Loss: 1.1277825832366943\n",
      "Iteration 42409 Loss: 1.033666968345642\n",
      "Iteration 42409 Loss: 0.9893924593925476\n",
      "Iteration 42410 Loss: 1.082723617553711\n",
      "Iteration 42411 Loss: 1.0515179634094238\n",
      "Iteration 42412 Loss: 0.744899332523346\n",
      "Iteration 42413 Loss: 1.0554077625274658\n",
      "Iteration 42414 Loss: 1.0905864238739014\n",
      "Iteration 42415 Loss: 1.121095895767212\n",
      "Iteration 42416 Loss: 0.7410138249397278\n",
      "Iteration 42417 Loss: 0.7803390026092529\n",
      "Iteration 42418 Loss: 1.0798897743225098\n",
      "Iteration 42419 Loss: 0.9905681610107422\n",
      "Iteration 42419 Loss: 0.9738041758537292\n",
      "Iteration 42420 Loss: 0.9715611338615417\n",
      "Iteration 42421 Loss: 1.1697726249694824\n",
      "Iteration 42422 Loss: 0.799163818359375\n",
      "Iteration 42423 Loss: 1.123884916305542\n",
      "Iteration 42424 Loss: 0.81719970703125\n",
      "Iteration 42425 Loss: 0.8956973552703857\n",
      "Iteration 42426 Loss: 0.7620688080787659\n",
      "Iteration 42427 Loss: 1.1656780242919922\n",
      "Iteration 42428 Loss: 0.8106199502944946\n",
      "Iteration 42429 Loss: 0.6067327260971069\n",
      "Iteration 42429 Loss: 0.9122379422187805\n",
      "Iteration 42430 Loss: 1.0001744031906128\n",
      "Iteration 42431 Loss: 0.9666983485221863\n",
      "Iteration 42432 Loss: 0.8299499750137329\n",
      "Iteration 42433 Loss: 0.9873694777488708\n",
      "Iteration 42434 Loss: 0.6187832951545715\n",
      "Iteration 42435 Loss: 0.7835037112236023\n",
      "Iteration 42436 Loss: 0.4105260968208313\n",
      "Iteration 42437 Loss: 0.6072208285331726\n",
      "Iteration 42438 Loss: 1.0422446727752686\n",
      "Iteration 42439 Loss: 1.1249250173568726\n",
      "Iteration 42439 Loss: 0.8371396064758301\n",
      "Iteration 42440 Loss: 1.026696801185608\n",
      "Iteration 42441 Loss: 0.9331130981445312\n",
      "Iteration 42442 Loss: 0.7700082063674927\n",
      "Iteration 42443 Loss: 1.0126776695251465\n",
      "Iteration 42444 Loss: 1.5159319639205933\n",
      "Iteration 42445 Loss: 0.6779732704162598\n",
      "Iteration 42446 Loss: 0.6836708188056946\n",
      "Iteration 42447 Loss: 0.8866851329803467\n",
      "Iteration 42448 Loss: 1.1676785945892334\n",
      "Iteration 42449 Loss: 0.9516803026199341\n",
      "Iteration 42449 Loss: 0.9626115560531616\n",
      "Iteration 42450 Loss: 1.07052481174469\n",
      "Iteration 42451 Loss: 0.8119205236434937\n",
      "Iteration 42452 Loss: 1.3464592695236206\n",
      "Iteration 42453 Loss: 1.2341759204864502\n",
      "Iteration 42454 Loss: 1.1044460535049438\n",
      "Iteration 42455 Loss: 1.3329570293426514\n",
      "Iteration 42456 Loss: 1.2513176202774048\n",
      "Iteration 42457 Loss: 0.7517165541648865\n",
      "Iteration 42458 Loss: 0.9190871715545654\n",
      "Iteration 42459 Loss: 1.1070424318313599\n",
      "Iteration 42459 Loss: 1.092964768409729\n",
      "Iteration 42460 Loss: 0.7481269240379333\n",
      "Iteration 42461 Loss: 1.0678222179412842\n",
      "Iteration 42462 Loss: 1.0711205005645752\n",
      "Iteration 42463 Loss: 0.5449349880218506\n",
      "Iteration 42464 Loss: 1.0010111331939697\n",
      "Iteration 42465 Loss: 1.0494498014450073\n",
      "Iteration 42466 Loss: 0.9270078539848328\n",
      "Iteration 42467 Loss: 0.8461732268333435\n",
      "Iteration 42468 Loss: 1.138799786567688\n",
      "Iteration 42469 Loss: 1.102648377418518\n",
      "Iteration 42469 Loss: 0.9497095346450806\n",
      "Iteration 42470 Loss: 0.7168753743171692\n",
      "Iteration 42471 Loss: 0.9589496850967407\n",
      "Iteration 42472 Loss: 1.2533429861068726\n",
      "Iteration 42473 Loss: 0.948051393032074\n",
      "Iteration 42474 Loss: 0.5676729679107666\n",
      "Iteration 42475 Loss: 0.8728672862052917\n",
      "Iteration 42476 Loss: 0.9197344183921814\n",
      "Iteration 42477 Loss: 0.9124609231948853\n",
      "Iteration 42478 Loss: 0.9777291417121887\n",
      "Iteration 42479 Loss: 0.6094067692756653\n",
      "Iteration 42479 Loss: 0.8737090826034546\n",
      "Iteration 42480 Loss: 1.1038631200790405\n",
      "Iteration 42481 Loss: 0.8992710709571838\n",
      "Iteration 42482 Loss: 0.8628658652305603\n",
      "Iteration 42483 Loss: 1.0293049812316895\n",
      "Iteration 42484 Loss: 0.9517714977264404\n",
      "Iteration 42485 Loss: 1.1467293500900269\n",
      "Iteration 42486 Loss: 1.0607104301452637\n",
      "Iteration 42487 Loss: 1.0663419961929321\n",
      "Iteration 42488 Loss: 1.07081937789917\n",
      "Iteration 42489 Loss: 0.7081197500228882\n",
      "Iteration 42489 Loss: 0.9899798631668091\n",
      "Iteration 42490 Loss: 1.0917541980743408\n",
      "Iteration 42491 Loss: 1.017212986946106\n",
      "Iteration 42492 Loss: 0.9246842861175537\n",
      "Iteration 42493 Loss: 0.8771702647209167\n",
      "Iteration 42494 Loss: 0.7863041758537292\n",
      "Iteration 42495 Loss: 0.990838885307312\n",
      "Iteration 42496 Loss: 1.1942241191864014\n",
      "Iteration 42497 Loss: 0.9029051065444946\n",
      "Iteration 42498 Loss: 1.109307885169983\n",
      "Iteration 42499 Loss: 0.8897054195404053\n",
      "Iteration 42499 Loss: 0.9784107208251953\n",
      "Iteration 42500 Loss: 1.0078872442245483\n",
      "Iteration 42501 Loss: 0.9683088064193726\n",
      "Iteration 42502 Loss: 0.9046670198440552\n",
      "Iteration 42503 Loss: 1.0033375024795532\n",
      "Iteration 42504 Loss: 1.1156554222106934\n",
      "Iteration 42505 Loss: 1.0313928127288818\n",
      "Iteration 42506 Loss: 1.071250557899475\n",
      "Iteration 42507 Loss: 0.4791731536388397\n",
      "Iteration 42508 Loss: 0.9949334263801575\n",
      "Iteration 42509 Loss: 0.8163937926292419\n",
      "Iteration 42509 Loss: 0.9392999410629272\n",
      "Iteration 42510 Loss: 1.0931092500686646\n",
      "Iteration 42511 Loss: 0.9017125964164734\n",
      "Iteration 42512 Loss: 1.0127999782562256\n",
      "Iteration 42513 Loss: 1.0024324655532837\n",
      "Iteration 42514 Loss: 0.8662365078926086\n",
      "Iteration 42515 Loss: 0.7897531390190125\n",
      "Iteration 42516 Loss: 0.7197774052619934\n",
      "Iteration 42517 Loss: 0.860183835029602\n",
      "Iteration 42518 Loss: 1.3004708290100098\n",
      "Iteration 42519 Loss: 1.0585951805114746\n",
      "Iteration 42519 Loss: 0.9605070948600769\n",
      "Iteration 42520 Loss: 0.9327331185340881\n",
      "Iteration 42521 Loss: 1.0618685483932495\n",
      "Iteration 42522 Loss: 1.1733088493347168\n",
      "Iteration 42523 Loss: 0.8992617726325989\n",
      "Iteration 42524 Loss: 1.1280275583267212\n",
      "Iteration 42525 Loss: 1.2141315937042236\n",
      "Iteration 42526 Loss: 0.9378821849822998\n",
      "Iteration 42527 Loss: 0.8207945823669434\n",
      "Iteration 42528 Loss: 1.1833113431930542\n",
      "Iteration 42529 Loss: 0.8506336808204651\n",
      "Iteration 42529 Loss: 1.0201953649520874\n",
      "Iteration 42530 Loss: 0.741950273513794\n",
      "Iteration 42531 Loss: 1.0544265508651733\n",
      "Iteration 42532 Loss: 1.1672643423080444\n",
      "Iteration 42533 Loss: 1.064148187637329\n",
      "Iteration 42534 Loss: 1.1410826444625854\n",
      "Iteration 42535 Loss: 1.0540211200714111\n",
      "Iteration 42536 Loss: 0.9388264417648315\n",
      "Iteration 42537 Loss: 0.8359491229057312\n",
      "Iteration 42538 Loss: 0.8813292980194092\n",
      "Iteration 42539 Loss: 1.0540881156921387\n",
      "Iteration 42539 Loss: 0.9933086633682251\n",
      "Iteration 42540 Loss: 0.6638336181640625\n",
      "Iteration 42541 Loss: 0.7110011577606201\n",
      "Iteration 42542 Loss: 0.950239360332489\n",
      "Iteration 42543 Loss: 0.5791987180709839\n",
      "Iteration 42544 Loss: 0.8756111860275269\n",
      "Iteration 42545 Loss: 0.9758691191673279\n",
      "Iteration 42546 Loss: 0.8395959138870239\n",
      "Iteration 42547 Loss: 1.0808213949203491\n",
      "Iteration 42548 Loss: 0.8904250264167786\n",
      "Iteration 42549 Loss: 0.8436626195907593\n",
      "Iteration 42549 Loss: 0.8410258293151855\n",
      "Iteration 42550 Loss: 0.9188145399093628\n",
      "Iteration 42551 Loss: 0.8251902461051941\n",
      "Iteration 42552 Loss: 0.7527150511741638\n",
      "Iteration 42553 Loss: 0.7663186192512512\n",
      "Iteration 42554 Loss: 0.9027991890907288\n",
      "Iteration 42555 Loss: 0.7525684833526611\n",
      "Iteration 42556 Loss: 1.203413963317871\n",
      "Iteration 42557 Loss: 1.2192673683166504\n",
      "Iteration 42558 Loss: 0.8852758407592773\n",
      "Iteration 42559 Loss: 0.8756616115570068\n",
      "Iteration 42559 Loss: 0.9102023839950562\n",
      "Iteration 42560 Loss: 0.9171074628829956\n",
      "Iteration 42561 Loss: 0.5310906767845154\n",
      "Iteration 42562 Loss: 0.9748706221580505\n",
      "Iteration 42563 Loss: 0.7221219539642334\n",
      "Iteration 42564 Loss: 0.7272970080375671\n",
      "Iteration 42565 Loss: 0.9367017149925232\n",
      "Iteration 42566 Loss: 0.8857121467590332\n",
      "Iteration 42567 Loss: 0.8008341789245605\n",
      "Iteration 42568 Loss: 0.8553469181060791\n",
      "Iteration 42569 Loss: 0.9338268041610718\n",
      "Iteration 42569 Loss: 0.8284909129142761\n",
      "Iteration 42570 Loss: 1.1872889995574951\n",
      "Iteration 42571 Loss: 1.159869909286499\n",
      "Iteration 42572 Loss: 1.028290867805481\n",
      "Iteration 42573 Loss: 0.7706120014190674\n",
      "Iteration 42574 Loss: 1.020032286643982\n",
      "Iteration 42575 Loss: 1.1478915214538574\n",
      "Iteration 42576 Loss: 0.9481227993965149\n",
      "Iteration 42577 Loss: 1.0699084997177124\n",
      "Iteration 42578 Loss: 0.9780596494674683\n",
      "Iteration 42579 Loss: 0.9932953119277954\n",
      "Iteration 42579 Loss: 1.0303370952606201\n",
      "Iteration 42580 Loss: 0.7592136859893799\n",
      "Iteration 42581 Loss: 1.1939477920532227\n",
      "Iteration 42582 Loss: 0.9773051142692566\n",
      "Iteration 42583 Loss: 1.1468136310577393\n",
      "Iteration 42584 Loss: 0.9318904280662537\n",
      "Iteration 42585 Loss: 0.8190245032310486\n",
      "Iteration 42586 Loss: 1.0155236721038818\n",
      "Iteration 42587 Loss: 1.227315068244934\n",
      "Iteration 42588 Loss: 0.771664023399353\n",
      "Iteration 42589 Loss: 0.6820014119148254\n",
      "Iteration 42589 Loss: 0.9524699449539185\n",
      "Iteration 42590 Loss: 1.0747582912445068\n",
      "Iteration 42591 Loss: 1.1281205415725708\n",
      "Iteration 42592 Loss: 0.8192129135131836\n",
      "Iteration 42593 Loss: 0.6907470226287842\n",
      "Iteration 42594 Loss: 0.6226018667221069\n",
      "Iteration 42595 Loss: 0.9380699396133423\n",
      "Iteration 42596 Loss: 0.9315976500511169\n",
      "Iteration 42597 Loss: 1.1949374675750732\n",
      "Iteration 42598 Loss: 0.7058698534965515\n",
      "Iteration 42599 Loss: 0.9221201539039612\n",
      "Iteration 42599 Loss: 0.9028035998344421\n",
      "Iteration 42600 Loss: 0.8984817862510681\n",
      "Iteration 42601 Loss: 1.1704505681991577\n",
      "Iteration 42602 Loss: 1.197256326675415\n",
      "Iteration 42603 Loss: 0.8238190412521362\n",
      "Iteration 42604 Loss: 0.8626273274421692\n",
      "Iteration 42605 Loss: 1.000641942024231\n",
      "Iteration 42606 Loss: 0.7711203098297119\n",
      "Iteration 42607 Loss: 0.7219435572624207\n",
      "Iteration 42608 Loss: 1.0429126024246216\n",
      "Iteration 42609 Loss: 0.705589234828949\n",
      "Iteration 42609 Loss: 0.9194843173027039\n",
      "Iteration 42610 Loss: 0.9194176197052002\n",
      "Iteration 42611 Loss: 1.0064616203308105\n",
      "Iteration 42612 Loss: 1.1646032333374023\n",
      "Iteration 42613 Loss: 1.2109744548797607\n",
      "Iteration 42614 Loss: 0.7412300705909729\n",
      "Iteration 42615 Loss: 0.8641287088394165\n",
      "Iteration 42616 Loss: 0.9477609992027283\n",
      "Iteration 42617 Loss: 0.7808515429496765\n",
      "Iteration 42618 Loss: 0.8046767711639404\n",
      "Iteration 42619 Loss: 0.8083951473236084\n",
      "Iteration 42619 Loss: 0.9248499870300293\n",
      "Iteration 42620 Loss: 1.193127155303955\n",
      "Iteration 42621 Loss: 0.9509229063987732\n",
      "Iteration 42622 Loss: 0.91659015417099\n",
      "Iteration 42623 Loss: 0.786820113658905\n",
      "Iteration 42624 Loss: 1.0764939785003662\n",
      "Iteration 42625 Loss: 1.040343999862671\n",
      "Iteration 42626 Loss: 0.9520606994628906\n",
      "Iteration 42627 Loss: 1.036233901977539\n",
      "Iteration 42628 Loss: 1.3129136562347412\n",
      "Iteration 42629 Loss: 0.8955886960029602\n",
      "Iteration 42629 Loss: 1.016109585762024\n",
      "Iteration 42630 Loss: 0.6365920901298523\n",
      "Iteration 42631 Loss: 0.8573938608169556\n",
      "Iteration 42632 Loss: 0.9234723448753357\n",
      "Iteration 42633 Loss: 1.1628351211547852\n",
      "Iteration 42634 Loss: 0.9838610887527466\n",
      "Iteration 42635 Loss: 0.8657228350639343\n",
      "Iteration 42636 Loss: 0.8701422214508057\n",
      "Iteration 42637 Loss: 0.9577581882476807\n",
      "Iteration 42638 Loss: 0.9633241891860962\n",
      "Iteration 42639 Loss: 1.0920140743255615\n",
      "Iteration 42639 Loss: 0.9313116073608398\n",
      "Iteration 42640 Loss: 0.7237271666526794\n",
      "Iteration 42641 Loss: 0.7157695293426514\n",
      "Iteration 42642 Loss: 0.783950924873352\n",
      "Iteration 42643 Loss: 0.618506669998169\n",
      "Iteration 42644 Loss: 0.9642212390899658\n",
      "Iteration 42645 Loss: 0.9553307294845581\n",
      "Iteration 42646 Loss: 0.9034391045570374\n",
      "Iteration 42647 Loss: 1.0851364135742188\n",
      "Iteration 42648 Loss: 0.9833104610443115\n",
      "Iteration 42649 Loss: 0.6826552152633667\n",
      "Iteration 42649 Loss: 0.8416048288345337\n",
      "Iteration 42650 Loss: 0.9422985315322876\n",
      "Iteration 42651 Loss: 1.1367807388305664\n",
      "Iteration 42652 Loss: 0.6658225059509277\n",
      "Iteration 42653 Loss: 0.6178650259971619\n",
      "Iteration 42654 Loss: 1.0404456853866577\n",
      "Iteration 42655 Loss: 0.9180017113685608\n",
      "Iteration 42656 Loss: 1.0384057760238647\n",
      "Iteration 42657 Loss: 0.7066762447357178\n",
      "Iteration 42658 Loss: 0.9758085012435913\n",
      "Iteration 42659 Loss: 0.9696995615959167\n",
      "Iteration 42659 Loss: 0.9011804461479187\n",
      "Iteration 42660 Loss: 0.9957447648048401\n",
      "Iteration 42661 Loss: 0.9859360456466675\n",
      "Iteration 42662 Loss: 1.103342056274414\n",
      "Iteration 42663 Loss: 0.8404029011726379\n",
      "Iteration 42664 Loss: 1.1251871585845947\n",
      "Iteration 42665 Loss: 0.8789154291152954\n",
      "Iteration 42666 Loss: 1.2643094062805176\n",
      "Iteration 42667 Loss: 0.8680646419525146\n",
      "Iteration 42668 Loss: 0.7959146499633789\n",
      "Iteration 42669 Loss: 0.9668161273002625\n",
      "Iteration 42669 Loss: 0.9824633598327637\n",
      "Iteration 42670 Loss: 1.0456788539886475\n",
      "Iteration 42671 Loss: 1.1391178369522095\n",
      "Iteration 42672 Loss: 1.2192524671554565\n",
      "Iteration 42673 Loss: 0.6641280055046082\n",
      "Iteration 42674 Loss: 0.764024019241333\n",
      "Iteration 42675 Loss: 1.0652804374694824\n",
      "Iteration 42676 Loss: 0.8053069114685059\n",
      "Iteration 42677 Loss: 1.0039650201797485\n",
      "Iteration 42678 Loss: 0.9437069296836853\n",
      "Iteration 42679 Loss: 1.0436922311782837\n",
      "Iteration 42679 Loss: 0.9694153070449829\n",
      "Iteration 42680 Loss: 0.8139205574989319\n",
      "Iteration 42681 Loss: 1.1752147674560547\n",
      "Iteration 42682 Loss: 1.3490811586380005\n",
      "Iteration 42683 Loss: 0.9466390013694763\n",
      "Iteration 42684 Loss: 0.9355683922767639\n",
      "Iteration 42685 Loss: 1.0667129755020142\n",
      "Iteration 42686 Loss: 1.3424252271652222\n",
      "Iteration 42687 Loss: 0.8095883727073669\n",
      "Iteration 42688 Loss: 0.9596834182739258\n",
      "Iteration 42689 Loss: 0.8503260612487793\n",
      "Iteration 42689 Loss: 1.0249159336090088\n",
      "Iteration 42690 Loss: 0.9477462768554688\n",
      "Iteration 42691 Loss: 0.9584854245185852\n",
      "Iteration 42692 Loss: 0.6722280979156494\n",
      "Iteration 42693 Loss: 0.8910207748413086\n",
      "Iteration 42694 Loss: 0.5271466374397278\n",
      "Iteration 42695 Loss: 0.7758793234825134\n",
      "Iteration 42696 Loss: 0.8354703783988953\n",
      "Iteration 42697 Loss: 1.0826929807662964\n",
      "Iteration 42698 Loss: 1.0099844932556152\n",
      "Iteration 42699 Loss: 1.0697320699691772\n",
      "Iteration 42699 Loss: 0.8770386576652527\n",
      "Iteration 42700 Loss: 1.1868988275527954\n",
      "Iteration 42701 Loss: 0.9299269318580627\n",
      "Iteration 42702 Loss: 0.7088017463684082\n",
      "Iteration 42703 Loss: 0.8929941058158875\n",
      "Iteration 42704 Loss: 0.7186610102653503\n",
      "Iteration 42705 Loss: 0.9067770838737488\n",
      "Iteration 42706 Loss: 0.8276541829109192\n",
      "Iteration 42707 Loss: 0.8961651921272278\n",
      "Iteration 42708 Loss: 1.150202989578247\n",
      "Iteration 42709 Loss: 0.9329895973205566\n",
      "Iteration 42709 Loss: 0.9151070713996887\n",
      "Iteration 42710 Loss: 0.9857763648033142\n",
      "Iteration 42711 Loss: 0.753544807434082\n",
      "Iteration 42712 Loss: 0.8842061161994934\n",
      "Iteration 42713 Loss: 1.2632238864898682\n",
      "Iteration 42714 Loss: 1.0964957475662231\n",
      "Iteration 42715 Loss: 1.1421840190887451\n",
      "Iteration 42716 Loss: 1.0474932193756104\n",
      "Iteration 42717 Loss: 0.8667222857475281\n",
      "Iteration 42718 Loss: 1.2588452100753784\n",
      "Iteration 42719 Loss: 1.1165579557418823\n",
      "Iteration 42719 Loss: 1.041504979133606\n",
      "Iteration 42720 Loss: 0.9716588258743286\n",
      "Iteration 42721 Loss: 1.0858862400054932\n",
      "Iteration 42722 Loss: 0.9490473866462708\n",
      "Iteration 42723 Loss: 0.9600276947021484\n",
      "Iteration 42724 Loss: 0.9258944392204285\n",
      "Iteration 42725 Loss: 0.9039382934570312\n",
      "Iteration 42726 Loss: 0.897297739982605\n",
      "Iteration 42727 Loss: 1.0177110433578491\n",
      "Iteration 42728 Loss: 1.111983060836792\n",
      "Iteration 42729 Loss: 1.2593371868133545\n",
      "Iteration 42729 Loss: 1.0082781314849854\n",
      "Iteration 42730 Loss: 1.00484299659729\n",
      "Iteration 42731 Loss: 0.8935134410858154\n",
      "Iteration 42732 Loss: 1.230765700340271\n",
      "Iteration 42733 Loss: 0.5793662667274475\n",
      "Iteration 42734 Loss: 1.136491298675537\n",
      "Iteration 42735 Loss: 0.673498272895813\n",
      "Iteration 42736 Loss: 0.844961404800415\n",
      "Iteration 42737 Loss: 1.1917446851730347\n",
      "Iteration 42738 Loss: 1.0226304531097412\n",
      "Iteration 42739 Loss: 0.7677443623542786\n",
      "Iteration 42739 Loss: 0.9345558881759644\n",
      "Iteration 42740 Loss: 1.090538501739502\n",
      "Iteration 42741 Loss: 0.73122239112854\n",
      "Iteration 42742 Loss: 0.9167922735214233\n",
      "Iteration 42743 Loss: 1.1759635210037231\n",
      "Iteration 42744 Loss: 1.3836894035339355\n",
      "Iteration 42745 Loss: 0.6169934868812561\n",
      "Iteration 42746 Loss: 0.4950636327266693\n",
      "Iteration 42747 Loss: 0.7782154679298401\n",
      "Iteration 42748 Loss: 1.0898526906967163\n",
      "Iteration 42749 Loss: 0.6173422932624817\n",
      "Iteration 42749 Loss: 0.8895673751831055\n",
      "Iteration 42750 Loss: 0.6769671440124512\n",
      "Iteration 42751 Loss: 1.2217251062393188\n",
      "Iteration 42752 Loss: 0.738085150718689\n",
      "Iteration 42753 Loss: 0.7680079936981201\n",
      "Iteration 42754 Loss: 1.0562105178833008\n",
      "Iteration 42755 Loss: 1.031045913696289\n",
      "Iteration 42756 Loss: 0.935900092124939\n",
      "Iteration 42757 Loss: 0.9745693802833557\n",
      "Iteration 42758 Loss: 0.9229359030723572\n",
      "Iteration 42759 Loss: 0.8531720638275146\n",
      "Iteration 42759 Loss: 0.9178619384765625\n",
      "Iteration 42760 Loss: 0.8028460741043091\n",
      "Iteration 42761 Loss: 0.9642517566680908\n",
      "Iteration 42762 Loss: 0.7766294479370117\n",
      "Iteration 42763 Loss: 0.5429512858390808\n",
      "Iteration 42764 Loss: 1.1618125438690186\n",
      "Iteration 42765 Loss: 0.975599467754364\n",
      "Iteration 42766 Loss: 0.9315763115882874\n",
      "Iteration 42767 Loss: 0.856777012348175\n",
      "Iteration 42768 Loss: 0.5682870745658875\n",
      "Iteration 42769 Loss: 0.7753984928131104\n",
      "Iteration 42769 Loss: 0.8356128931045532\n",
      "Iteration 42770 Loss: 1.1847953796386719\n",
      "Iteration 42771 Loss: 0.6947464346885681\n",
      "Iteration 42772 Loss: 0.927537202835083\n",
      "Iteration 42773 Loss: 0.9502411484718323\n",
      "Iteration 42774 Loss: 1.0004022121429443\n",
      "Iteration 42775 Loss: 0.8298018574714661\n",
      "Iteration 42776 Loss: 0.731023907661438\n",
      "Iteration 42777 Loss: 0.6980785727500916\n",
      "Iteration 42778 Loss: 0.8178452253341675\n",
      "Iteration 42779 Loss: 0.828345000743866\n",
      "Iteration 42779 Loss: 0.8662816882133484\n",
      "Iteration 42780 Loss: 1.2224507331848145\n",
      "Iteration 42781 Loss: 0.9554688930511475\n",
      "Iteration 42782 Loss: 1.2135400772094727\n",
      "Iteration 42783 Loss: 0.6934962272644043\n",
      "Iteration 42784 Loss: 1.1252955198287964\n",
      "Iteration 42785 Loss: 0.7012302279472351\n",
      "Iteration 42786 Loss: 0.5758830308914185\n",
      "Iteration 42787 Loss: 0.9888622760772705\n",
      "Iteration 42788 Loss: 1.0099472999572754\n",
      "Iteration 42789 Loss: 1.1863529682159424\n",
      "Iteration 42789 Loss: 0.9672527313232422\n",
      "Iteration 42790 Loss: 0.6796894669532776\n",
      "Iteration 42791 Loss: 1.0647157430648804\n",
      "Iteration 42792 Loss: 1.1067520380020142\n",
      "Iteration 42793 Loss: 0.9304237365722656\n",
      "Iteration 42794 Loss: 0.7168546319007874\n",
      "Iteration 42795 Loss: 1.1217706203460693\n",
      "Iteration 42796 Loss: 1.2415622472763062\n",
      "Iteration 42797 Loss: 0.7287028431892395\n",
      "Iteration 42798 Loss: 1.246671199798584\n",
      "Iteration 42799 Loss: 1.1429744958877563\n",
      "Iteration 42799 Loss: 0.998011589050293\n",
      "Iteration 42800 Loss: 0.9831604361534119\n",
      "Iteration 42801 Loss: 0.9383729696273804\n",
      "Iteration 42802 Loss: 0.8995486497879028\n",
      "Iteration 42803 Loss: 0.7123962640762329\n",
      "Iteration 42804 Loss: 1.2229788303375244\n",
      "Iteration 42805 Loss: 0.7088807225227356\n",
      "Iteration 42806 Loss: 0.9230489134788513\n",
      "Iteration 42807 Loss: 0.7632850408554077\n",
      "Iteration 42808 Loss: 0.8825097680091858\n",
      "Iteration 42809 Loss: 1.0098319053649902\n",
      "Iteration 42809 Loss: 0.9044013023376465\n",
      "Iteration 42810 Loss: 1.135380744934082\n",
      "Iteration 42811 Loss: 1.0138698816299438\n",
      "Iteration 42812 Loss: 0.9193968772888184\n",
      "Iteration 42813 Loss: 1.0194536447525024\n",
      "Iteration 42814 Loss: 0.6429170966148376\n",
      "Iteration 42815 Loss: 0.9308921694755554\n",
      "Iteration 42816 Loss: 0.6312599778175354\n",
      "Iteration 42817 Loss: 0.9561458826065063\n",
      "Iteration 42818 Loss: 0.8055050373077393\n",
      "Iteration 42819 Loss: 0.6228827238082886\n",
      "Iteration 42819 Loss: 0.8677703738212585\n",
      "Iteration 42820 Loss: 0.857511043548584\n",
      "Iteration 42821 Loss: 1.0881065130233765\n",
      "Iteration 42822 Loss: 1.1678532361984253\n",
      "Iteration 42823 Loss: 1.006441354751587\n",
      "Iteration 42824 Loss: 0.8888362646102905\n",
      "Iteration 42825 Loss: 0.9950700998306274\n",
      "Iteration 42826 Loss: 0.9336364269256592\n",
      "Iteration 42827 Loss: 0.8076915740966797\n",
      "Iteration 42828 Loss: 1.3209718465805054\n",
      "Iteration 42829 Loss: 0.7532992362976074\n",
      "Iteration 42829 Loss: 0.981941819190979\n",
      "Iteration 42830 Loss: 0.7491394877433777\n",
      "Iteration 42831 Loss: 0.8290895819664001\n",
      "Iteration 42832 Loss: 1.0662919282913208\n",
      "Iteration 42833 Loss: 0.9183385968208313\n",
      "Iteration 42834 Loss: 1.221072793006897\n",
      "Iteration 42835 Loss: 1.0409104824066162\n",
      "Iteration 42836 Loss: 1.0897846221923828\n",
      "Iteration 42837 Loss: 1.009498119354248\n",
      "Iteration 42838 Loss: 0.7076674103736877\n",
      "Iteration 42839 Loss: 0.707938015460968\n",
      "Iteration 42839 Loss: 0.9339731335639954\n",
      "Iteration 42840 Loss: 0.8834774494171143\n",
      "Iteration 42841 Loss: 1.057469129562378\n",
      "Iteration 42842 Loss: 1.1730390787124634\n",
      "Iteration 42843 Loss: 1.1285099983215332\n",
      "Iteration 42844 Loss: 0.803577184677124\n",
      "Iteration 42845 Loss: 1.0830230712890625\n",
      "Iteration 42846 Loss: 0.5818581581115723\n",
      "Iteration 42847 Loss: 0.923753559589386\n",
      "Iteration 42848 Loss: 1.023364782333374\n",
      "Iteration 42849 Loss: 0.6794809699058533\n",
      "Iteration 42849 Loss: 0.9337552785873413\n",
      "Iteration 42850 Loss: 0.9906635284423828\n",
      "Iteration 42851 Loss: 1.1437726020812988\n",
      "Iteration 42852 Loss: 0.9631392359733582\n",
      "Iteration 42853 Loss: 1.031337857246399\n",
      "Iteration 42854 Loss: 1.140026569366455\n",
      "Iteration 42855 Loss: 0.8298633098602295\n",
      "Iteration 42856 Loss: 0.926124632358551\n",
      "Iteration 42857 Loss: 1.0440795421600342\n",
      "Iteration 42858 Loss: 1.003697156906128\n",
      "Iteration 42859 Loss: 1.0015735626220703\n",
      "Iteration 42859 Loss: 1.0074278116226196\n",
      "Iteration 42860 Loss: 0.9681424498558044\n",
      "Iteration 42861 Loss: 0.9098535180091858\n",
      "Iteration 42862 Loss: 1.017058253288269\n",
      "Iteration 42863 Loss: 1.2210534811019897\n",
      "Iteration 42864 Loss: 0.8790171146392822\n",
      "Iteration 42865 Loss: 0.9921401143074036\n",
      "Iteration 42866 Loss: 0.8450360298156738\n",
      "Iteration 42867 Loss: 0.8058066368103027\n",
      "Iteration 42868 Loss: 0.8310298919677734\n",
      "Iteration 42869 Loss: 1.11797297000885\n",
      "Iteration 42869 Loss: 0.9587110280990601\n",
      "Iteration 42870 Loss: 0.9273979663848877\n",
      "Iteration 42871 Loss: 0.6916014552116394\n",
      "Iteration 42872 Loss: 0.39523062109947205\n",
      "Iteration 42873 Loss: 1.0325011014938354\n",
      "Iteration 42874 Loss: 1.1667087078094482\n",
      "Iteration 42875 Loss: 0.7997540235519409\n",
      "Iteration 42876 Loss: 0.9412767291069031\n",
      "Iteration 42877 Loss: 0.923592746257782\n",
      "Iteration 42878 Loss: 0.865587592124939\n",
      "Iteration 42879 Loss: 0.8233275413513184\n",
      "Iteration 42879 Loss: 0.8566978573799133\n",
      "Iteration 42880 Loss: 0.8990890383720398\n",
      "Iteration 42881 Loss: 0.9363062381744385\n",
      "Iteration 42882 Loss: 1.240544319152832\n",
      "Iteration 42883 Loss: 0.8128431439399719\n",
      "Iteration 42884 Loss: 0.8547239303588867\n",
      "Iteration 42885 Loss: 1.0844011306762695\n",
      "Iteration 42886 Loss: 1.1996415853500366\n",
      "Iteration 42887 Loss: 0.907842755317688\n",
      "Iteration 42888 Loss: 1.007528305053711\n",
      "Iteration 42889 Loss: 1.0584299564361572\n",
      "Iteration 42889 Loss: 1.000135064125061\n",
      "Iteration 42890 Loss: 1.0578691959381104\n",
      "Iteration 42891 Loss: 1.0297702550888062\n",
      "Iteration 42892 Loss: 0.7502696514129639\n",
      "Iteration 42893 Loss: 0.5780606865882874\n",
      "Iteration 42894 Loss: 0.7543579339981079\n",
      "Iteration 42895 Loss: 0.8929832577705383\n",
      "Iteration 42896 Loss: 0.95304274559021\n",
      "Iteration 42897 Loss: 1.0210736989974976\n",
      "Iteration 42898 Loss: 1.008452296257019\n",
      "Iteration 42899 Loss: 0.6715229749679565\n",
      "Iteration 42899 Loss: 0.8717402219772339\n",
      "Iteration 42900 Loss: 0.70542311668396\n",
      "Iteration 42901 Loss: 0.640890896320343\n",
      "Iteration 42902 Loss: 0.8050128817558289\n",
      "Iteration 42903 Loss: 1.0206589698791504\n",
      "Iteration 42904 Loss: 0.7586446404457092\n",
      "Iteration 42905 Loss: 0.861617386341095\n",
      "Iteration 42906 Loss: 0.8530699610710144\n",
      "Iteration 42907 Loss: 1.1674976348876953\n",
      "Iteration 42908 Loss: 1.208435297012329\n",
      "Iteration 42909 Loss: 0.7573006749153137\n",
      "Iteration 42909 Loss: 0.877855122089386\n",
      "Iteration 42910 Loss: 0.928040087223053\n",
      "Iteration 42911 Loss: 0.9117316007614136\n",
      "Iteration 42912 Loss: 0.9703290462493896\n",
      "Iteration 42913 Loss: 0.6053116321563721\n",
      "Iteration 42914 Loss: 1.0063447952270508\n",
      "Iteration 42915 Loss: 0.7382041215896606\n",
      "Iteration 42916 Loss: 0.9433972835540771\n",
      "Iteration 42917 Loss: 0.8034651279449463\n",
      "Iteration 42918 Loss: 0.8820708394050598\n",
      "Iteration 42919 Loss: 1.210597276687622\n",
      "Iteration 42919 Loss: 0.8999491930007935\n",
      "Iteration 42920 Loss: 0.9447423815727234\n",
      "Iteration 42921 Loss: 0.8045613169670105\n",
      "Iteration 42922 Loss: 1.0123584270477295\n",
      "Iteration 42923 Loss: 0.8655756711959839\n",
      "Iteration 42924 Loss: 1.0568631887435913\n",
      "Iteration 42925 Loss: 0.6479657888412476\n",
      "Iteration 42926 Loss: 1.3457005023956299\n",
      "Iteration 42927 Loss: 0.8191360831260681\n",
      "Iteration 42928 Loss: 1.4276599884033203\n",
      "Iteration 42929 Loss: 0.9183934926986694\n",
      "Iteration 42929 Loss: 0.984295666217804\n",
      "Iteration 42930 Loss: 1.0448588132858276\n",
      "Iteration 42931 Loss: 0.8796610236167908\n",
      "Iteration 42932 Loss: 0.8584063053131104\n",
      "Iteration 42933 Loss: 0.9324656128883362\n",
      "Iteration 42934 Loss: 0.7317822575569153\n",
      "Iteration 42935 Loss: 1.0769885778427124\n",
      "Iteration 42936 Loss: 0.9867820739746094\n",
      "Iteration 42937 Loss: 1.2631534337997437\n",
      "Iteration 42938 Loss: 0.8233135938644409\n",
      "Iteration 42939 Loss: 0.9100802540779114\n",
      "Iteration 42939 Loss: 0.9507492184638977\n",
      "Iteration 42940 Loss: 1.078183650970459\n",
      "Iteration 42941 Loss: 0.7555448412895203\n",
      "Iteration 42942 Loss: 0.8568912148475647\n",
      "Iteration 42943 Loss: 0.6270327568054199\n",
      "Iteration 42944 Loss: 1.3643743991851807\n",
      "Iteration 42945 Loss: 0.8825156092643738\n",
      "Iteration 42946 Loss: 0.6786965727806091\n",
      "Iteration 42947 Loss: 1.04155433177948\n",
      "Iteration 42948 Loss: 0.9022293090820312\n",
      "Iteration 42949 Loss: 0.8215674161911011\n",
      "Iteration 42949 Loss: 0.900858998298645\n",
      "Iteration 42950 Loss: 0.7956759929656982\n",
      "Iteration 42951 Loss: 1.1246720552444458\n",
      "Iteration 42952 Loss: 0.913677990436554\n",
      "Iteration 42953 Loss: 1.0974534749984741\n",
      "Iteration 42954 Loss: 0.9460346698760986\n",
      "Iteration 42955 Loss: 1.0424085855484009\n",
      "Iteration 42956 Loss: 1.2169162034988403\n",
      "Iteration 42957 Loss: 0.938838541507721\n",
      "Iteration 42958 Loss: 0.8887499570846558\n",
      "Iteration 42959 Loss: 0.5856974124908447\n",
      "Iteration 42959 Loss: 0.9550126194953918\n",
      "Iteration 42960 Loss: 0.9793778657913208\n",
      "Iteration 42961 Loss: 1.009534239768982\n",
      "Iteration 42962 Loss: 0.972794771194458\n",
      "Iteration 42963 Loss: 0.927242636680603\n",
      "Iteration 42964 Loss: 1.1483628749847412\n",
      "Iteration 42965 Loss: 0.9851306676864624\n",
      "Iteration 42966 Loss: 1.4365605115890503\n",
      "Iteration 42967 Loss: 1.2033792734146118\n",
      "Iteration 42968 Loss: 0.9192937612533569\n",
      "Iteration 42969 Loss: 0.7513629198074341\n",
      "Iteration 42969 Loss: 1.03330397605896\n",
      "Iteration 42970 Loss: 0.9987024664878845\n",
      "Iteration 42971 Loss: 0.9764349460601807\n",
      "Iteration 42972 Loss: 0.7395274639129639\n",
      "Iteration 42973 Loss: 0.9564606547355652\n",
      "Iteration 42974 Loss: 0.8012790679931641\n",
      "Iteration 42975 Loss: 1.1009514331817627\n",
      "Iteration 42976 Loss: 1.0428756475448608\n",
      "Iteration 42977 Loss: 1.1690644025802612\n",
      "Iteration 42978 Loss: 1.1473710536956787\n",
      "Iteration 42979 Loss: 0.9057363271713257\n",
      "Iteration 42979 Loss: 0.9838403463363647\n",
      "Iteration 42980 Loss: 1.0635839700698853\n",
      "Iteration 42981 Loss: 1.0525481700897217\n",
      "Iteration 42982 Loss: 0.6560077667236328\n",
      "Iteration 42983 Loss: 0.9485064744949341\n",
      "Iteration 42984 Loss: 0.9469174742698669\n",
      "Iteration 42985 Loss: 0.8332715630531311\n",
      "Iteration 42986 Loss: 0.8401007652282715\n",
      "Iteration 42987 Loss: 0.7531261444091797\n",
      "Iteration 42988 Loss: 0.9408283233642578\n",
      "Iteration 42989 Loss: 1.2777098417282104\n",
      "Iteration 42989 Loss: 0.9312599897384644\n",
      "Iteration 42990 Loss: 0.8696739673614502\n",
      "Iteration 42991 Loss: 1.1775455474853516\n",
      "Iteration 42992 Loss: 0.7111445069313049\n",
      "Iteration 42993 Loss: 0.7070447206497192\n",
      "Iteration 42994 Loss: 1.1331628561019897\n",
      "Iteration 42995 Loss: 0.6215373873710632\n",
      "Iteration 42996 Loss: 1.0235142707824707\n",
      "Iteration 42997 Loss: 0.7008051872253418\n",
      "Iteration 42998 Loss: 1.0249481201171875\n",
      "Iteration 42999 Loss: 0.7344161868095398\n",
      "Iteration 42999 Loss: 0.8703792691230774\n",
      "Iteration 43000 Loss: 0.7926397323608398\n",
      "Iteration 43001 Loss: 0.8080642223358154\n",
      "Iteration 43002 Loss: 1.389549970626831\n",
      "Iteration 43003 Loss: 1.0201356410980225\n",
      "Iteration 43004 Loss: 0.9286989569664001\n",
      "Iteration 43005 Loss: 1.0188372135162354\n",
      "Iteration 43006 Loss: 0.6803375482559204\n",
      "Iteration 43007 Loss: 0.7941937446594238\n",
      "Iteration 43008 Loss: 0.7590707540512085\n",
      "Iteration 43009 Loss: 0.7282747030258179\n",
      "Iteration 43009 Loss: 0.8919803500175476\n",
      "Iteration 43010 Loss: 1.3541150093078613\n",
      "Iteration 43011 Loss: 1.0754376649856567\n",
      "Iteration 43012 Loss: 0.8114715814590454\n",
      "Iteration 43013 Loss: 1.218898892402649\n",
      "Iteration 43014 Loss: 0.7219125032424927\n",
      "Iteration 43015 Loss: 0.8757482767105103\n",
      "Iteration 43016 Loss: 1.0455740690231323\n",
      "Iteration 43017 Loss: 0.7681058049201965\n",
      "Iteration 43018 Loss: 1.3907415866851807\n",
      "Iteration 43019 Loss: 0.6771358251571655\n",
      "Iteration 43019 Loss: 0.993914008140564\n",
      "Iteration 43020 Loss: 0.9433432817459106\n",
      "Iteration 43021 Loss: 1.0405488014221191\n",
      "Iteration 43022 Loss: 1.105101227760315\n",
      "Iteration 43023 Loss: 0.6820618510246277\n",
      "Iteration 43024 Loss: 0.9556330442428589\n",
      "Iteration 43025 Loss: 0.8663502931594849\n",
      "Iteration 43026 Loss: 0.9125943183898926\n",
      "Iteration 43027 Loss: 0.8473102450370789\n",
      "Iteration 43028 Loss: 1.052017092704773\n",
      "Iteration 43029 Loss: 0.5880948305130005\n",
      "Iteration 43029 Loss: 0.8993054628372192\n",
      "Iteration 43030 Loss: 0.759901762008667\n",
      "Iteration 43031 Loss: 1.1184407472610474\n",
      "Iteration 43032 Loss: 1.2931627035140991\n",
      "Iteration 43033 Loss: 1.1152925491333008\n",
      "Iteration 43034 Loss: 1.2144662141799927\n",
      "Iteration 43035 Loss: 0.7891286611557007\n",
      "Iteration 43036 Loss: 0.906951904296875\n",
      "Iteration 43037 Loss: 0.8877784013748169\n",
      "Iteration 43038 Loss: 1.061052680015564\n",
      "Iteration 43039 Loss: 0.8254509568214417\n",
      "Iteration 43039 Loss: 0.9971626400947571\n",
      "Iteration 43040 Loss: 0.6839300394058228\n",
      "Iteration 43041 Loss: 1.000165581703186\n",
      "Iteration 43042 Loss: 0.8647082448005676\n",
      "Iteration 43043 Loss: 0.5380513072013855\n",
      "Iteration 43044 Loss: 0.8090472221374512\n",
      "Iteration 43045 Loss: 0.7247689962387085\n",
      "Iteration 43046 Loss: 0.8717655539512634\n",
      "Iteration 43047 Loss: 0.9689579606056213\n",
      "Iteration 43048 Loss: 1.2331407070159912\n",
      "Iteration 43049 Loss: 1.2178720235824585\n",
      "Iteration 43049 Loss: 0.8912407755851746\n",
      "Iteration 43050 Loss: 1.0338399410247803\n",
      "Iteration 43051 Loss: 0.773699164390564\n",
      "Iteration 43052 Loss: 0.7436559200286865\n",
      "Iteration 43053 Loss: 0.5286998748779297\n",
      "Iteration 43054 Loss: 1.1848260164260864\n",
      "Iteration 43055 Loss: 0.7705358266830444\n",
      "Iteration 43056 Loss: 0.6389662623405457\n",
      "Iteration 43057 Loss: 0.86309415102005\n",
      "Iteration 43058 Loss: 0.8983168005943298\n",
      "Iteration 43059 Loss: 1.2909705638885498\n",
      "Iteration 43059 Loss: 0.8726604580879211\n",
      "Iteration 43060 Loss: 0.7548093795776367\n",
      "Iteration 43061 Loss: 0.9849978089332581\n",
      "Iteration 43062 Loss: 1.1666193008422852\n",
      "Iteration 43063 Loss: 1.1401286125183105\n",
      "Iteration 43064 Loss: 0.5837547779083252\n",
      "Iteration 43065 Loss: 0.8268640041351318\n",
      "Iteration 43066 Loss: 0.5760887265205383\n",
      "Iteration 43067 Loss: 0.9651131629943848\n",
      "Iteration 43068 Loss: 1.234969139099121\n",
      "Iteration 43069 Loss: 1.3116309642791748\n",
      "Iteration 43069 Loss: 0.9544976949691772\n",
      "Iteration 43070 Loss: 1.0289883613586426\n",
      "Iteration 43071 Loss: 1.231768012046814\n",
      "Iteration 43072 Loss: 0.8808668255805969\n",
      "Iteration 43073 Loss: 0.9051592946052551\n",
      "Iteration 43074 Loss: 0.9968836903572083\n",
      "Iteration 43075 Loss: 1.089771032333374\n",
      "Iteration 43076 Loss: 0.7824980616569519\n",
      "Iteration 43077 Loss: 0.8801900148391724\n",
      "Iteration 43078 Loss: 1.115635871887207\n",
      "Iteration 43079 Loss: 1.1335011720657349\n",
      "Iteration 43079 Loss: 1.0045263767242432\n",
      "Iteration 43080 Loss: 1.1332290172576904\n",
      "Iteration 43081 Loss: 1.0478500127792358\n",
      "Iteration 43082 Loss: 0.9758942723274231\n",
      "Iteration 43083 Loss: 0.794479250907898\n",
      "Iteration 43084 Loss: 0.8712673783302307\n",
      "Iteration 43085 Loss: 0.8652125000953674\n",
      "Iteration 43086 Loss: 0.9104508757591248\n",
      "Iteration 43087 Loss: 1.0433802604675293\n",
      "Iteration 43088 Loss: 0.6345383524894714\n",
      "Iteration 43089 Loss: 0.8377049565315247\n",
      "Iteration 43089 Loss: 0.9114006161689758\n",
      "Iteration 43090 Loss: 0.8874917030334473\n",
      "Iteration 43091 Loss: 0.961130678653717\n",
      "Iteration 43092 Loss: 1.002075433731079\n",
      "Iteration 43093 Loss: 1.0055651664733887\n",
      "Iteration 43094 Loss: 1.1024696826934814\n",
      "Iteration 43095 Loss: 0.46206432580947876\n",
      "Iteration 43096 Loss: 1.2774173021316528\n",
      "Iteration 43097 Loss: 0.6521331667900085\n",
      "Iteration 43098 Loss: 0.9701408743858337\n",
      "Iteration 43099 Loss: 1.167661190032959\n",
      "Iteration 43099 Loss: 0.9488149881362915\n",
      "Iteration 43100 Loss: 0.9682546854019165\n",
      "Iteration 43101 Loss: 1.0118387937545776\n",
      "Iteration 43102 Loss: 1.2135612964630127\n",
      "Iteration 43103 Loss: 1.063401222229004\n",
      "Iteration 43104 Loss: 0.9473604559898376\n",
      "Iteration 43105 Loss: 0.7125168442726135\n",
      "Iteration 43106 Loss: 0.8102565407752991\n",
      "Iteration 43107 Loss: 0.7312474846839905\n",
      "Iteration 43108 Loss: 1.0151417255401611\n",
      "Iteration 43109 Loss: 0.8652353882789612\n",
      "Iteration 43109 Loss: 0.9338814616203308\n",
      "Iteration 43110 Loss: 1.055675983428955\n",
      "Iteration 43111 Loss: 1.2528607845306396\n",
      "Iteration 43112 Loss: 0.8453851342201233\n",
      "Iteration 43113 Loss: 0.9158049821853638\n",
      "Iteration 43114 Loss: 0.7235803008079529\n",
      "Iteration 43115 Loss: 1.0377310514450073\n",
      "Iteration 43116 Loss: 0.9156269431114197\n",
      "Iteration 43117 Loss: 0.9217837452888489\n",
      "Iteration 43118 Loss: 0.8754326105117798\n",
      "Iteration 43119 Loss: 1.0930860042572021\n",
      "Iteration 43119 Loss: 0.9636967778205872\n",
      "Iteration 43120 Loss: 0.8917264938354492\n",
      "Iteration 43121 Loss: 0.9768341779708862\n",
      "Iteration 43122 Loss: 1.0582515001296997\n",
      "Iteration 43123 Loss: 0.7261679172515869\n",
      "Iteration 43124 Loss: 0.6698479652404785\n",
      "Iteration 43125 Loss: 0.8908217549324036\n",
      "Iteration 43126 Loss: 0.9946686625480652\n",
      "Iteration 43127 Loss: 1.0578960180282593\n",
      "Iteration 43128 Loss: 1.2227283716201782\n",
      "Iteration 43129 Loss: 0.593870222568512\n",
      "Iteration 43129 Loss: 0.9082813262939453\n",
      "Iteration 43130 Loss: 1.0282258987426758\n",
      "Iteration 43131 Loss: 0.9440904259681702\n",
      "Iteration 43132 Loss: 0.6636315584182739\n",
      "Iteration 43133 Loss: 0.8684417009353638\n",
      "Iteration 43134 Loss: 0.8697880506515503\n",
      "Iteration 43135 Loss: 0.5128970146179199\n",
      "Iteration 43136 Loss: 0.8943862915039062\n",
      "Iteration 43137 Loss: 0.8087589740753174\n",
      "Iteration 43138 Loss: 1.2972933053970337\n",
      "Iteration 43139 Loss: 0.6884459853172302\n",
      "Iteration 43139 Loss: 0.8575959205627441\n",
      "Iteration 43140 Loss: 0.6590073108673096\n",
      "Iteration 43141 Loss: 1.1507189273834229\n",
      "Iteration 43142 Loss: 0.9395765066146851\n",
      "Iteration 43143 Loss: 0.8266466856002808\n",
      "Iteration 43144 Loss: 0.8417425751686096\n",
      "Iteration 43145 Loss: 1.1221407651901245\n",
      "Iteration 43146 Loss: 0.6932470202445984\n",
      "Iteration 43147 Loss: 0.7668790817260742\n",
      "Iteration 43148 Loss: 0.7384693622589111\n",
      "Iteration 43149 Loss: 1.0760776996612549\n",
      "Iteration 43149 Loss: 0.8814506530761719\n",
      "Iteration 43150 Loss: 0.9820504784584045\n",
      "Iteration 43151 Loss: 1.1505277156829834\n",
      "Iteration 43152 Loss: 0.8363893032073975\n",
      "Iteration 43153 Loss: 0.9461983442306519\n",
      "Iteration 43154 Loss: 1.031341791152954\n",
      "Iteration 43155 Loss: 0.966725766658783\n",
      "Iteration 43156 Loss: 0.5776312947273254\n",
      "Iteration 43157 Loss: 1.069616675376892\n",
      "Iteration 43158 Loss: 0.7972880005836487\n",
      "Iteration 43159 Loss: 1.1657614707946777\n",
      "Iteration 43159 Loss: 0.9523531198501587\n",
      "Iteration 43160 Loss: 0.9299936294555664\n",
      "Iteration 43161 Loss: 0.6423512101173401\n",
      "Iteration 43162 Loss: 0.6654952168464661\n",
      "Iteration 43163 Loss: 0.7735683917999268\n",
      "Iteration 43164 Loss: 0.8655548691749573\n",
      "Iteration 43165 Loss: 1.3298699855804443\n",
      "Iteration 43166 Loss: 0.8692359924316406\n",
      "Iteration 43167 Loss: 0.6887729167938232\n",
      "Iteration 43168 Loss: 1.0088471174240112\n",
      "Iteration 43169 Loss: 0.9717270135879517\n",
      "Iteration 43169 Loss: 0.8745416402816772\n",
      "Iteration 43170 Loss: 1.3493562936782837\n",
      "Iteration 43171 Loss: 1.10905122756958\n",
      "Iteration 43172 Loss: 1.0944139957427979\n",
      "Iteration 43173 Loss: 0.8522652983665466\n",
      "Iteration 43174 Loss: 0.8709580302238464\n",
      "Iteration 43175 Loss: 1.052607774734497\n",
      "Iteration 43176 Loss: 1.018663763999939\n",
      "Iteration 43177 Loss: 0.9191515445709229\n",
      "Iteration 43178 Loss: 1.2471235990524292\n",
      "Iteration 43179 Loss: 0.8385543823242188\n",
      "Iteration 43179 Loss: 1.0352145433425903\n",
      "Iteration 43180 Loss: 0.93879634141922\n",
      "Iteration 43181 Loss: 1.1400725841522217\n",
      "Iteration 43182 Loss: 1.157007098197937\n",
      "Iteration 43183 Loss: 0.8476017117500305\n",
      "Iteration 43184 Loss: 0.6961276531219482\n",
      "Iteration 43185 Loss: 0.6490697264671326\n",
      "Iteration 43186 Loss: 1.0145877599716187\n",
      "Iteration 43187 Loss: 1.0988495349884033\n",
      "Iteration 43188 Loss: 0.7479841113090515\n",
      "Iteration 43189 Loss: 0.7282669544219971\n",
      "Iteration 43189 Loss: 0.9018363952636719\n",
      "Iteration 43190 Loss: 0.8125709891319275\n",
      "Iteration 43191 Loss: 0.8830379843711853\n",
      "Iteration 43192 Loss: 0.9141824245452881\n",
      "Iteration 43193 Loss: 0.7956438660621643\n",
      "Iteration 43194 Loss: 1.3244937658309937\n",
      "Iteration 43195 Loss: 0.5097777247428894\n",
      "Iteration 43196 Loss: 0.7577682137489319\n",
      "Iteration 43197 Loss: 0.9241424202919006\n",
      "Iteration 43198 Loss: 0.8693863749504089\n",
      "Iteration 43199 Loss: 0.739388108253479\n",
      "Iteration 43199 Loss: 0.8530391454696655\n",
      "Iteration 43200 Loss: 0.9248682856559753\n",
      "Iteration 43201 Loss: 1.0344055891036987\n",
      "Iteration 43202 Loss: 0.8166141510009766\n",
      "Iteration 43203 Loss: 1.2749122381210327\n",
      "Iteration 43204 Loss: 0.7663311958312988\n",
      "Iteration 43205 Loss: 0.9463353157043457\n",
      "Iteration 43206 Loss: 1.2759873867034912\n",
      "Iteration 43207 Loss: 1.1234211921691895\n",
      "Iteration 43208 Loss: 0.6782065629959106\n",
      "Iteration 43209 Loss: 1.1482011079788208\n",
      "Iteration 43209 Loss: 0.9989282488822937\n",
      "Iteration 43210 Loss: 0.8592535257339478\n",
      "Iteration 43211 Loss: 1.137929081916809\n",
      "Iteration 43212 Loss: 0.7636718153953552\n",
      "Iteration 43213 Loss: 0.9773653745651245\n",
      "Iteration 43214 Loss: 0.897463858127594\n",
      "Iteration 43215 Loss: 1.0047177076339722\n",
      "Iteration 43216 Loss: 1.0934085845947266\n",
      "Iteration 43217 Loss: 0.7245561480522156\n",
      "Iteration 43218 Loss: 0.8808205723762512\n",
      "Iteration 43219 Loss: 1.069288969039917\n",
      "Iteration 43219 Loss: 0.9408475756645203\n",
      "Iteration 43220 Loss: 1.0714411735534668\n",
      "Iteration 43221 Loss: 0.8487062454223633\n",
      "Iteration 43222 Loss: 0.6067550778388977\n",
      "Iteration 43223 Loss: 0.864467203617096\n",
      "Iteration 43224 Loss: 0.836054801940918\n",
      "Iteration 43225 Loss: 0.8870331048965454\n",
      "Iteration 43226 Loss: 1.0775609016418457\n",
      "Iteration 43227 Loss: 0.663173258304596\n",
      "Iteration 43228 Loss: 0.7518395185470581\n",
      "Iteration 43229 Loss: 0.899182915687561\n",
      "Iteration 43229 Loss: 0.8506214022636414\n",
      "Iteration 43230 Loss: 1.1399668455123901\n",
      "Iteration 43231 Loss: 0.7224767208099365\n",
      "Iteration 43232 Loss: 0.9933488368988037\n",
      "Iteration 43233 Loss: 0.9778318405151367\n",
      "Iteration 43234 Loss: 1.1220946311950684\n",
      "Iteration 43235 Loss: 1.0440388917922974\n",
      "Iteration 43236 Loss: 0.7782679796218872\n",
      "Iteration 43237 Loss: 1.120164155960083\n",
      "Iteration 43238 Loss: 1.1057424545288086\n",
      "Iteration 43239 Loss: 1.207520604133606\n",
      "Iteration 43239 Loss: 1.0211453437805176\n",
      "Iteration 43240 Loss: 0.9868773221969604\n",
      "Iteration 43241 Loss: 0.7138532996177673\n",
      "Iteration 43242 Loss: 0.8975037336349487\n",
      "Iteration 43243 Loss: 1.1200616359710693\n",
      "Iteration 43244 Loss: 1.067038655281067\n",
      "Iteration 43245 Loss: 0.8836925029754639\n",
      "Iteration 43246 Loss: 0.7267566323280334\n",
      "Iteration 43247 Loss: 0.7519839406013489\n",
      "Iteration 43248 Loss: 0.9251869916915894\n",
      "Iteration 43249 Loss: 1.1872949600219727\n",
      "Iteration 43249 Loss: 0.9260250329971313\n",
      "Iteration 43250 Loss: 0.8948578238487244\n",
      "Iteration 43251 Loss: 0.9546532034873962\n",
      "Iteration 43252 Loss: 0.7987096905708313\n",
      "Iteration 43253 Loss: 0.9566113352775574\n",
      "Iteration 43254 Loss: 0.5799421072006226\n",
      "Iteration 43255 Loss: 0.9530752301216125\n",
      "Iteration 43256 Loss: 0.595880925655365\n",
      "Iteration 43257 Loss: 0.8952208757400513\n",
      "Iteration 43258 Loss: 0.9429093599319458\n",
      "Iteration 43259 Loss: 1.0342611074447632\n",
      "Iteration 43259 Loss: 0.8606122136116028\n",
      "Iteration 43260 Loss: 0.8826937079429626\n",
      "Iteration 43261 Loss: 0.8445028066635132\n",
      "Iteration 43262 Loss: 0.9088635444641113\n",
      "Iteration 43263 Loss: 0.7840744853019714\n",
      "Iteration 43264 Loss: 1.0401266813278198\n",
      "Iteration 43265 Loss: 0.9123148918151855\n",
      "Iteration 43266 Loss: 0.8359118700027466\n",
      "Iteration 43267 Loss: 1.1515406370162964\n",
      "Iteration 43268 Loss: 0.8103024959564209\n",
      "Iteration 43269 Loss: 0.5923637747764587\n",
      "Iteration 43269 Loss: 0.8762694597244263\n",
      "Iteration 43270 Loss: 1.0320792198181152\n",
      "Iteration 43271 Loss: 1.0057032108306885\n",
      "Iteration 43272 Loss: 0.8721765279769897\n",
      "Iteration 43273 Loss: 0.9918125867843628\n",
      "Iteration 43274 Loss: 0.8831995129585266\n",
      "Iteration 43275 Loss: 0.6310001611709595\n",
      "Iteration 43276 Loss: 1.009846568107605\n",
      "Iteration 43277 Loss: 0.8619092106819153\n",
      "Iteration 43278 Loss: 0.8460732698440552\n",
      "Iteration 43279 Loss: 0.8812769055366516\n",
      "Iteration 43279 Loss: 0.9015076756477356\n",
      "Iteration 43280 Loss: 0.8464933633804321\n",
      "Iteration 43281 Loss: 1.0607693195343018\n",
      "Iteration 43282 Loss: 0.8898674845695496\n",
      "Iteration 43283 Loss: 1.0636245012283325\n",
      "Iteration 43284 Loss: 0.7225416898727417\n",
      "Iteration 43285 Loss: 0.8064972758293152\n",
      "Iteration 43286 Loss: 0.6880369782447815\n",
      "Iteration 43287 Loss: 0.937504768371582\n",
      "Iteration 43288 Loss: 1.0849037170410156\n",
      "Iteration 43289 Loss: 0.7828978896141052\n",
      "Iteration 43289 Loss: 0.8883136510848999\n",
      "Iteration 43290 Loss: 0.9140610694885254\n",
      "Iteration 43291 Loss: 1.1560406684875488\n",
      "Iteration 43292 Loss: 1.148436188697815\n",
      "Iteration 43293 Loss: 0.9944671392440796\n",
      "Iteration 43294 Loss: 0.8219925761222839\n",
      "Iteration 43295 Loss: 0.857234001159668\n",
      "Iteration 43296 Loss: 0.6911629438400269\n",
      "Iteration 43297 Loss: 0.8494597673416138\n",
      "Iteration 43298 Loss: 1.0945111513137817\n",
      "Iteration 43299 Loss: 1.2179316282272339\n",
      "Iteration 43299 Loss: 0.9745296239852905\n",
      "Iteration 43300 Loss: 0.7921590805053711\n",
      "Iteration 43301 Loss: 1.0968964099884033\n",
      "Iteration 43302 Loss: 0.8289763927459717\n",
      "Iteration 43303 Loss: 0.9543195366859436\n",
      "Iteration 43304 Loss: 1.01120126247406\n",
      "Iteration 43305 Loss: 0.8948201537132263\n",
      "Iteration 43306 Loss: 0.6533010005950928\n",
      "Iteration 43307 Loss: 0.5490573048591614\n",
      "Iteration 43308 Loss: 0.8534680604934692\n",
      "Iteration 43309 Loss: 1.0022344589233398\n",
      "Iteration 43309 Loss: 0.8636433482170105\n",
      "Iteration 43310 Loss: 1.3034735918045044\n",
      "Iteration 43311 Loss: 1.1476776599884033\n",
      "Iteration 43312 Loss: 1.056630253791809\n",
      "Iteration 43313 Loss: 1.2249211072921753\n",
      "Iteration 43314 Loss: 0.6264322400093079\n",
      "Iteration 43315 Loss: 1.0007991790771484\n",
      "Iteration 43316 Loss: 0.8714435696601868\n",
      "Iteration 43317 Loss: 1.1391202211380005\n",
      "Iteration 43318 Loss: 0.811863899230957\n",
      "Iteration 43319 Loss: 0.9708037972450256\n",
      "Iteration 43319 Loss: 1.0153166055679321\n",
      "Iteration 43320 Loss: 1.1146774291992188\n",
      "Iteration 43321 Loss: 0.8329452276229858\n",
      "Iteration 43322 Loss: 0.8091927766799927\n",
      "Iteration 43323 Loss: 0.7790618538856506\n",
      "Iteration 43324 Loss: 1.4424904584884644\n",
      "Iteration 43325 Loss: 0.8927175402641296\n",
      "Iteration 43326 Loss: 0.820054292678833\n",
      "Iteration 43327 Loss: 1.223656415939331\n",
      "Iteration 43328 Loss: 0.9503875374794006\n",
      "Iteration 43329 Loss: 0.6602448225021362\n",
      "Iteration 43329 Loss: 0.9525429010391235\n",
      "Iteration 43330 Loss: 1.0430887937545776\n",
      "Iteration 43331 Loss: 1.1765631437301636\n",
      "Iteration 43332 Loss: 0.9576588869094849\n",
      "Iteration 43333 Loss: 0.8821219801902771\n",
      "Iteration 43334 Loss: 1.1800974607467651\n",
      "Iteration 43335 Loss: 0.9788472056388855\n",
      "Iteration 43336 Loss: 0.8022080659866333\n",
      "Iteration 43337 Loss: 0.8519328236579895\n",
      "Iteration 43338 Loss: 0.8727738261222839\n",
      "Iteration 43339 Loss: 0.8485552668571472\n",
      "Iteration 43339 Loss: 0.9593847393989563\n",
      "Iteration 43340 Loss: 1.2805125713348389\n",
      "Iteration 43341 Loss: 0.7836201190948486\n",
      "Iteration 43342 Loss: 1.0907320976257324\n",
      "Iteration 43343 Loss: 1.0913037061691284\n",
      "Iteration 43344 Loss: 1.0314722061157227\n",
      "Iteration 43345 Loss: 1.0012547969818115\n",
      "Iteration 43346 Loss: 0.9806477427482605\n",
      "Iteration 43347 Loss: 1.2230005264282227\n",
      "Iteration 43348 Loss: 0.7072842717170715\n",
      "Iteration 43349 Loss: 1.2093449831008911\n",
      "Iteration 43349 Loss: 1.0399173498153687\n",
      "Iteration 43350 Loss: 1.089518427848816\n",
      "Iteration 43351 Loss: 1.0114409923553467\n",
      "Iteration 43352 Loss: 1.0954309701919556\n",
      "Iteration 43353 Loss: 0.702954888343811\n",
      "Iteration 43354 Loss: 1.122225046157837\n",
      "Iteration 43355 Loss: 1.0600560903549194\n",
      "Iteration 43356 Loss: 1.077413558959961\n",
      "Iteration 43357 Loss: 0.8604153990745544\n",
      "Iteration 43358 Loss: 0.9018286466598511\n",
      "Iteration 43359 Loss: 0.6678807735443115\n",
      "Iteration 43359 Loss: 0.9589165449142456\n",
      "Iteration 43360 Loss: 1.11099374294281\n",
      "Iteration 43361 Loss: 0.6672371029853821\n",
      "Iteration 43362 Loss: 0.921572744846344\n",
      "Iteration 43363 Loss: 0.9131073951721191\n",
      "Iteration 43364 Loss: 0.724824845790863\n",
      "Iteration 43365 Loss: 0.8809946179389954\n",
      "Iteration 43366 Loss: 0.7921369671821594\n",
      "Iteration 43367 Loss: 0.9879682064056396\n",
      "Iteration 43368 Loss: 0.9713762998580933\n",
      "Iteration 43369 Loss: 0.7933273911476135\n",
      "Iteration 43369 Loss: 0.8763540387153625\n",
      "Iteration 43370 Loss: 0.95189368724823\n",
      "Iteration 43371 Loss: 1.1400492191314697\n",
      "Iteration 43372 Loss: 0.9992361068725586\n",
      "Iteration 43373 Loss: 0.7309953570365906\n",
      "Iteration 43374 Loss: 1.1268621683120728\n",
      "Iteration 43375 Loss: 0.8259304165840149\n",
      "Iteration 43376 Loss: 1.0296247005462646\n",
      "Iteration 43377 Loss: 1.048020362854004\n",
      "Iteration 43378 Loss: 1.1448016166687012\n",
      "Iteration 43379 Loss: 0.6805509924888611\n",
      "Iteration 43379 Loss: 0.9677964448928833\n",
      "Iteration 43380 Loss: 1.1766715049743652\n",
      "Iteration 43381 Loss: 1.1834272146224976\n",
      "Iteration 43382 Loss: 1.1184526681900024\n",
      "Iteration 43383 Loss: 1.055330753326416\n",
      "Iteration 43384 Loss: 0.9296435713768005\n",
      "Iteration 43385 Loss: 1.1533269882202148\n",
      "Iteration 43386 Loss: 0.9289206862449646\n",
      "Iteration 43387 Loss: 1.2403088808059692\n",
      "Iteration 43388 Loss: 1.0112868547439575\n",
      "Iteration 43389 Loss: 0.713214099407196\n",
      "Iteration 43389 Loss: 1.051058292388916\n",
      "Iteration 43390 Loss: 0.8484940528869629\n",
      "Iteration 43391 Loss: 0.9761872887611389\n",
      "Iteration 43392 Loss: 1.0014742612838745\n",
      "Iteration 43393 Loss: 1.0033869743347168\n",
      "Iteration 43394 Loss: 1.265774130821228\n",
      "Iteration 43395 Loss: 0.8209367990493774\n",
      "Iteration 43396 Loss: 1.1163557767868042\n",
      "Iteration 43397 Loss: 1.1321463584899902\n",
      "Iteration 43398 Loss: 0.7727850675582886\n",
      "Iteration 43399 Loss: 0.8122864365577698\n",
      "Iteration 43399 Loss: 0.9749826192855835\n",
      "Iteration 43400 Loss: 0.8720139265060425\n",
      "Iteration 43401 Loss: 0.7857514023780823\n",
      "Iteration 43402 Loss: 0.9537047147750854\n",
      "Iteration 43403 Loss: 0.9155810475349426\n",
      "Iteration 43404 Loss: 0.9448045492172241\n",
      "Iteration 43405 Loss: 0.9646596908569336\n",
      "Iteration 43406 Loss: 0.9378360509872437\n",
      "Iteration 43407 Loss: 0.821029782295227\n",
      "Iteration 43408 Loss: 0.8367425203323364\n",
      "Iteration 43409 Loss: 0.7644262313842773\n",
      "Iteration 43409 Loss: 0.8796550631523132\n",
      "Iteration 43410 Loss: 1.0038602352142334\n",
      "Iteration 43411 Loss: 1.2847331762313843\n",
      "Iteration 43412 Loss: 0.8169777393341064\n",
      "Iteration 43413 Loss: 0.693926215171814\n",
      "Iteration 43414 Loss: 0.8620672225952148\n",
      "Iteration 43415 Loss: 1.3752936124801636\n",
      "Iteration 43416 Loss: 1.1672296524047852\n",
      "Iteration 43417 Loss: 0.7083811163902283\n",
      "Iteration 43418 Loss: 0.7914724946022034\n",
      "Iteration 43419 Loss: 1.002211332321167\n",
      "Iteration 43419 Loss: 0.9706152677536011\n",
      "Iteration 43420 Loss: 0.903693437576294\n",
      "Iteration 43421 Loss: 0.8285321593284607\n",
      "Iteration 43422 Loss: 0.761084794998169\n",
      "Iteration 43423 Loss: 0.9612603187561035\n",
      "Iteration 43424 Loss: 0.8995420932769775\n",
      "Iteration 43425 Loss: 0.9876155853271484\n",
      "Iteration 43426 Loss: 1.0703953504562378\n",
      "Iteration 43427 Loss: 1.0119965076446533\n",
      "Iteration 43428 Loss: 0.8910010457038879\n",
      "Iteration 43429 Loss: 1.087140440940857\n",
      "Iteration 43429 Loss: 0.9402261972427368\n",
      "Iteration 43430 Loss: 1.307248592376709\n",
      "Iteration 43431 Loss: 1.0222828388214111\n",
      "Iteration 43432 Loss: 0.7611230611801147\n",
      "Iteration 43433 Loss: 0.6933578848838806\n",
      "Iteration 43434 Loss: 1.2881594896316528\n",
      "Iteration 43435 Loss: 0.833500325679779\n",
      "Iteration 43436 Loss: 0.6991003751754761\n",
      "Iteration 43437 Loss: 0.903815507888794\n",
      "Iteration 43438 Loss: 0.7837762236595154\n",
      "Iteration 43439 Loss: 1.1758593320846558\n",
      "Iteration 43439 Loss: 0.9468223452568054\n",
      "Iteration 43440 Loss: 0.9674467444419861\n",
      "Iteration 43441 Loss: 1.0756319761276245\n",
      "Iteration 43442 Loss: 1.1891437768936157\n",
      "Iteration 43443 Loss: 0.871060311794281\n",
      "Iteration 43444 Loss: 0.9138712286949158\n",
      "Iteration 43445 Loss: 1.0122641324996948\n",
      "Iteration 43446 Loss: 1.4416158199310303\n",
      "Iteration 43447 Loss: 0.9319778084754944\n",
      "Iteration 43448 Loss: 0.9525547623634338\n",
      "Iteration 43449 Loss: 0.6497896313667297\n",
      "Iteration 43449 Loss: 1.0005357265472412\n",
      "Iteration 43450 Loss: 1.4550212621688843\n",
      "Iteration 43451 Loss: 0.9625526666641235\n",
      "Iteration 43452 Loss: 1.1855697631835938\n",
      "Iteration 43453 Loss: 0.6009032130241394\n",
      "Iteration 43454 Loss: 0.6659202575683594\n",
      "Iteration 43455 Loss: 0.822704553604126\n",
      "Iteration 43456 Loss: 1.2436316013336182\n",
      "Iteration 43457 Loss: 1.3374229669570923\n",
      "Iteration 43458 Loss: 0.7590498328208923\n",
      "Iteration 43459 Loss: 0.9697157740592957\n",
      "Iteration 43459 Loss: 1.0002491474151611\n",
      "Iteration 43460 Loss: 1.2188953161239624\n",
      "Iteration 43461 Loss: 1.3377116918563843\n",
      "Iteration 43462 Loss: 1.1014753580093384\n",
      "Iteration 43463 Loss: 1.097336769104004\n",
      "Iteration 43464 Loss: 0.7531036138534546\n",
      "Iteration 43465 Loss: 1.0137094259262085\n",
      "Iteration 43466 Loss: 0.8612278699874878\n",
      "Iteration 43467 Loss: 0.727642834186554\n",
      "Iteration 43468 Loss: 1.015628695487976\n",
      "Iteration 43469 Loss: 0.862040102481842\n",
      "Iteration 43469 Loss: 0.998877227306366\n",
      "Iteration 43470 Loss: 0.936855673789978\n",
      "Iteration 43471 Loss: 0.9337635040283203\n",
      "Iteration 43472 Loss: 1.1237428188323975\n",
      "Iteration 43473 Loss: 1.1451916694641113\n",
      "Iteration 43474 Loss: 1.136620044708252\n",
      "Iteration 43475 Loss: 0.5735006332397461\n",
      "Iteration 43476 Loss: 1.0157227516174316\n",
      "Iteration 43477 Loss: 1.020395278930664\n",
      "Iteration 43478 Loss: 0.6979427933692932\n",
      "Iteration 43479 Loss: 0.872765064239502\n",
      "Iteration 43479 Loss: 0.9456501007080078\n",
      "Iteration 43480 Loss: 1.1308339834213257\n",
      "Iteration 43481 Loss: 0.4132815897464752\n",
      "Iteration 43482 Loss: 0.8632281422615051\n",
      "Iteration 43483 Loss: 0.6992568969726562\n",
      "Iteration 43484 Loss: 1.126735806465149\n",
      "Iteration 43485 Loss: 0.9930053353309631\n",
      "Iteration 43486 Loss: 1.064333200454712\n",
      "Iteration 43487 Loss: 0.7954496741294861\n",
      "Iteration 43488 Loss: 0.9297296404838562\n",
      "Iteration 43489 Loss: 0.780709445476532\n",
      "Iteration 43489 Loss: 0.8796563148498535\n",
      "Iteration 43490 Loss: 0.977276623249054\n",
      "Iteration 43491 Loss: 1.0754739046096802\n",
      "Iteration 43492 Loss: 0.8400923609733582\n",
      "Iteration 43493 Loss: 0.9698527455329895\n",
      "Iteration 43494 Loss: 1.0125855207443237\n",
      "Iteration 43495 Loss: 1.0394433736801147\n",
      "Iteration 43496 Loss: 0.7515071034431458\n",
      "Iteration 43497 Loss: 0.7781915664672852\n",
      "Iteration 43498 Loss: 0.5849857926368713\n",
      "Iteration 43499 Loss: 0.9205417633056641\n",
      "Iteration 43499 Loss: 0.8949950933456421\n",
      "Iteration 43500 Loss: 0.788537323474884\n",
      "Iteration 43501 Loss: 0.6161069273948669\n",
      "Iteration 43502 Loss: 0.8133524656295776\n",
      "Iteration 43503 Loss: 0.8524446487426758\n",
      "Iteration 43504 Loss: 0.9367860555648804\n",
      "Iteration 43505 Loss: 1.2297677993774414\n",
      "Iteration 43506 Loss: 0.9008500576019287\n",
      "Iteration 43507 Loss: 0.6627361178398132\n",
      "Iteration 43508 Loss: 1.1930243968963623\n",
      "Iteration 43509 Loss: 1.009392499923706\n",
      "Iteration 43509 Loss: 0.9002998471260071\n",
      "Iteration 43510 Loss: 0.9281715154647827\n",
      "Iteration 43511 Loss: 1.3551667928695679\n",
      "Iteration 43512 Loss: 1.1444029808044434\n",
      "Iteration 43513 Loss: 0.8356326222419739\n",
      "Iteration 43514 Loss: 0.9650332927703857\n",
      "Iteration 43515 Loss: 0.9229273796081543\n",
      "Iteration 43516 Loss: 1.1030278205871582\n",
      "Iteration 43517 Loss: 1.0712052583694458\n",
      "Iteration 43518 Loss: 1.1496721506118774\n",
      "Iteration 43519 Loss: 0.8683285117149353\n",
      "Iteration 43519 Loss: 1.0343568325042725\n",
      "Iteration 43520 Loss: 0.7516500353813171\n",
      "Iteration 43521 Loss: 1.1935527324676514\n",
      "Iteration 43522 Loss: 1.0058790445327759\n",
      "Iteration 43523 Loss: 0.566091001033783\n",
      "Iteration 43524 Loss: 1.116796851158142\n",
      "Iteration 43525 Loss: 1.2010297775268555\n",
      "Iteration 43526 Loss: 0.940911591053009\n",
      "Iteration 43527 Loss: 1.039543628692627\n",
      "Iteration 43528 Loss: 0.8115151524543762\n",
      "Iteration 43529 Loss: 1.1885614395141602\n",
      "Iteration 43529 Loss: 0.9815530776977539\n",
      "Iteration 43530 Loss: 1.0579040050506592\n",
      "Iteration 43531 Loss: 0.8288566470146179\n",
      "Iteration 43532 Loss: 1.2584673166275024\n",
      "Iteration 43533 Loss: 1.2433041334152222\n",
      "Iteration 43534 Loss: 0.8710993528366089\n",
      "Iteration 43535 Loss: 1.0537670850753784\n",
      "Iteration 43536 Loss: 1.3377436399459839\n",
      "Iteration 43537 Loss: 0.9647836685180664\n",
      "Iteration 43538 Loss: 1.0909217596054077\n",
      "Iteration 43539 Loss: 0.9283286333084106\n",
      "Iteration 43539 Loss: 1.063517689704895\n",
      "Iteration 43540 Loss: 0.8153801560401917\n",
      "Iteration 43541 Loss: 0.8525465130805969\n",
      "Iteration 43542 Loss: 0.9941191673278809\n",
      "Iteration 43543 Loss: 0.84698486328125\n",
      "Iteration 43544 Loss: 1.040414810180664\n",
      "Iteration 43545 Loss: 1.0725593566894531\n",
      "Iteration 43546 Loss: 0.95907062292099\n",
      "Iteration 43547 Loss: 1.14032781124115\n",
      "Iteration 43548 Loss: 1.004945158958435\n",
      "Iteration 43549 Loss: 1.0315762758255005\n",
      "Iteration 43549 Loss: 0.975792407989502\n",
      "Iteration 43550 Loss: 0.6986254453659058\n",
      "Iteration 43551 Loss: 0.9711284637451172\n",
      "Iteration 43552 Loss: 1.1415843963623047\n",
      "Iteration 43553 Loss: 0.8202837109565735\n",
      "Iteration 43554 Loss: 0.7998397350311279\n",
      "Iteration 43555 Loss: 1.0384489297866821\n",
      "Iteration 43556 Loss: 1.0517902374267578\n",
      "Iteration 43557 Loss: 0.8521726727485657\n",
      "Iteration 43558 Loss: 0.9613345861434937\n",
      "Iteration 43559 Loss: 1.041491150856018\n",
      "Iteration 43559 Loss: 0.9376699328422546\n",
      "Iteration 43560 Loss: 0.9724000692367554\n",
      "Iteration 43561 Loss: 1.1910196542739868\n",
      "Iteration 43562 Loss: 1.3100401163101196\n",
      "Iteration 43563 Loss: 0.812341034412384\n",
      "Iteration 43564 Loss: 0.8034681677818298\n",
      "Iteration 43565 Loss: 0.9540391564369202\n",
      "Iteration 43566 Loss: 0.6632974743843079\n",
      "Iteration 43567 Loss: 1.0083158016204834\n",
      "Iteration 43568 Loss: 1.007252812385559\n",
      "Iteration 43569 Loss: 1.0595695972442627\n",
      "Iteration 43569 Loss: 0.9781745076179504\n",
      "Iteration 43570 Loss: 0.8186047673225403\n",
      "Iteration 43571 Loss: 0.7496958374977112\n",
      "Iteration 43572 Loss: 0.5901452898979187\n",
      "Iteration 43573 Loss: 1.1094557046890259\n",
      "Iteration 43574 Loss: 0.8854743838310242\n",
      "Iteration 43575 Loss: 0.9285980463027954\n",
      "Iteration 43576 Loss: 1.133748173713684\n",
      "Iteration 43577 Loss: 0.9851562976837158\n",
      "Iteration 43578 Loss: 0.8259778618812561\n",
      "Iteration 43579 Loss: 1.2532098293304443\n",
      "Iteration 43579 Loss: 0.9280065298080444\n",
      "Iteration 43580 Loss: 1.2870090007781982\n",
      "Iteration 43581 Loss: 1.0264812707901\n",
      "Iteration 43582 Loss: 0.5258118510246277\n",
      "Iteration 43583 Loss: 1.2821890115737915\n",
      "Iteration 43584 Loss: 0.951675295829773\n",
      "Iteration 43585 Loss: 0.8152670860290527\n",
      "Iteration 43586 Loss: 0.7758083343505859\n",
      "Iteration 43587 Loss: 0.9630444645881653\n",
      "Iteration 43588 Loss: 0.8888861536979675\n",
      "Iteration 43589 Loss: 0.9589530229568481\n",
      "Iteration 43589 Loss: 0.9475125074386597\n",
      "Iteration 43590 Loss: 0.9792892932891846\n",
      "Iteration 43591 Loss: 1.045997142791748\n",
      "Iteration 43592 Loss: 0.8747743368148804\n",
      "Iteration 43593 Loss: 1.093860149383545\n",
      "Iteration 43594 Loss: 0.8542411923408508\n",
      "Iteration 43595 Loss: 0.5843070149421692\n",
      "Iteration 43596 Loss: 0.9961076974868774\n",
      "Iteration 43597 Loss: 0.9179869294166565\n",
      "Iteration 43598 Loss: 0.9439100027084351\n",
      "Iteration 43599 Loss: 1.1075432300567627\n",
      "Iteration 43599 Loss: 0.9398016929626465\n",
      "Iteration 43600 Loss: 0.9011569023132324\n",
      "Iteration 43601 Loss: 0.9474616050720215\n",
      "Iteration 43602 Loss: 1.1138134002685547\n",
      "Iteration 43603 Loss: 1.3005765676498413\n",
      "Iteration 43604 Loss: 1.0316742658615112\n",
      "Iteration 43605 Loss: 0.9959313273429871\n",
      "Iteration 43606 Loss: 0.9302059412002563\n",
      "Iteration 43607 Loss: 0.9836819171905518\n",
      "Iteration 43608 Loss: 1.0316085815429688\n",
      "Iteration 43609 Loss: 0.5714054703712463\n",
      "Iteration 43609 Loss: 0.980751633644104\n",
      "Iteration 43610 Loss: 1.3851439952850342\n",
      "Iteration 43611 Loss: 0.8803625106811523\n",
      "Iteration 43612 Loss: 0.819408655166626\n",
      "Iteration 43613 Loss: 0.9062234163284302\n",
      "Iteration 43614 Loss: 1.110077977180481\n",
      "Iteration 43615 Loss: 0.8028929233551025\n",
      "Iteration 43616 Loss: 1.403847098350525\n",
      "Iteration 43617 Loss: 1.0348771810531616\n",
      "Iteration 43618 Loss: 1.0014538764953613\n",
      "Iteration 43619 Loss: 0.8941807746887207\n",
      "Iteration 43619 Loss: 1.0238467454910278\n",
      "Iteration 43620 Loss: 1.062058687210083\n",
      "Iteration 43621 Loss: 1.0716519355773926\n",
      "Iteration 43622 Loss: 0.7924935817718506\n",
      "Iteration 43623 Loss: 0.8772929310798645\n",
      "Iteration 43624 Loss: 0.8402029871940613\n",
      "Iteration 43625 Loss: 0.923481822013855\n",
      "Iteration 43626 Loss: 0.8874674439430237\n",
      "Iteration 43627 Loss: 1.0910282135009766\n",
      "Iteration 43628 Loss: 0.5823038220405579\n",
      "Iteration 43629 Loss: 0.7698352336883545\n",
      "Iteration 43629 Loss: 0.889781653881073\n",
      "Iteration 43630 Loss: 0.816283106803894\n",
      "Iteration 43631 Loss: 1.0751900672912598\n",
      "Iteration 43632 Loss: 1.0581856966018677\n",
      "Iteration 43633 Loss: 1.1840819120407104\n",
      "Iteration 43634 Loss: 0.8394386768341064\n",
      "Iteration 43635 Loss: 0.8957075476646423\n",
      "Iteration 43636 Loss: 0.7290050387382507\n",
      "Iteration 43637 Loss: 0.9971026182174683\n",
      "Iteration 43638 Loss: 0.9299818873405457\n",
      "Iteration 43639 Loss: 0.9534937739372253\n",
      "Iteration 43639 Loss: 0.9478470683097839\n",
      "Iteration 43640 Loss: 1.1068354845046997\n",
      "Iteration 43641 Loss: 0.8345146179199219\n",
      "Iteration 43642 Loss: 0.9461908340454102\n",
      "Iteration 43643 Loss: 0.885300874710083\n",
      "Iteration 43644 Loss: 1.2039518356323242\n",
      "Iteration 43645 Loss: 1.1664035320281982\n",
      "Iteration 43646 Loss: 1.1890463829040527\n",
      "Iteration 43647 Loss: 0.9436305165290833\n",
      "Iteration 43648 Loss: 1.0325146913528442\n",
      "Iteration 43649 Loss: 0.9891243577003479\n",
      "Iteration 43649 Loss: 1.0297514200210571\n",
      "Iteration 43650 Loss: 0.6826161742210388\n",
      "Iteration 43651 Loss: 0.866940975189209\n",
      "Iteration 43652 Loss: 0.8354500532150269\n",
      "Iteration 43653 Loss: 1.0354324579238892\n",
      "Iteration 43654 Loss: 0.7760259509086609\n",
      "Iteration 43655 Loss: 1.156253695487976\n",
      "Iteration 43656 Loss: 1.0257985591888428\n",
      "Iteration 43657 Loss: 0.6035103797912598\n",
      "Iteration 43658 Loss: 1.0881562232971191\n",
      "Iteration 43659 Loss: 1.1353702545166016\n",
      "Iteration 43659 Loss: 0.9205554723739624\n",
      "Iteration 43660 Loss: 1.2237509489059448\n",
      "Iteration 43661 Loss: 0.7081438302993774\n",
      "Iteration 43662 Loss: 1.0443006753921509\n",
      "Iteration 43663 Loss: 1.0726494789123535\n",
      "Iteration 43664 Loss: 1.0432896614074707\n",
      "Iteration 43665 Loss: 0.9464089274406433\n",
      "Iteration 43666 Loss: 1.280207633972168\n",
      "Iteration 43667 Loss: 0.8242596983909607\n",
      "Iteration 43668 Loss: 1.0126677751541138\n",
      "Iteration 43669 Loss: 0.848075270652771\n",
      "Iteration 43669 Loss: 1.0003753900527954\n",
      "Iteration 43670 Loss: 0.900812566280365\n",
      "Iteration 43671 Loss: 1.3279764652252197\n",
      "Iteration 43672 Loss: 0.899535059928894\n",
      "Iteration 43673 Loss: 1.4325001239776611\n",
      "Iteration 43674 Loss: 1.163818597793579\n",
      "Iteration 43675 Loss: 0.9434840679168701\n",
      "Iteration 43676 Loss: 0.8423687219619751\n",
      "Iteration 43677 Loss: 0.9326041340827942\n",
      "Iteration 43678 Loss: 0.8601130247116089\n",
      "Iteration 43679 Loss: 0.9693915843963623\n",
      "Iteration 43679 Loss: 1.027260422706604\n",
      "Iteration 43680 Loss: 0.8714936375617981\n",
      "Iteration 43681 Loss: 0.952102541923523\n",
      "Iteration 43682 Loss: 0.7503691911697388\n",
      "Iteration 43683 Loss: 0.7189633846282959\n",
      "Iteration 43684 Loss: 1.04933762550354\n",
      "Iteration 43685 Loss: 1.063641905784607\n",
      "Iteration 43686 Loss: 0.905855655670166\n",
      "Iteration 43687 Loss: 0.8604913949966431\n",
      "Iteration 43688 Loss: 1.4588313102722168\n",
      "Iteration 43689 Loss: 0.7811710238456726\n",
      "Iteration 43689 Loss: 0.9412258267402649\n",
      "Iteration 43690 Loss: 0.9220046997070312\n",
      "Iteration 43691 Loss: 1.0243315696716309\n",
      "Iteration 43692 Loss: 0.9700585603713989\n",
      "Iteration 43693 Loss: 1.0866626501083374\n",
      "Iteration 43694 Loss: 1.047961711883545\n",
      "Iteration 43695 Loss: 1.1501576900482178\n",
      "Iteration 43696 Loss: 0.9294768571853638\n",
      "Iteration 43697 Loss: 0.8929553031921387\n",
      "Iteration 43698 Loss: 0.9726372361183167\n",
      "Iteration 43699 Loss: 0.9448421597480774\n",
      "Iteration 43699 Loss: 0.9941087961196899\n",
      "Iteration 43700 Loss: 1.2441928386688232\n",
      "Iteration 43701 Loss: 0.7780783176422119\n",
      "Iteration 43702 Loss: 1.0609718561172485\n",
      "Iteration 43703 Loss: 1.2738984823226929\n",
      "Iteration 43704 Loss: 0.8933606743812561\n",
      "Iteration 43705 Loss: 0.9887576699256897\n",
      "Iteration 43706 Loss: 0.8685603737831116\n",
      "Iteration 43707 Loss: 1.078131914138794\n",
      "Iteration 43708 Loss: 1.0502270460128784\n",
      "Iteration 43709 Loss: 1.0294115543365479\n",
      "Iteration 43709 Loss: 1.0265591144561768\n",
      "Iteration 43710 Loss: 1.2909411191940308\n",
      "Iteration 43711 Loss: 0.6822566986083984\n",
      "Iteration 43712 Loss: 0.9577770829200745\n",
      "Iteration 43713 Loss: 0.7492480874061584\n",
      "Iteration 43714 Loss: 1.449028730392456\n",
      "Iteration 43715 Loss: 0.7459108829498291\n",
      "Iteration 43716 Loss: 0.8899781703948975\n",
      "Iteration 43717 Loss: 0.9634146690368652\n",
      "Iteration 43718 Loss: 1.2948682308197021\n",
      "Iteration 43719 Loss: 0.7319726943969727\n",
      "Iteration 43719 Loss: 0.9755395650863647\n",
      "Iteration 43720 Loss: 0.7423699498176575\n",
      "Iteration 43721 Loss: 1.1103475093841553\n",
      "Iteration 43722 Loss: 1.1852829456329346\n",
      "Iteration 43723 Loss: 0.8071033954620361\n",
      "Iteration 43724 Loss: 1.055517554283142\n",
      "Iteration 43725 Loss: 0.7943990230560303\n",
      "Iteration 43726 Loss: 0.9744402766227722\n",
      "Iteration 43727 Loss: 1.0504814386367798\n",
      "Iteration 43728 Loss: 0.9478830099105835\n",
      "Iteration 43729 Loss: 0.829693078994751\n",
      "Iteration 43729 Loss: 0.9497518539428711\n",
      "Iteration 43730 Loss: 1.3426578044891357\n",
      "Iteration 43731 Loss: 0.8413646221160889\n",
      "Iteration 43732 Loss: 0.9452982544898987\n",
      "Iteration 43733 Loss: 0.844936192035675\n",
      "Iteration 43734 Loss: 0.9760240316390991\n",
      "Iteration 43735 Loss: 0.8769959807395935\n",
      "Iteration 43736 Loss: 1.2718719244003296\n",
      "Iteration 43737 Loss: 0.5801202654838562\n",
      "Iteration 43738 Loss: 0.946618378162384\n",
      "Iteration 43739 Loss: 0.9634900093078613\n",
      "Iteration 43739 Loss: 0.9589377641677856\n",
      "Iteration 43740 Loss: 1.078343152999878\n",
      "Iteration 43741 Loss: 1.1357462406158447\n",
      "Iteration 43742 Loss: 0.6850282549858093\n",
      "Iteration 43743 Loss: 1.0633563995361328\n",
      "Iteration 43744 Loss: 1.0904128551483154\n",
      "Iteration 43745 Loss: 0.9295781254768372\n",
      "Iteration 43746 Loss: 0.7178927659988403\n",
      "Iteration 43747 Loss: 1.1619805097579956\n",
      "Iteration 43748 Loss: 0.5854904651641846\n",
      "Iteration 43749 Loss: 0.9122591614723206\n",
      "Iteration 43749 Loss: 0.9360088109970093\n",
      "Iteration 43750 Loss: 1.4359962940216064\n",
      "Iteration 43751 Loss: 0.7602070569992065\n",
      "Iteration 43752 Loss: 0.959432065486908\n",
      "Iteration 43753 Loss: 1.02440345287323\n",
      "Iteration 43754 Loss: 0.8631364703178406\n",
      "Iteration 43755 Loss: 0.7210169434547424\n",
      "Iteration 43756 Loss: 0.7976899147033691\n",
      "Iteration 43757 Loss: 1.1545970439910889\n",
      "Iteration 43758 Loss: 1.1943466663360596\n",
      "Iteration 43759 Loss: 1.1552828550338745\n",
      "Iteration 43759 Loss: 1.0066109895706177\n",
      "Iteration 43760 Loss: 0.8589223027229309\n",
      "Iteration 43761 Loss: 0.8774860501289368\n",
      "Iteration 43762 Loss: 1.080997109413147\n",
      "Iteration 43763 Loss: 0.8857088088989258\n",
      "Iteration 43764 Loss: 0.7299418449401855\n",
      "Iteration 43765 Loss: 1.148106575012207\n",
      "Iteration 43766 Loss: 0.8626843094825745\n",
      "Iteration 43767 Loss: 0.7265030741691589\n",
      "Iteration 43768 Loss: 1.0311858654022217\n",
      "Iteration 43769 Loss: 1.1136831045150757\n",
      "Iteration 43769 Loss: 0.9315218925476074\n",
      "Iteration 43770 Loss: 1.1070818901062012\n",
      "Iteration 43771 Loss: 1.0791736841201782\n",
      "Iteration 43772 Loss: 0.9438132047653198\n",
      "Iteration 43773 Loss: 1.0370079278945923\n",
      "Iteration 43774 Loss: 1.0002262592315674\n",
      "Iteration 43775 Loss: 0.8343439102172852\n",
      "Iteration 43776 Loss: 0.7623319625854492\n",
      "Iteration 43777 Loss: 0.7283971905708313\n",
      "Iteration 43778 Loss: 1.082872986793518\n",
      "Iteration 43779 Loss: 0.8692773580551147\n",
      "Iteration 43779 Loss: 0.9444526433944702\n",
      "Iteration 43780 Loss: 1.1831358671188354\n",
      "Iteration 43781 Loss: 0.9246335029602051\n",
      "Iteration 43782 Loss: 0.8761135339736938\n",
      "Iteration 43783 Loss: 0.7595043778419495\n",
      "Iteration 43784 Loss: 1.1498514413833618\n",
      "Iteration 43785 Loss: 0.8972671031951904\n",
      "Iteration 43786 Loss: 0.7659541368484497\n",
      "Iteration 43787 Loss: 0.9471377730369568\n",
      "Iteration 43788 Loss: 0.5836287140846252\n",
      "Iteration 43789 Loss: 1.171751856803894\n",
      "Iteration 43789 Loss: 0.9258977770805359\n",
      "Iteration 43790 Loss: 0.8031496405601501\n",
      "Iteration 43791 Loss: 0.8506789207458496\n",
      "Iteration 43792 Loss: 0.8655467629432678\n",
      "Iteration 43793 Loss: 0.8241074085235596\n",
      "Iteration 43794 Loss: 0.9781851768493652\n",
      "Iteration 43795 Loss: 0.9529196619987488\n",
      "Iteration 43796 Loss: 0.9463761448860168\n",
      "Iteration 43797 Loss: 1.3424454927444458\n",
      "Iteration 43798 Loss: 0.9668585658073425\n",
      "Iteration 43799 Loss: 0.581192135810852\n",
      "Iteration 43799 Loss: 0.9111460447311401\n",
      "Iteration 43800 Loss: 1.022145390510559\n",
      "Iteration 43801 Loss: 1.4114032983779907\n",
      "Iteration 43802 Loss: 1.0540547370910645\n",
      "Iteration 43803 Loss: 1.15146803855896\n",
      "Iteration 43804 Loss: 1.14845871925354\n",
      "Iteration 43805 Loss: 0.9295180439949036\n",
      "Iteration 43806 Loss: 0.8736963868141174\n",
      "Iteration 43807 Loss: 1.1237295866012573\n",
      "Iteration 43808 Loss: 0.8859806060791016\n",
      "Iteration 43809 Loss: 0.889817476272583\n",
      "Iteration 43809 Loss: 1.0490272045135498\n",
      "Iteration 43810 Loss: 1.0635261535644531\n",
      "Iteration 43811 Loss: 0.908208429813385\n",
      "Iteration 43812 Loss: 1.1642072200775146\n",
      "Iteration 43813 Loss: 1.0732409954071045\n",
      "Iteration 43814 Loss: 0.8060153126716614\n",
      "Iteration 43815 Loss: 0.6377202868461609\n",
      "Iteration 43816 Loss: 1.0173970460891724\n",
      "Iteration 43817 Loss: 0.7679375410079956\n",
      "Iteration 43818 Loss: 1.0956547260284424\n",
      "Iteration 43819 Loss: 0.6453701257705688\n",
      "Iteration 43819 Loss: 0.9179278612136841\n",
      "Iteration 43820 Loss: 0.8841438293457031\n",
      "Iteration 43821 Loss: 1.1676312685012817\n",
      "Iteration 43822 Loss: 0.7485364675521851\n",
      "Iteration 43823 Loss: 0.9791937470436096\n",
      "Iteration 43824 Loss: 1.591124176979065\n",
      "Iteration 43825 Loss: 1.0689619779586792\n",
      "Iteration 43826 Loss: 0.6472895741462708\n",
      "Iteration 43827 Loss: 0.9044583439826965\n",
      "Iteration 43828 Loss: 0.6423413157463074\n",
      "Iteration 43829 Loss: 0.9359508156776428\n",
      "Iteration 43829 Loss: 0.956963062286377\n",
      "Iteration 43830 Loss: 1.0272623300552368\n",
      "Iteration 43831 Loss: 0.9619647860527039\n",
      "Iteration 43832 Loss: 0.799256443977356\n",
      "Iteration 43833 Loss: 1.0818458795547485\n",
      "Iteration 43834 Loss: 1.0600329637527466\n",
      "Iteration 43835 Loss: 0.8184415102005005\n",
      "Iteration 43836 Loss: 0.8979347348213196\n",
      "Iteration 43837 Loss: 0.743291974067688\n",
      "Iteration 43838 Loss: 1.3031038045883179\n",
      "Iteration 43839 Loss: 1.1136884689331055\n",
      "Iteration 43839 Loss: 0.9806822538375854\n",
      "Iteration 43840 Loss: 0.9126593470573425\n",
      "Iteration 43841 Loss: 0.7836713194847107\n",
      "Iteration 43842 Loss: 1.1814545392990112\n",
      "Iteration 43843 Loss: 0.6210089921951294\n",
      "Iteration 43844 Loss: 0.8631154894828796\n",
      "Iteration 43845 Loss: 1.3170349597930908\n",
      "Iteration 43846 Loss: 0.7758897542953491\n",
      "Iteration 43847 Loss: 0.8791642785072327\n",
      "Iteration 43848 Loss: 1.0402684211730957\n",
      "Iteration 43849 Loss: 0.7669910192489624\n",
      "Iteration 43849 Loss: 0.9141258001327515\n",
      "Iteration 43850 Loss: 1.0424046516418457\n",
      "Iteration 43851 Loss: 0.7839027047157288\n",
      "Iteration 43852 Loss: 0.7593795657157898\n",
      "Iteration 43853 Loss: 1.0107449293136597\n",
      "Iteration 43854 Loss: 1.0323574542999268\n",
      "Iteration 43855 Loss: 0.8833567500114441\n",
      "Iteration 43856 Loss: 1.1003440618515015\n",
      "Iteration 43857 Loss: 0.8016343116760254\n",
      "Iteration 43858 Loss: 0.6962072849273682\n",
      "Iteration 43859 Loss: 0.8699179887771606\n",
      "Iteration 43859 Loss: 0.8980249166488647\n",
      "Iteration 43860 Loss: 0.8009601831436157\n",
      "Iteration 43861 Loss: 1.0781121253967285\n",
      "Iteration 43862 Loss: 0.7827032208442688\n",
      "Iteration 43863 Loss: 0.8278362154960632\n",
      "Iteration 43864 Loss: 0.8320587277412415\n",
      "Iteration 43865 Loss: 0.8961192965507507\n",
      "Iteration 43866 Loss: 0.7832595109939575\n",
      "Iteration 43867 Loss: 0.902815043926239\n",
      "Iteration 43868 Loss: 0.4497659504413605\n",
      "Iteration 43869 Loss: 0.8091537356376648\n",
      "Iteration 43869 Loss: 0.816278338432312\n",
      "Iteration 43870 Loss: 1.2702430486679077\n",
      "Iteration 43871 Loss: 0.7707986235618591\n",
      "Iteration 43872 Loss: 1.1930369138717651\n",
      "Iteration 43873 Loss: 0.9639408588409424\n",
      "Iteration 43874 Loss: 0.9520371556282043\n",
      "Iteration 43875 Loss: 0.7945354580879211\n",
      "Iteration 43876 Loss: 0.39938491582870483\n",
      "Iteration 43877 Loss: 1.1475569009780884\n",
      "Iteration 43878 Loss: 0.7440416216850281\n",
      "Iteration 43879 Loss: 0.7759963870048523\n",
      "Iteration 43879 Loss: 0.9011572599411011\n",
      "Iteration 43880 Loss: 0.8131842613220215\n",
      "Iteration 43881 Loss: 1.3005064725875854\n",
      "Iteration 43882 Loss: 0.8500046730041504\n",
      "Iteration 43883 Loss: 1.0413814783096313\n",
      "Iteration 43884 Loss: 1.4129325151443481\n",
      "Iteration 43885 Loss: 1.0832622051239014\n",
      "Iteration 43886 Loss: 0.8511892557144165\n",
      "Iteration 43887 Loss: 0.9098222851753235\n",
      "Iteration 43888 Loss: 0.7555387020111084\n",
      "Iteration 43889 Loss: 0.8031586408615112\n",
      "Iteration 43889 Loss: 0.9820981025695801\n",
      "Iteration 43890 Loss: 0.6486791372299194\n",
      "Iteration 43891 Loss: 1.0456403493881226\n",
      "Iteration 43892 Loss: 0.9259436130523682\n",
      "Iteration 43893 Loss: 0.8665903210639954\n",
      "Iteration 43894 Loss: 0.8447062969207764\n",
      "Iteration 43895 Loss: 0.8514132499694824\n",
      "Iteration 43896 Loss: 0.7694995999336243\n",
      "Iteration 43897 Loss: 1.1097100973129272\n",
      "Iteration 43898 Loss: 0.9567052125930786\n",
      "Iteration 43899 Loss: 1.1184839010238647\n",
      "Iteration 43899 Loss: 0.9137371778488159\n",
      "Iteration 43900 Loss: 0.7057327628135681\n",
      "Iteration 43901 Loss: 0.9956950545310974\n",
      "Iteration 43902 Loss: 0.5697250962257385\n",
      "Iteration 43903 Loss: 1.2174108028411865\n",
      "Iteration 43904 Loss: 1.109548568725586\n",
      "Iteration 43905 Loss: 1.4306561946868896\n",
      "Iteration 43906 Loss: 1.017563819885254\n",
      "Iteration 43907 Loss: 0.7860801219940186\n",
      "Iteration 43908 Loss: 0.6764401197433472\n",
      "Iteration 43909 Loss: 0.9311127662658691\n",
      "Iteration 43909 Loss: 0.9439965486526489\n",
      "Iteration 43910 Loss: 0.9643658399581909\n",
      "Iteration 43911 Loss: 1.1008578538894653\n",
      "Iteration 43912 Loss: 0.7145341038703918\n",
      "Iteration 43913 Loss: 0.8651688098907471\n",
      "Iteration 43914 Loss: 1.1816214323043823\n",
      "Iteration 43915 Loss: 0.6220173835754395\n",
      "Iteration 43916 Loss: 1.1054880619049072\n",
      "Iteration 43917 Loss: 1.010258674621582\n",
      "Iteration 43918 Loss: 1.0989254713058472\n",
      "Iteration 43919 Loss: 0.8198882937431335\n",
      "Iteration 43919 Loss: 0.9483126401901245\n",
      "Iteration 43920 Loss: 1.107427716255188\n",
      "Iteration 43921 Loss: 0.7995386719703674\n",
      "Iteration 43922 Loss: 0.7966110110282898\n",
      "Iteration 43923 Loss: 0.8386850357055664\n",
      "Iteration 43924 Loss: 1.1911345720291138\n",
      "Iteration 43925 Loss: 0.8791136741638184\n",
      "Iteration 43926 Loss: 0.7941411733627319\n",
      "Iteration 43927 Loss: 0.7480400800704956\n",
      "Iteration 43928 Loss: 0.9839277267456055\n",
      "Iteration 43929 Loss: 0.7896451950073242\n",
      "Iteration 43929 Loss: 0.8928264379501343\n",
      "Iteration 43930 Loss: 0.7896833419799805\n",
      "Iteration 43931 Loss: 0.8015926480293274\n",
      "Iteration 43932 Loss: 1.0602470636367798\n",
      "Iteration 43933 Loss: 0.5601357221603394\n",
      "Iteration 43934 Loss: 0.7268672585487366\n",
      "Iteration 43935 Loss: 1.196393609046936\n",
      "Iteration 43936 Loss: 1.3012232780456543\n",
      "Iteration 43937 Loss: 1.178358793258667\n",
      "Iteration 43938 Loss: 0.8738905191421509\n",
      "Iteration 43939 Loss: 0.9746266007423401\n",
      "Iteration 43939 Loss: 0.9463019371032715\n",
      "Iteration 43940 Loss: 0.7358974814414978\n",
      "Iteration 43941 Loss: 1.1667780876159668\n",
      "Iteration 43942 Loss: 0.7191154956817627\n",
      "Iteration 43943 Loss: 0.9272157549858093\n",
      "Iteration 43944 Loss: 1.1782649755477905\n",
      "Iteration 43945 Loss: 0.8968148827552795\n",
      "Iteration 43946 Loss: 0.6663757562637329\n",
      "Iteration 43947 Loss: 1.3211669921875\n",
      "Iteration 43948 Loss: 1.2202364206314087\n",
      "Iteration 43949 Loss: 0.7938569188117981\n",
      "Iteration 43949 Loss: 0.9625722765922546\n",
      "Iteration 43950 Loss: 1.1066827774047852\n",
      "Iteration 43951 Loss: 1.3394638299942017\n",
      "Iteration 43952 Loss: 1.0759512186050415\n",
      "Iteration 43953 Loss: 0.7779171466827393\n",
      "Iteration 43954 Loss: 1.0788731575012207\n",
      "Iteration 43955 Loss: 0.7821196913719177\n",
      "Iteration 43956 Loss: 1.1657140254974365\n",
      "Iteration 43957 Loss: 1.0470502376556396\n",
      "Iteration 43958 Loss: 0.7436483502388\n",
      "Iteration 43959 Loss: 0.9084560871124268\n",
      "Iteration 43959 Loss: 1.0025875568389893\n",
      "Iteration 43960 Loss: 0.7521237134933472\n",
      "Iteration 43961 Loss: 0.9092458486557007\n",
      "Iteration 43962 Loss: 1.1326055526733398\n",
      "Iteration 43963 Loss: 0.8256515264511108\n",
      "Iteration 43964 Loss: 0.8539844155311584\n",
      "Iteration 43965 Loss: 1.0490931272506714\n",
      "Iteration 43966 Loss: 0.6852112412452698\n",
      "Iteration 43967 Loss: 0.8480916619300842\n",
      "Iteration 43968 Loss: 0.8711935877799988\n",
      "Iteration 43969 Loss: 0.8854277729988098\n",
      "Iteration 43969 Loss: 0.8812628984451294\n",
      "Iteration 43970 Loss: 0.7972084283828735\n",
      "Iteration 43971 Loss: 1.0152804851531982\n",
      "Iteration 43972 Loss: 1.3253161907196045\n",
      "Iteration 43973 Loss: 0.9822820425033569\n",
      "Iteration 43974 Loss: 1.2693707942962646\n",
      "Iteration 43975 Loss: 1.1051256656646729\n",
      "Iteration 43976 Loss: 0.8880389332771301\n",
      "Iteration 43977 Loss: 0.6288609504699707\n",
      "Iteration 43978 Loss: 0.7901946902275085\n",
      "Iteration 43979 Loss: 0.5844275951385498\n",
      "Iteration 43979 Loss: 0.9386104345321655\n",
      "Iteration 43980 Loss: 1.2419331073760986\n",
      "Iteration 43981 Loss: 0.8536121249198914\n",
      "Iteration 43982 Loss: 0.8473072648048401\n",
      "Iteration 43983 Loss: 0.8121841549873352\n",
      "Iteration 43984 Loss: 1.3926870822906494\n",
      "Iteration 43985 Loss: 0.9624100923538208\n",
      "Iteration 43986 Loss: 0.8641335964202881\n",
      "Iteration 43987 Loss: 0.9942041039466858\n",
      "Iteration 43988 Loss: 0.9492840766906738\n",
      "Iteration 43989 Loss: 0.9661518335342407\n",
      "Iteration 43989 Loss: 0.9883908033370972\n",
      "Iteration 43990 Loss: 0.8411104083061218\n",
      "Iteration 43991 Loss: 0.9228688478469849\n",
      "Iteration 43992 Loss: 1.0507835149765015\n",
      "Iteration 43993 Loss: 1.1484911441802979\n",
      "Iteration 43994 Loss: 0.7696391940116882\n",
      "Iteration 43995 Loss: 1.0501701831817627\n",
      "Iteration 43996 Loss: 1.292478084564209\n",
      "Iteration 43997 Loss: 0.8135771155357361\n",
      "Iteration 43998 Loss: 0.9184133410453796\n",
      "Iteration 43999 Loss: 1.0169568061828613\n",
      "Iteration 43999 Loss: 0.9824488759040833\n",
      "Iteration 44000 Loss: 1.340531587600708\n",
      "Iteration 44001 Loss: 1.0264486074447632\n",
      "Iteration 44002 Loss: 1.1150866746902466\n",
      "Iteration 44003 Loss: 1.075160264968872\n",
      "Iteration 44004 Loss: 0.6813754439353943\n",
      "Iteration 44005 Loss: 1.086621880531311\n",
      "Iteration 44006 Loss: 0.7592937350273132\n",
      "Iteration 44007 Loss: 0.8135965466499329\n",
      "Iteration 44008 Loss: 1.0845717191696167\n",
      "Iteration 44009 Loss: 0.8251113295555115\n",
      "Iteration 44009 Loss: 0.9807798266410828\n",
      "Iteration 44010 Loss: 0.965458333492279\n",
      "Iteration 44011 Loss: 1.1708484888076782\n",
      "Iteration 44012 Loss: 0.9481428861618042\n",
      "Iteration 44013 Loss: 0.7155711650848389\n",
      "Iteration 44014 Loss: 0.7562681436538696\n",
      "Iteration 44015 Loss: 0.8599244952201843\n",
      "Iteration 44016 Loss: 1.0693882703781128\n",
      "Iteration 44017 Loss: 0.7890538573265076\n",
      "Iteration 44018 Loss: 1.0338222980499268\n",
      "Iteration 44019 Loss: 0.7735176682472229\n",
      "Iteration 44019 Loss: 0.9081994891166687\n",
      "Iteration 44020 Loss: 1.1069767475128174\n",
      "Iteration 44021 Loss: 0.964512288570404\n",
      "Iteration 44022 Loss: 1.0914404392242432\n",
      "Iteration 44023 Loss: 0.8806964159011841\n",
      "Iteration 44024 Loss: 1.1979598999023438\n",
      "Iteration 44025 Loss: 1.1496890783309937\n",
      "Iteration 44026 Loss: 0.7529101371765137\n",
      "Iteration 44027 Loss: 1.062605381011963\n",
      "Iteration 44028 Loss: 1.1079730987548828\n",
      "Iteration 44029 Loss: 0.9064830541610718\n",
      "Iteration 44029 Loss: 1.0221245288848877\n",
      "Iteration 44030 Loss: 0.6702643036842346\n",
      "Iteration 44031 Loss: 0.732822835445404\n",
      "Iteration 44032 Loss: 1.0419797897338867\n",
      "Iteration 44033 Loss: 1.1514040231704712\n",
      "Iteration 44034 Loss: 0.923058807849884\n",
      "Iteration 44035 Loss: 0.5648857951164246\n",
      "Iteration 44036 Loss: 1.3323208093643188\n",
      "Iteration 44037 Loss: 0.6548784375190735\n",
      "Iteration 44038 Loss: 0.8813431262969971\n",
      "Iteration 44039 Loss: 1.2734395265579224\n",
      "Iteration 44039 Loss: 0.9226397275924683\n",
      "Iteration 44040 Loss: 0.9423004388809204\n",
      "Iteration 44041 Loss: 1.2679275274276733\n",
      "Iteration 44042 Loss: 0.5692102313041687\n",
      "Iteration 44043 Loss: 0.7733412384986877\n",
      "Iteration 44044 Loss: 1.0829135179519653\n",
      "Iteration 44045 Loss: 0.9490879774093628\n",
      "Iteration 44046 Loss: 0.7295090556144714\n",
      "Iteration 44047 Loss: 0.8084468841552734\n",
      "Iteration 44048 Loss: 1.130919337272644\n",
      "Iteration 44049 Loss: 0.959330141544342\n",
      "Iteration 44049 Loss: 0.921298623085022\n",
      "Iteration 44050 Loss: 0.9340137839317322\n",
      "Iteration 44051 Loss: 0.8999022245407104\n",
      "Iteration 44052 Loss: 0.7869821786880493\n",
      "Iteration 44053 Loss: 1.3981869220733643\n",
      "Iteration 44054 Loss: 1.0660152435302734\n",
      "Iteration 44055 Loss: 1.094523549079895\n",
      "Iteration 44056 Loss: 0.9176166653633118\n",
      "Iteration 44057 Loss: 1.2947795391082764\n",
      "Iteration 44058 Loss: 1.012717366218567\n",
      "Iteration 44059 Loss: 0.907782793045044\n",
      "Iteration 44059 Loss: 1.0312520265579224\n",
      "Iteration 44060 Loss: 0.9335944056510925\n",
      "Iteration 44061 Loss: 0.9327946305274963\n",
      "Iteration 44062 Loss: 1.0702084302902222\n",
      "Iteration 44063 Loss: 1.2227476835250854\n",
      "Iteration 44064 Loss: 1.1382856369018555\n",
      "Iteration 44065 Loss: 0.9763022065162659\n",
      "Iteration 44066 Loss: 0.8722137808799744\n",
      "Iteration 44067 Loss: 0.9116894006729126\n",
      "Iteration 44068 Loss: 0.5778830051422119\n",
      "Iteration 44069 Loss: 0.6763125658035278\n",
      "Iteration 44069 Loss: 0.9312031865119934\n",
      "Iteration 44070 Loss: 1.049617052078247\n",
      "Iteration 44071 Loss: 0.8949929475784302\n",
      "Iteration 44072 Loss: 0.9288184642791748\n",
      "Iteration 44073 Loss: 0.9239774942398071\n",
      "Iteration 44074 Loss: 0.7518360614776611\n",
      "Iteration 44075 Loss: 0.8978993892669678\n",
      "Iteration 44076 Loss: 0.8532555103302002\n",
      "Iteration 44077 Loss: 1.1472817659378052\n",
      "Iteration 44078 Loss: 1.2926204204559326\n",
      "Iteration 44079 Loss: 1.007449746131897\n",
      "Iteration 44079 Loss: 0.974774956703186\n",
      "Iteration 44080 Loss: 0.8831323385238647\n",
      "Iteration 44081 Loss: 1.0264079570770264\n",
      "Iteration 44082 Loss: 0.7948125600814819\n",
      "Iteration 44083 Loss: 1.248461127281189\n",
      "Iteration 44084 Loss: 1.0260266065597534\n",
      "Iteration 44085 Loss: 1.136327862739563\n",
      "Iteration 44086 Loss: 0.9341651201248169\n",
      "Iteration 44087 Loss: 1.1271487474441528\n",
      "Iteration 44088 Loss: 0.8313683867454529\n",
      "Iteration 44089 Loss: 0.9076814651489258\n",
      "Iteration 44089 Loss: 0.9915531873703003\n",
      "Iteration 44090 Loss: 0.7923177480697632\n",
      "Iteration 44091 Loss: 1.068009853363037\n",
      "Iteration 44092 Loss: 0.8929186463356018\n",
      "Iteration 44093 Loss: 1.4597201347351074\n",
      "Iteration 44094 Loss: 0.8519098162651062\n",
      "Iteration 44095 Loss: 1.063097596168518\n",
      "Iteration 44096 Loss: 0.9399706721305847\n",
      "Iteration 44097 Loss: 1.1744871139526367\n",
      "Iteration 44098 Loss: 1.2566291093826294\n",
      "Iteration 44099 Loss: 1.1012455224990845\n",
      "Iteration 44099 Loss: 1.0600306987762451\n",
      "Iteration 44100 Loss: 0.9903553128242493\n",
      "Iteration 44101 Loss: 0.71729975938797\n",
      "Iteration 44102 Loss: 0.7832896113395691\n",
      "Iteration 44103 Loss: 0.8524126410484314\n",
      "Iteration 44104 Loss: 1.1475337743759155\n",
      "Iteration 44105 Loss: 1.1865135431289673\n",
      "Iteration 44106 Loss: 0.8411216139793396\n",
      "Iteration 44107 Loss: 0.7867165803909302\n",
      "Iteration 44108 Loss: 0.7587775588035583\n",
      "Iteration 44109 Loss: 0.9970250129699707\n",
      "Iteration 44109 Loss: 0.9061044454574585\n",
      "Iteration 44110 Loss: 0.981677234172821\n",
      "Iteration 44111 Loss: 0.9320023655891418\n",
      "Iteration 44112 Loss: 1.0267990827560425\n",
      "Iteration 44113 Loss: 1.0313886404037476\n",
      "Iteration 44114 Loss: 0.8056781888008118\n",
      "Iteration 44115 Loss: 0.8484747409820557\n",
      "Iteration 44116 Loss: 1.0455043315887451\n",
      "Iteration 44117 Loss: 0.8857441544532776\n",
      "Iteration 44118 Loss: 0.9325016140937805\n",
      "Iteration 44119 Loss: 0.6861351728439331\n",
      "Iteration 44119 Loss: 0.9175906181335449\n",
      "Iteration 44120 Loss: 0.6112238764762878\n",
      "Iteration 44121 Loss: 0.9293906688690186\n",
      "Iteration 44122 Loss: 1.2587441205978394\n",
      "Iteration 44123 Loss: 0.75299072265625\n",
      "Iteration 44124 Loss: 0.8809894323348999\n",
      "Iteration 44125 Loss: 1.221514344215393\n",
      "Iteration 44126 Loss: 0.7022049427032471\n",
      "Iteration 44127 Loss: 0.8129535913467407\n",
      "Iteration 44128 Loss: 0.7475927472114563\n",
      "Iteration 44129 Loss: 1.073281168937683\n",
      "Iteration 44129 Loss: 0.8990885615348816\n",
      "Iteration 44130 Loss: 0.7265796065330505\n",
      "Iteration 44131 Loss: 0.9994001388549805\n",
      "Iteration 44132 Loss: 1.066772699356079\n",
      "Iteration 44133 Loss: 0.8411557674407959\n",
      "Iteration 44134 Loss: 1.0034518241882324\n",
      "Iteration 44135 Loss: 1.1370192766189575\n",
      "Iteration 44136 Loss: 0.8643286228179932\n",
      "Iteration 44137 Loss: 0.8734649419784546\n",
      "Iteration 44138 Loss: 0.8004789352416992\n",
      "Iteration 44139 Loss: 1.0872026681900024\n",
      "Iteration 44139 Loss: 0.939985454082489\n",
      "Iteration 44140 Loss: 1.1580495834350586\n",
      "Iteration 44141 Loss: 0.9263081550598145\n",
      "Iteration 44142 Loss: 0.7701266407966614\n",
      "Iteration 44143 Loss: 0.8489405512809753\n",
      "Iteration 44144 Loss: 1.0758181810379028\n",
      "Iteration 44145 Loss: 1.134649395942688\n",
      "Iteration 44146 Loss: 0.6589795351028442\n",
      "Iteration 44147 Loss: 1.082335352897644\n",
      "Iteration 44148 Loss: 0.9259596467018127\n",
      "Iteration 44149 Loss: 0.702350378036499\n",
      "Iteration 44149 Loss: 0.9283517003059387\n",
      "Iteration 44150 Loss: 1.2864381074905396\n",
      "Iteration 44151 Loss: 0.797884464263916\n",
      "Iteration 44152 Loss: 0.9607440233230591\n",
      "Iteration 44153 Loss: 0.8937457203865051\n",
      "Iteration 44154 Loss: 0.8154528737068176\n",
      "Iteration 44155 Loss: 0.9040011763572693\n",
      "Iteration 44156 Loss: 0.818699836730957\n",
      "Iteration 44157 Loss: 1.091627597808838\n",
      "Iteration 44158 Loss: 1.1925219297409058\n",
      "Iteration 44159 Loss: 0.8966079950332642\n",
      "Iteration 44159 Loss: 0.9657724499702454\n",
      "Iteration 44160 Loss: 0.9128710627555847\n",
      "Iteration 44161 Loss: 1.3345754146575928\n",
      "Iteration 44162 Loss: 0.9061325788497925\n",
      "Iteration 44163 Loss: 0.9264706969261169\n",
      "Iteration 44164 Loss: 0.7580839991569519\n",
      "Iteration 44165 Loss: 1.126749873161316\n",
      "Iteration 44166 Loss: 1.0751922130584717\n",
      "Iteration 44167 Loss: 1.0070886611938477\n",
      "Iteration 44168 Loss: 1.3554422855377197\n",
      "Iteration 44169 Loss: 0.916584849357605\n",
      "Iteration 44169 Loss: 1.031919240951538\n",
      "Iteration 44170 Loss: 0.6107194423675537\n",
      "Iteration 44171 Loss: 1.0723050832748413\n",
      "Iteration 44172 Loss: 1.165808916091919\n",
      "Iteration 44173 Loss: 1.1681385040283203\n",
      "Iteration 44174 Loss: 1.2179232835769653\n",
      "Iteration 44175 Loss: 1.0636852979660034\n",
      "Iteration 44176 Loss: 1.0358580350875854\n",
      "Iteration 44177 Loss: 0.9790782928466797\n",
      "Iteration 44178 Loss: 1.0937312841415405\n",
      "Iteration 44179 Loss: 0.8268319964408875\n",
      "Iteration 44179 Loss: 1.023408055305481\n",
      "Iteration 44180 Loss: 0.9049667119979858\n",
      "Iteration 44181 Loss: 0.9819146990776062\n",
      "Iteration 44182 Loss: 0.8748698830604553\n",
      "Iteration 44183 Loss: 1.1660572290420532\n",
      "Iteration 44184 Loss: 0.7607248425483704\n",
      "Iteration 44185 Loss: 1.406460165977478\n",
      "Iteration 44186 Loss: 0.7481966018676758\n",
      "Iteration 44187 Loss: 0.5470675230026245\n",
      "Iteration 44188 Loss: 0.7540250420570374\n",
      "Iteration 44189 Loss: 0.6071255803108215\n",
      "Iteration 44189 Loss: 0.8751408457756042\n",
      "Iteration 44190 Loss: 0.9492860436439514\n",
      "Iteration 44191 Loss: 0.7813969254493713\n",
      "Iteration 44192 Loss: 0.9690685272216797\n",
      "Iteration 44193 Loss: 0.9036063551902771\n",
      "Iteration 44194 Loss: 1.2536741495132446\n",
      "Iteration 44195 Loss: 1.077003836631775\n",
      "Iteration 44196 Loss: 1.074733018875122\n",
      "Iteration 44197 Loss: 1.021333932876587\n",
      "Iteration 44198 Loss: 0.7939254641532898\n",
      "Iteration 44199 Loss: 1.0933388471603394\n",
      "Iteration 44199 Loss: 0.9917367100715637\n",
      "Iteration 44200 Loss: 1.0030553340911865\n",
      "Iteration 44201 Loss: 1.3531687259674072\n",
      "Iteration 44202 Loss: 0.8977599740028381\n",
      "Iteration 44203 Loss: 0.8652118444442749\n",
      "Iteration 44204 Loss: 1.1825698614120483\n",
      "Iteration 44205 Loss: 1.0176044702529907\n",
      "Iteration 44206 Loss: 0.85661381483078\n",
      "Iteration 44207 Loss: 0.7658177614212036\n",
      "Iteration 44208 Loss: 0.9400709271430969\n",
      "Iteration 44209 Loss: 1.1484448909759521\n",
      "Iteration 44209 Loss: 1.003031849861145\n",
      "Iteration 44210 Loss: 0.781806230545044\n",
      "Iteration 44211 Loss: 0.976324200630188\n",
      "Iteration 44212 Loss: 0.89528888463974\n",
      "Iteration 44213 Loss: 0.8199254870414734\n",
      "Iteration 44214 Loss: 0.9102261662483215\n",
      "Iteration 44215 Loss: 0.9772822856903076\n",
      "Iteration 44216 Loss: 1.0551023483276367\n",
      "Iteration 44217 Loss: 1.1460216045379639\n",
      "Iteration 44218 Loss: 0.9955440759658813\n",
      "Iteration 44219 Loss: 1.0728551149368286\n",
      "Iteration 44219 Loss: 0.9630376696586609\n",
      "Iteration 44220 Loss: 0.702674388885498\n",
      "Iteration 44221 Loss: 0.8093053102493286\n",
      "Iteration 44222 Loss: 0.7627119421958923\n",
      "Iteration 44223 Loss: 1.1308252811431885\n",
      "Iteration 44224 Loss: 0.8628518581390381\n",
      "Iteration 44225 Loss: 0.7327276468276978\n",
      "Iteration 44226 Loss: 0.7817195653915405\n",
      "Iteration 44227 Loss: 0.9426054954528809\n",
      "Iteration 44228 Loss: 1.0030295848846436\n",
      "Iteration 44229 Loss: 0.9959884881973267\n",
      "Iteration 44229 Loss: 0.8724439740180969\n",
      "Iteration 44230 Loss: 1.0546493530273438\n",
      "Iteration 44231 Loss: 1.0039581060409546\n",
      "Iteration 44232 Loss: 1.0551469326019287\n",
      "Iteration 44233 Loss: 0.8612555861473083\n",
      "Iteration 44234 Loss: 0.6906517148017883\n",
      "Iteration 44235 Loss: 1.2002207040786743\n",
      "Iteration 44236 Loss: 1.0422334671020508\n",
      "Iteration 44237 Loss: 1.2966269254684448\n",
      "Iteration 44238 Loss: 0.9906352758407593\n",
      "Iteration 44239 Loss: 0.9887571930885315\n",
      "Iteration 44239 Loss: 1.0184135437011719\n",
      "Iteration 44240 Loss: 0.9184134602546692\n",
      "Iteration 44241 Loss: 1.151915192604065\n",
      "Iteration 44242 Loss: 1.0603249073028564\n",
      "Iteration 44243 Loss: 1.0383139848709106\n",
      "Iteration 44244 Loss: 0.8926962614059448\n",
      "Iteration 44245 Loss: 0.920371949672699\n",
      "Iteration 44246 Loss: 0.9163935780525208\n",
      "Iteration 44247 Loss: 0.9379408359527588\n",
      "Iteration 44248 Loss: 1.0041955709457397\n",
      "Iteration 44249 Loss: 0.9490056037902832\n",
      "Iteration 44249 Loss: 0.9789570569992065\n",
      "Iteration 44250 Loss: 0.7245842814445496\n",
      "Iteration 44251 Loss: 0.8242673873901367\n",
      "Iteration 44252 Loss: 1.093968152999878\n",
      "Iteration 44253 Loss: 0.9566214084625244\n",
      "Iteration 44254 Loss: 0.8230874538421631\n",
      "Iteration 44255 Loss: 0.9978848695755005\n",
      "Iteration 44256 Loss: 0.7641109824180603\n",
      "Iteration 44257 Loss: 0.8742069005966187\n",
      "Iteration 44258 Loss: 0.8588176965713501\n",
      "Iteration 44259 Loss: 0.8783445954322815\n",
      "Iteration 44259 Loss: 0.8795893788337708\n",
      "Iteration 44260 Loss: 0.9982134699821472\n",
      "Iteration 44261 Loss: 0.8862512707710266\n",
      "Iteration 44262 Loss: 0.9554932117462158\n",
      "Iteration 44263 Loss: 0.9712123870849609\n",
      "Iteration 44264 Loss: 0.7908396124839783\n",
      "Iteration 44265 Loss: 1.0415198802947998\n",
      "Iteration 44266 Loss: 1.1833487749099731\n",
      "Iteration 44267 Loss: 0.6801952719688416\n",
      "Iteration 44268 Loss: 1.0603009462356567\n",
      "Iteration 44269 Loss: 1.0924720764160156\n",
      "Iteration 44269 Loss: 0.9659846425056458\n",
      "Iteration 44270 Loss: 0.6261380314826965\n",
      "Iteration 44271 Loss: 0.8230521082878113\n",
      "Iteration 44272 Loss: 0.6142606735229492\n",
      "Iteration 44273 Loss: 1.0774338245391846\n",
      "Iteration 44274 Loss: 0.9441906213760376\n",
      "Iteration 44275 Loss: 0.8804008960723877\n",
      "Iteration 44276 Loss: 0.674764096736908\n",
      "Iteration 44277 Loss: 0.7013273239135742\n",
      "Iteration 44278 Loss: 0.9687276482582092\n",
      "Iteration 44279 Loss: 1.1429781913757324\n",
      "Iteration 44279 Loss: 0.8453273773193359\n",
      "Iteration 44280 Loss: 1.278426170349121\n",
      "Iteration 44281 Loss: 0.6329332590103149\n",
      "Iteration 44282 Loss: 1.054999828338623\n",
      "Iteration 44283 Loss: 0.9293871521949768\n",
      "Iteration 44284 Loss: 1.0563920736312866\n",
      "Iteration 44285 Loss: 1.231830358505249\n",
      "Iteration 44286 Loss: 0.8311125636100769\n",
      "Iteration 44287 Loss: 1.1801671981811523\n",
      "Iteration 44288 Loss: 0.7637019753456116\n",
      "Iteration 44289 Loss: 1.241298794746399\n",
      "Iteration 44289 Loss: 1.0200250148773193\n",
      "Iteration 44290 Loss: 0.7515657544136047\n",
      "Iteration 44291 Loss: 1.1176539659500122\n",
      "Iteration 44292 Loss: 0.7635251879692078\n",
      "Iteration 44293 Loss: 0.6754804253578186\n",
      "Iteration 44294 Loss: 0.8323700428009033\n",
      "Iteration 44295 Loss: 1.0937448740005493\n",
      "Iteration 44296 Loss: 0.8543975353240967\n",
      "Iteration 44297 Loss: 0.8879522085189819\n",
      "Iteration 44298 Loss: 1.1798088550567627\n",
      "Iteration 44299 Loss: 1.1381503343582153\n",
      "Iteration 44299 Loss: 0.9294648170471191\n",
      "Iteration 44300 Loss: 0.9883342385292053\n",
      "Iteration 44301 Loss: 1.0857069492340088\n",
      "Iteration 44302 Loss: 0.8480551242828369\n",
      "Iteration 44303 Loss: 0.8195860385894775\n",
      "Iteration 44304 Loss: 0.5638980269432068\n",
      "Iteration 44305 Loss: 0.9420291781425476\n",
      "Iteration 44306 Loss: 0.8818468451499939\n",
      "Iteration 44307 Loss: 0.846991240978241\n",
      "Iteration 44308 Loss: 0.7679239511489868\n",
      "Iteration 44309 Loss: 0.7591400742530823\n",
      "Iteration 44309 Loss: 0.8503511548042297\n",
      "Iteration 44310 Loss: 0.8163228631019592\n",
      "Iteration 44311 Loss: 1.0852179527282715\n",
      "Iteration 44312 Loss: 1.0782418251037598\n",
      "Iteration 44313 Loss: 1.0825316905975342\n",
      "Iteration 44314 Loss: 0.9807044267654419\n",
      "Iteration 44315 Loss: 0.8335895538330078\n",
      "Iteration 44316 Loss: 0.9001896381378174\n",
      "Iteration 44317 Loss: 1.2842565774917603\n",
      "Iteration 44318 Loss: 0.9781963229179382\n",
      "Iteration 44319 Loss: 1.0231242179870605\n",
      "Iteration 44319 Loss: 1.006237506866455\n",
      "Iteration 44320 Loss: 0.9561749696731567\n",
      "Iteration 44321 Loss: 1.183752417564392\n",
      "Iteration 44322 Loss: 0.9488946199417114\n",
      "Iteration 44323 Loss: 0.9885224103927612\n",
      "Iteration 44324 Loss: 0.8551305532455444\n",
      "Iteration 44325 Loss: 0.9906585216522217\n",
      "Iteration 44326 Loss: 0.785131573677063\n",
      "Iteration 44327 Loss: 0.6358088254928589\n",
      "Iteration 44328 Loss: 0.871934175491333\n",
      "Iteration 44329 Loss: 0.821780800819397\n",
      "Iteration 44329 Loss: 0.9037789106369019\n",
      "Iteration 44330 Loss: 0.8463748097419739\n",
      "Iteration 44331 Loss: 0.974402666091919\n",
      "Iteration 44332 Loss: 1.175484538078308\n",
      "Iteration 44333 Loss: 0.8750097751617432\n",
      "Iteration 44334 Loss: 0.8183976411819458\n",
      "Iteration 44335 Loss: 1.035690188407898\n",
      "Iteration 44336 Loss: 1.007171630859375\n",
      "Iteration 44337 Loss: 1.1277450323104858\n",
      "Iteration 44338 Loss: 0.7027384638786316\n",
      "Iteration 44339 Loss: 1.101851224899292\n",
      "Iteration 44339 Loss: 0.9664865732192993\n",
      "Iteration 44340 Loss: 0.9658818244934082\n",
      "Iteration 44341 Loss: 0.7959197759628296\n",
      "Iteration 44342 Loss: 0.8554402589797974\n",
      "Iteration 44343 Loss: 1.0366098880767822\n",
      "Iteration 44344 Loss: 1.057585597038269\n",
      "Iteration 44345 Loss: 0.8728975057601929\n",
      "Iteration 44346 Loss: 0.9009121656417847\n",
      "Iteration 44347 Loss: 0.9598667025566101\n",
      "Iteration 44348 Loss: 0.982094943523407\n",
      "Iteration 44349 Loss: 0.9216968417167664\n",
      "Iteration 44349 Loss: 0.9348905682563782\n",
      "Iteration 44350 Loss: 1.219008207321167\n",
      "Iteration 44351 Loss: 0.9297055006027222\n",
      "Iteration 44352 Loss: 1.1757498979568481\n",
      "Iteration 44353 Loss: 1.2584224939346313\n",
      "Iteration 44354 Loss: 0.9462006688117981\n",
      "Iteration 44355 Loss: 0.7947548627853394\n",
      "Iteration 44356 Loss: 1.0087456703186035\n",
      "Iteration 44357 Loss: 1.086493730545044\n",
      "Iteration 44358 Loss: 1.0875194072723389\n",
      "Iteration 44359 Loss: 0.9120975732803345\n",
      "Iteration 44359 Loss: 1.041869878768921\n",
      "Iteration 44360 Loss: 0.9093257188796997\n",
      "Iteration 44361 Loss: 1.0494900941848755\n",
      "Iteration 44362 Loss: 0.7220712304115295\n",
      "Iteration 44363 Loss: 0.7863897681236267\n",
      "Iteration 44364 Loss: 1.0193324089050293\n",
      "Iteration 44365 Loss: 0.919766366481781\n",
      "Iteration 44366 Loss: 0.9673490524291992\n",
      "Iteration 44367 Loss: 0.6859217882156372\n",
      "Iteration 44368 Loss: 0.923626720905304\n",
      "Iteration 44369 Loss: 0.9896931052207947\n",
      "Iteration 44369 Loss: 0.8972966074943542\n",
      "Iteration 44370 Loss: 0.7081210017204285\n",
      "Iteration 44371 Loss: 0.9700050354003906\n",
      "Iteration 44372 Loss: 1.079867959022522\n",
      "Iteration 44373 Loss: 0.8349021673202515\n",
      "Iteration 44374 Loss: 0.9904770851135254\n",
      "Iteration 44375 Loss: 1.2089534997940063\n",
      "Iteration 44376 Loss: 1.060287594795227\n",
      "Iteration 44377 Loss: 1.1821985244750977\n",
      "Iteration 44378 Loss: 0.9530195593833923\n",
      "Iteration 44379 Loss: 0.9809079170227051\n",
      "Iteration 44379 Loss: 0.9968740344047546\n",
      "Iteration 44380 Loss: 0.8539856672286987\n",
      "Iteration 44381 Loss: 0.8080233335494995\n",
      "Iteration 44382 Loss: 0.9841116666793823\n",
      "Iteration 44383 Loss: 1.2349963188171387\n",
      "Iteration 44384 Loss: 0.8042652606964111\n",
      "Iteration 44385 Loss: 1.1036145687103271\n",
      "Iteration 44386 Loss: 0.7262604236602783\n",
      "Iteration 44387 Loss: 1.0470056533813477\n",
      "Iteration 44388 Loss: 0.8709961175918579\n",
      "Iteration 44389 Loss: 0.9926774501800537\n",
      "Iteration 44389 Loss: 0.9425936937332153\n",
      "Iteration 44390 Loss: 0.7584594488143921\n",
      "Iteration 44391 Loss: 0.9447821974754333\n",
      "Iteration 44392 Loss: 0.8183568716049194\n",
      "Iteration 44393 Loss: 0.7404537200927734\n",
      "Iteration 44394 Loss: 1.1076284646987915\n",
      "Iteration 44395 Loss: 0.7776169180870056\n",
      "Iteration 44396 Loss: 1.120461344718933\n",
      "Iteration 44397 Loss: 1.192385196685791\n",
      "Iteration 44398 Loss: 1.2589523792266846\n",
      "Iteration 44399 Loss: 0.8448538184165955\n",
      "Iteration 44399 Loss: 0.9563949704170227\n",
      "Iteration 44400 Loss: 0.8840853571891785\n",
      "Iteration 44401 Loss: 0.9571326375007629\n",
      "Iteration 44402 Loss: 1.0861865282058716\n",
      "Iteration 44403 Loss: 1.0319907665252686\n",
      "Iteration 44404 Loss: 1.081937551498413\n",
      "Iteration 44405 Loss: 1.0535966157913208\n",
      "Iteration 44406 Loss: 1.0916630029678345\n",
      "Iteration 44407 Loss: 1.0473721027374268\n",
      "Iteration 44408 Loss: 0.5268116593360901\n",
      "Iteration 44409 Loss: 0.8615624308586121\n",
      "Iteration 44409 Loss: 0.9622339010238647\n",
      "Iteration 44410 Loss: 1.064801812171936\n",
      "Iteration 44411 Loss: 1.1447930335998535\n",
      "Iteration 44412 Loss: 1.0605953931808472\n",
      "Iteration 44413 Loss: 1.0555614233016968\n",
      "Iteration 44414 Loss: 0.7981692552566528\n",
      "Iteration 44415 Loss: 1.1557812690734863\n",
      "Iteration 44416 Loss: 0.8191354274749756\n",
      "Iteration 44417 Loss: 0.9820348024368286\n",
      "Iteration 44418 Loss: 1.1456819772720337\n",
      "Iteration 44419 Loss: 0.7344115376472473\n",
      "Iteration 44419 Loss: 0.9960966110229492\n",
      "Iteration 44420 Loss: 0.8705106973648071\n",
      "Iteration 44421 Loss: 1.0889675617218018\n",
      "Iteration 44422 Loss: 1.0584608316421509\n",
      "Iteration 44423 Loss: 0.9880831837654114\n",
      "Iteration 44424 Loss: 0.6934128999710083\n",
      "Iteration 44425 Loss: 1.022453784942627\n",
      "Iteration 44426 Loss: 1.0298699140548706\n",
      "Iteration 44427 Loss: 1.0119826793670654\n",
      "Iteration 44428 Loss: 0.9057154059410095\n",
      "Iteration 44429 Loss: 1.1523585319519043\n",
      "Iteration 44429 Loss: 0.9821815490722656\n",
      "Iteration 44430 Loss: 0.8651678562164307\n",
      "Iteration 44431 Loss: 1.0241472721099854\n",
      "Iteration 44432 Loss: 0.7197256684303284\n",
      "Iteration 44433 Loss: 1.2101783752441406\n",
      "Iteration 44434 Loss: 0.9251620769500732\n",
      "Iteration 44435 Loss: 0.987911581993103\n",
      "Iteration 44436 Loss: 1.0330300331115723\n",
      "Iteration 44437 Loss: 0.9162665605545044\n",
      "Iteration 44438 Loss: 0.8532416820526123\n",
      "Iteration 44439 Loss: 0.7818714380264282\n",
      "Iteration 44439 Loss: 0.9316703081130981\n",
      "Iteration 44440 Loss: 0.9586449265480042\n",
      "Iteration 44441 Loss: 1.0652070045471191\n",
      "Iteration 44442 Loss: 0.6584561467170715\n",
      "Iteration 44443 Loss: 0.795647382736206\n",
      "Iteration 44444 Loss: 0.9708794355392456\n",
      "Iteration 44445 Loss: 0.8594448566436768\n",
      "Iteration 44446 Loss: 1.1823830604553223\n",
      "Iteration 44447 Loss: 1.2372392416000366\n",
      "Iteration 44448 Loss: 0.7778275609016418\n",
      "Iteration 44449 Loss: 1.2170867919921875\n",
      "Iteration 44449 Loss: 0.9722815752029419\n",
      "Iteration 44450 Loss: 0.8491186499595642\n",
      "Iteration 44451 Loss: 0.7898528575897217\n",
      "Iteration 44452 Loss: 0.6934081315994263\n",
      "Iteration 44453 Loss: 0.6721394062042236\n",
      "Iteration 44454 Loss: 0.9824289679527283\n",
      "Iteration 44455 Loss: 0.7549708485603333\n",
      "Iteration 44456 Loss: 0.9757076501846313\n",
      "Iteration 44457 Loss: 0.966145396232605\n",
      "Iteration 44458 Loss: 1.0130140781402588\n",
      "Iteration 44459 Loss: 1.0565364360809326\n",
      "Iteration 44459 Loss: 0.875332236289978\n",
      "Iteration 44460 Loss: 0.8525072932243347\n",
      "Iteration 44461 Loss: 0.8096073865890503\n",
      "Iteration 44462 Loss: 0.8946412205696106\n",
      "Iteration 44463 Loss: 0.9471046924591064\n",
      "Iteration 44464 Loss: 0.7913531064987183\n",
      "Iteration 44465 Loss: 0.9094790816307068\n",
      "Iteration 44466 Loss: 1.1307852268218994\n",
      "Iteration 44467 Loss: 0.946839451789856\n",
      "Iteration 44468 Loss: 0.7106274366378784\n",
      "Iteration 44469 Loss: 0.7710440158843994\n",
      "Iteration 44469 Loss: 0.8763988614082336\n",
      "Iteration 44470 Loss: 0.9600309729576111\n",
      "Iteration 44471 Loss: 1.1006087064743042\n",
      "Iteration 44472 Loss: 0.8938820362091064\n",
      "Iteration 44473 Loss: 0.5750731825828552\n",
      "Iteration 44474 Loss: 0.9601486921310425\n",
      "Iteration 44475 Loss: 1.0322580337524414\n",
      "Iteration 44476 Loss: 0.9422739148139954\n",
      "Iteration 44477 Loss: 0.7328416705131531\n",
      "Iteration 44478 Loss: 0.9429304003715515\n",
      "Iteration 44479 Loss: 1.1047029495239258\n",
      "Iteration 44479 Loss: 0.9244750738143921\n",
      "Iteration 44480 Loss: 0.6399909853935242\n",
      "Iteration 44481 Loss: 0.7551831603050232\n",
      "Iteration 44482 Loss: 0.9056944251060486\n",
      "Iteration 44483 Loss: 1.0821576118469238\n",
      "Iteration 44484 Loss: 0.8093780279159546\n",
      "Iteration 44485 Loss: 0.7497979998588562\n",
      "Iteration 44486 Loss: 0.8438252210617065\n",
      "Iteration 44487 Loss: 0.785734236240387\n",
      "Iteration 44488 Loss: 1.1166963577270508\n",
      "Iteration 44489 Loss: 0.7671470046043396\n",
      "Iteration 44489 Loss: 0.8455605506896973\n",
      "Iteration 44490 Loss: 1.1836544275283813\n",
      "Iteration 44491 Loss: 0.9271538853645325\n",
      "Iteration 44492 Loss: 0.791165828704834\n",
      "Iteration 44493 Loss: 0.8565748929977417\n",
      "Iteration 44494 Loss: 0.8008261919021606\n",
      "Iteration 44495 Loss: 1.0442719459533691\n",
      "Iteration 44496 Loss: 0.8612533211708069\n",
      "Iteration 44497 Loss: 0.6769596934318542\n",
      "Iteration 44498 Loss: 0.9809348583221436\n",
      "Iteration 44499 Loss: 1.0855095386505127\n",
      "Iteration 44499 Loss: 0.9208305478096008\n",
      "Iteration 44500 Loss: 0.9186714291572571\n",
      "Iteration 44501 Loss: 0.9198084473609924\n",
      "Iteration 44502 Loss: 0.7969857454299927\n",
      "Iteration 44503 Loss: 0.7433547377586365\n",
      "Iteration 44504 Loss: 0.7455471158027649\n",
      "Iteration 44505 Loss: 1.0134543180465698\n",
      "Iteration 44506 Loss: 1.0971559286117554\n",
      "Iteration 44507 Loss: 0.821505606174469\n",
      "Iteration 44508 Loss: 0.8025981187820435\n",
      "Iteration 44509 Loss: 0.8067011833190918\n",
      "Iteration 44509 Loss: 0.8665782809257507\n",
      "Iteration 44510 Loss: 0.9345633387565613\n",
      "Iteration 44511 Loss: 1.1015448570251465\n",
      "Iteration 44512 Loss: 0.8919187784194946\n",
      "Iteration 44513 Loss: 0.8450947403907776\n",
      "Iteration 44514 Loss: 1.1365439891815186\n",
      "Iteration 44515 Loss: 0.8957981467247009\n",
      "Iteration 44516 Loss: 0.94053715467453\n",
      "Iteration 44517 Loss: 0.6935175061225891\n",
      "Iteration 44518 Loss: 1.2755045890808105\n",
      "Iteration 44519 Loss: 0.7959353923797607\n",
      "Iteration 44519 Loss: 0.9510958790779114\n",
      "Iteration 44520 Loss: 0.9134835004806519\n",
      "Iteration 44521 Loss: 1.141723871231079\n",
      "Iteration 44522 Loss: 0.8504136204719543\n",
      "Iteration 44523 Loss: 0.7610226273536682\n",
      "Iteration 44524 Loss: 0.9621118903160095\n",
      "Iteration 44525 Loss: 0.7694841027259827\n",
      "Iteration 44526 Loss: 0.7409881353378296\n",
      "Iteration 44527 Loss: 1.004664421081543\n",
      "Iteration 44528 Loss: 1.0516858100891113\n",
      "Iteration 44529 Loss: 1.0507177114486694\n",
      "Iteration 44529 Loss: 0.9246295690536499\n",
      "Iteration 44530 Loss: 0.8248611688613892\n",
      "Iteration 44531 Loss: 0.9114411473274231\n",
      "Iteration 44532 Loss: 0.9279690980911255\n",
      "Iteration 44533 Loss: 1.1594183444976807\n",
      "Iteration 44534 Loss: 1.0056583881378174\n",
      "Iteration 44535 Loss: 0.737881600856781\n",
      "Iteration 44536 Loss: 1.1641672849655151\n",
      "Iteration 44537 Loss: 0.8775548934936523\n",
      "Iteration 44538 Loss: 0.9048620462417603\n",
      "Iteration 44539 Loss: 0.7875215411186218\n",
      "Iteration 44539 Loss: 0.9301335215568542\n",
      "Iteration 44540 Loss: 1.0248206853866577\n",
      "Iteration 44541 Loss: 0.7087476253509521\n",
      "Iteration 44542 Loss: 0.940704345703125\n",
      "Iteration 44543 Loss: 0.8470597863197327\n",
      "Iteration 44544 Loss: 0.9362020492553711\n",
      "Iteration 44545 Loss: 0.8300802111625671\n",
      "Iteration 44546 Loss: 1.1200916767120361\n",
      "Iteration 44547 Loss: 1.3677228689193726\n",
      "Iteration 44548 Loss: 0.8357674479484558\n",
      "Iteration 44549 Loss: 0.8757572770118713\n",
      "Iteration 44549 Loss: 0.9486953616142273\n",
      "Iteration 44550 Loss: 0.7453624606132507\n",
      "Iteration 44551 Loss: 0.8555065989494324\n",
      "Iteration 44552 Loss: 0.778571367263794\n",
      "Iteration 44553 Loss: 0.9171735644340515\n",
      "Iteration 44554 Loss: 1.1348134279251099\n",
      "Iteration 44555 Loss: 0.8333289623260498\n",
      "Iteration 44556 Loss: 0.981330931186676\n",
      "Iteration 44557 Loss: 0.7669866681098938\n",
      "Iteration 44558 Loss: 0.9780701994895935\n",
      "Iteration 44559 Loss: 0.8728539943695068\n",
      "Iteration 44559 Loss: 0.8863998651504517\n",
      "Iteration 44560 Loss: 0.8239558339118958\n",
      "Iteration 44561 Loss: 0.6895252466201782\n",
      "Iteration 44562 Loss: 0.9362162351608276\n",
      "Iteration 44563 Loss: 0.8647147417068481\n",
      "Iteration 44564 Loss: 1.1970843076705933\n",
      "Iteration 44565 Loss: 0.9530847668647766\n",
      "Iteration 44566 Loss: 0.7087223529815674\n",
      "Iteration 44567 Loss: 0.9699916839599609\n",
      "Iteration 44568 Loss: 0.8349342346191406\n",
      "Iteration 44569 Loss: 1.0244468450546265\n",
      "Iteration 44569 Loss: 0.9002677202224731\n",
      "Iteration 44570 Loss: 0.8342049717903137\n",
      "Iteration 44571 Loss: 1.136534571647644\n",
      "Iteration 44572 Loss: 1.0521384477615356\n",
      "Iteration 44573 Loss: 1.1766732931137085\n",
      "Iteration 44574 Loss: 0.7412066459655762\n",
      "Iteration 44575 Loss: 1.2004742622375488\n",
      "Iteration 44576 Loss: 0.9931609034538269\n",
      "Iteration 44577 Loss: 1.0437958240509033\n",
      "Iteration 44578 Loss: 1.0272468328475952\n",
      "Iteration 44579 Loss: 1.1147037744522095\n",
      "Iteration 44579 Loss: 1.0320138931274414\n",
      "Iteration 44580 Loss: 1.3453450202941895\n",
      "Iteration 44581 Loss: 0.625221848487854\n",
      "Iteration 44582 Loss: 0.7378920912742615\n",
      "Iteration 44583 Loss: 1.161014199256897\n",
      "Iteration 44584 Loss: 1.0054423809051514\n",
      "Iteration 44585 Loss: 1.1655548810958862\n",
      "Iteration 44586 Loss: 0.9865854978561401\n",
      "Iteration 44587 Loss: 0.8814508318901062\n",
      "Iteration 44588 Loss: 0.9012415409088135\n",
      "Iteration 44589 Loss: 0.9763643145561218\n",
      "Iteration 44589 Loss: 0.9786112904548645\n",
      "Iteration 44590 Loss: 0.6113684773445129\n",
      "Iteration 44591 Loss: 0.9987552762031555\n",
      "Iteration 44592 Loss: 0.7922016382217407\n",
      "Iteration 44593 Loss: 1.1551169157028198\n",
      "Iteration 44594 Loss: 0.5598111748695374\n",
      "Iteration 44595 Loss: 1.0910091400146484\n",
      "Iteration 44596 Loss: 1.466917872428894\n",
      "Iteration 44597 Loss: 0.7374260425567627\n",
      "Iteration 44598 Loss: 1.0862102508544922\n",
      "Iteration 44599 Loss: 1.1367701292037964\n",
      "Iteration 44599 Loss: 0.9635586738586426\n",
      "Iteration 44600 Loss: 0.5379576086997986\n",
      "Iteration 44601 Loss: 0.8992934226989746\n",
      "Iteration 44602 Loss: 1.3320919275283813\n",
      "Iteration 44603 Loss: 1.285781741142273\n",
      "Iteration 44604 Loss: 1.3037885427474976\n",
      "Iteration 44605 Loss: 0.8249568343162537\n",
      "Iteration 44606 Loss: 1.0843449831008911\n",
      "Iteration 44607 Loss: 0.9987764358520508\n",
      "Iteration 44608 Loss: 1.130440592765808\n",
      "Iteration 44609 Loss: 1.1169906854629517\n",
      "Iteration 44609 Loss: 1.051442265510559\n",
      "Iteration 44610 Loss: 1.0907137393951416\n",
      "Iteration 44611 Loss: 0.7763715982437134\n",
      "Iteration 44612 Loss: 0.7887939214706421\n",
      "Iteration 44613 Loss: 0.9016736745834351\n",
      "Iteration 44614 Loss: 1.0800409317016602\n",
      "Iteration 44615 Loss: 1.055957555770874\n",
      "Iteration 44616 Loss: 0.7050831317901611\n",
      "Iteration 44617 Loss: 0.9524256587028503\n",
      "Iteration 44618 Loss: 0.7545249462127686\n",
      "Iteration 44619 Loss: 1.1998873949050903\n",
      "Iteration 44619 Loss: 0.9305472373962402\n",
      "Iteration 44620 Loss: 1.062697172164917\n",
      "Iteration 44621 Loss: 0.7487463355064392\n",
      "Iteration 44622 Loss: 1.0792347192764282\n",
      "Iteration 44623 Loss: 1.101427435874939\n",
      "Iteration 44624 Loss: 0.8699081540107727\n",
      "Iteration 44625 Loss: 1.0393810272216797\n",
      "Iteration 44626 Loss: 0.6618704199790955\n",
      "Iteration 44627 Loss: 1.1233469247817993\n",
      "Iteration 44628 Loss: 0.9536691308021545\n",
      "Iteration 44629 Loss: 0.9218094348907471\n",
      "Iteration 44629 Loss: 0.9562090635299683\n",
      "Iteration 44630 Loss: 1.02881920337677\n",
      "Iteration 44631 Loss: 1.1510426998138428\n",
      "Iteration 44632 Loss: 0.6105155348777771\n",
      "Iteration 44633 Loss: 0.8716710209846497\n",
      "Iteration 44634 Loss: 0.6904736161231995\n",
      "Iteration 44635 Loss: 0.8252516388893127\n",
      "Iteration 44636 Loss: 0.941569447517395\n",
      "Iteration 44637 Loss: 0.71036696434021\n",
      "Iteration 44638 Loss: 1.1026324033737183\n",
      "Iteration 44639 Loss: 0.8766956329345703\n",
      "Iteration 44639 Loss: 0.8809038996696472\n",
      "Iteration 44640 Loss: 0.6971537470817566\n",
      "Iteration 44641 Loss: 1.0584502220153809\n",
      "Iteration 44642 Loss: 0.9803504943847656\n",
      "Iteration 44643 Loss: 0.9769625067710876\n",
      "Iteration 44644 Loss: 0.7971946597099304\n",
      "Iteration 44645 Loss: 0.4719918966293335\n",
      "Iteration 44646 Loss: 1.194858193397522\n",
      "Iteration 44647 Loss: 0.7281360030174255\n",
      "Iteration 44648 Loss: 1.1463068723678589\n",
      "Iteration 44649 Loss: 0.9276703596115112\n",
      "Iteration 44649 Loss: 0.897907555103302\n",
      "Iteration 44650 Loss: 0.7311622500419617\n",
      "Iteration 44651 Loss: 1.2696856260299683\n",
      "Iteration 44652 Loss: 1.3785245418548584\n",
      "Iteration 44653 Loss: 0.7901350855827332\n",
      "Iteration 44654 Loss: 0.8248957991600037\n",
      "Iteration 44655 Loss: 0.9772747755050659\n",
      "Iteration 44656 Loss: 0.8160731792449951\n",
      "Iteration 44657 Loss: 0.9333324432373047\n",
      "Iteration 44658 Loss: 0.8602607846260071\n",
      "Iteration 44659 Loss: 0.7180848121643066\n",
      "Iteration 44659 Loss: 0.9299429059028625\n",
      "Iteration 44660 Loss: 1.1042081117630005\n",
      "Iteration 44661 Loss: 1.00599205493927\n",
      "Iteration 44662 Loss: 0.8885108232498169\n",
      "Iteration 44663 Loss: 0.8316599130630493\n",
      "Iteration 44664 Loss: 0.9815096259117126\n",
      "Iteration 44665 Loss: 0.82298344373703\n",
      "Iteration 44666 Loss: 1.1309806108474731\n",
      "Iteration 44667 Loss: 1.0868964195251465\n",
      "Iteration 44668 Loss: 0.9482446908950806\n",
      "Iteration 44669 Loss: 0.8834312558174133\n",
      "Iteration 44669 Loss: 0.9684416055679321\n",
      "Iteration 44670 Loss: 1.2632731199264526\n",
      "Iteration 44671 Loss: 0.9705067276954651\n",
      "Iteration 44672 Loss: 1.1197376251220703\n",
      "Iteration 44673 Loss: 0.6761524081230164\n",
      "Iteration 44674 Loss: 0.89449143409729\n",
      "Iteration 44675 Loss: 0.8258844614028931\n",
      "Iteration 44676 Loss: 0.8675782680511475\n",
      "Iteration 44677 Loss: 0.7879104018211365\n",
      "Iteration 44678 Loss: 1.0863730907440186\n",
      "Iteration 44679 Loss: 1.0874587297439575\n",
      "Iteration 44679 Loss: 0.9579365849494934\n",
      "Iteration 44680 Loss: 1.1135364770889282\n",
      "Iteration 44681 Loss: 1.001672625541687\n",
      "Iteration 44682 Loss: 0.8585417866706848\n",
      "Iteration 44683 Loss: 0.8191924095153809\n",
      "Iteration 44684 Loss: 0.8889086246490479\n",
      "Iteration 44685 Loss: 1.1130856275558472\n",
      "Iteration 44686 Loss: 1.0633846521377563\n",
      "Iteration 44687 Loss: 0.8827714323997498\n",
      "Iteration 44688 Loss: 1.154502272605896\n",
      "Iteration 44689 Loss: 0.967085599899292\n",
      "Iteration 44689 Loss: 0.9862682223320007\n",
      "Iteration 44690 Loss: 0.8146165013313293\n",
      "Iteration 44691 Loss: 1.2951130867004395\n",
      "Iteration 44692 Loss: 1.0686970949172974\n",
      "Iteration 44693 Loss: 1.1296530961990356\n",
      "Iteration 44694 Loss: 0.7989111542701721\n",
      "Iteration 44695 Loss: 1.2166646718978882\n",
      "Iteration 44696 Loss: 0.9762915372848511\n",
      "Iteration 44697 Loss: 0.7972700595855713\n",
      "Iteration 44698 Loss: 1.0310441255569458\n",
      "Iteration 44699 Loss: 0.6910957098007202\n",
      "Iteration 44699 Loss: 0.9819356799125671\n",
      "Iteration 44700 Loss: 1.1484755277633667\n",
      "Iteration 44701 Loss: 0.7113566994667053\n",
      "Iteration 44702 Loss: 0.9739412069320679\n",
      "Iteration 44703 Loss: 0.8715606331825256\n",
      "Iteration 44704 Loss: 0.6153054237365723\n",
      "Iteration 44705 Loss: 0.8989875912666321\n",
      "Iteration 44706 Loss: 0.8410254716873169\n",
      "Iteration 44707 Loss: 1.075719952583313\n",
      "Iteration 44708 Loss: 1.0118982791900635\n",
      "Iteration 44709 Loss: 1.0083204507827759\n",
      "Iteration 44709 Loss: 0.9156591296195984\n",
      "Iteration 44710 Loss: 1.0999581813812256\n",
      "Iteration 44711 Loss: 0.8883355855941772\n",
      "Iteration 44712 Loss: 1.3690763711929321\n",
      "Iteration 44713 Loss: 0.8438169956207275\n",
      "Iteration 44714 Loss: 0.6520491242408752\n",
      "Iteration 44715 Loss: 1.0302938222885132\n",
      "Iteration 44716 Loss: 1.0553618669509888\n",
      "Iteration 44717 Loss: 1.1265209913253784\n",
      "Iteration 44718 Loss: 1.3033790588378906\n",
      "Iteration 44719 Loss: 0.7290295958518982\n",
      "Iteration 44719 Loss: 1.0097821950912476\n",
      "Iteration 44720 Loss: 1.0496728420257568\n",
      "Iteration 44721 Loss: 1.1071947813034058\n",
      "Iteration 44722 Loss: 1.2286347150802612\n",
      "Iteration 44723 Loss: 0.9126380085945129\n",
      "Iteration 44724 Loss: 1.0789880752563477\n",
      "Iteration 44725 Loss: 1.062650442123413\n",
      "Iteration 44726 Loss: 1.0840976238250732\n",
      "Iteration 44727 Loss: 0.8962199687957764\n",
      "Iteration 44728 Loss: 0.9940524101257324\n",
      "Iteration 44729 Loss: 0.9317312240600586\n",
      "Iteration 44729 Loss: 1.034588098526001\n",
      "Iteration 44730 Loss: 1.1619900465011597\n",
      "Iteration 44731 Loss: 0.7020708918571472\n",
      "Iteration 44732 Loss: 1.1536623239517212\n",
      "Iteration 44733 Loss: 0.7124354243278503\n",
      "Iteration 44734 Loss: 0.9096992611885071\n",
      "Iteration 44735 Loss: 0.8339654207229614\n",
      "Iteration 44736 Loss: 1.0867552757263184\n",
      "Iteration 44737 Loss: 0.5264458656311035\n",
      "Iteration 44738 Loss: 0.8809629082679749\n",
      "Iteration 44739 Loss: 1.3226701021194458\n",
      "Iteration 44739 Loss: 0.9290657043457031\n",
      "Iteration 44740 Loss: 1.2268935441970825\n",
      "Iteration 44741 Loss: 1.0051556825637817\n",
      "Iteration 44742 Loss: 0.9491571187973022\n",
      "Iteration 44743 Loss: 0.7631771564483643\n",
      "Iteration 44744 Loss: 1.1033607721328735\n",
      "Iteration 44745 Loss: 0.9423586130142212\n",
      "Iteration 44746 Loss: 1.1006685495376587\n",
      "Iteration 44747 Loss: 0.5998032689094543\n",
      "Iteration 44748 Loss: 1.0183244943618774\n",
      "Iteration 44749 Loss: 1.1376489400863647\n",
      "Iteration 44749 Loss: 0.9846547842025757\n",
      "Iteration 44750 Loss: 1.2833877801895142\n",
      "Iteration 44751 Loss: 0.6460028886795044\n",
      "Iteration 44752 Loss: 0.9781537055969238\n",
      "Iteration 44753 Loss: 0.8223331570625305\n",
      "Iteration 44754 Loss: 0.6133316159248352\n",
      "Iteration 44755 Loss: 0.7348142266273499\n",
      "Iteration 44756 Loss: 0.9820792078971863\n",
      "Iteration 44757 Loss: 0.9495249390602112\n",
      "Iteration 44758 Loss: 1.2040632963180542\n",
      "Iteration 44759 Loss: 1.0169882774353027\n",
      "Iteration 44759 Loss: 0.9230679273605347\n",
      "Iteration 44760 Loss: 1.1253411769866943\n",
      "Iteration 44761 Loss: 0.7354421615600586\n",
      "Iteration 44762 Loss: 1.018827199935913\n",
      "Iteration 44763 Loss: 1.0881824493408203\n",
      "Iteration 44764 Loss: 1.03734290599823\n",
      "Iteration 44765 Loss: 1.2206557989120483\n",
      "Iteration 44766 Loss: 1.246170997619629\n",
      "Iteration 44767 Loss: 1.138711929321289\n",
      "Iteration 44768 Loss: 0.6815105676651001\n",
      "Iteration 44769 Loss: 0.9165289998054504\n",
      "Iteration 44769 Loss: 1.0208714008331299\n",
      "Iteration 44770 Loss: 0.940019965171814\n",
      "Iteration 44771 Loss: 0.6172021627426147\n",
      "Iteration 44772 Loss: 1.2312750816345215\n",
      "Iteration 44773 Loss: 0.826067328453064\n",
      "Iteration 44774 Loss: 0.9152780175209045\n",
      "Iteration 44775 Loss: 1.2411150932312012\n",
      "Iteration 44776 Loss: 0.983148992061615\n",
      "Iteration 44777 Loss: 0.9924859404563904\n",
      "Iteration 44778 Loss: 1.0338846445083618\n",
      "Iteration 44779 Loss: 0.8022889494895935\n",
      "Iteration 44779 Loss: 0.958276629447937\n",
      "Iteration 44780 Loss: 1.075830101966858\n",
      "Iteration 44781 Loss: 0.9631988406181335\n",
      "Iteration 44782 Loss: 0.49220389127731323\n",
      "Iteration 44783 Loss: 0.9308449029922485\n",
      "Iteration 44784 Loss: 0.9052562713623047\n",
      "Iteration 44785 Loss: 1.1002672910690308\n",
      "Iteration 44786 Loss: 0.7112205028533936\n",
      "Iteration 44787 Loss: 0.9295458197593689\n",
      "Iteration 44788 Loss: 0.5449504256248474\n",
      "Iteration 44789 Loss: 0.9157571792602539\n",
      "Iteration 44789 Loss: 0.8569074869155884\n",
      "Iteration 44790 Loss: 0.8829613924026489\n",
      "Iteration 44791 Loss: 0.8348267078399658\n",
      "Iteration 44792 Loss: 0.6828470826148987\n",
      "Iteration 44793 Loss: 1.0378272533416748\n",
      "Iteration 44794 Loss: 1.0508619546890259\n",
      "Iteration 44795 Loss: 0.886692464351654\n",
      "Iteration 44796 Loss: 0.8877962231636047\n",
      "Iteration 44797 Loss: 0.9024460911750793\n",
      "Iteration 44798 Loss: 0.9502725005149841\n",
      "Iteration 44799 Loss: 0.7024631500244141\n",
      "Iteration 44799 Loss: 0.8818994760513306\n",
      "Iteration 44800 Loss: 1.0255566835403442\n",
      "Iteration 44801 Loss: 0.7575041055679321\n",
      "Iteration 44802 Loss: 0.7094584107398987\n",
      "Iteration 44803 Loss: 1.0839033126831055\n",
      "Iteration 44804 Loss: 0.7498228549957275\n",
      "Iteration 44805 Loss: 0.9750025868415833\n",
      "Iteration 44806 Loss: 0.6919859647750854\n",
      "Iteration 44807 Loss: 0.713591456413269\n",
      "Iteration 44808 Loss: 1.1673654317855835\n",
      "Iteration 44809 Loss: 0.9536815285682678\n",
      "Iteration 44809 Loss: 0.8827872276306152\n",
      "Iteration 44810 Loss: 0.6277057528495789\n",
      "Iteration 44811 Loss: 1.2365834712982178\n",
      "Iteration 44812 Loss: 1.2560526132583618\n",
      "Iteration 44813 Loss: 0.7727102637290955\n",
      "Iteration 44814 Loss: 1.0560336112976074\n",
      "Iteration 44815 Loss: 0.44849857687950134\n",
      "Iteration 44816 Loss: 1.0059095621109009\n",
      "Iteration 44817 Loss: 0.7620262503623962\n",
      "Iteration 44818 Loss: 0.7152830958366394\n",
      "Iteration 44819 Loss: 0.6936540603637695\n",
      "Iteration 44819 Loss: 0.8574457168579102\n",
      "Iteration 44820 Loss: 0.828605592250824\n",
      "Iteration 44821 Loss: 1.01372230052948\n",
      "Iteration 44822 Loss: 0.3587324917316437\n",
      "Iteration 44823 Loss: 0.8818132877349854\n",
      "Iteration 44824 Loss: 0.84075528383255\n",
      "Iteration 44825 Loss: 1.2598700523376465\n",
      "Iteration 44826 Loss: 1.1107792854309082\n",
      "Iteration 44827 Loss: 1.2162742614746094\n",
      "Iteration 44828 Loss: 1.2496578693389893\n",
      "Iteration 44829 Loss: 0.9654703140258789\n",
      "Iteration 44829 Loss: 0.972568154335022\n",
      "Iteration 44830 Loss: 0.8213213682174683\n",
      "Iteration 44831 Loss: 1.2049615383148193\n",
      "Iteration 44832 Loss: 0.8324286937713623\n",
      "Iteration 44833 Loss: 1.0185145139694214\n",
      "Iteration 44834 Loss: 0.9190624952316284\n",
      "Iteration 44835 Loss: 0.8521832823753357\n",
      "Iteration 44836 Loss: 0.9482386112213135\n",
      "Iteration 44837 Loss: 0.7859383225440979\n",
      "Iteration 44838 Loss: 1.2496800422668457\n",
      "Iteration 44839 Loss: 1.030432105064392\n",
      "Iteration 44839 Loss: 0.9662761688232422\n",
      "Iteration 44840 Loss: 1.0679709911346436\n",
      "Iteration 44841 Loss: 1.0189950466156006\n",
      "Iteration 44842 Loss: 0.9912208318710327\n",
      "Iteration 44843 Loss: 1.064439058303833\n",
      "Iteration 44844 Loss: 0.8642224669456482\n",
      "Iteration 44845 Loss: 0.9779161214828491\n",
      "Iteration 44846 Loss: 1.0017735958099365\n",
      "Iteration 44847 Loss: 0.8912263512611389\n",
      "Iteration 44848 Loss: 1.635023593902588\n",
      "Iteration 44849 Loss: 0.9693671464920044\n",
      "Iteration 44849 Loss: 1.048215627670288\n",
      "Iteration 44850 Loss: 0.8760603070259094\n",
      "Iteration 44851 Loss: 0.964357316493988\n",
      "Iteration 44852 Loss: 0.6753026843070984\n",
      "Iteration 44853 Loss: 1.0472981929779053\n",
      "Iteration 44854 Loss: 1.0276509523391724\n",
      "Iteration 44855 Loss: 0.9368337988853455\n",
      "Iteration 44856 Loss: 0.5464082360267639\n",
      "Iteration 44857 Loss: 0.705287754535675\n",
      "Iteration 44858 Loss: 0.6725431680679321\n",
      "Iteration 44859 Loss: 0.9622688889503479\n",
      "Iteration 44859 Loss: 0.8414011001586914\n",
      "Iteration 44860 Loss: 0.8687501549720764\n",
      "Iteration 44861 Loss: 1.1603418588638306\n",
      "Iteration 44862 Loss: 1.1989448070526123\n",
      "Iteration 44863 Loss: 1.2003153562545776\n",
      "Iteration 44864 Loss: 0.7466621398925781\n",
      "Iteration 44865 Loss: 0.8031134009361267\n",
      "Iteration 44866 Loss: 0.8818676471710205\n",
      "Iteration 44867 Loss: 0.933427095413208\n",
      "Iteration 44868 Loss: 0.48514991998672485\n",
      "Iteration 44869 Loss: 0.93104487657547\n",
      "Iteration 44869 Loss: 0.9209617376327515\n",
      "Iteration 44870 Loss: 0.981866180896759\n",
      "Iteration 44871 Loss: 1.090008020401001\n",
      "Iteration 44872 Loss: 0.9637237191200256\n",
      "Iteration 44873 Loss: 1.0378526449203491\n",
      "Iteration 44874 Loss: 0.7713045477867126\n",
      "Iteration 44875 Loss: 1.0712941884994507\n",
      "Iteration 44876 Loss: 1.2309937477111816\n",
      "Iteration 44877 Loss: 1.0912632942199707\n",
      "Iteration 44878 Loss: 0.9104930758476257\n",
      "Iteration 44879 Loss: 0.7935277223587036\n",
      "Iteration 44879 Loss: 0.9942327737808228\n",
      "Iteration 44880 Loss: 1.0188218355178833\n",
      "Iteration 44881 Loss: 0.9414258599281311\n",
      "Iteration 44882 Loss: 0.8493520617485046\n",
      "Iteration 44883 Loss: 1.224706768989563\n",
      "Iteration 44884 Loss: 0.854130208492279\n",
      "Iteration 44885 Loss: 0.8360893130302429\n",
      "Iteration 44886 Loss: 1.1197433471679688\n",
      "Iteration 44887 Loss: 1.3128575086593628\n",
      "Iteration 44888 Loss: 1.0825263261795044\n",
      "Iteration 44889 Loss: 0.6961151361465454\n",
      "Iteration 44889 Loss: 0.9935768246650696\n",
      "Iteration 44890 Loss: 0.5818286538124084\n",
      "Iteration 44891 Loss: 0.9821800589561462\n",
      "Iteration 44892 Loss: 0.9899901151657104\n",
      "Iteration 44893 Loss: 0.7780515551567078\n",
      "Iteration 44894 Loss: 0.8076670169830322\n",
      "Iteration 44895 Loss: 1.0460476875305176\n",
      "Iteration 44896 Loss: 0.7453211545944214\n",
      "Iteration 44897 Loss: 0.6731736063957214\n",
      "Iteration 44898 Loss: 0.9100674986839294\n",
      "Iteration 44899 Loss: 1.0067485570907593\n",
      "Iteration 44899 Loss: 0.8521075248718262\n",
      "Iteration 44900 Loss: 0.7975011467933655\n",
      "Iteration 44901 Loss: 0.6376737952232361\n",
      "Iteration 44902 Loss: 1.2260972261428833\n",
      "Iteration 44903 Loss: 1.1709493398666382\n",
      "Iteration 44904 Loss: 1.0104247331619263\n",
      "Iteration 44905 Loss: 0.974041759967804\n",
      "Iteration 44906 Loss: 0.6564059853553772\n",
      "Iteration 44907 Loss: 0.865850031375885\n",
      "Iteration 44908 Loss: 0.8679444789886475\n",
      "Iteration 44909 Loss: 1.013282299041748\n",
      "Iteration 44909 Loss: 0.9220172166824341\n",
      "Iteration 44910 Loss: 0.909827470779419\n",
      "Iteration 44911 Loss: 0.8084830641746521\n",
      "Iteration 44912 Loss: 0.9760255217552185\n",
      "Iteration 44913 Loss: 0.722913384437561\n",
      "Iteration 44914 Loss: 1.0790520906448364\n",
      "Iteration 44915 Loss: 0.8666794896125793\n",
      "Iteration 44916 Loss: 0.8391427993774414\n",
      "Iteration 44917 Loss: 0.9879079461097717\n",
      "Iteration 44918 Loss: 0.9391859173774719\n",
      "Iteration 44919 Loss: 0.8553109169006348\n",
      "Iteration 44919 Loss: 0.898452877998352\n",
      "Iteration 44920 Loss: 0.8753511309623718\n",
      "Iteration 44921 Loss: 0.9963425397872925\n",
      "Iteration 44922 Loss: 1.1677361726760864\n",
      "Iteration 44923 Loss: 0.9814242124557495\n",
      "Iteration 44924 Loss: 1.0313587188720703\n",
      "Iteration 44925 Loss: 0.7630174160003662\n",
      "Iteration 44926 Loss: 1.3322083950042725\n",
      "Iteration 44927 Loss: 0.7886139154434204\n",
      "Iteration 44928 Loss: 0.897167980670929\n",
      "Iteration 44929 Loss: 0.8636712431907654\n",
      "Iteration 44929 Loss: 0.9696892499923706\n",
      "Iteration 44930 Loss: 1.0181776285171509\n",
      "Iteration 44931 Loss: 0.6866322159767151\n",
      "Iteration 44932 Loss: 0.9498081803321838\n",
      "Iteration 44933 Loss: 0.7622641921043396\n",
      "Iteration 44934 Loss: 0.6327202916145325\n",
      "Iteration 44935 Loss: 0.9618174433708191\n",
      "Iteration 44936 Loss: 1.0233973264694214\n",
      "Iteration 44937 Loss: 0.8475214838981628\n",
      "Iteration 44938 Loss: 0.8835552930831909\n",
      "Iteration 44939 Loss: 0.8368434309959412\n",
      "Iteration 44939 Loss: 0.8602737188339233\n",
      "Iteration 44940 Loss: 0.9212518334388733\n",
      "Iteration 44941 Loss: 0.5550560355186462\n",
      "Iteration 44942 Loss: 1.0858060121536255\n",
      "Iteration 44943 Loss: 0.8542017340660095\n",
      "Iteration 44944 Loss: 1.184385061264038\n",
      "Iteration 44945 Loss: 0.9377251267433167\n",
      "Iteration 44946 Loss: 0.9022771716117859\n",
      "Iteration 44947 Loss: 0.6354403495788574\n",
      "Iteration 44948 Loss: 1.0372729301452637\n",
      "Iteration 44949 Loss: 0.8755722641944885\n",
      "Iteration 44949 Loss: 0.8988988995552063\n",
      "Iteration 44950 Loss: 1.1568554639816284\n",
      "Iteration 44951 Loss: 1.2240455150604248\n",
      "Iteration 44952 Loss: 0.770196795463562\n",
      "Iteration 44953 Loss: 1.021018147468567\n",
      "Iteration 44954 Loss: 0.9866943359375\n",
      "Iteration 44955 Loss: 1.0048997402191162\n",
      "Iteration 44956 Loss: 0.9985015392303467\n",
      "Iteration 44957 Loss: 0.7539355754852295\n",
      "Iteration 44958 Loss: 1.2069720029830933\n",
      "Iteration 44959 Loss: 1.1412707567214966\n",
      "Iteration 44959 Loss: 1.0264390707015991\n",
      "Iteration 44960 Loss: 1.0459322929382324\n",
      "Iteration 44961 Loss: 0.8525108695030212\n",
      "Iteration 44962 Loss: 1.0934276580810547\n",
      "Iteration 44963 Loss: 0.7461424469947815\n",
      "Iteration 44964 Loss: 0.810182511806488\n",
      "Iteration 44965 Loss: 1.032846450805664\n",
      "Iteration 44966 Loss: 0.9128062129020691\n",
      "Iteration 44967 Loss: 0.6068487167358398\n",
      "Iteration 44968 Loss: 1.2779699563980103\n",
      "Iteration 44969 Loss: 0.930602490901947\n",
      "Iteration 44969 Loss: 0.9309269785881042\n",
      "Iteration 44970 Loss: 0.955299437046051\n",
      "Iteration 44971 Loss: 0.8555341362953186\n",
      "Iteration 44972 Loss: 0.7539144158363342\n",
      "Iteration 44973 Loss: 0.9204872250556946\n",
      "Iteration 44974 Loss: 0.8355517387390137\n",
      "Iteration 44975 Loss: 1.440880298614502\n",
      "Iteration 44976 Loss: 1.022950291633606\n",
      "Iteration 44977 Loss: 0.8345722556114197\n",
      "Iteration 44978 Loss: 1.2733604907989502\n",
      "Iteration 44979 Loss: 0.7240157127380371\n",
      "Iteration 44979 Loss: 0.9616565704345703\n",
      "Iteration 44980 Loss: 0.890184223651886\n",
      "Iteration 44981 Loss: 1.0927371978759766\n",
      "Iteration 44982 Loss: 1.2439703941345215\n",
      "Iteration 44983 Loss: 0.9307969808578491\n",
      "Iteration 44984 Loss: 1.0838452577590942\n",
      "Iteration 44985 Loss: 0.4570564031600952\n",
      "Iteration 44986 Loss: 1.2798488140106201\n",
      "Iteration 44987 Loss: 1.0101523399353027\n",
      "Iteration 44988 Loss: 1.0401875972747803\n",
      "Iteration 44989 Loss: 1.2908142805099487\n",
      "Iteration 44989 Loss: 1.0319592952728271\n",
      "Iteration 44990 Loss: 1.3233799934387207\n",
      "Iteration 44991 Loss: 0.8595811128616333\n",
      "Iteration 44992 Loss: 0.7998636960983276\n",
      "Iteration 44993 Loss: 0.8473563194274902\n",
      "Iteration 44994 Loss: 0.8964650630950928\n",
      "Iteration 44995 Loss: 0.9367337822914124\n",
      "Iteration 44996 Loss: 0.8393470644950867\n",
      "Iteration 44997 Loss: 0.854889452457428\n",
      "Iteration 44998 Loss: 1.1503342390060425\n",
      "Iteration 44999 Loss: 0.6147923469543457\n",
      "Iteration 44999 Loss: 0.9122743606567383\n",
      "Iteration 45000 Loss: 0.8353074193000793\n",
      "Iteration 45001 Loss: 0.8711165189743042\n",
      "Iteration 45002 Loss: 1.0629221200942993\n",
      "Iteration 45003 Loss: 0.7720555067062378\n",
      "Iteration 45004 Loss: 0.8952106237411499\n",
      "Iteration 45005 Loss: 0.621629536151886\n",
      "Iteration 45006 Loss: 1.0545463562011719\n",
      "Iteration 45007 Loss: 0.8575448393821716\n",
      "Iteration 45008 Loss: 1.2035000324249268\n",
      "Iteration 45009 Loss: 0.8777644634246826\n",
      "Iteration 45009 Loss: 0.9051597714424133\n",
      "Iteration 45010 Loss: 1.2160669565200806\n",
      "Iteration 45011 Loss: 1.1529076099395752\n",
      "Iteration 45012 Loss: 1.000138521194458\n",
      "Iteration 45013 Loss: 0.8255196213722229\n",
      "Iteration 45014 Loss: 0.7720575928688049\n",
      "Iteration 45015 Loss: 1.3382130861282349\n",
      "Iteration 45016 Loss: 1.1473075151443481\n",
      "Iteration 45017 Loss: 1.179760217666626\n",
      "Iteration 45018 Loss: 0.9019522070884705\n",
      "Iteration 45019 Loss: 0.9501383304595947\n",
      "Iteration 45019 Loss: 1.0484061241149902\n",
      "Iteration 45020 Loss: 0.9534177780151367\n",
      "Iteration 45021 Loss: 0.954897940158844\n",
      "Iteration 45022 Loss: 0.6435421109199524\n",
      "Iteration 45023 Loss: 0.666993260383606\n",
      "Iteration 45024 Loss: 1.2411386966705322\n",
      "Iteration 45025 Loss: 0.9586281180381775\n",
      "Iteration 45026 Loss: 0.9138338565826416\n",
      "Iteration 45027 Loss: 1.239128589630127\n",
      "Iteration 45028 Loss: 1.2498373985290527\n",
      "Iteration 45029 Loss: 1.10358464717865\n",
      "Iteration 45029 Loss: 0.9925003051757812\n",
      "Iteration 45030 Loss: 0.8560224771499634\n",
      "Iteration 45031 Loss: 0.7914456129074097\n",
      "Iteration 45032 Loss: 1.3337899446487427\n",
      "Iteration 45033 Loss: 0.9077460169792175\n",
      "Iteration 45034 Loss: 0.6897016167640686\n",
      "Iteration 45035 Loss: 0.791446328163147\n",
      "Iteration 45036 Loss: 0.9765570163726807\n",
      "Iteration 45037 Loss: 0.600883424282074\n",
      "Iteration 45038 Loss: 1.099210500717163\n",
      "Iteration 45039 Loss: 0.7849924564361572\n",
      "Iteration 45039 Loss: 0.8831794857978821\n",
      "Iteration 45040 Loss: 1.3906265497207642\n",
      "Iteration 45041 Loss: 0.9764312505722046\n",
      "Iteration 45042 Loss: 0.6220372915267944\n",
      "Iteration 45043 Loss: 0.8231428861618042\n",
      "Iteration 45044 Loss: 0.9179599285125732\n",
      "Iteration 45045 Loss: 0.9364600777626038\n",
      "Iteration 45046 Loss: 1.4795093536376953\n",
      "Iteration 45047 Loss: 1.0242257118225098\n",
      "Iteration 45048 Loss: 0.9974517822265625\n",
      "Iteration 45049 Loss: 1.1810531616210938\n",
      "Iteration 45049 Loss: 1.034889817237854\n",
      "Iteration 45050 Loss: 0.6631303429603577\n",
      "Iteration 45051 Loss: 1.2029565572738647\n",
      "Iteration 45052 Loss: 1.0670772790908813\n",
      "Iteration 45053 Loss: 1.1156431436538696\n",
      "Iteration 45054 Loss: 0.9255005717277527\n",
      "Iteration 45055 Loss: 1.2236018180847168\n",
      "Iteration 45056 Loss: 1.0470725297927856\n",
      "Iteration 45057 Loss: 0.5653461217880249\n",
      "Iteration 45058 Loss: 1.1208137273788452\n",
      "Iteration 45059 Loss: 0.6309080123901367\n",
      "Iteration 45059 Loss: 0.9562050104141235\n",
      "Iteration 45060 Loss: 0.8040134310722351\n",
      "Iteration 45061 Loss: 0.8238924741744995\n",
      "Iteration 45062 Loss: 1.1344999074935913\n",
      "Iteration 45063 Loss: 0.8448356986045837\n",
      "Iteration 45064 Loss: 1.1391266584396362\n",
      "Iteration 45065 Loss: 0.7455452680587769\n",
      "Iteration 45066 Loss: 0.6732051968574524\n",
      "Iteration 45067 Loss: 0.911056637763977\n",
      "Iteration 45068 Loss: 0.7402164340019226\n",
      "Iteration 45069 Loss: 0.8171660304069519\n",
      "Iteration 45069 Loss: 0.863355815410614\n",
      "Iteration 45070 Loss: 1.2412831783294678\n",
      "Iteration 45071 Loss: 0.842419445514679\n",
      "Iteration 45072 Loss: 1.0010164976119995\n",
      "Iteration 45073 Loss: 0.762898862361908\n",
      "Iteration 45074 Loss: 1.0664772987365723\n",
      "Iteration 45075 Loss: 0.9004050493240356\n",
      "Iteration 45076 Loss: 1.147457480430603\n",
      "Iteration 45077 Loss: 0.7677393555641174\n",
      "Iteration 45078 Loss: 1.0646371841430664\n",
      "Iteration 45079 Loss: 0.9333651065826416\n",
      "Iteration 45079 Loss: 0.9727699160575867\n",
      "Iteration 45080 Loss: 1.1354016065597534\n",
      "Iteration 45081 Loss: 0.7578862905502319\n",
      "Iteration 45082 Loss: 0.8035377860069275\n",
      "Iteration 45083 Loss: 1.2925664186477661\n",
      "Iteration 45084 Loss: 0.9509648680686951\n",
      "Iteration 45085 Loss: 1.2374215126037598\n",
      "Iteration 45086 Loss: 0.7326015830039978\n",
      "Iteration 45087 Loss: 1.0169694423675537\n",
      "Iteration 45088 Loss: 1.0749894380569458\n",
      "Iteration 45089 Loss: 1.0037577152252197\n",
      "Iteration 45089 Loss: 1.0006096363067627\n",
      "Iteration 45090 Loss: 0.870130181312561\n",
      "Iteration 45091 Loss: 0.7841798067092896\n",
      "Iteration 45092 Loss: 0.7990288138389587\n",
      "Iteration 45093 Loss: 1.041444182395935\n",
      "Iteration 45094 Loss: 1.0615344047546387\n",
      "Iteration 45095 Loss: 0.9303523302078247\n",
      "Iteration 45096 Loss: 0.9696816205978394\n",
      "Iteration 45097 Loss: 0.9985396862030029\n",
      "Iteration 45098 Loss: 1.1247199773788452\n",
      "Iteration 45099 Loss: 0.8531034588813782\n",
      "Iteration 45099 Loss: 0.9432714581489563\n",
      "Iteration 45100 Loss: 1.045839548110962\n",
      "Iteration 45101 Loss: 0.8089848160743713\n",
      "Iteration 45102 Loss: 1.04351007938385\n",
      "Iteration 45103 Loss: 1.0215405225753784\n",
      "Iteration 45104 Loss: 0.8640448451042175\n",
      "Iteration 45105 Loss: 0.8999534249305725\n",
      "Iteration 45106 Loss: 1.1361240148544312\n",
      "Iteration 45107 Loss: 1.2052491903305054\n",
      "Iteration 45108 Loss: 0.8094830513000488\n",
      "Iteration 45109 Loss: 1.1183810234069824\n",
      "Iteration 45109 Loss: 0.9953109622001648\n",
      "Iteration 45110 Loss: 0.852953314781189\n",
      "Iteration 45111 Loss: 0.7700515985488892\n",
      "Iteration 45112 Loss: 0.9843001961708069\n",
      "Iteration 45113 Loss: 1.2022883892059326\n",
      "Iteration 45114 Loss: 0.873326301574707\n",
      "Iteration 45115 Loss: 0.9803856611251831\n",
      "Iteration 45116 Loss: 1.188636302947998\n",
      "Iteration 45117 Loss: 0.8096879124641418\n",
      "Iteration 45118 Loss: 0.9313292503356934\n",
      "Iteration 45119 Loss: 0.7471264004707336\n",
      "Iteration 45119 Loss: 0.9340084791183472\n",
      "Iteration 45120 Loss: 0.8249379396438599\n",
      "Iteration 45121 Loss: 0.5793619751930237\n",
      "Iteration 45122 Loss: 1.0837597846984863\n",
      "Iteration 45123 Loss: 0.9508246779441833\n",
      "Iteration 45124 Loss: 0.4902939200401306\n",
      "Iteration 45125 Loss: 0.5296577215194702\n",
      "Iteration 45126 Loss: 1.1958091259002686\n",
      "Iteration 45127 Loss: 0.8554505705833435\n",
      "Iteration 45128 Loss: 0.7367990016937256\n",
      "Iteration 45129 Loss: 1.16304349899292\n",
      "Iteration 45129 Loss: 0.8409938812255859\n",
      "Iteration 45130 Loss: 1.0636049509048462\n",
      "Iteration 45131 Loss: 0.9757069945335388\n",
      "Iteration 45132 Loss: 0.9457462430000305\n",
      "Iteration 45133 Loss: 0.8785646557807922\n",
      "Iteration 45134 Loss: 0.8478663563728333\n",
      "Iteration 45135 Loss: 0.552265465259552\n",
      "Iteration 45136 Loss: 1.0141873359680176\n",
      "Iteration 45137 Loss: 0.7672047019004822\n",
      "Iteration 45138 Loss: 0.9253499507904053\n",
      "Iteration 45139 Loss: 0.7376945614814758\n",
      "Iteration 45139 Loss: 0.8708192110061646\n",
      "Iteration 45140 Loss: 1.0693786144256592\n",
      "Iteration 45141 Loss: 0.8499258160591125\n",
      "Iteration 45142 Loss: 1.0963705778121948\n",
      "Iteration 45143 Loss: 0.9057552218437195\n",
      "Iteration 45144 Loss: 0.9718486070632935\n",
      "Iteration 45145 Loss: 1.2774155139923096\n",
      "Iteration 45146 Loss: 1.0680307149887085\n",
      "Iteration 45147 Loss: 0.9296939373016357\n",
      "Iteration 45148 Loss: 1.198999047279358\n",
      "Iteration 45149 Loss: 0.9268491268157959\n",
      "Iteration 45149 Loss: 1.0294266939163208\n",
      "Iteration 45150 Loss: 0.8396435379981995\n",
      "Iteration 45151 Loss: 0.635226845741272\n",
      "Iteration 45152 Loss: 0.8011237978935242\n",
      "Iteration 45153 Loss: 0.8393015265464783\n",
      "Iteration 45154 Loss: 0.8337492346763611\n",
      "Iteration 45155 Loss: 0.9741647839546204\n",
      "Iteration 45156 Loss: 0.893143355846405\n",
      "Iteration 45157 Loss: 0.7396417260169983\n",
      "Iteration 45158 Loss: 1.066192865371704\n",
      "Iteration 45159 Loss: 0.7118768692016602\n",
      "Iteration 45159 Loss: 0.8334064483642578\n",
      "Iteration 45160 Loss: 1.152147889137268\n",
      "Iteration 45161 Loss: 0.9547173976898193\n",
      "Iteration 45162 Loss: 0.7994869351387024\n",
      "Iteration 45163 Loss: 1.036853551864624\n",
      "Iteration 45164 Loss: 1.016968011856079\n",
      "Iteration 45165 Loss: 1.242133378982544\n",
      "Iteration 45166 Loss: 0.811676025390625\n",
      "Iteration 45167 Loss: 0.9502668380737305\n",
      "Iteration 45168 Loss: 0.8872853517532349\n",
      "Iteration 45169 Loss: 1.1216307878494263\n",
      "Iteration 45169 Loss: 0.9973166584968567\n",
      "Iteration 45170 Loss: 1.0209465026855469\n",
      "Iteration 45171 Loss: 0.8894418478012085\n",
      "Iteration 45172 Loss: 0.8853650093078613\n",
      "Iteration 45173 Loss: 0.745469868183136\n",
      "Iteration 45174 Loss: 0.6542445421218872\n",
      "Iteration 45175 Loss: 0.7520946264266968\n",
      "Iteration 45176 Loss: 0.8243815898895264\n",
      "Iteration 45177 Loss: 0.8960089683532715\n",
      "Iteration 45178 Loss: 1.1371914148330688\n",
      "Iteration 45179 Loss: 0.6968507170677185\n",
      "Iteration 45179 Loss: 0.8501995205879211\n",
      "Iteration 45180 Loss: 1.086677074432373\n",
      "Iteration 45181 Loss: 1.019681453704834\n",
      "Iteration 45182 Loss: 0.8171513080596924\n",
      "Iteration 45183 Loss: 0.9609981179237366\n",
      "Iteration 45184 Loss: 1.176699161529541\n",
      "Iteration 45185 Loss: 0.8749738335609436\n",
      "Iteration 45186 Loss: 1.0860685110092163\n",
      "Iteration 45187 Loss: 0.9755698442459106\n",
      "Iteration 45188 Loss: 1.1315500736236572\n",
      "Iteration 45189 Loss: 0.7119481563568115\n",
      "Iteration 45189 Loss: 0.9841316938400269\n",
      "Iteration 45190 Loss: 1.025073528289795\n",
      "Iteration 45191 Loss: 0.9269580841064453\n",
      "Iteration 45192 Loss: 1.1300618648529053\n",
      "Iteration 45193 Loss: 0.829158365726471\n",
      "Iteration 45194 Loss: 0.9302495121955872\n",
      "Iteration 45195 Loss: 0.9292476177215576\n",
      "Iteration 45196 Loss: 0.9772630333900452\n",
      "Iteration 45197 Loss: 0.8377406597137451\n",
      "Iteration 45198 Loss: 0.9660999178886414\n",
      "Iteration 45199 Loss: 0.8011235594749451\n",
      "Iteration 45199 Loss: 0.9352976679801941\n",
      "Iteration 45200 Loss: 0.9253959655761719\n",
      "Iteration 45201 Loss: 0.8524407744407654\n",
      "Iteration 45202 Loss: 0.7710092663764954\n",
      "Iteration 45203 Loss: 0.8001991510391235\n",
      "Iteration 45204 Loss: 0.9263353943824768\n",
      "Iteration 45205 Loss: 1.074055552482605\n",
      "Iteration 45206 Loss: 0.7922685146331787\n",
      "Iteration 45207 Loss: 0.885984480381012\n",
      "Iteration 45208 Loss: 1.048933744430542\n",
      "Iteration 45209 Loss: 0.8989723920822144\n",
      "Iteration 45209 Loss: 0.8975595235824585\n",
      "Iteration 45210 Loss: 1.085457444190979\n",
      "Iteration 45211 Loss: 0.871502161026001\n",
      "Iteration 45212 Loss: 1.2702267169952393\n",
      "Iteration 45213 Loss: 1.065443515777588\n",
      "Iteration 45214 Loss: 1.0838572978973389\n",
      "Iteration 45215 Loss: 0.8598114848136902\n",
      "Iteration 45216 Loss: 0.9084406495094299\n",
      "Iteration 45217 Loss: 0.9564602971076965\n",
      "Iteration 45218 Loss: 0.90741366147995\n",
      "Iteration 45219 Loss: 0.9646775722503662\n",
      "Iteration 45219 Loss: 0.99732905626297\n",
      "Iteration 45220 Loss: 0.9168060421943665\n",
      "Iteration 45221 Loss: 1.2106916904449463\n",
      "Iteration 45222 Loss: 0.8700256943702698\n",
      "Iteration 45223 Loss: 0.9948712587356567\n",
      "Iteration 45224 Loss: 0.8354225158691406\n",
      "Iteration 45225 Loss: 0.9561784863471985\n",
      "Iteration 45226 Loss: 1.2222775220870972\n",
      "Iteration 45227 Loss: 0.7154897451400757\n",
      "Iteration 45228 Loss: 1.006631851196289\n",
      "Iteration 45229 Loss: 0.6869233846664429\n",
      "Iteration 45229 Loss: 0.941531777381897\n",
      "Iteration 45230 Loss: 0.9790151119232178\n",
      "Iteration 45231 Loss: 1.4481680393218994\n",
      "Iteration 45232 Loss: 1.0594487190246582\n",
      "Iteration 45233 Loss: 0.9744786024093628\n",
      "Iteration 45234 Loss: 0.8954383730888367\n",
      "Iteration 45235 Loss: 0.9204496145248413\n",
      "Iteration 45236 Loss: 0.7817659974098206\n",
      "Iteration 45237 Loss: 1.1230769157409668\n",
      "Iteration 45238 Loss: 0.7696422934532166\n",
      "Iteration 45239 Loss: 1.0873619318008423\n",
      "Iteration 45239 Loss: 1.0038845539093018\n",
      "Iteration 45240 Loss: 1.2799139022827148\n",
      "Iteration 45241 Loss: 1.1001267433166504\n",
      "Iteration 45242 Loss: 1.385715365409851\n",
      "Iteration 45243 Loss: 0.7572634816169739\n",
      "Iteration 45244 Loss: 0.7479044198989868\n",
      "Iteration 45245 Loss: 0.6361488103866577\n",
      "Iteration 45246 Loss: 0.8029122352600098\n",
      "Iteration 45247 Loss: 0.9102203249931335\n",
      "Iteration 45248 Loss: 1.0394989252090454\n",
      "Iteration 45249 Loss: 0.7693647742271423\n",
      "Iteration 45249 Loss: 0.9429068565368652\n",
      "Iteration 45250 Loss: 1.444631576538086\n",
      "Iteration 45251 Loss: 1.180830955505371\n",
      "Iteration 45252 Loss: 0.8444333672523499\n",
      "Iteration 45253 Loss: 0.8150277137756348\n",
      "Iteration 45254 Loss: 1.2382856607437134\n",
      "Iteration 45255 Loss: 1.3679637908935547\n",
      "Iteration 45256 Loss: 1.0298724174499512\n",
      "Iteration 45257 Loss: 0.927850604057312\n",
      "Iteration 45258 Loss: 1.2307112216949463\n",
      "Iteration 45259 Loss: 1.3436367511749268\n",
      "Iteration 45259 Loss: 1.142324447631836\n",
      "Iteration 45260 Loss: 0.8458693623542786\n",
      "Iteration 45261 Loss: 0.8762205243110657\n",
      "Iteration 45262 Loss: 0.6414960026741028\n",
      "Iteration 45263 Loss: 0.7562379837036133\n",
      "Iteration 45264 Loss: 0.9873777031898499\n",
      "Iteration 45265 Loss: 0.7258650064468384\n",
      "Iteration 45266 Loss: 1.1491749286651611\n",
      "Iteration 45267 Loss: 0.9615382552146912\n",
      "Iteration 45268 Loss: 1.0583475828170776\n",
      "Iteration 45269 Loss: 0.9241260290145874\n",
      "Iteration 45269 Loss: 0.8926253318786621\n",
      "Iteration 45270 Loss: 0.8635775446891785\n",
      "Iteration 45271 Loss: 0.9002736210823059\n",
      "Iteration 45272 Loss: 1.0848920345306396\n",
      "Iteration 45273 Loss: 0.7652451992034912\n",
      "Iteration 45274 Loss: 0.5964646935462952\n",
      "Iteration 45275 Loss: 0.6776161193847656\n",
      "Iteration 45276 Loss: 0.8771195411682129\n",
      "Iteration 45277 Loss: 1.0392398834228516\n",
      "Iteration 45278 Loss: 0.781165599822998\n",
      "Iteration 45279 Loss: 1.0550180673599243\n",
      "Iteration 45279 Loss: 0.8640612363815308\n",
      "Iteration 45280 Loss: 1.0954947471618652\n",
      "Iteration 45281 Loss: 1.0523607730865479\n",
      "Iteration 45282 Loss: 1.0077977180480957\n",
      "Iteration 45283 Loss: 0.9362296462059021\n",
      "Iteration 45284 Loss: 0.734732985496521\n",
      "Iteration 45285 Loss: 1.1115431785583496\n",
      "Iteration 45286 Loss: 0.9715220332145691\n",
      "Iteration 45287 Loss: 0.7885707020759583\n",
      "Iteration 45288 Loss: 1.052357792854309\n",
      "Iteration 45289 Loss: 0.8128177523612976\n",
      "Iteration 45289 Loss: 0.9563426971435547\n",
      "Iteration 45290 Loss: 0.6018020510673523\n",
      "Iteration 45291 Loss: 0.8732128143310547\n",
      "Iteration 45292 Loss: 0.5738321542739868\n",
      "Iteration 45293 Loss: 0.7362720966339111\n",
      "Iteration 45294 Loss: 1.1539157629013062\n",
      "Iteration 45295 Loss: 0.6856220960617065\n",
      "Iteration 45296 Loss: 0.9844818711280823\n",
      "Iteration 45297 Loss: 0.7818240523338318\n",
      "Iteration 45298 Loss: 0.8248996734619141\n",
      "Iteration 45299 Loss: 0.7262182235717773\n",
      "Iteration 45299 Loss: 0.7942081093788147\n",
      "Iteration 45300 Loss: 0.8668258786201477\n",
      "Iteration 45301 Loss: 1.0165982246398926\n",
      "Iteration 45302 Loss: 1.0018174648284912\n",
      "Iteration 45303 Loss: 0.7467061877250671\n",
      "Iteration 45304 Loss: 0.9131616353988647\n",
      "Iteration 45305 Loss: 0.8929938077926636\n",
      "Iteration 45306 Loss: 0.9697668552398682\n",
      "Iteration 45307 Loss: 0.5040896534919739\n",
      "Iteration 45308 Loss: 0.7803298234939575\n",
      "Iteration 45309 Loss: 0.9689571857452393\n",
      "Iteration 45309 Loss: 0.8661246299743652\n",
      "Iteration 45310 Loss: 1.0564754009246826\n",
      "Iteration 45311 Loss: 0.9830785989761353\n",
      "Iteration 45312 Loss: 0.7526318430900574\n",
      "Iteration 45313 Loss: 1.0363397598266602\n",
      "Iteration 45314 Loss: 1.0180734395980835\n",
      "Iteration 45315 Loss: 1.0750477313995361\n",
      "Iteration 45316 Loss: 1.2952967882156372\n",
      "Iteration 45317 Loss: 0.6953747868537903\n",
      "Iteration 45318 Loss: 0.8413275480270386\n",
      "Iteration 45319 Loss: 0.9285624623298645\n",
      "Iteration 45319 Loss: 0.9682208299636841\n",
      "Iteration 45320 Loss: 1.0165554285049438\n",
      "Iteration 45321 Loss: 1.286670446395874\n",
      "Iteration 45322 Loss: 0.897865891456604\n",
      "Iteration 45323 Loss: 0.8834042549133301\n",
      "Iteration 45324 Loss: 1.2421231269836426\n",
      "Iteration 45325 Loss: 1.0176641941070557\n",
      "Iteration 45326 Loss: 0.8748492002487183\n",
      "Iteration 45327 Loss: 0.8045662045478821\n",
      "Iteration 45328 Loss: 1.4446089267730713\n",
      "Iteration 45329 Loss: 0.6688427925109863\n",
      "Iteration 45329 Loss: 1.0137150287628174\n",
      "Iteration 45330 Loss: 0.9830486178398132\n",
      "Iteration 45331 Loss: 1.0152323246002197\n",
      "Iteration 45332 Loss: 0.7089577913284302\n",
      "Iteration 45333 Loss: 0.8184924125671387\n",
      "Iteration 45334 Loss: 0.7924680113792419\n",
      "Iteration 45335 Loss: 1.2016648054122925\n",
      "Iteration 45336 Loss: 0.9749616384506226\n",
      "Iteration 45337 Loss: 0.9419437050819397\n",
      "Iteration 45338 Loss: 0.7585620880126953\n",
      "Iteration 45339 Loss: 0.826840877532959\n",
      "Iteration 45339 Loss: 0.9022172689437866\n",
      "Iteration 45340 Loss: 0.7043603658676147\n",
      "Iteration 45341 Loss: 0.8794376254081726\n",
      "Iteration 45342 Loss: 0.9253678321838379\n",
      "Iteration 45343 Loss: 1.3777658939361572\n",
      "Iteration 45344 Loss: 0.7728046774864197\n",
      "Iteration 45345 Loss: 0.5895098447799683\n",
      "Iteration 45346 Loss: 0.9970057010650635\n",
      "Iteration 45347 Loss: 0.9679778218269348\n",
      "Iteration 45348 Loss: 1.1309044361114502\n",
      "Iteration 45349 Loss: 0.7657232880592346\n",
      "Iteration 45349 Loss: 0.9110857248306274\n",
      "Iteration 45350 Loss: 0.6751589179039001\n",
      "Iteration 45351 Loss: 0.8823991417884827\n",
      "Iteration 45352 Loss: 1.037936806678772\n",
      "Iteration 45353 Loss: 1.2785624265670776\n",
      "Iteration 45354 Loss: 1.003222942352295\n",
      "Iteration 45355 Loss: 0.9195371866226196\n",
      "Iteration 45356 Loss: 0.918673574924469\n",
      "Iteration 45357 Loss: 1.1069879531860352\n",
      "Iteration 45358 Loss: 0.7036328315734863\n",
      "Iteration 45359 Loss: 1.1211612224578857\n",
      "Iteration 45359 Loss: 0.9647272825241089\n",
      "Iteration 45360 Loss: 1.2416059970855713\n",
      "Iteration 45361 Loss: 0.9972432851791382\n",
      "Iteration 45362 Loss: 0.7993406653404236\n",
      "Iteration 45363 Loss: 1.052545189857483\n",
      "Iteration 45364 Loss: 1.0111159086227417\n",
      "Iteration 45365 Loss: 0.9261420965194702\n",
      "Iteration 45366 Loss: 0.7842637300491333\n",
      "Iteration 45367 Loss: 0.7700352668762207\n",
      "Iteration 45368 Loss: 0.8647698760032654\n",
      "Iteration 45369 Loss: 0.6854931116104126\n",
      "Iteration 45369 Loss: 0.913255512714386\n",
      "Iteration 45370 Loss: 0.9631784558296204\n",
      "Iteration 45371 Loss: 0.9664222598075867\n",
      "Iteration 45372 Loss: 1.0299673080444336\n",
      "Iteration 45373 Loss: 0.9082133769989014\n",
      "Iteration 45374 Loss: 0.6272428035736084\n",
      "Iteration 45375 Loss: 0.9435470700263977\n",
      "Iteration 45376 Loss: 0.9877744913101196\n",
      "Iteration 45377 Loss: 1.040368914604187\n",
      "Iteration 45378 Loss: 0.8524417877197266\n",
      "Iteration 45379 Loss: 0.8705885410308838\n",
      "Iteration 45379 Loss: 0.9189745783805847\n",
      "Iteration 45380 Loss: 0.5606655478477478\n",
      "Iteration 45381 Loss: 0.7533078193664551\n",
      "Iteration 45382 Loss: 1.1329646110534668\n",
      "Iteration 45383 Loss: 0.7783622741699219\n",
      "Iteration 45384 Loss: 0.7777031660079956\n",
      "Iteration 45385 Loss: 0.897929847240448\n",
      "Iteration 45386 Loss: 0.7631765007972717\n",
      "Iteration 45387 Loss: 0.7022739052772522\n",
      "Iteration 45388 Loss: 0.7197595238685608\n",
      "Iteration 45389 Loss: 0.7295471429824829\n",
      "Iteration 45389 Loss: 0.7815690636634827\n",
      "Iteration 45390 Loss: 1.1707831621170044\n",
      "Iteration 45391 Loss: 0.9527937769889832\n",
      "Iteration 45392 Loss: 1.208225131034851\n",
      "Iteration 45393 Loss: 1.0213804244995117\n",
      "Iteration 45394 Loss: 0.8025737404823303\n",
      "Iteration 45395 Loss: 0.7805337309837341\n",
      "Iteration 45396 Loss: 0.743615984916687\n",
      "Iteration 45397 Loss: 0.7994899749755859\n",
      "Iteration 45398 Loss: 1.1332945823669434\n",
      "Iteration 45399 Loss: 0.9461061954498291\n",
      "Iteration 45399 Loss: 0.9558796882629395\n",
      "Iteration 45400 Loss: 1.0058858394622803\n",
      "Iteration 45401 Loss: 0.9813497066497803\n",
      "Iteration 45402 Loss: 0.8212261199951172\n",
      "Iteration 45403 Loss: 1.0589673519134521\n",
      "Iteration 45404 Loss: 1.1669918298721313\n",
      "Iteration 45405 Loss: 0.9278765320777893\n",
      "Iteration 45406 Loss: 0.832635223865509\n",
      "Iteration 45407 Loss: 1.0209113359451294\n",
      "Iteration 45408 Loss: 0.990046501159668\n",
      "Iteration 45409 Loss: 0.9502259492874146\n",
      "Iteration 45409 Loss: 0.975611686706543\n",
      "Iteration 45410 Loss: 0.9198723435401917\n",
      "Iteration 45411 Loss: 1.0563669204711914\n",
      "Iteration 45412 Loss: 0.8186859488487244\n",
      "Iteration 45413 Loss: 0.8529247641563416\n",
      "Iteration 45414 Loss: 1.1862820386886597\n",
      "Iteration 45415 Loss: 1.1214505434036255\n",
      "Iteration 45416 Loss: 0.9805234670639038\n",
      "Iteration 45417 Loss: 1.3689453601837158\n",
      "Iteration 45418 Loss: 0.7135054469108582\n",
      "Iteration 45419 Loss: 0.6540094017982483\n",
      "Iteration 45419 Loss: 0.9672565460205078\n",
      "Iteration 45420 Loss: 1.3583364486694336\n",
      "Iteration 45421 Loss: 0.7459198236465454\n",
      "Iteration 45422 Loss: 0.9028980135917664\n",
      "Iteration 45423 Loss: 1.0287892818450928\n",
      "Iteration 45424 Loss: 0.8906198143959045\n",
      "Iteration 45425 Loss: 0.8148719668388367\n",
      "Iteration 45426 Loss: 1.0899351835250854\n",
      "Iteration 45427 Loss: 1.1764615774154663\n",
      "Iteration 45428 Loss: 0.9032349586486816\n",
      "Iteration 45429 Loss: 0.6689053177833557\n",
      "Iteration 45429 Loss: 0.9579971432685852\n",
      "Iteration 45430 Loss: 1.0661346912384033\n",
      "Iteration 45431 Loss: 0.576955258846283\n",
      "Iteration 45432 Loss: 0.9787640571594238\n",
      "Iteration 45433 Loss: 0.5101648569107056\n",
      "Iteration 45434 Loss: 1.1577290296554565\n",
      "Iteration 45435 Loss: 0.803562343120575\n",
      "Iteration 45436 Loss: 1.2501004934310913\n",
      "Iteration 45437 Loss: 0.9362472295761108\n",
      "Iteration 45438 Loss: 0.9059703350067139\n",
      "Iteration 45439 Loss: 1.2757335901260376\n",
      "Iteration 45439 Loss: 0.9461361765861511\n",
      "Iteration 45440 Loss: 0.8601250052452087\n",
      "Iteration 45441 Loss: 0.7159265875816345\n",
      "Iteration 45442 Loss: 0.9129427671432495\n",
      "Iteration 45443 Loss: 1.1192800998687744\n",
      "Iteration 45444 Loss: 1.13925039768219\n",
      "Iteration 45445 Loss: 1.0069133043289185\n",
      "Iteration 45446 Loss: 0.8459492325782776\n",
      "Iteration 45447 Loss: 0.8132681250572205\n",
      "Iteration 45448 Loss: 0.793651819229126\n",
      "Iteration 45449 Loss: 0.8003841638565063\n",
      "Iteration 45449 Loss: 0.9007691144943237\n",
      "Iteration 45450 Loss: 0.6263494491577148\n",
      "Iteration 45451 Loss: 0.8367115259170532\n",
      "Iteration 45452 Loss: 1.1007044315338135\n",
      "Iteration 45453 Loss: 0.9570629000663757\n",
      "Iteration 45454 Loss: 1.0066249370574951\n",
      "Iteration 45455 Loss: 0.8388929963111877\n",
      "Iteration 45456 Loss: 0.8786623477935791\n",
      "Iteration 45457 Loss: 0.780480682849884\n",
      "Iteration 45458 Loss: 0.9039276242256165\n",
      "Iteration 45459 Loss: 0.5192528963088989\n",
      "Iteration 45459 Loss: 0.844866931438446\n",
      "Iteration 45460 Loss: 0.5722419619560242\n",
      "Iteration 45461 Loss: 0.9637042880058289\n",
      "Iteration 45462 Loss: 0.6394128799438477\n",
      "Iteration 45463 Loss: 0.5832949876785278\n",
      "Iteration 45464 Loss: 0.8223157525062561\n",
      "Iteration 45465 Loss: 1.0708296298980713\n",
      "Iteration 45466 Loss: 1.0045198202133179\n",
      "Iteration 45467 Loss: 1.105429768562317\n",
      "Iteration 45468 Loss: 1.0958693027496338\n",
      "Iteration 45469 Loss: 0.9197415113449097\n",
      "Iteration 45469 Loss: 0.87773597240448\n",
      "Iteration 45470 Loss: 0.7135054469108582\n",
      "Iteration 45471 Loss: 0.6912652850151062\n",
      "Iteration 45472 Loss: 1.0447386503219604\n",
      "Iteration 45473 Loss: 1.03611421585083\n",
      "Iteration 45474 Loss: 0.9326863884925842\n",
      "Iteration 45475 Loss: 0.9517843723297119\n",
      "Iteration 45476 Loss: 1.1251654624938965\n",
      "Iteration 45477 Loss: 0.7484869956970215\n",
      "Iteration 45478 Loss: 0.9422688484191895\n",
      "Iteration 45479 Loss: 0.9446192979812622\n",
      "Iteration 45479 Loss: 0.913063645362854\n",
      "Iteration 45480 Loss: 0.9036365151405334\n",
      "Iteration 45481 Loss: 0.9647404551506042\n",
      "Iteration 45482 Loss: 0.8800575137138367\n",
      "Iteration 45483 Loss: 0.8234463334083557\n",
      "Iteration 45484 Loss: 1.045889973640442\n",
      "Iteration 45485 Loss: 0.9451310038566589\n",
      "Iteration 45486 Loss: 0.785905659198761\n",
      "Iteration 45487 Loss: 1.1322370767593384\n",
      "Iteration 45488 Loss: 1.512137532234192\n",
      "Iteration 45489 Loss: 0.7469238042831421\n",
      "Iteration 45489 Loss: 0.9740105867385864\n",
      "Iteration 45490 Loss: 0.9233055114746094\n",
      "Iteration 45491 Loss: 0.8381765484809875\n",
      "Iteration 45492 Loss: 1.0756126642227173\n",
      "Iteration 45493 Loss: 0.8724556565284729\n",
      "Iteration 45494 Loss: 0.7823446393013\n",
      "Iteration 45495 Loss: 1.0204908847808838\n",
      "Iteration 45496 Loss: 0.7876684665679932\n",
      "Iteration 45497 Loss: 0.9546874165534973\n",
      "Iteration 45498 Loss: 0.7426773905754089\n",
      "Iteration 45499 Loss: 1.0489715337753296\n",
      "Iteration 45499 Loss: 0.9046390652656555\n",
      "Iteration 45500 Loss: 1.0626952648162842\n",
      "Iteration 45501 Loss: 0.9639906287193298\n",
      "Iteration 45502 Loss: 1.0282493829727173\n",
      "Iteration 45503 Loss: 0.6409538388252258\n",
      "Iteration 45504 Loss: 1.0259084701538086\n",
      "Iteration 45505 Loss: 1.0043622255325317\n",
      "Iteration 45506 Loss: 0.8797023296356201\n",
      "Iteration 45507 Loss: 1.0104310512542725\n",
      "Iteration 45508 Loss: 0.7219492197036743\n",
      "Iteration 45509 Loss: 1.2971552610397339\n",
      "Iteration 45509 Loss: 0.9635397791862488\n",
      "Iteration 45510 Loss: 0.739185631275177\n",
      "Iteration 45511 Loss: 0.7665953040122986\n",
      "Iteration 45512 Loss: 0.7851979732513428\n",
      "Iteration 45513 Loss: 1.190518856048584\n",
      "Iteration 45514 Loss: 0.6532238125801086\n",
      "Iteration 45515 Loss: 0.9314736723899841\n",
      "Iteration 45516 Loss: 1.3030134439468384\n",
      "Iteration 45517 Loss: 0.7077088356018066\n",
      "Iteration 45518 Loss: 1.0313773155212402\n",
      "Iteration 45519 Loss: 0.697776734828949\n",
      "Iteration 45519 Loss: 0.8806072473526001\n",
      "Iteration 45520 Loss: 1.0809396505355835\n",
      "Iteration 45521 Loss: 0.8746030926704407\n",
      "Iteration 45522 Loss: 0.7697688937187195\n",
      "Iteration 45523 Loss: 1.0278481245040894\n",
      "Iteration 45524 Loss: 1.0496160984039307\n",
      "Iteration 45525 Loss: 1.0189744234085083\n",
      "Iteration 45526 Loss: 1.114561915397644\n",
      "Iteration 45527 Loss: 1.026239037513733\n",
      "Iteration 45528 Loss: 0.7947932481765747\n",
      "Iteration 45529 Loss: 1.2211390733718872\n",
      "Iteration 45529 Loss: 0.997848391532898\n",
      "Iteration 45530 Loss: 0.8124879598617554\n",
      "Iteration 45531 Loss: 0.929258406162262\n",
      "Iteration 45532 Loss: 0.8678361177444458\n",
      "Iteration 45533 Loss: 1.0039445161819458\n",
      "Iteration 45534 Loss: 0.5728046894073486\n",
      "Iteration 45535 Loss: 0.8310521245002747\n",
      "Iteration 45536 Loss: 0.8143409490585327\n",
      "Iteration 45537 Loss: 0.8247605562210083\n",
      "Iteration 45538 Loss: 0.9010246396064758\n",
      "Iteration 45539 Loss: 0.840273916721344\n",
      "Iteration 45539 Loss: 0.8397784233093262\n",
      "Iteration 45540 Loss: 0.8311213254928589\n",
      "Iteration 45541 Loss: 0.594187319278717\n",
      "Iteration 45542 Loss: 0.8391513228416443\n",
      "Iteration 45543 Loss: 0.793036937713623\n",
      "Iteration 45544 Loss: 0.7030368447303772\n",
      "Iteration 45545 Loss: 0.8721127510070801\n",
      "Iteration 45546 Loss: 0.6805546879768372\n",
      "Iteration 45547 Loss: 0.9665718674659729\n",
      "Iteration 45548 Loss: 0.8238409161567688\n",
      "Iteration 45549 Loss: 0.9935857057571411\n",
      "Iteration 45549 Loss: 0.8097200393676758\n",
      "Iteration 45550 Loss: 1.2359598875045776\n",
      "Iteration 45551 Loss: 0.9229021668434143\n",
      "Iteration 45552 Loss: 0.9113502502441406\n",
      "Iteration 45553 Loss: 0.44220590591430664\n",
      "Iteration 45554 Loss: 0.8069093227386475\n",
      "Iteration 45555 Loss: 0.7849990129470825\n",
      "Iteration 45556 Loss: 0.6767823100090027\n",
      "Iteration 45557 Loss: 1.140198826789856\n",
      "Iteration 45558 Loss: 1.0038529634475708\n",
      "Iteration 45559 Loss: 1.124113917350769\n",
      "Iteration 45559 Loss: 0.9049274325370789\n",
      "Iteration 45560 Loss: 1.2031290531158447\n",
      "Iteration 45561 Loss: 1.2176152467727661\n",
      "Iteration 45562 Loss: 0.947434663772583\n",
      "Iteration 45563 Loss: 1.1013578176498413\n",
      "Iteration 45564 Loss: 1.1046375036239624\n",
      "Iteration 45565 Loss: 0.9007216095924377\n",
      "Iteration 45566 Loss: 1.0878095626831055\n",
      "Iteration 45567 Loss: 0.8665158748626709\n",
      "Iteration 45568 Loss: 0.8927647471427917\n",
      "Iteration 45569 Loss: 0.9103792309761047\n",
      "Iteration 45569 Loss: 1.0232365131378174\n",
      "Iteration 45570 Loss: 0.824622631072998\n",
      "Iteration 45571 Loss: 0.8115229606628418\n",
      "Iteration 45572 Loss: 1.0845855474472046\n",
      "Iteration 45573 Loss: 1.0343726873397827\n",
      "Iteration 45574 Loss: 1.0499910116195679\n",
      "Iteration 45575 Loss: 1.2142415046691895\n",
      "Iteration 45576 Loss: 1.1898142099380493\n",
      "Iteration 45577 Loss: 1.152402639389038\n",
      "Iteration 45578 Loss: 1.0714592933654785\n",
      "Iteration 45579 Loss: 0.6550973057746887\n",
      "Iteration 45579 Loss: 1.0088109970092773\n",
      "Iteration 45580 Loss: 0.9338045120239258\n",
      "Iteration 45581 Loss: 1.164103627204895\n",
      "Iteration 45582 Loss: 0.8810356259346008\n",
      "Iteration 45583 Loss: 0.8991107940673828\n",
      "Iteration 45584 Loss: 0.8316303491592407\n",
      "Iteration 45585 Loss: 0.9378297328948975\n",
      "Iteration 45586 Loss: 0.8884832859039307\n",
      "Iteration 45587 Loss: 1.0370285511016846\n",
      "Iteration 45588 Loss: 0.9349684119224548\n",
      "Iteration 45589 Loss: 1.0254809856414795\n",
      "Iteration 45589 Loss: 0.9533475637435913\n",
      "Iteration 45590 Loss: 1.1347585916519165\n",
      "Iteration 45591 Loss: 0.8848778009414673\n",
      "Iteration 45592 Loss: 0.9168456792831421\n",
      "Iteration 45593 Loss: 0.8942805528640747\n",
      "Iteration 45594 Loss: 0.7163347601890564\n",
      "Iteration 45595 Loss: 0.6455012559890747\n",
      "Iteration 45596 Loss: 1.0794209241867065\n",
      "Iteration 45597 Loss: 0.9698488116264343\n",
      "Iteration 45598 Loss: 0.9968660473823547\n",
      "Iteration 45599 Loss: 0.8130601048469543\n",
      "Iteration 45599 Loss: 0.9051793813705444\n",
      "Iteration 45600 Loss: 1.1157647371292114\n",
      "Iteration 45601 Loss: 0.8012491464614868\n",
      "Iteration 45602 Loss: 1.2148149013519287\n",
      "Iteration 45603 Loss: 0.9716618061065674\n",
      "Iteration 45604 Loss: 0.818468451499939\n",
      "Iteration 45605 Loss: 1.0707383155822754\n",
      "Iteration 45606 Loss: 1.2300703525543213\n",
      "Iteration 45607 Loss: 1.3185359239578247\n",
      "Iteration 45608 Loss: 0.5967907905578613\n",
      "Iteration 45609 Loss: 1.1558921337127686\n",
      "Iteration 45609 Loss: 1.0293986797332764\n",
      "Iteration 45610 Loss: 0.8967287540435791\n",
      "Iteration 45611 Loss: 0.8507841229438782\n",
      "Iteration 45612 Loss: 0.9201237559318542\n",
      "Iteration 45613 Loss: 0.8659966588020325\n",
      "Iteration 45614 Loss: 0.7704509496688843\n",
      "Iteration 45615 Loss: 0.777277946472168\n",
      "Iteration 45616 Loss: 0.8745915293693542\n",
      "Iteration 45617 Loss: 1.0108213424682617\n",
      "Iteration 45618 Loss: 0.9074638485908508\n",
      "Iteration 45619 Loss: 0.6551505327224731\n",
      "Iteration 45619 Loss: 0.8529389500617981\n",
      "Iteration 45620 Loss: 0.7827281355857849\n",
      "Iteration 45621 Loss: 1.1805331707000732\n",
      "Iteration 45622 Loss: 1.1557986736297607\n",
      "Iteration 45623 Loss: 1.100454330444336\n",
      "Iteration 45624 Loss: 0.7901280522346497\n",
      "Iteration 45625 Loss: 1.0150970220565796\n",
      "Iteration 45626 Loss: 0.9653428792953491\n",
      "Iteration 45627 Loss: 1.0482807159423828\n",
      "Iteration 45628 Loss: 0.6086868047714233\n",
      "Iteration 45629 Loss: 0.8207731246948242\n",
      "Iteration 45629 Loss: 0.9467822909355164\n",
      "Iteration 45630 Loss: 0.7146033644676208\n",
      "Iteration 45631 Loss: 1.1678904294967651\n",
      "Iteration 45632 Loss: 1.0833871364593506\n",
      "Iteration 45633 Loss: 0.818954586982727\n",
      "Iteration 45634 Loss: 0.8077310919761658\n",
      "Iteration 45635 Loss: 0.8676963448524475\n",
      "Iteration 45636 Loss: 0.8127909898757935\n",
      "Iteration 45637 Loss: 0.8015481233596802\n",
      "Iteration 45638 Loss: 0.8920890092849731\n",
      "Iteration 45639 Loss: 0.8235732913017273\n",
      "Iteration 45639 Loss: 0.8790264129638672\n",
      "Iteration 45640 Loss: 1.0449211597442627\n",
      "Iteration 45641 Loss: 0.8060603141784668\n",
      "Iteration 45642 Loss: 1.003548502922058\n",
      "Iteration 45643 Loss: 1.0406972169876099\n",
      "Iteration 45644 Loss: 0.8535114526748657\n",
      "Iteration 45645 Loss: 0.7350282073020935\n",
      "Iteration 45646 Loss: 0.8805136680603027\n",
      "Iteration 45647 Loss: 0.6986402273178101\n",
      "Iteration 45648 Loss: 1.271746277809143\n",
      "Iteration 45649 Loss: 0.771466851234436\n",
      "Iteration 45649 Loss: 0.9106133580207825\n",
      "Iteration 45650 Loss: 1.0573232173919678\n",
      "Iteration 45651 Loss: 0.9794944524765015\n",
      "Iteration 45652 Loss: 0.9194154143333435\n",
      "Iteration 45653 Loss: 0.939372718334198\n",
      "Iteration 45654 Loss: 0.8514490127563477\n",
      "Iteration 45655 Loss: 0.9661616683006287\n",
      "Iteration 45656 Loss: 0.9841005206108093\n",
      "Iteration 45657 Loss: 0.7795303463935852\n",
      "Iteration 45658 Loss: 0.7343481779098511\n",
      "Iteration 45659 Loss: 0.9426472783088684\n",
      "Iteration 45659 Loss: 0.9153842926025391\n",
      "Iteration 45660 Loss: 0.8488707542419434\n",
      "Iteration 45661 Loss: 0.9538021683692932\n",
      "Iteration 45662 Loss: 0.7402200698852539\n",
      "Iteration 45663 Loss: 1.2068092823028564\n",
      "Iteration 45664 Loss: 0.8701667189598083\n",
      "Iteration 45665 Loss: 0.8342251181602478\n",
      "Iteration 45666 Loss: 0.9674402475357056\n",
      "Iteration 45667 Loss: 0.7096154093742371\n",
      "Iteration 45668 Loss: 1.1204777956008911\n",
      "Iteration 45669 Loss: 1.2137949466705322\n",
      "Iteration 45669 Loss: 0.9465422630310059\n",
      "Iteration 45670 Loss: 0.9577853679656982\n",
      "Iteration 45671 Loss: 1.0155891180038452\n",
      "Iteration 45672 Loss: 0.9344601631164551\n",
      "Iteration 45673 Loss: 0.864332914352417\n",
      "Iteration 45674 Loss: 0.994270384311676\n",
      "Iteration 45675 Loss: 0.9953829646110535\n",
      "Iteration 45676 Loss: 0.8390823006629944\n",
      "Iteration 45677 Loss: 0.913661539554596\n",
      "Iteration 45678 Loss: 0.9650916457176208\n",
      "Iteration 45679 Loss: 0.9686831831932068\n",
      "Iteration 45679 Loss: 0.9448339343070984\n",
      "Iteration 45680 Loss: 0.9649633765220642\n",
      "Iteration 45681 Loss: 0.8157918453216553\n",
      "Iteration 45682 Loss: 0.9359079599380493\n",
      "Iteration 45683 Loss: 0.688225507736206\n",
      "Iteration 45684 Loss: 0.8851599097251892\n",
      "Iteration 45685 Loss: 0.7742210030555725\n",
      "Iteration 45686 Loss: 0.8504738807678223\n",
      "Iteration 45687 Loss: 1.1986433267593384\n",
      "Iteration 45688 Loss: 0.6219875812530518\n",
      "Iteration 45689 Loss: 0.9187332391738892\n",
      "Iteration 45689 Loss: 0.8654106855392456\n",
      "Iteration 45690 Loss: 1.6191741228103638\n",
      "Iteration 45691 Loss: 0.9573931097984314\n",
      "Iteration 45692 Loss: 1.0500868558883667\n",
      "Iteration 45693 Loss: 0.8683426976203918\n",
      "Iteration 45694 Loss: 0.8351026177406311\n",
      "Iteration 45695 Loss: 0.8797093033790588\n",
      "Iteration 45696 Loss: 0.9372086524963379\n",
      "Iteration 45697 Loss: 0.9821106791496277\n",
      "Iteration 45698 Loss: 0.9715633392333984\n",
      "Iteration 45699 Loss: 1.51363205909729\n",
      "Iteration 45699 Loss: 1.0614324808120728\n",
      "Iteration 45700 Loss: 0.8600276708602905\n",
      "Iteration 45701 Loss: 0.9712513089179993\n",
      "Iteration 45702 Loss: 0.9341964721679688\n",
      "Iteration 45703 Loss: 1.1118775606155396\n",
      "Iteration 45704 Loss: 0.981840968132019\n",
      "Iteration 45705 Loss: 1.2316244840621948\n",
      "Iteration 45706 Loss: 0.9104258418083191\n",
      "Iteration 45707 Loss: 0.923098623752594\n",
      "Iteration 45708 Loss: 0.798900306224823\n",
      "Iteration 45709 Loss: 0.791619598865509\n",
      "Iteration 45709 Loss: 0.9514862895011902\n",
      "Iteration 45710 Loss: 1.084856629371643\n",
      "Iteration 45711 Loss: 1.4510074853897095\n",
      "Iteration 45712 Loss: 1.3314073085784912\n",
      "Iteration 45713 Loss: 1.0059071779251099\n",
      "Iteration 45714 Loss: 1.0971859693527222\n",
      "Iteration 45715 Loss: 0.8537019491195679\n",
      "Iteration 45716 Loss: 0.7567309141159058\n",
      "Iteration 45717 Loss: 0.9719738960266113\n",
      "Iteration 45718 Loss: 1.2919974327087402\n",
      "Iteration 45719 Loss: 0.8424572348594666\n",
      "Iteration 45719 Loss: 1.0687224864959717\n",
      "Iteration 45720 Loss: 1.0437227487564087\n",
      "Iteration 45721 Loss: 0.9649518132209778\n",
      "Iteration 45722 Loss: 0.9381270408630371\n",
      "Iteration 45723 Loss: 0.7265215516090393\n",
      "Iteration 45724 Loss: 0.7726744413375854\n",
      "Iteration 45725 Loss: 0.8822960257530212\n",
      "Iteration 45726 Loss: 1.0645850896835327\n",
      "Iteration 45727 Loss: 1.035184383392334\n",
      "Iteration 45728 Loss: 0.820497453212738\n",
      "Iteration 45729 Loss: 0.9533789157867432\n",
      "Iteration 45729 Loss: 0.9201940298080444\n",
      "Iteration 45730 Loss: 1.203818440437317\n",
      "Iteration 45731 Loss: 1.3083103895187378\n",
      "Iteration 45732 Loss: 1.1055591106414795\n",
      "Iteration 45733 Loss: 0.7305138111114502\n",
      "Iteration 45734 Loss: 1.034927487373352\n",
      "Iteration 45735 Loss: 0.8399290442466736\n",
      "Iteration 45736 Loss: 0.545340359210968\n",
      "Iteration 45737 Loss: 0.5372081995010376\n",
      "Iteration 45738 Loss: 0.7441736459732056\n",
      "Iteration 45739 Loss: 1.0888527631759644\n",
      "Iteration 45739 Loss: 0.9138633012771606\n",
      "Iteration 45740 Loss: 1.1042574644088745\n",
      "Iteration 45741 Loss: 0.8874899744987488\n",
      "Iteration 45742 Loss: 1.0577712059020996\n",
      "Iteration 45743 Loss: 0.7948623299598694\n",
      "Iteration 45744 Loss: 0.8836801052093506\n",
      "Iteration 45745 Loss: 1.168122410774231\n",
      "Iteration 45746 Loss: 1.3292328119277954\n",
      "Iteration 45747 Loss: 1.1000957489013672\n",
      "Iteration 45748 Loss: 0.7725522518157959\n",
      "Iteration 45749 Loss: 0.8902129530906677\n",
      "Iteration 45749 Loss: 0.9988277554512024\n",
      "Iteration 45750 Loss: 0.7383121848106384\n",
      "Iteration 45751 Loss: 0.9702909588813782\n",
      "Iteration 45752 Loss: 0.9519786834716797\n",
      "Iteration 45753 Loss: 0.8766750693321228\n",
      "Iteration 45754 Loss: 1.274312138557434\n",
      "Iteration 45755 Loss: 0.8841380476951599\n",
      "Iteration 45756 Loss: 1.1175729036331177\n",
      "Iteration 45757 Loss: 0.890183687210083\n",
      "Iteration 45758 Loss: 1.0164185762405396\n",
      "Iteration 45759 Loss: 1.065033197402954\n",
      "Iteration 45759 Loss: 0.9784916043281555\n",
      "Iteration 45760 Loss: 0.8424800634384155\n",
      "Iteration 45761 Loss: 0.9254805445671082\n",
      "Iteration 45762 Loss: 0.9739962220191956\n",
      "Iteration 45763 Loss: 0.8908923268318176\n",
      "Iteration 45764 Loss: 0.8536937832832336\n",
      "Iteration 45765 Loss: 1.100790023803711\n",
      "Iteration 45766 Loss: 0.6832193732261658\n",
      "Iteration 45767 Loss: 0.9163781404495239\n",
      "Iteration 45768 Loss: 0.7916059494018555\n",
      "Iteration 45769 Loss: 1.177003264427185\n",
      "Iteration 45769 Loss: 0.9155539274215698\n",
      "Iteration 45770 Loss: 0.9266192317008972\n",
      "Iteration 45771 Loss: 1.1663048267364502\n",
      "Iteration 45772 Loss: 0.9416040182113647\n",
      "Iteration 45773 Loss: 1.1868712902069092\n",
      "Iteration 45774 Loss: 1.0924677848815918\n",
      "Iteration 45775 Loss: 0.7510731816291809\n",
      "Iteration 45776 Loss: 1.2061405181884766\n",
      "Iteration 45777 Loss: 0.8446480631828308\n",
      "Iteration 45778 Loss: 0.8442143201828003\n",
      "Iteration 45779 Loss: 0.8986749053001404\n",
      "Iteration 45779 Loss: 0.9858618974685669\n",
      "Iteration 45780 Loss: 1.1156013011932373\n",
      "Iteration 45781 Loss: 0.841131329536438\n",
      "Iteration 45782 Loss: 0.9253900647163391\n",
      "Iteration 45783 Loss: 0.8716385960578918\n",
      "Iteration 45784 Loss: 0.9906383752822876\n",
      "Iteration 45785 Loss: 0.9177485704421997\n",
      "Iteration 45786 Loss: 0.7433913350105286\n",
      "Iteration 45787 Loss: 0.8985941410064697\n",
      "Iteration 45788 Loss: 0.9840887784957886\n",
      "Iteration 45789 Loss: 0.8830112218856812\n",
      "Iteration 45789 Loss: 0.9171233177185059\n",
      "Iteration 45790 Loss: 1.05211341381073\n",
      "Iteration 45791 Loss: 0.7781336903572083\n",
      "Iteration 45792 Loss: 1.038226842880249\n",
      "Iteration 45793 Loss: 0.945476770401001\n",
      "Iteration 45794 Loss: 0.8688321113586426\n",
      "Iteration 45795 Loss: 1.3199572563171387\n",
      "Iteration 45796 Loss: 1.2021528482437134\n",
      "Iteration 45797 Loss: 1.046572208404541\n",
      "Iteration 45798 Loss: 0.8111733198165894\n",
      "Iteration 45799 Loss: 0.9093149304389954\n",
      "Iteration 45799 Loss: 0.9971954226493835\n",
      "Iteration 45800 Loss: 0.9141145348548889\n",
      "Iteration 45801 Loss: 0.9481297731399536\n",
      "Iteration 45802 Loss: 1.026995301246643\n",
      "Iteration 45803 Loss: 1.1727172136306763\n",
      "Iteration 45804 Loss: 0.9801191687583923\n",
      "Iteration 45805 Loss: 0.8607816696166992\n",
      "Iteration 45806 Loss: 0.9748550653457642\n",
      "Iteration 45807 Loss: 1.1327931880950928\n",
      "Iteration 45808 Loss: 0.7268768548965454\n",
      "Iteration 45809 Loss: 1.2918797731399536\n",
      "Iteration 45809 Loss: 1.002926230430603\n",
      "Iteration 45810 Loss: 0.8778138756752014\n",
      "Iteration 45811 Loss: 1.1963659524917603\n",
      "Iteration 45812 Loss: 1.2375775575637817\n",
      "Iteration 45813 Loss: 1.133435606956482\n",
      "Iteration 45814 Loss: 1.0708975791931152\n",
      "Iteration 45815 Loss: 0.8167028427124023\n",
      "Iteration 45816 Loss: 1.2449933290481567\n",
      "Iteration 45817 Loss: 0.9608603119850159\n",
      "Iteration 45818 Loss: 0.7448444366455078\n",
      "Iteration 45819 Loss: 0.5949968695640564\n",
      "Iteration 45819 Loss: 0.9878488779067993\n",
      "Iteration 45820 Loss: 0.9885967969894409\n",
      "Iteration 45821 Loss: 0.9749323129653931\n",
      "Iteration 45822 Loss: 0.708276093006134\n",
      "Iteration 45823 Loss: 0.9101449847221375\n",
      "Iteration 45824 Loss: 0.8686344027519226\n",
      "Iteration 45825 Loss: 0.8883930444717407\n",
      "Iteration 45826 Loss: 1.6087039709091187\n",
      "Iteration 45827 Loss: 1.0385243892669678\n",
      "Iteration 45828 Loss: 0.8556028008460999\n",
      "Iteration 45829 Loss: 0.8999449610710144\n",
      "Iteration 45829 Loss: 0.9741753339767456\n",
      "Iteration 45830 Loss: 1.085422396659851\n",
      "Iteration 45831 Loss: 1.0511374473571777\n",
      "Iteration 45832 Loss: 0.9600924849510193\n",
      "Iteration 45833 Loss: 0.8888670802116394\n",
      "Iteration 45834 Loss: 1.006593108177185\n",
      "Iteration 45835 Loss: 1.0016056299209595\n",
      "Iteration 45836 Loss: 0.730445921421051\n",
      "Iteration 45837 Loss: 0.8951501846313477\n",
      "Iteration 45838 Loss: 1.0069897174835205\n",
      "Iteration 45839 Loss: 1.1204864978790283\n",
      "Iteration 45839 Loss: 0.9746791124343872\n",
      "Iteration 45840 Loss: 0.9722724556922913\n",
      "Iteration 45841 Loss: 0.6840053796768188\n",
      "Iteration 45842 Loss: 0.6281097531318665\n",
      "Iteration 45843 Loss: 0.9491912126541138\n",
      "Iteration 45844 Loss: 0.9794711470603943\n",
      "Iteration 45845 Loss: 0.8498143553733826\n",
      "Iteration 45846 Loss: 0.6864643096923828\n",
      "Iteration 45847 Loss: 0.9105693101882935\n",
      "Iteration 45848 Loss: 0.6651904582977295\n",
      "Iteration 45849 Loss: 1.1411439180374146\n",
      "Iteration 45849 Loss: 0.8466232419013977\n",
      "Iteration 45850 Loss: 0.9279115200042725\n",
      "Iteration 45851 Loss: 1.1969890594482422\n",
      "Iteration 45852 Loss: 1.1349902153015137\n",
      "Iteration 45853 Loss: 1.0486732721328735\n",
      "Iteration 45854 Loss: 0.8472468852996826\n",
      "Iteration 45855 Loss: 1.0704618692398071\n",
      "Iteration 45856 Loss: 1.2232582569122314\n",
      "Iteration 45857 Loss: 0.8667575120925903\n",
      "Iteration 45858 Loss: 0.6856416463851929\n",
      "Iteration 45859 Loss: 0.9431182146072388\n",
      "Iteration 45859 Loss: 0.9945048093795776\n",
      "Iteration 45860 Loss: 0.9410717487335205\n",
      "Iteration 45861 Loss: 1.136751651763916\n",
      "Iteration 45862 Loss: 1.0463083982467651\n",
      "Iteration 45863 Loss: 1.05630362033844\n",
      "Iteration 45864 Loss: 0.858302652835846\n",
      "Iteration 45865 Loss: 0.7213510870933533\n",
      "Iteration 45866 Loss: 1.074520230293274\n",
      "Iteration 45867 Loss: 0.9984387159347534\n",
      "Iteration 45868 Loss: 1.098482608795166\n",
      "Iteration 45869 Loss: 1.0462217330932617\n",
      "Iteration 45869 Loss: 0.9977752566337585\n",
      "Iteration 45870 Loss: 0.6596863865852356\n",
      "Iteration 45871 Loss: 0.9191579818725586\n",
      "Iteration 45872 Loss: 0.9379733204841614\n",
      "Iteration 45873 Loss: 0.9971386194229126\n",
      "Iteration 45874 Loss: 0.8474525809288025\n",
      "Iteration 45875 Loss: 0.8422799706459045\n",
      "Iteration 45876 Loss: 0.9073407649993896\n",
      "Iteration 45877 Loss: 1.0044606924057007\n",
      "Iteration 45878 Loss: 0.8207729458808899\n",
      "Iteration 45879 Loss: 0.8878408074378967\n",
      "Iteration 45879 Loss: 0.8824103474617004\n",
      "Iteration 45880 Loss: 0.9621913433074951\n",
      "Iteration 45881 Loss: 0.8971297740936279\n",
      "Iteration 45882 Loss: 1.404410481452942\n",
      "Iteration 45883 Loss: 1.2446630001068115\n",
      "Iteration 45884 Loss: 0.8485198020935059\n",
      "Iteration 45885 Loss: 0.9472153186798096\n",
      "Iteration 45886 Loss: 0.721560001373291\n",
      "Iteration 45887 Loss: 1.2219483852386475\n",
      "Iteration 45888 Loss: 1.0464473962783813\n",
      "Iteration 45889 Loss: 1.0281453132629395\n",
      "Iteration 45889 Loss: 1.032223105430603\n",
      "Iteration 45890 Loss: 1.1066155433654785\n",
      "Iteration 45891 Loss: 1.0263514518737793\n",
      "Iteration 45892 Loss: 0.9856124520301819\n",
      "Iteration 45893 Loss: 0.7755054831504822\n",
      "Iteration 45894 Loss: 0.9576239585876465\n",
      "Iteration 45895 Loss: 0.9330652952194214\n",
      "Iteration 45896 Loss: 0.7268741130828857\n",
      "Iteration 45897 Loss: 0.902748703956604\n",
      "Iteration 45898 Loss: 1.0086826086044312\n",
      "Iteration 45899 Loss: 0.9630534052848816\n",
      "Iteration 45899 Loss: 0.9386132955551147\n",
      "Iteration 45900 Loss: 0.5957849621772766\n",
      "Iteration 45901 Loss: 1.0039373636245728\n",
      "Iteration 45902 Loss: 0.9829089045524597\n",
      "Iteration 45903 Loss: 0.7763238549232483\n",
      "Iteration 45904 Loss: 1.3664436340332031\n",
      "Iteration 45905 Loss: 0.8103039264678955\n",
      "Iteration 45906 Loss: 0.9842731952667236\n",
      "Iteration 45907 Loss: 1.2407982349395752\n",
      "Iteration 45908 Loss: 1.1740312576293945\n",
      "Iteration 45909 Loss: 0.7431305050849915\n",
      "Iteration 45909 Loss: 0.9677934646606445\n",
      "Iteration 45910 Loss: 0.7239567637443542\n",
      "Iteration 45911 Loss: 1.2337298393249512\n",
      "Iteration 45912 Loss: 1.0676125288009644\n",
      "Iteration 45913 Loss: 0.897625207901001\n",
      "Iteration 45914 Loss: 1.1336556673049927\n",
      "Iteration 45915 Loss: 1.0888384580612183\n",
      "Iteration 45916 Loss: 0.8827253580093384\n",
      "Iteration 45917 Loss: 0.9772446155548096\n",
      "Iteration 45918 Loss: 0.8967538475990295\n",
      "Iteration 45919 Loss: 0.8766630291938782\n",
      "Iteration 45919 Loss: 0.977880597114563\n",
      "Iteration 45920 Loss: 0.9251695871353149\n",
      "Iteration 45921 Loss: 0.7005787491798401\n",
      "Iteration 45922 Loss: 1.146963357925415\n",
      "Iteration 45923 Loss: 1.1607604026794434\n",
      "Iteration 45924 Loss: 1.2479116916656494\n",
      "Iteration 45925 Loss: 0.9152254462242126\n",
      "Iteration 45926 Loss: 1.0883808135986328\n",
      "Iteration 45927 Loss: 0.8657518625259399\n",
      "Iteration 45928 Loss: 0.8951862454414368\n",
      "Iteration 45929 Loss: 0.8455657958984375\n",
      "Iteration 45929 Loss: 0.9791495203971863\n",
      "Iteration 45930 Loss: 0.7896543741226196\n",
      "Iteration 45931 Loss: 1.2142870426177979\n",
      "Iteration 45932 Loss: 0.8773797154426575\n",
      "Iteration 45933 Loss: 1.055272102355957\n",
      "Iteration 45934 Loss: 0.759931206703186\n",
      "Iteration 45935 Loss: 0.9608569741249084\n",
      "Iteration 45936 Loss: 0.9243296384811401\n",
      "Iteration 45937 Loss: 0.5770964622497559\n",
      "Iteration 45938 Loss: 1.0399103164672852\n",
      "Iteration 45939 Loss: 0.728209376335144\n",
      "Iteration 45939 Loss: 0.8926927447319031\n",
      "Iteration 45940 Loss: 1.0972570180892944\n",
      "Iteration 45941 Loss: 0.8924311399459839\n",
      "Iteration 45942 Loss: 0.977925717830658\n",
      "Iteration 45943 Loss: 0.9644930958747864\n",
      "Iteration 45944 Loss: 1.1133447885513306\n",
      "Iteration 45945 Loss: 1.4199953079223633\n",
      "Iteration 45946 Loss: 0.7779495120048523\n",
      "Iteration 45947 Loss: 0.6815428733825684\n",
      "Iteration 45948 Loss: 1.0699294805526733\n",
      "Iteration 45949 Loss: 1.044032096862793\n",
      "Iteration 45949 Loss: 1.003890037536621\n",
      "Iteration 45950 Loss: 0.7498780488967896\n",
      "Iteration 45951 Loss: 1.0088577270507812\n",
      "Iteration 45952 Loss: 0.6934599280357361\n",
      "Iteration 45953 Loss: 1.0114613771438599\n",
      "Iteration 45954 Loss: 0.6405895948410034\n",
      "Iteration 45955 Loss: 1.162713885307312\n",
      "Iteration 45956 Loss: 0.7942752242088318\n",
      "Iteration 45957 Loss: 1.0710525512695312\n",
      "Iteration 45958 Loss: 1.1290613412857056\n",
      "Iteration 45959 Loss: 0.8885465860366821\n",
      "Iteration 45959 Loss: 0.9149896502494812\n",
      "Iteration 45960 Loss: 1.2463915348052979\n",
      "Iteration 45961 Loss: 1.0197056531906128\n",
      "Iteration 45962 Loss: 0.9387044310569763\n",
      "Iteration 45963 Loss: 0.9208151698112488\n",
      "Iteration 45964 Loss: 1.1412943601608276\n",
      "Iteration 45965 Loss: 1.2645529508590698\n",
      "Iteration 45966 Loss: 1.2253309488296509\n",
      "Iteration 45967 Loss: 1.0623432397842407\n",
      "Iteration 45968 Loss: 1.1580252647399902\n",
      "Iteration 45969 Loss: 0.9142891764640808\n",
      "Iteration 45969 Loss: 1.0891454219818115\n",
      "Iteration 45970 Loss: 0.7736955881118774\n",
      "Iteration 45971 Loss: 0.6810193657875061\n",
      "Iteration 45972 Loss: 0.5849500894546509\n",
      "Iteration 45973 Loss: 0.5499857664108276\n",
      "Iteration 45974 Loss: 0.8266790509223938\n",
      "Iteration 45975 Loss: 0.97380530834198\n",
      "Iteration 45976 Loss: 0.6816539168357849\n",
      "Iteration 45977 Loss: 0.8576757907867432\n",
      "Iteration 45978 Loss: 1.2482537031173706\n",
      "Iteration 45979 Loss: 0.8023844957351685\n",
      "Iteration 45979 Loss: 0.7980103492736816\n",
      "Iteration 45980 Loss: 0.9895569086074829\n",
      "Iteration 45981 Loss: 0.9643917083740234\n",
      "Iteration 45982 Loss: 1.1783363819122314\n",
      "Iteration 45983 Loss: 0.8090371489524841\n",
      "Iteration 45984 Loss: 0.8148541450500488\n",
      "Iteration 45985 Loss: 0.5062994956970215\n",
      "Iteration 45986 Loss: 1.459175705909729\n",
      "Iteration 45987 Loss: 0.8947758674621582\n",
      "Iteration 45988 Loss: 0.9187662601470947\n",
      "Iteration 45989 Loss: 1.1602492332458496\n",
      "Iteration 45989 Loss: 0.9695444107055664\n",
      "Iteration 45990 Loss: 0.9148726463317871\n",
      "Iteration 45991 Loss: 0.7229185700416565\n",
      "Iteration 45992 Loss: 1.0280495882034302\n",
      "Iteration 45993 Loss: 0.7755564451217651\n",
      "Iteration 45994 Loss: 0.7897623777389526\n",
      "Iteration 45995 Loss: 1.0313318967819214\n",
      "Iteration 45996 Loss: 0.7278281450271606\n",
      "Iteration 45997 Loss: 1.1015777587890625\n",
      "Iteration 45998 Loss: 0.6859959959983826\n",
      "Iteration 45999 Loss: 0.7207640409469604\n",
      "Iteration 45999 Loss: 0.849865734577179\n",
      "Iteration 46000 Loss: 0.8608627319335938\n",
      "Iteration 46001 Loss: 0.7462902665138245\n",
      "Iteration 46002 Loss: 0.9506345391273499\n",
      "Iteration 46003 Loss: 1.1240261793136597\n",
      "Iteration 46004 Loss: 0.8276718258857727\n",
      "Iteration 46005 Loss: 0.7668229937553406\n",
      "Iteration 46006 Loss: 0.9154219627380371\n",
      "Iteration 46007 Loss: 1.017715334892273\n",
      "Iteration 46008 Loss: 0.9382955431938171\n",
      "Iteration 46009 Loss: 0.7103449106216431\n",
      "Iteration 46009 Loss: 0.8858086466789246\n",
      "Iteration 46010 Loss: 0.4734150767326355\n",
      "Iteration 46011 Loss: 1.2263820171356201\n",
      "Iteration 46012 Loss: 0.9558440446853638\n",
      "Iteration 46013 Loss: 0.8678210973739624\n",
      "Iteration 46014 Loss: 0.9504655003547668\n",
      "Iteration 46015 Loss: 0.6111323833465576\n",
      "Iteration 46016 Loss: 0.8114440441131592\n",
      "Iteration 46017 Loss: 1.2464159727096558\n",
      "Iteration 46018 Loss: 0.7651159167289734\n",
      "Iteration 46019 Loss: 1.2115486860275269\n",
      "Iteration 46019 Loss: 0.9119585156440735\n",
      "Iteration 46020 Loss: 1.0993497371673584\n",
      "Iteration 46021 Loss: 1.0196937322616577\n",
      "Iteration 46022 Loss: 0.6371996402740479\n",
      "Iteration 46023 Loss: 1.2720541954040527\n",
      "Iteration 46024 Loss: 0.8107669949531555\n",
      "Iteration 46025 Loss: 1.2706650495529175\n",
      "Iteration 46026 Loss: 1.0700424909591675\n",
      "Iteration 46027 Loss: 0.7757765054702759\n",
      "Iteration 46028 Loss: 1.0753616094589233\n",
      "Iteration 46029 Loss: 0.7883724570274353\n",
      "Iteration 46029 Loss: 0.9819282293319702\n",
      "Iteration 46030 Loss: 0.9439219832420349\n",
      "Iteration 46031 Loss: 0.6852044463157654\n",
      "Iteration 46032 Loss: 0.7282794713973999\n",
      "Iteration 46033 Loss: 0.9231343269348145\n",
      "Iteration 46034 Loss: 1.0392636060714722\n",
      "Iteration 46035 Loss: 0.6790511608123779\n",
      "Iteration 46036 Loss: 1.1315999031066895\n",
      "Iteration 46037 Loss: 1.140630841255188\n",
      "Iteration 46038 Loss: 0.8635697364807129\n",
      "Iteration 46039 Loss: 0.8826519250869751\n",
      "Iteration 46039 Loss: 0.9017308354377747\n",
      "Iteration 46040 Loss: 1.288831353187561\n",
      "Iteration 46041 Loss: 1.1797807216644287\n",
      "Iteration 46042 Loss: 0.9503426551818848\n",
      "Iteration 46043 Loss: 1.0035083293914795\n",
      "Iteration 46044 Loss: 0.890571653842926\n",
      "Iteration 46045 Loss: 0.9345647692680359\n",
      "Iteration 46046 Loss: 0.8572787642478943\n",
      "Iteration 46047 Loss: 0.849743127822876\n",
      "Iteration 46048 Loss: 1.21783447265625\n",
      "Iteration 46049 Loss: 0.7499747276306152\n",
      "Iteration 46049 Loss: 0.9922429919242859\n",
      "Iteration 46050 Loss: 0.7805444002151489\n",
      "Iteration 46051 Loss: 1.0647467374801636\n",
      "Iteration 46052 Loss: 1.103028416633606\n",
      "Iteration 46053 Loss: 0.9083329439163208\n",
      "Iteration 46054 Loss: 1.0704643726348877\n",
      "Iteration 46055 Loss: 0.9082850813865662\n",
      "Iteration 46056 Loss: 1.1403322219848633\n",
      "Iteration 46057 Loss: 0.9523977041244507\n",
      "Iteration 46058 Loss: 0.8348333239555359\n",
      "Iteration 46059 Loss: 0.8064344525337219\n",
      "Iteration 46059 Loss: 0.9569398760795593\n",
      "Iteration 46060 Loss: 1.1487103700637817\n",
      "Iteration 46061 Loss: 1.041469693183899\n",
      "Iteration 46062 Loss: 0.8822096586227417\n",
      "Iteration 46063 Loss: 1.0104979276657104\n",
      "Iteration 46064 Loss: 1.1523187160491943\n",
      "Iteration 46065 Loss: 1.3524647951126099\n",
      "Iteration 46066 Loss: 0.9581159353256226\n",
      "Iteration 46067 Loss: 0.8507445454597473\n",
      "Iteration 46068 Loss: 0.8211588263511658\n",
      "Iteration 46069 Loss: 1.004733920097351\n",
      "Iteration 46069 Loss: 1.0222423076629639\n",
      "Iteration 46070 Loss: 0.7481836080551147\n",
      "Iteration 46071 Loss: 1.3931506872177124\n",
      "Iteration 46072 Loss: 0.8322902321815491\n",
      "Iteration 46073 Loss: 0.5674872994422913\n",
      "Iteration 46074 Loss: 0.8441582322120667\n",
      "Iteration 46075 Loss: 1.1791822910308838\n",
      "Iteration 46076 Loss: 0.931688129901886\n",
      "Iteration 46077 Loss: 1.0585918426513672\n",
      "Iteration 46078 Loss: 0.8260952830314636\n",
      "Iteration 46079 Loss: 0.8256193995475769\n",
      "Iteration 46079 Loss: 0.9206446409225464\n",
      "Iteration 46080 Loss: 1.2697932720184326\n",
      "Iteration 46081 Loss: 0.9262922406196594\n",
      "Iteration 46082 Loss: 0.8178946375846863\n",
      "Iteration 46083 Loss: 0.9255163073539734\n",
      "Iteration 46084 Loss: 1.1225566864013672\n",
      "Iteration 46085 Loss: 0.9379484057426453\n",
      "Iteration 46086 Loss: 1.1562026739120483\n",
      "Iteration 46087 Loss: 0.8577465415000916\n",
      "Iteration 46088 Loss: 1.2267167568206787\n",
      "Iteration 46089 Loss: 0.8950809240341187\n",
      "Iteration 46089 Loss: 1.0135747194290161\n",
      "Iteration 46090 Loss: 0.8587576746940613\n",
      "Iteration 46091 Loss: 1.0624128580093384\n",
      "Iteration 46092 Loss: 0.8893402814865112\n",
      "Iteration 46093 Loss: 1.2557607889175415\n",
      "Iteration 46094 Loss: 0.9931711554527283\n",
      "Iteration 46095 Loss: 0.8170896172523499\n",
      "Iteration 46096 Loss: 1.2112104892730713\n",
      "Iteration 46097 Loss: 0.7083803415298462\n",
      "Iteration 46098 Loss: 0.7673333883285522\n",
      "Iteration 46099 Loss: 0.8602393865585327\n",
      "Iteration 46099 Loss: 0.9423696398735046\n",
      "Iteration 46100 Loss: 0.8730159997940063\n",
      "Iteration 46101 Loss: 1.1162821054458618\n",
      "Iteration 46102 Loss: 0.7835277318954468\n",
      "Iteration 46103 Loss: 0.9633562564849854\n",
      "Iteration 46104 Loss: 0.9373162388801575\n",
      "Iteration 46105 Loss: 0.9404277205467224\n",
      "Iteration 46106 Loss: 0.7697434425354004\n",
      "Iteration 46107 Loss: 0.5662952661514282\n",
      "Iteration 46108 Loss: 1.0321086645126343\n",
      "Iteration 46109 Loss: 1.1577510833740234\n",
      "Iteration 46109 Loss: 0.9139825105667114\n",
      "Iteration 46110 Loss: 1.2612560987472534\n",
      "Iteration 46111 Loss: 0.8146066665649414\n",
      "Iteration 46112 Loss: 1.0868875980377197\n",
      "Iteration 46113 Loss: 0.8559252023696899\n",
      "Iteration 46114 Loss: 0.9707097411155701\n",
      "Iteration 46115 Loss: 1.025158166885376\n",
      "Iteration 46116 Loss: 0.9666902422904968\n",
      "Iteration 46117 Loss: 0.8022275567054749\n",
      "Iteration 46118 Loss: 0.8989129662513733\n",
      "Iteration 46119 Loss: 0.8619210124015808\n",
      "Iteration 46119 Loss: 0.9544295072555542\n",
      "Iteration 46120 Loss: 1.2582309246063232\n",
      "Iteration 46121 Loss: 0.9757686257362366\n",
      "Iteration 46122 Loss: 1.3147889375686646\n",
      "Iteration 46123 Loss: 0.9970582723617554\n",
      "Iteration 46124 Loss: 1.041040062904358\n",
      "Iteration 46125 Loss: 0.4868892431259155\n",
      "Iteration 46126 Loss: 0.8748246431350708\n",
      "Iteration 46127 Loss: 0.7767193913459778\n",
      "Iteration 46128 Loss: 1.0992610454559326\n",
      "Iteration 46129 Loss: 1.1183356046676636\n",
      "Iteration 46129 Loss: 0.9942916035652161\n",
      "Iteration 46130 Loss: 0.8525853157043457\n",
      "Iteration 46131 Loss: 1.0957356691360474\n",
      "Iteration 46132 Loss: 0.6640704870223999\n",
      "Iteration 46133 Loss: 0.7893800139427185\n",
      "Iteration 46134 Loss: 0.7264341115951538\n",
      "Iteration 46135 Loss: 0.8476839065551758\n",
      "Iteration 46136 Loss: 1.0694466829299927\n",
      "Iteration 46137 Loss: 0.8183907270431519\n",
      "Iteration 46138 Loss: 1.0587736368179321\n",
      "Iteration 46139 Loss: 0.7059440016746521\n",
      "Iteration 46139 Loss: 0.8628444671630859\n",
      "Iteration 46140 Loss: 1.0733500719070435\n",
      "Iteration 46141 Loss: 1.2205936908721924\n",
      "Iteration 46142 Loss: 1.0662949085235596\n",
      "Iteration 46143 Loss: 1.0164412260055542\n",
      "Iteration 46144 Loss: 0.8190000057220459\n",
      "Iteration 46145 Loss: 0.9786249399185181\n",
      "Iteration 46146 Loss: 1.2871767282485962\n",
      "Iteration 46147 Loss: 1.3173197507858276\n",
      "Iteration 46148 Loss: 1.046827793121338\n",
      "Iteration 46149 Loss: 1.066558599472046\n",
      "Iteration 46149 Loss: 1.0892188549041748\n",
      "Iteration 46150 Loss: 1.0319703817367554\n",
      "Iteration 46151 Loss: 0.7536810040473938\n",
      "Iteration 46152 Loss: 0.9888930320739746\n",
      "Iteration 46153 Loss: 1.1528172492980957\n",
      "Iteration 46154 Loss: 0.8090469241142273\n",
      "Iteration 46155 Loss: 0.6406815648078918\n",
      "Iteration 46156 Loss: 1.0088430643081665\n",
      "Iteration 46157 Loss: 1.0407187938690186\n",
      "Iteration 46158 Loss: 1.1502256393432617\n",
      "Iteration 46159 Loss: 0.8133291006088257\n",
      "Iteration 46159 Loss: 0.9390207529067993\n",
      "Iteration 46160 Loss: 0.884563148021698\n",
      "Iteration 46161 Loss: 0.6517120599746704\n",
      "Iteration 46162 Loss: 1.05661141872406\n",
      "Iteration 46163 Loss: 0.8213750720024109\n",
      "Iteration 46164 Loss: 0.840815007686615\n",
      "Iteration 46165 Loss: 0.9431165456771851\n",
      "Iteration 46166 Loss: 1.034502387046814\n",
      "Iteration 46167 Loss: 0.9752916693687439\n",
      "Iteration 46168 Loss: 0.6336047053337097\n",
      "Iteration 46169 Loss: 1.1239523887634277\n",
      "Iteration 46169 Loss: 0.8965544700622559\n",
      "Iteration 46170 Loss: 0.9639471769332886\n",
      "Iteration 46171 Loss: 0.9019213318824768\n",
      "Iteration 46172 Loss: 0.9432029724121094\n",
      "Iteration 46173 Loss: 0.9404358863830566\n",
      "Iteration 46174 Loss: 0.7890614867210388\n",
      "Iteration 46175 Loss: 1.2113865613937378\n",
      "Iteration 46176 Loss: 0.9021322727203369\n",
      "Iteration 46177 Loss: 1.0674855709075928\n",
      "Iteration 46178 Loss: 1.122230052947998\n",
      "Iteration 46179 Loss: 0.7452501058578491\n",
      "Iteration 46179 Loss: 0.9587054252624512\n",
      "Iteration 46180 Loss: 0.9427813291549683\n",
      "Iteration 46181 Loss: 0.8202061057090759\n",
      "Iteration 46182 Loss: 1.1313709020614624\n",
      "Iteration 46183 Loss: 0.8990927934646606\n",
      "Iteration 46184 Loss: 0.7603951096534729\n",
      "Iteration 46185 Loss: 1.053432822227478\n",
      "Iteration 46186 Loss: 0.7121468782424927\n",
      "Iteration 46187 Loss: 1.2522995471954346\n",
      "Iteration 46188 Loss: 0.7007369995117188\n",
      "Iteration 46189 Loss: 0.940201997756958\n",
      "Iteration 46189 Loss: 0.9212664365768433\n",
      "Iteration 46190 Loss: 1.1356873512268066\n",
      "Iteration 46191 Loss: 0.9917454123497009\n",
      "Iteration 46192 Loss: 0.8729480504989624\n",
      "Iteration 46193 Loss: 0.8045244216918945\n",
      "Iteration 46194 Loss: 0.6127210259437561\n",
      "Iteration 46195 Loss: 1.0211968421936035\n",
      "Iteration 46196 Loss: 0.9107016921043396\n",
      "Iteration 46197 Loss: 0.9312158823013306\n",
      "Iteration 46198 Loss: 1.1460875272750854\n",
      "Iteration 46199 Loss: 0.8829464912414551\n",
      "Iteration 46199 Loss: 0.9309775233268738\n",
      "Iteration 46200 Loss: 0.8964243531227112\n",
      "Iteration 46201 Loss: 0.9239616394042969\n",
      "Iteration 46202 Loss: 0.6193352341651917\n",
      "Iteration 46203 Loss: 0.8920838832855225\n",
      "Iteration 46204 Loss: 1.2799888849258423\n",
      "Iteration 46205 Loss: 1.0281790494918823\n",
      "Iteration 46206 Loss: 1.1033893823623657\n",
      "Iteration 46207 Loss: 0.8105395436286926\n",
      "Iteration 46208 Loss: 0.8252315521240234\n",
      "Iteration 46209 Loss: 0.9560291171073914\n",
      "Iteration 46209 Loss: 0.9335162043571472\n",
      "Iteration 46210 Loss: 0.924876868724823\n",
      "Iteration 46211 Loss: 0.9199777245521545\n",
      "Iteration 46212 Loss: 0.8643352389335632\n",
      "Iteration 46213 Loss: 0.7357481718063354\n",
      "Iteration 46214 Loss: 1.1515690088272095\n",
      "Iteration 46215 Loss: 1.0081969499588013\n",
      "Iteration 46216 Loss: 0.7789073586463928\n",
      "Iteration 46217 Loss: 0.7456540465354919\n",
      "Iteration 46218 Loss: 0.8877008557319641\n",
      "Iteration 46219 Loss: 1.0312825441360474\n",
      "Iteration 46219 Loss: 0.9048249125480652\n",
      "Iteration 46220 Loss: 0.9405405521392822\n",
      "Iteration 46221 Loss: 1.322748064994812\n",
      "Iteration 46222 Loss: 0.9475263357162476\n",
      "Iteration 46223 Loss: 0.9282780885696411\n",
      "Iteration 46224 Loss: 1.1513971090316772\n",
      "Iteration 46225 Loss: 0.7674617171287537\n",
      "Iteration 46226 Loss: 1.035836935043335\n",
      "Iteration 46227 Loss: 1.1281888484954834\n",
      "Iteration 46228 Loss: 0.9168814420700073\n",
      "Iteration 46229 Loss: 1.2290830612182617\n",
      "Iteration 46229 Loss: 1.0367943048477173\n",
      "Iteration 46230 Loss: 1.142500400543213\n",
      "Iteration 46231 Loss: 0.8478374481201172\n",
      "Iteration 46232 Loss: 1.1673604249954224\n",
      "Iteration 46233 Loss: 1.0534801483154297\n",
      "Iteration 46234 Loss: 1.2514923810958862\n",
      "Iteration 46235 Loss: 0.9483199715614319\n",
      "Iteration 46236 Loss: 1.1469385623931885\n",
      "Iteration 46237 Loss: 0.5415491461753845\n",
      "Iteration 46238 Loss: 0.8961056470870972\n",
      "Iteration 46239 Loss: 0.7580745220184326\n",
      "Iteration 46239 Loss: 0.9753658175468445\n",
      "Iteration 46240 Loss: 0.7465206980705261\n",
      "Iteration 46241 Loss: 0.8313190340995789\n",
      "Iteration 46242 Loss: 0.8933301568031311\n",
      "Iteration 46243 Loss: 1.1682069301605225\n",
      "Iteration 46244 Loss: 0.7720673680305481\n",
      "Iteration 46245 Loss: 1.3778283596038818\n",
      "Iteration 46246 Loss: 0.6787377595901489\n",
      "Iteration 46247 Loss: 1.1504203081130981\n",
      "Iteration 46248 Loss: 0.7464069724082947\n",
      "Iteration 46249 Loss: 1.269342303276062\n",
      "Iteration 46249 Loss: 0.9634180068969727\n",
      "Iteration 46250 Loss: 0.9115937948226929\n",
      "Iteration 46251 Loss: 0.7445407509803772\n",
      "Iteration 46252 Loss: 1.0137994289398193\n",
      "Iteration 46253 Loss: 0.7129889726638794\n",
      "Iteration 46254 Loss: 0.7788896560668945\n",
      "Iteration 46255 Loss: 0.6632130146026611\n",
      "Iteration 46256 Loss: 0.9249207377433777\n",
      "Iteration 46257 Loss: 0.994041919708252\n",
      "Iteration 46258 Loss: 1.0908095836639404\n",
      "Iteration 46259 Loss: 1.1401898860931396\n",
      "Iteration 46259 Loss: 0.8974987268447876\n",
      "Iteration 46260 Loss: 0.946374773979187\n",
      "Iteration 46261 Loss: 0.5829575657844543\n",
      "Iteration 46262 Loss: 1.1230255365371704\n",
      "Iteration 46263 Loss: 0.7594263553619385\n",
      "Iteration 46264 Loss: 0.8550313711166382\n",
      "Iteration 46265 Loss: 0.9477196931838989\n",
      "Iteration 46266 Loss: 0.9836376905441284\n",
      "Iteration 46267 Loss: 0.8544380068778992\n",
      "Iteration 46268 Loss: 0.8906049132347107\n",
      "Iteration 46269 Loss: 0.8415411114692688\n",
      "Iteration 46269 Loss: 0.8784757852554321\n",
      "Iteration 46270 Loss: 1.0618427991867065\n",
      "Iteration 46271 Loss: 0.8873181343078613\n",
      "Iteration 46272 Loss: 0.6508288383483887\n",
      "Iteration 46273 Loss: 1.0062952041625977\n",
      "Iteration 46274 Loss: 1.0557451248168945\n",
      "Iteration 46275 Loss: 0.5849719643592834\n",
      "Iteration 46276 Loss: 1.020100712776184\n",
      "Iteration 46277 Loss: 1.0547949075698853\n",
      "Iteration 46278 Loss: 0.8294889330863953\n",
      "Iteration 46279 Loss: 0.9321425557136536\n",
      "Iteration 46279 Loss: 0.9083529710769653\n",
      "Iteration 46280 Loss: 1.186317801475525\n",
      "Iteration 46281 Loss: 0.7533658146858215\n",
      "Iteration 46282 Loss: 1.143733263015747\n",
      "Iteration 46283 Loss: 1.3271628618240356\n",
      "Iteration 46284 Loss: 0.8289601802825928\n",
      "Iteration 46285 Loss: 1.0531005859375\n",
      "Iteration 46286 Loss: 0.662117063999176\n",
      "Iteration 46287 Loss: 1.1603621244430542\n",
      "Iteration 46288 Loss: 0.799541711807251\n",
      "Iteration 46289 Loss: 0.8728405237197876\n",
      "Iteration 46289 Loss: 0.9787502288818359\n",
      "Iteration 46290 Loss: 0.7133288979530334\n",
      "Iteration 46291 Loss: 1.0763282775878906\n",
      "Iteration 46292 Loss: 1.1214768886566162\n",
      "Iteration 46293 Loss: 0.9249945878982544\n",
      "Iteration 46294 Loss: 1.0561597347259521\n",
      "Iteration 46295 Loss: 0.9230073690414429\n",
      "Iteration 46296 Loss: 0.6451997756958008\n",
      "Iteration 46297 Loss: 0.52293461561203\n",
      "Iteration 46298 Loss: 0.7617493271827698\n",
      "Iteration 46299 Loss: 0.8898497819900513\n",
      "Iteration 46299 Loss: 0.8635029792785645\n",
      "Iteration 46300 Loss: 0.6651250123977661\n",
      "Iteration 46301 Loss: 1.2023979425430298\n",
      "Iteration 46302 Loss: 1.1206058263778687\n",
      "Iteration 46303 Loss: 0.933930516242981\n",
      "Iteration 46304 Loss: 0.8565244674682617\n",
      "Iteration 46305 Loss: 1.1423776149749756\n",
      "Iteration 46306 Loss: 1.047067403793335\n",
      "Iteration 46307 Loss: 0.7021620869636536\n",
      "Iteration 46308 Loss: 0.8719373345375061\n",
      "Iteration 46309 Loss: 0.9578984379768372\n",
      "Iteration 46309 Loss: 0.9500026702880859\n",
      "Iteration 46310 Loss: 0.9656192660331726\n",
      "Iteration 46311 Loss: 0.5977354645729065\n",
      "Iteration 46312 Loss: 1.0858166217803955\n",
      "Iteration 46313 Loss: 0.9082381129264832\n",
      "Iteration 46314 Loss: 0.9557703733444214\n",
      "Iteration 46315 Loss: 1.1435389518737793\n",
      "Iteration 46316 Loss: 0.9345454573631287\n",
      "Iteration 46317 Loss: 1.096330165863037\n",
      "Iteration 46318 Loss: 1.0334161520004272\n",
      "Iteration 46319 Loss: 0.8234777450561523\n",
      "Iteration 46319 Loss: 0.9544488787651062\n",
      "Iteration 46320 Loss: 0.7852298617362976\n",
      "Iteration 46321 Loss: 1.132541298866272\n",
      "Iteration 46322 Loss: 1.2286187410354614\n",
      "Iteration 46323 Loss: 0.8977779746055603\n",
      "Iteration 46324 Loss: 1.071733832359314\n",
      "Iteration 46325 Loss: 1.0448373556137085\n",
      "Iteration 46326 Loss: 0.9786273837089539\n",
      "Iteration 46327 Loss: 0.7632309198379517\n",
      "Iteration 46328 Loss: 0.7080169916152954\n",
      "Iteration 46329 Loss: 1.1469846963882446\n",
      "Iteration 46329 Loss: 0.9757598638534546\n",
      "Iteration 46330 Loss: 1.0048246383666992\n",
      "Iteration 46331 Loss: 0.7103429436683655\n",
      "Iteration 46332 Loss: 0.9092383980751038\n",
      "Iteration 46333 Loss: 0.8807704448699951\n",
      "Iteration 46334 Loss: 0.9628757238388062\n",
      "Iteration 46335 Loss: 0.7552610635757446\n",
      "Iteration 46336 Loss: 0.8309979438781738\n",
      "Iteration 46337 Loss: 1.0533181428909302\n",
      "Iteration 46338 Loss: 0.7073909640312195\n",
      "Iteration 46339 Loss: 0.8721014857292175\n",
      "Iteration 46339 Loss: 0.8687122464179993\n",
      "Iteration 46340 Loss: 1.2241771221160889\n",
      "Iteration 46341 Loss: 0.753746747970581\n",
      "Iteration 46342 Loss: 1.0363445281982422\n",
      "Iteration 46343 Loss: 0.989375650882721\n",
      "Iteration 46344 Loss: 1.1051387786865234\n",
      "Iteration 46345 Loss: 0.9665719866752625\n",
      "Iteration 46346 Loss: 0.7472390532493591\n",
      "Iteration 46347 Loss: 0.7714701294898987\n",
      "Iteration 46348 Loss: 0.6853635907173157\n",
      "Iteration 46349 Loss: 0.6852439045906067\n",
      "Iteration 46349 Loss: 0.8964670896530151\n",
      "Iteration 46350 Loss: 0.5662232637405396\n",
      "Iteration 46351 Loss: 0.8573165535926819\n",
      "Iteration 46352 Loss: 0.7128497958183289\n",
      "Iteration 46353 Loss: 0.8448567390441895\n",
      "Iteration 46354 Loss: 0.9811031818389893\n",
      "Iteration 46355 Loss: 0.9338606595993042\n",
      "Iteration 46356 Loss: 0.7983313202857971\n",
      "Iteration 46357 Loss: 0.8098279237747192\n",
      "Iteration 46358 Loss: 0.8388608694076538\n",
      "Iteration 46359 Loss: 1.0515261888504028\n",
      "Iteration 46359 Loss: 0.8394756317138672\n",
      "Iteration 46360 Loss: 0.838957667350769\n",
      "Iteration 46361 Loss: 0.8193803429603577\n",
      "Iteration 46362 Loss: 1.1971036195755005\n",
      "Iteration 46363 Loss: 0.7348831295967102\n",
      "Iteration 46364 Loss: 0.9584637880325317\n",
      "Iteration 46365 Loss: 0.8637998104095459\n",
      "Iteration 46366 Loss: 1.0137262344360352\n",
      "Iteration 46367 Loss: 1.083267092704773\n",
      "Iteration 46368 Loss: 0.7251690030097961\n",
      "Iteration 46369 Loss: 1.129581332206726\n",
      "Iteration 46369 Loss: 0.9364331960678101\n",
      "Iteration 46370 Loss: 1.1160789728164673\n",
      "Iteration 46371 Loss: 0.8234567046165466\n",
      "Iteration 46372 Loss: 0.6840843558311462\n",
      "Iteration 46373 Loss: 0.8312826752662659\n",
      "Iteration 46374 Loss: 1.362714409828186\n",
      "Iteration 46375 Loss: 0.8564091920852661\n",
      "Iteration 46376 Loss: 1.319998860359192\n",
      "Iteration 46377 Loss: 1.1747461557388306\n",
      "Iteration 46378 Loss: 1.1539462804794312\n",
      "Iteration 46379 Loss: 1.0540246963500977\n",
      "Iteration 46379 Loss: 1.0376741886138916\n",
      "Iteration 46380 Loss: 0.878948986530304\n",
      "Iteration 46381 Loss: 0.7458760142326355\n",
      "Iteration 46382 Loss: 1.1951559782028198\n",
      "Iteration 46383 Loss: 1.0164796113967896\n",
      "Iteration 46384 Loss: 0.9941041469573975\n",
      "Iteration 46385 Loss: 0.8641591668128967\n",
      "Iteration 46386 Loss: 0.9349645972251892\n",
      "Iteration 46387 Loss: 0.9912073612213135\n",
      "Iteration 46388 Loss: 0.6181732416152954\n",
      "Iteration 46389 Loss: 0.870182991027832\n",
      "Iteration 46389 Loss: 0.9109252095222473\n",
      "Iteration 46390 Loss: 0.9206532835960388\n",
      "Iteration 46391 Loss: 1.274853229522705\n",
      "Iteration 46392 Loss: 0.9998784065246582\n",
      "Iteration 46393 Loss: 1.180898666381836\n",
      "Iteration 46394 Loss: 1.1397570371627808\n",
      "Iteration 46395 Loss: 1.1495552062988281\n",
      "Iteration 46396 Loss: 0.8313930034637451\n",
      "Iteration 46397 Loss: 1.0831321477890015\n",
      "Iteration 46398 Loss: 1.2332628965377808\n",
      "Iteration 46399 Loss: 0.9676153063774109\n",
      "Iteration 46399 Loss: 1.0780999660491943\n",
      "Iteration 46400 Loss: 1.170942783355713\n",
      "Iteration 46401 Loss: 0.8421764969825745\n",
      "Iteration 46402 Loss: 0.8959175944328308\n",
      "Iteration 46403 Loss: 0.9679654836654663\n",
      "Iteration 46404 Loss: 1.1671693325042725\n",
      "Iteration 46405 Loss: 1.0142550468444824\n",
      "Iteration 46406 Loss: 0.8599856495857239\n",
      "Iteration 46407 Loss: 1.328244686126709\n",
      "Iteration 46408 Loss: 0.8330299258232117\n",
      "Iteration 46409 Loss: 0.607412576675415\n",
      "Iteration 46409 Loss: 0.9687099456787109\n",
      "Iteration 46410 Loss: 0.8411983251571655\n",
      "Iteration 46411 Loss: 0.9751348495483398\n",
      "Iteration 46412 Loss: 0.699174702167511\n",
      "Iteration 46413 Loss: 0.8106898665428162\n",
      "Iteration 46414 Loss: 0.982970654964447\n",
      "Iteration 46415 Loss: 0.775670051574707\n",
      "Iteration 46416 Loss: 0.9016745090484619\n",
      "Iteration 46417 Loss: 0.8050887584686279\n",
      "Iteration 46418 Loss: 1.1379787921905518\n",
      "Iteration 46419 Loss: 1.0320545434951782\n",
      "Iteration 46419 Loss: 0.8961635828018188\n",
      "Iteration 46420 Loss: 0.860115647315979\n",
      "Iteration 46421 Loss: 1.0546183586120605\n",
      "Iteration 46422 Loss: 1.1428916454315186\n",
      "Iteration 46423 Loss: 0.8431162238121033\n",
      "Iteration 46424 Loss: 0.8553770780563354\n",
      "Iteration 46425 Loss: 0.554382860660553\n",
      "Iteration 46426 Loss: 0.802620530128479\n",
      "Iteration 46427 Loss: 0.9761919975280762\n",
      "Iteration 46428 Loss: 0.46465036273002625\n",
      "Iteration 46429 Loss: 0.6198269128799438\n",
      "Iteration 46429 Loss: 0.8173791766166687\n",
      "Iteration 46430 Loss: 1.0427191257476807\n",
      "Iteration 46431 Loss: 1.0726360082626343\n",
      "Iteration 46432 Loss: 0.7684201598167419\n",
      "Iteration 46433 Loss: 0.785590410232544\n",
      "Iteration 46434 Loss: 1.1600935459136963\n",
      "Iteration 46435 Loss: 0.885033905506134\n",
      "Iteration 46436 Loss: 0.8579933047294617\n",
      "Iteration 46437 Loss: 1.045526146888733\n",
      "Iteration 46438 Loss: 0.7813540101051331\n",
      "Iteration 46439 Loss: 1.2268644571304321\n",
      "Iteration 46439 Loss: 0.9626232385635376\n",
      "Iteration 46440 Loss: 0.7026004791259766\n",
      "Iteration 46441 Loss: 1.1049528121948242\n",
      "Iteration 46442 Loss: 0.6384057402610779\n",
      "Iteration 46443 Loss: 0.9531674981117249\n",
      "Iteration 46444 Loss: 0.9800919890403748\n",
      "Iteration 46445 Loss: 1.2674777507781982\n",
      "Iteration 46446 Loss: 1.0909149646759033\n",
      "Iteration 46447 Loss: 1.3668919801712036\n",
      "Iteration 46448 Loss: 0.891889214515686\n",
      "Iteration 46449 Loss: 0.767309308052063\n",
      "Iteration 46449 Loss: 0.9763701558113098\n",
      "Iteration 46450 Loss: 1.2389181852340698\n",
      "Iteration 46451 Loss: 1.0312433242797852\n",
      "Iteration 46452 Loss: 1.0634796619415283\n",
      "Iteration 46453 Loss: 0.891745388507843\n",
      "Iteration 46454 Loss: 0.8249242305755615\n",
      "Iteration 46455 Loss: 1.03396475315094\n",
      "Iteration 46456 Loss: 1.0697124004364014\n",
      "Iteration 46457 Loss: 0.6144046783447266\n",
      "Iteration 46458 Loss: 1.0527799129486084\n",
      "Iteration 46459 Loss: 0.7593693137168884\n",
      "Iteration 46459 Loss: 0.9580541849136353\n",
      "Iteration 46460 Loss: 1.0252482891082764\n",
      "Iteration 46461 Loss: 1.4240549802780151\n",
      "Iteration 46462 Loss: 0.871376097202301\n",
      "Iteration 46463 Loss: 0.9359225630760193\n",
      "Iteration 46464 Loss: 1.0231647491455078\n",
      "Iteration 46465 Loss: 0.9295750856399536\n",
      "Iteration 46466 Loss: 1.1940500736236572\n",
      "Iteration 46467 Loss: 1.1751117706298828\n",
      "Iteration 46468 Loss: 0.9949629902839661\n",
      "Iteration 46469 Loss: 0.8933055996894836\n",
      "Iteration 46469 Loss: 1.0466772317886353\n",
      "Iteration 46470 Loss: 0.8522806167602539\n",
      "Iteration 46471 Loss: 0.9552329182624817\n",
      "Iteration 46472 Loss: 0.9082770347595215\n",
      "Iteration 46473 Loss: 1.0383661985397339\n",
      "Iteration 46474 Loss: 1.005176067352295\n",
      "Iteration 46475 Loss: 1.1332648992538452\n",
      "Iteration 46476 Loss: 1.321055293083191\n",
      "Iteration 46477 Loss: 0.8593729734420776\n",
      "Iteration 46478 Loss: 0.7745707035064697\n",
      "Iteration 46479 Loss: 1.0254734754562378\n",
      "Iteration 46479 Loss: 0.987307071685791\n",
      "Iteration 46480 Loss: 1.0507452487945557\n",
      "Iteration 46481 Loss: 1.1283830404281616\n",
      "Iteration 46482 Loss: 0.7395755648612976\n",
      "Iteration 46483 Loss: 1.1587496995925903\n",
      "Iteration 46484 Loss: 0.9965841770172119\n",
      "Iteration 46485 Loss: 0.9993786215782166\n",
      "Iteration 46486 Loss: 0.9916465878486633\n",
      "Iteration 46487 Loss: 0.8588646650314331\n",
      "Iteration 46488 Loss: 0.8984477519989014\n",
      "Iteration 46489 Loss: 1.0395493507385254\n",
      "Iteration 46489 Loss: 0.9861924052238464\n",
      "Iteration 46490 Loss: 0.6031144261360168\n",
      "Iteration 46491 Loss: 0.9021581411361694\n",
      "Iteration 46492 Loss: 0.47834083437919617\n",
      "Iteration 46493 Loss: 0.7173067331314087\n",
      "Iteration 46494 Loss: 1.3469388484954834\n",
      "Iteration 46495 Loss: 0.9294213652610779\n",
      "Iteration 46496 Loss: 0.7238571047782898\n",
      "Iteration 46497 Loss: 1.1548722982406616\n",
      "Iteration 46498 Loss: 0.9570255279541016\n",
      "Iteration 46499 Loss: 0.6676610708236694\n",
      "Iteration 46499 Loss: 0.8480695486068726\n",
      "Iteration 46500 Loss: 0.9822767376899719\n",
      "Iteration 46501 Loss: 0.8932521343231201\n",
      "Iteration 46502 Loss: 0.8252919316291809\n",
      "Iteration 46503 Loss: 0.9172728061676025\n",
      "Iteration 46504 Loss: 1.0540227890014648\n",
      "Iteration 46505 Loss: 0.9797601103782654\n",
      "Iteration 46506 Loss: 0.8989484906196594\n",
      "Iteration 46507 Loss: 0.8843740224838257\n",
      "Iteration 46508 Loss: 0.7913395166397095\n",
      "Iteration 46509 Loss: 0.918521523475647\n",
      "Iteration 46509 Loss: 0.9145059585571289\n",
      "Iteration 46510 Loss: 1.0271185636520386\n",
      "Iteration 46511 Loss: 1.199081301689148\n",
      "Iteration 46512 Loss: 0.7510961294174194\n",
      "Iteration 46513 Loss: 0.840805172920227\n",
      "Iteration 46514 Loss: 0.9897211790084839\n",
      "Iteration 46515 Loss: 0.7138813138008118\n",
      "Iteration 46516 Loss: 1.166961669921875\n",
      "Iteration 46517 Loss: 0.45607343316078186\n",
      "Iteration 46518 Loss: 0.8238818049430847\n",
      "Iteration 46519 Loss: 0.9078461527824402\n",
      "Iteration 46519 Loss: 0.8876466751098633\n",
      "Iteration 46520 Loss: 0.8205277919769287\n",
      "Iteration 46521 Loss: 1.1109120845794678\n",
      "Iteration 46522 Loss: 0.6965256333351135\n",
      "Iteration 46523 Loss: 0.9864156246185303\n",
      "Iteration 46524 Loss: 0.7870203852653503\n",
      "Iteration 46525 Loss: 0.8685739040374756\n",
      "Iteration 46526 Loss: 0.9814460873603821\n",
      "Iteration 46527 Loss: 0.7932231426239014\n",
      "Iteration 46528 Loss: 0.828751802444458\n",
      "Iteration 46529 Loss: 0.8279287815093994\n",
      "Iteration 46529 Loss: 0.870132565498352\n",
      "Iteration 46530 Loss: 1.2135305404663086\n",
      "Iteration 46531 Loss: 1.0779409408569336\n",
      "Iteration 46532 Loss: 1.10720956325531\n",
      "Iteration 46533 Loss: 1.0129749774932861\n",
      "Iteration 46534 Loss: 1.0188833475112915\n",
      "Iteration 46535 Loss: 0.9129902720451355\n",
      "Iteration 46536 Loss: 1.0960954427719116\n",
      "Iteration 46537 Loss: 1.076535701751709\n",
      "Iteration 46538 Loss: 0.995103120803833\n",
      "Iteration 46539 Loss: 0.9926254153251648\n",
      "Iteration 46539 Loss: 1.0503889322280884\n",
      "Iteration 46540 Loss: 0.7578480243682861\n",
      "Iteration 46541 Loss: 0.980997622013092\n",
      "Iteration 46542 Loss: 0.66058349609375\n",
      "Iteration 46543 Loss: 1.1702324151992798\n",
      "Iteration 46544 Loss: 0.8294113874435425\n",
      "Iteration 46545 Loss: 1.0991979837417603\n",
      "Iteration 46546 Loss: 0.8253252506256104\n",
      "Iteration 46547 Loss: 1.0317611694335938\n",
      "Iteration 46548 Loss: 1.1358753442764282\n",
      "Iteration 46549 Loss: 0.8722173571586609\n",
      "Iteration 46549 Loss: 0.9363449811935425\n",
      "Iteration 46550 Loss: 0.7539830207824707\n",
      "Iteration 46551 Loss: 0.6408459544181824\n",
      "Iteration 46552 Loss: 1.0725635290145874\n",
      "Iteration 46553 Loss: 1.1979910135269165\n",
      "Iteration 46554 Loss: 0.8740203380584717\n",
      "Iteration 46555 Loss: 1.1320304870605469\n",
      "Iteration 46556 Loss: 0.6965523958206177\n",
      "Iteration 46557 Loss: 0.9114764332771301\n",
      "Iteration 46558 Loss: 0.9096137285232544\n",
      "Iteration 46559 Loss: 0.7225411534309387\n",
      "Iteration 46559 Loss: 0.8911617398262024\n",
      "Iteration 46560 Loss: 0.8855936527252197\n",
      "Iteration 46561 Loss: 0.8165178298950195\n",
      "Iteration 46562 Loss: 0.8727718591690063\n",
      "Iteration 46563 Loss: 0.8260844945907593\n",
      "Iteration 46564 Loss: 0.681262195110321\n",
      "Iteration 46565 Loss: 0.9447426795959473\n",
      "Iteration 46566 Loss: 0.94449383020401\n",
      "Iteration 46567 Loss: 0.9346783757209778\n",
      "Iteration 46568 Loss: 0.729630172252655\n",
      "Iteration 46569 Loss: 0.6956866979598999\n",
      "Iteration 46569 Loss: 0.8331462144851685\n",
      "Iteration 46570 Loss: 1.171004295349121\n",
      "Iteration 46571 Loss: 1.2024049758911133\n",
      "Iteration 46572 Loss: 0.9434661865234375\n",
      "Iteration 46573 Loss: 0.7574927806854248\n",
      "Iteration 46574 Loss: 0.8076973557472229\n",
      "Iteration 46575 Loss: 0.7106700539588928\n",
      "Iteration 46576 Loss: 1.023348331451416\n",
      "Iteration 46577 Loss: 0.9913410544395447\n",
      "Iteration 46578 Loss: 1.1740002632141113\n",
      "Iteration 46579 Loss: 1.0917893648147583\n",
      "Iteration 46579 Loss: 0.9873213768005371\n",
      "Iteration 46580 Loss: 1.0909980535507202\n",
      "Iteration 46581 Loss: 0.8359678983688354\n",
      "Iteration 46582 Loss: 0.9648551940917969\n",
      "Iteration 46583 Loss: 0.8606112599372864\n",
      "Iteration 46584 Loss: 1.0234835147857666\n",
      "Iteration 46585 Loss: 0.8728013038635254\n",
      "Iteration 46586 Loss: 0.8944870829582214\n",
      "Iteration 46587 Loss: 0.8357686996459961\n",
      "Iteration 46588 Loss: 0.9907093644142151\n",
      "Iteration 46589 Loss: 1.1082992553710938\n",
      "Iteration 46589 Loss: 0.9477981328964233\n",
      "Iteration 46590 Loss: 0.9777700304985046\n",
      "Iteration 46591 Loss: 0.8734005093574524\n",
      "Iteration 46592 Loss: 0.6563920378684998\n",
      "Iteration 46593 Loss: 0.6695830225944519\n",
      "Iteration 46594 Loss: 1.0207343101501465\n",
      "Iteration 46595 Loss: 1.0037739276885986\n",
      "Iteration 46596 Loss: 0.7531782388687134\n",
      "Iteration 46597 Loss: 0.9692012667655945\n",
      "Iteration 46598 Loss: 1.3873220682144165\n",
      "Iteration 46599 Loss: 1.1648285388946533\n",
      "Iteration 46599 Loss: 0.9476183652877808\n",
      "Iteration 46600 Loss: 1.1161094903945923\n",
      "Iteration 46601 Loss: 1.0987778902053833\n",
      "Iteration 46602 Loss: 0.9967314004898071\n",
      "Iteration 46603 Loss: 1.214684247970581\n",
      "Iteration 46604 Loss: 0.9173445701599121\n",
      "Iteration 46605 Loss: 0.9999952912330627\n",
      "Iteration 46606 Loss: 1.0721800327301025\n",
      "Iteration 46607 Loss: 0.7519745230674744\n",
      "Iteration 46608 Loss: 0.6787616014480591\n",
      "Iteration 46609 Loss: 0.944908618927002\n",
      "Iteration 46609 Loss: 0.9791467785835266\n",
      "Iteration 46610 Loss: 0.8792052268981934\n",
      "Iteration 46611 Loss: 0.8019178509712219\n",
      "Iteration 46612 Loss: 1.1193844079971313\n",
      "Iteration 46613 Loss: 0.7361329197883606\n",
      "Iteration 46614 Loss: 0.9886838793754578\n",
      "Iteration 46615 Loss: 0.8961860537528992\n",
      "Iteration 46616 Loss: 1.142317533493042\n",
      "Iteration 46617 Loss: 0.7257939577102661\n",
      "Iteration 46618 Loss: 0.7629488706588745\n",
      "Iteration 46619 Loss: 0.9901861548423767\n",
      "Iteration 46619 Loss: 0.9042755961418152\n",
      "Iteration 46620 Loss: 0.8962584733963013\n",
      "Iteration 46621 Loss: 1.1964390277862549\n",
      "Iteration 46622 Loss: 1.1153960227966309\n",
      "Iteration 46623 Loss: 0.9947830438613892\n",
      "Iteration 46624 Loss: 1.142922043800354\n",
      "Iteration 46625 Loss: 1.1429399251937866\n",
      "Iteration 46626 Loss: 0.9166927337646484\n",
      "Iteration 46627 Loss: 1.1545623540878296\n",
      "Iteration 46628 Loss: 1.041196346282959\n",
      "Iteration 46629 Loss: 1.3076531887054443\n",
      "Iteration 46629 Loss: 1.0908843278884888\n",
      "Iteration 46630 Loss: 0.8388552069664001\n",
      "Iteration 46631 Loss: 1.1722331047058105\n",
      "Iteration 46632 Loss: 0.8882459402084351\n",
      "Iteration 46633 Loss: 1.1026394367218018\n",
      "Iteration 46634 Loss: 0.7752081155776978\n",
      "Iteration 46635 Loss: 0.7615231275558472\n",
      "Iteration 46636 Loss: 0.9254610538482666\n",
      "Iteration 46637 Loss: 0.8709023594856262\n",
      "Iteration 46638 Loss: 0.7801888585090637\n",
      "Iteration 46639 Loss: 0.9989250898361206\n",
      "Iteration 46639 Loss: 0.9114182591438293\n",
      "Iteration 46640 Loss: 1.130101203918457\n",
      "Iteration 46641 Loss: 1.1800378561019897\n",
      "Iteration 46642 Loss: 0.9327307939529419\n",
      "Iteration 46643 Loss: 0.9081010818481445\n",
      "Iteration 46644 Loss: 0.9940988421440125\n",
      "Iteration 46645 Loss: 0.976426899433136\n",
      "Iteration 46646 Loss: 1.3151479959487915\n",
      "Iteration 46647 Loss: 1.0322034358978271\n",
      "Iteration 46648 Loss: 0.759735643863678\n",
      "Iteration 46649 Loss: 0.7042807340621948\n",
      "Iteration 46649 Loss: 0.9932864904403687\n",
      "Iteration 46650 Loss: 0.7039087414741516\n",
      "Iteration 46651 Loss: 0.7902399897575378\n",
      "Iteration 46652 Loss: 0.9845498204231262\n",
      "Iteration 46653 Loss: 0.85638427734375\n",
      "Iteration 46654 Loss: 0.8958100080490112\n",
      "Iteration 46655 Loss: 0.7910463809967041\n",
      "Iteration 46656 Loss: 1.0534331798553467\n",
      "Iteration 46657 Loss: 0.972630500793457\n",
      "Iteration 46658 Loss: 0.8845426440238953\n",
      "Iteration 46659 Loss: 0.8683692812919617\n",
      "Iteration 46659 Loss: 0.8800915479660034\n",
      "Iteration 46660 Loss: 1.1818695068359375\n",
      "Iteration 46661 Loss: 1.06299889087677\n",
      "Iteration 46662 Loss: 1.0475986003875732\n",
      "Iteration 46663 Loss: 0.8522528409957886\n",
      "Iteration 46664 Loss: 1.0667952299118042\n",
      "Iteration 46665 Loss: 1.1684138774871826\n",
      "Iteration 46666 Loss: 0.9171726703643799\n",
      "Iteration 46667 Loss: 1.1160712242126465\n",
      "Iteration 46668 Loss: 0.9548080563545227\n",
      "Iteration 46669 Loss: 0.8549706339836121\n",
      "Iteration 46669 Loss: 1.0222952365875244\n",
      "Iteration 46670 Loss: 0.8344935774803162\n",
      "Iteration 46671 Loss: 0.9138168096542358\n",
      "Iteration 46672 Loss: 0.889092206954956\n",
      "Iteration 46673 Loss: 0.94080650806427\n",
      "Iteration 46674 Loss: 0.7868556976318359\n",
      "Iteration 46675 Loss: 0.8252488374710083\n",
      "Iteration 46676 Loss: 1.123818039894104\n",
      "Iteration 46677 Loss: 0.6264898180961609\n",
      "Iteration 46678 Loss: 1.1116851568222046\n",
      "Iteration 46679 Loss: 1.2153414487838745\n",
      "Iteration 46679 Loss: 0.9267647862434387\n",
      "Iteration 46680 Loss: 0.7932833433151245\n",
      "Iteration 46681 Loss: 0.9555088877677917\n",
      "Iteration 46682 Loss: 0.8537928462028503\n",
      "Iteration 46683 Loss: 1.3107600212097168\n",
      "Iteration 46684 Loss: 1.1451218128204346\n",
      "Iteration 46685 Loss: 0.9464058876037598\n",
      "Iteration 46686 Loss: 1.127851963043213\n",
      "Iteration 46687 Loss: 0.7792754173278809\n",
      "Iteration 46688 Loss: 0.8601157665252686\n",
      "Iteration 46689 Loss: 1.402834415435791\n",
      "Iteration 46689 Loss: 1.0174949169158936\n",
      "Iteration 46690 Loss: 0.9211102724075317\n",
      "Iteration 46691 Loss: 1.1202641725540161\n",
      "Iteration 46692 Loss: 1.0291961431503296\n",
      "Iteration 46693 Loss: 1.1288368701934814\n",
      "Iteration 46694 Loss: 1.0528233051300049\n",
      "Iteration 46695 Loss: 0.8270984292030334\n",
      "Iteration 46696 Loss: 0.9701822996139526\n",
      "Iteration 46697 Loss: 1.0028162002563477\n",
      "Iteration 46698 Loss: 1.2266618013381958\n",
      "Iteration 46699 Loss: 1.1442325115203857\n",
      "Iteration 46699 Loss: 1.0423222780227661\n",
      "Iteration 46700 Loss: 1.0739601850509644\n",
      "Iteration 46701 Loss: 0.8175597786903381\n",
      "Iteration 46702 Loss: 1.0508885383605957\n",
      "Iteration 46703 Loss: 1.0387704372406006\n",
      "Iteration 46704 Loss: 1.1715145111083984\n",
      "Iteration 46705 Loss: 0.6982707977294922\n",
      "Iteration 46706 Loss: 0.5079988241195679\n",
      "Iteration 46707 Loss: 0.9360094666481018\n",
      "Iteration 46708 Loss: 1.1030848026275635\n",
      "Iteration 46709 Loss: 1.086061716079712\n",
      "Iteration 46709 Loss: 0.9484118223190308\n",
      "Iteration 46710 Loss: 0.9278652667999268\n",
      "Iteration 46711 Loss: 1.1331517696380615\n",
      "Iteration 46712 Loss: 0.8962754607200623\n",
      "Iteration 46713 Loss: 1.0861847400665283\n",
      "Iteration 46714 Loss: 0.745140016078949\n",
      "Iteration 46715 Loss: 0.8241326212882996\n",
      "Iteration 46716 Loss: 0.8041703701019287\n",
      "Iteration 46717 Loss: 0.7525089383125305\n",
      "Iteration 46718 Loss: 0.6950836181640625\n",
      "Iteration 46719 Loss: 1.143189787864685\n",
      "Iteration 46719 Loss: 0.9007701873779297\n",
      "Iteration 46720 Loss: 0.9232561588287354\n",
      "Iteration 46721 Loss: 0.8135219216346741\n",
      "Iteration 46722 Loss: 1.1861661672592163\n",
      "Iteration 46723 Loss: 1.0610318183898926\n",
      "Iteration 46724 Loss: 0.9752159118652344\n",
      "Iteration 46725 Loss: 1.149389624595642\n",
      "Iteration 46726 Loss: 0.898178219795227\n",
      "Iteration 46727 Loss: 0.41222551465034485\n",
      "Iteration 46728 Loss: 0.8213870525360107\n",
      "Iteration 46729 Loss: 1.1406314373016357\n",
      "Iteration 46729 Loss: 0.9381003379821777\n",
      "Iteration 46730 Loss: 1.2443856000900269\n",
      "Iteration 46731 Loss: 0.7754653692245483\n",
      "Iteration 46732 Loss: 0.9979947805404663\n",
      "Iteration 46733 Loss: 1.0773690938949585\n",
      "Iteration 46734 Loss: 1.1205726861953735\n",
      "Iteration 46735 Loss: 0.9611648321151733\n",
      "Iteration 46736 Loss: 1.094247579574585\n",
      "Iteration 46737 Loss: 0.7141273021697998\n",
      "Iteration 46738 Loss: 0.9666067361831665\n",
      "Iteration 46739 Loss: 0.673463761806488\n",
      "Iteration 46739 Loss: 0.9625398516654968\n",
      "Iteration 46740 Loss: 1.0153138637542725\n",
      "Iteration 46741 Loss: 0.7485259771347046\n",
      "Iteration 46742 Loss: 1.1761891841888428\n",
      "Iteration 46743 Loss: 0.9666371941566467\n",
      "Iteration 46744 Loss: 0.9680600166320801\n",
      "Iteration 46745 Loss: 1.0815656185150146\n",
      "Iteration 46746 Loss: 0.8938759565353394\n",
      "Iteration 46747 Loss: 0.8828628659248352\n",
      "Iteration 46748 Loss: 1.014470100402832\n",
      "Iteration 46749 Loss: 0.972427487373352\n",
      "Iteration 46749 Loss: 0.9719928503036499\n",
      "Iteration 46750 Loss: 0.6906760931015015\n",
      "Iteration 46751 Loss: 1.0539740324020386\n",
      "Iteration 46752 Loss: 0.8942630290985107\n",
      "Iteration 46753 Loss: 0.928632378578186\n",
      "Iteration 46754 Loss: 1.0011146068572998\n",
      "Iteration 46755 Loss: 0.7831750512123108\n",
      "Iteration 46756 Loss: 0.783038318157196\n",
      "Iteration 46757 Loss: 0.6336315870285034\n",
      "Iteration 46758 Loss: 1.0136913061141968\n",
      "Iteration 46759 Loss: 0.960233211517334\n",
      "Iteration 46759 Loss: 0.8742429614067078\n",
      "Iteration 46760 Loss: 0.9428001642227173\n",
      "Iteration 46761 Loss: 0.8764858245849609\n",
      "Iteration 46762 Loss: 0.7403213381767273\n",
      "Iteration 46763 Loss: 0.868720531463623\n",
      "Iteration 46764 Loss: 1.0548176765441895\n",
      "Iteration 46765 Loss: 0.9074080586433411\n",
      "Iteration 46766 Loss: 1.1000596284866333\n",
      "Iteration 46767 Loss: 1.2018195390701294\n",
      "Iteration 46768 Loss: 0.9310235977172852\n",
      "Iteration 46769 Loss: 1.3210234642028809\n",
      "Iteration 46769 Loss: 0.9944480061531067\n",
      "Iteration 46770 Loss: 0.8704990148544312\n",
      "Iteration 46771 Loss: 0.7961536049842834\n",
      "Iteration 46772 Loss: 1.1160120964050293\n",
      "Iteration 46773 Loss: 0.9308754205703735\n",
      "Iteration 46774 Loss: 0.9771807193756104\n",
      "Iteration 46775 Loss: 0.8716849088668823\n",
      "Iteration 46776 Loss: 1.149310827255249\n",
      "Iteration 46777 Loss: 0.8652850985527039\n",
      "Iteration 46778 Loss: 0.9678603410720825\n",
      "Iteration 46779 Loss: 0.7905403971672058\n",
      "Iteration 46779 Loss: 0.9335402250289917\n",
      "Iteration 46780 Loss: 1.0674787759780884\n",
      "Iteration 46781 Loss: 0.9911777377128601\n",
      "Iteration 46782 Loss: 1.0361695289611816\n",
      "Iteration 46783 Loss: 1.143065333366394\n",
      "Iteration 46784 Loss: 0.9121783375740051\n",
      "Iteration 46785 Loss: 1.111603856086731\n",
      "Iteration 46786 Loss: 0.9055623412132263\n",
      "Iteration 46787 Loss: 0.82846599817276\n",
      "Iteration 46788 Loss: 0.7644472718238831\n",
      "Iteration 46789 Loss: 1.1386228799819946\n",
      "Iteration 46789 Loss: 0.9898773431777954\n",
      "Iteration 46790 Loss: 0.8882138133049011\n",
      "Iteration 46791 Loss: 1.1096603870391846\n",
      "Iteration 46792 Loss: 0.8846589922904968\n",
      "Iteration 46793 Loss: 0.6574731469154358\n",
      "Iteration 46794 Loss: 1.0156517028808594\n",
      "Iteration 46795 Loss: 1.1237324476242065\n",
      "Iteration 46796 Loss: 0.6624873280525208\n",
      "Iteration 46797 Loss: 1.0514730215072632\n",
      "Iteration 46798 Loss: 0.8070821762084961\n",
      "Iteration 46799 Loss: 0.9739285707473755\n",
      "Iteration 46799 Loss: 0.9174361228942871\n",
      "Iteration 46800 Loss: 1.1022529602050781\n",
      "Iteration 46801 Loss: 1.091659426689148\n",
      "Iteration 46802 Loss: 0.8593707084655762\n",
      "Iteration 46803 Loss: 0.5903297066688538\n",
      "Iteration 46804 Loss: 1.0975042581558228\n",
      "Iteration 46805 Loss: 0.8914949297904968\n",
      "Iteration 46806 Loss: 0.9982121586799622\n",
      "Iteration 46807 Loss: 0.9946862459182739\n",
      "Iteration 46808 Loss: 0.8041151762008667\n",
      "Iteration 46809 Loss: 0.7797145843505859\n",
      "Iteration 46809 Loss: 0.9209340214729309\n",
      "Iteration 46810 Loss: 0.6977605819702148\n",
      "Iteration 46811 Loss: 0.8512246608734131\n",
      "Iteration 46812 Loss: 0.9631638526916504\n",
      "Iteration 46813 Loss: 0.8004605770111084\n",
      "Iteration 46814 Loss: 0.7440283298492432\n",
      "Iteration 46815 Loss: 0.8475801348686218\n",
      "Iteration 46816 Loss: 1.3775495290756226\n",
      "Iteration 46817 Loss: 0.9196383357048035\n",
      "Iteration 46818 Loss: 0.859129786491394\n",
      "Iteration 46819 Loss: 0.922990083694458\n",
      "Iteration 46819 Loss: 0.8983525037765503\n",
      "Iteration 46820 Loss: 0.8274762630462646\n",
      "Iteration 46821 Loss: 1.051296353340149\n",
      "Iteration 46822 Loss: 0.7101435661315918\n",
      "Iteration 46823 Loss: 0.7135351300239563\n",
      "Iteration 46824 Loss: 0.9963544011116028\n",
      "Iteration 46825 Loss: 1.014041781425476\n",
      "Iteration 46826 Loss: 1.350777506828308\n",
      "Iteration 46827 Loss: 0.8344454169273376\n",
      "Iteration 46828 Loss: 1.1339863538742065\n",
      "Iteration 46829 Loss: 0.735040545463562\n",
      "Iteration 46829 Loss: 0.9367097020149231\n",
      "Iteration 46830 Loss: 0.652224063873291\n",
      "Iteration 46831 Loss: 0.9194494485855103\n",
      "Iteration 46832 Loss: 1.022387981414795\n",
      "Iteration 46833 Loss: 1.068493127822876\n",
      "Iteration 46834 Loss: 0.8194537162780762\n",
      "Iteration 46835 Loss: 0.9999334812164307\n",
      "Iteration 46836 Loss: 0.6733905673027039\n",
      "Iteration 46837 Loss: 1.1382578611373901\n",
      "Iteration 46838 Loss: 1.1153230667114258\n",
      "Iteration 46839 Loss: 1.0272026062011719\n",
      "Iteration 46839 Loss: 0.9436116218566895\n",
      "Iteration 46840 Loss: 1.0371781587600708\n",
      "Iteration 46841 Loss: 0.8739515542984009\n",
      "Iteration 46842 Loss: 1.1166789531707764\n",
      "Iteration 46843 Loss: 0.7010114192962646\n",
      "Iteration 46844 Loss: 0.8966712355613708\n",
      "Iteration 46845 Loss: 1.1950513124465942\n",
      "Iteration 46846 Loss: 1.2137452363967896\n",
      "Iteration 46847 Loss: 0.9102421998977661\n",
      "Iteration 46848 Loss: 0.8194267153739929\n",
      "Iteration 46849 Loss: 1.1921724081039429\n",
      "Iteration 46849 Loss: 0.9956129193305969\n",
      "Iteration 46850 Loss: 0.7551649212837219\n",
      "Iteration 46851 Loss: 1.3541123867034912\n",
      "Iteration 46852 Loss: 0.9441415667533875\n",
      "Iteration 46853 Loss: 1.0351170301437378\n",
      "Iteration 46854 Loss: 0.8670335412025452\n",
      "Iteration 46855 Loss: 1.184980869293213\n",
      "Iteration 46856 Loss: 0.9621984958648682\n",
      "Iteration 46857 Loss: 1.0149012804031372\n",
      "Iteration 46858 Loss: 1.0501153469085693\n",
      "Iteration 46859 Loss: 1.311917781829834\n",
      "Iteration 46859 Loss: 1.0479682683944702\n",
      "Iteration 46860 Loss: 0.627606987953186\n",
      "Iteration 46861 Loss: 0.8900890946388245\n",
      "Iteration 46862 Loss: 1.056821346282959\n",
      "Iteration 46863 Loss: 0.8180733323097229\n",
      "Iteration 46864 Loss: 1.2420272827148438\n",
      "Iteration 46865 Loss: 0.9886073470115662\n",
      "Iteration 46866 Loss: 0.9111469984054565\n",
      "Iteration 46867 Loss: 0.7147610187530518\n",
      "Iteration 46868 Loss: 0.7891508936882019\n",
      "Iteration 46869 Loss: 0.9467754364013672\n",
      "Iteration 46869 Loss: 0.8985059857368469\n",
      "Iteration 46870 Loss: 1.1508088111877441\n",
      "Iteration 46871 Loss: 1.2024887800216675\n",
      "Iteration 46872 Loss: 0.8364148139953613\n",
      "Iteration 46873 Loss: 0.9408270716667175\n",
      "Iteration 46874 Loss: 0.720658004283905\n",
      "Iteration 46875 Loss: 1.0682754516601562\n",
      "Iteration 46876 Loss: 0.8750163316726685\n",
      "Iteration 46877 Loss: 1.1489530801773071\n",
      "Iteration 46878 Loss: 1.2112222909927368\n",
      "Iteration 46879 Loss: 0.8589079976081848\n",
      "Iteration 46879 Loss: 1.0013573169708252\n",
      "Iteration 46880 Loss: 0.69721519947052\n",
      "Iteration 46881 Loss: 0.7752484083175659\n",
      "Iteration 46882 Loss: 0.8554127812385559\n",
      "Iteration 46883 Loss: 0.9016955494880676\n",
      "Iteration 46884 Loss: 0.9972165822982788\n",
      "Iteration 46885 Loss: 1.1108185052871704\n",
      "Iteration 46886 Loss: 0.9519055485725403\n",
      "Iteration 46887 Loss: 1.214469313621521\n",
      "Iteration 46888 Loss: 0.8712115287780762\n",
      "Iteration 46889 Loss: 0.8223884105682373\n",
      "Iteration 46889 Loss: 0.9197582006454468\n",
      "Iteration 46890 Loss: 0.7725399732589722\n",
      "Iteration 46891 Loss: 1.1596299409866333\n",
      "Iteration 46892 Loss: 1.0942891836166382\n",
      "Iteration 46893 Loss: 0.5946778655052185\n",
      "Iteration 46894 Loss: 1.1173069477081299\n",
      "Iteration 46895 Loss: 1.1179721355438232\n",
      "Iteration 46896 Loss: 0.5804780721664429\n",
      "Iteration 46897 Loss: 1.106679916381836\n",
      "Iteration 46898 Loss: 1.1254353523254395\n",
      "Iteration 46899 Loss: 0.8867300748825073\n",
      "Iteration 46899 Loss: 0.9555739164352417\n",
      "Iteration 46900 Loss: 0.9764934182167053\n",
      "Iteration 46901 Loss: 0.7682598233222961\n",
      "Iteration 46902 Loss: 0.8987250328063965\n",
      "Iteration 46903 Loss: 1.1457901000976562\n",
      "Iteration 46904 Loss: 0.8624180555343628\n",
      "Iteration 46905 Loss: 0.9715500473976135\n",
      "Iteration 46906 Loss: 1.0228244066238403\n",
      "Iteration 46907 Loss: 1.0379204750061035\n",
      "Iteration 46908 Loss: 0.983132541179657\n",
      "Iteration 46909 Loss: 1.2646123170852661\n",
      "Iteration 46909 Loss: 0.9931726455688477\n",
      "Iteration 46910 Loss: 0.8410672545433044\n",
      "Iteration 46911 Loss: 0.8382645845413208\n",
      "Iteration 46912 Loss: 0.8731575608253479\n",
      "Iteration 46913 Loss: 1.0299999713897705\n",
      "Iteration 46914 Loss: 1.1812705993652344\n",
      "Iteration 46915 Loss: 0.923670768737793\n",
      "Iteration 46916 Loss: 1.2419642210006714\n",
      "Iteration 46917 Loss: 0.8476349711418152\n",
      "Iteration 46918 Loss: 1.1010557413101196\n",
      "Iteration 46919 Loss: 0.49568063020706177\n",
      "Iteration 46919 Loss: 0.9373766779899597\n",
      "Iteration 46920 Loss: 0.7469146847724915\n",
      "Iteration 46921 Loss: 1.1504348516464233\n",
      "Iteration 46922 Loss: 0.5833192467689514\n",
      "Iteration 46923 Loss: 1.07489812374115\n",
      "Iteration 46924 Loss: 1.1146126985549927\n",
      "Iteration 46925 Loss: 0.9250775575637817\n",
      "Iteration 46926 Loss: 1.089335322380066\n",
      "Iteration 46927 Loss: 0.991114616394043\n",
      "Iteration 46928 Loss: 0.9597353935241699\n",
      "Iteration 46929 Loss: 0.9934357404708862\n",
      "Iteration 46929 Loss: 0.9628877639770508\n",
      "Iteration 46930 Loss: 1.1646909713745117\n",
      "Iteration 46931 Loss: 0.5733893513679504\n",
      "Iteration 46932 Loss: 0.9050004482269287\n",
      "Iteration 46933 Loss: 1.215121865272522\n",
      "Iteration 46934 Loss: 0.9062815308570862\n",
      "Iteration 46935 Loss: 0.9811863899230957\n",
      "Iteration 46936 Loss: 0.7328742146492004\n",
      "Iteration 46937 Loss: 1.1225237846374512\n",
      "Iteration 46938 Loss: 0.8814071416854858\n",
      "Iteration 46939 Loss: 1.0895872116088867\n",
      "Iteration 46939 Loss: 0.9572063684463501\n",
      "Iteration 46940 Loss: 1.006780743598938\n",
      "Iteration 46941 Loss: 0.7919800877571106\n",
      "Iteration 46942 Loss: 0.9719277024269104\n",
      "Iteration 46943 Loss: 0.7786179780960083\n",
      "Iteration 46944 Loss: 0.7135809659957886\n",
      "Iteration 46945 Loss: 1.159209966659546\n",
      "Iteration 46946 Loss: 0.8740543723106384\n",
      "Iteration 46947 Loss: 0.7769402265548706\n",
      "Iteration 46948 Loss: 0.7240528464317322\n",
      "Iteration 46949 Loss: 0.797722339630127\n",
      "Iteration 46949 Loss: 0.8594866991043091\n",
      "Iteration 46950 Loss: 0.9388885498046875\n",
      "Iteration 46951 Loss: 0.7697299718856812\n",
      "Iteration 46952 Loss: 0.9347796440124512\n",
      "Iteration 46953 Loss: 0.736522376537323\n",
      "Iteration 46954 Loss: 0.8784840106964111\n",
      "Iteration 46955 Loss: 0.8811138272285461\n",
      "Iteration 46956 Loss: 0.6352004408836365\n",
      "Iteration 46957 Loss: 1.1096136569976807\n",
      "Iteration 46958 Loss: 1.0745208263397217\n",
      "Iteration 46959 Loss: 1.01887047290802\n",
      "Iteration 46959 Loss: 0.8977724313735962\n",
      "Iteration 46960 Loss: 0.9629361033439636\n",
      "Iteration 46961 Loss: 1.218916893005371\n",
      "Iteration 46962 Loss: 0.7573602795600891\n",
      "Iteration 46963 Loss: 1.033414602279663\n",
      "Iteration 46964 Loss: 0.9717611074447632\n",
      "Iteration 46965 Loss: 0.8942638039588928\n",
      "Iteration 46966 Loss: 1.0074745416641235\n",
      "Iteration 46967 Loss: 0.8778089284896851\n",
      "Iteration 46968 Loss: 0.8785207867622375\n",
      "Iteration 46969 Loss: 1.0295498371124268\n",
      "Iteration 46969 Loss: 0.9632007479667664\n",
      "Iteration 46970 Loss: 0.863804042339325\n",
      "Iteration 46971 Loss: 1.108114242553711\n",
      "Iteration 46972 Loss: 1.0938736200332642\n",
      "Iteration 46973 Loss: 0.8490694165229797\n",
      "Iteration 46974 Loss: 0.9540446996688843\n",
      "Iteration 46975 Loss: 1.291983723640442\n",
      "Iteration 46976 Loss: 0.7490587830543518\n",
      "Iteration 46977 Loss: 0.6633203625679016\n",
      "Iteration 46978 Loss: 1.0808039903640747\n",
      "Iteration 46979 Loss: 1.1763038635253906\n",
      "Iteration 46979 Loss: 0.9830376505851746\n",
      "Iteration 46980 Loss: 0.7379519939422607\n",
      "Iteration 46981 Loss: 0.769780695438385\n",
      "Iteration 46982 Loss: 0.689608633518219\n",
      "Iteration 46983 Loss: 0.9962238669395447\n",
      "Iteration 46984 Loss: 0.8219265937805176\n",
      "Iteration 46985 Loss: 1.0876520872116089\n",
      "Iteration 46986 Loss: 1.0020138025283813\n",
      "Iteration 46987 Loss: 0.7389838695526123\n",
      "Iteration 46988 Loss: 1.0331647396087646\n",
      "Iteration 46989 Loss: 1.0731747150421143\n",
      "Iteration 46989 Loss: 0.8950481414794922\n",
      "Iteration 46990 Loss: 1.1089069843292236\n",
      "Iteration 46991 Loss: 0.9852860569953918\n",
      "Iteration 46992 Loss: 1.0212502479553223\n",
      "Iteration 46993 Loss: 0.9601166248321533\n",
      "Iteration 46994 Loss: 1.297041416168213\n",
      "Iteration 46995 Loss: 0.9782240986824036\n",
      "Iteration 46996 Loss: 0.9549858570098877\n",
      "Iteration 46997 Loss: 1.1199136972427368\n",
      "Iteration 46998 Loss: 1.1685330867767334\n",
      "Iteration 46999 Loss: 1.138305425643921\n",
      "Iteration 46999 Loss: 1.0732563734054565\n",
      "Iteration 47000 Loss: 0.7866560220718384\n",
      "Iteration 47001 Loss: 0.9525901675224304\n",
      "Iteration 47002 Loss: 0.962297797203064\n",
      "Iteration 47003 Loss: 1.11283540725708\n",
      "Iteration 47004 Loss: 1.0880411863327026\n",
      "Iteration 47005 Loss: 0.6689810156822205\n",
      "Iteration 47006 Loss: 0.8399825096130371\n",
      "Iteration 47007 Loss: 0.9753134250640869\n",
      "Iteration 47008 Loss: 0.9465317726135254\n",
      "Iteration 47009 Loss: 0.7362877130508423\n",
      "Iteration 47009 Loss: 0.9069517254829407\n",
      "Iteration 47010 Loss: 1.0478248596191406\n",
      "Iteration 47011 Loss: 0.8659897446632385\n",
      "Iteration 47012 Loss: 0.5192928910255432\n",
      "Iteration 47013 Loss: 1.2647945880889893\n",
      "Iteration 47014 Loss: 1.0456361770629883\n",
      "Iteration 47015 Loss: 0.8298598527908325\n",
      "Iteration 47016 Loss: 0.842626690864563\n",
      "Iteration 47017 Loss: 0.8982973098754883\n",
      "Iteration 47018 Loss: 0.7906759977340698\n",
      "Iteration 47019 Loss: 0.827921450138092\n",
      "Iteration 47019 Loss: 0.8932919502258301\n",
      "Iteration 47020 Loss: 1.240270972251892\n",
      "Iteration 47021 Loss: 0.9280986785888672\n",
      "Iteration 47022 Loss: 1.2331570386886597\n",
      "Iteration 47023 Loss: 1.2848175764083862\n",
      "Iteration 47024 Loss: 0.880108654499054\n",
      "Iteration 47025 Loss: 1.1092194318771362\n",
      "Iteration 47026 Loss: 1.005784273147583\n",
      "Iteration 47027 Loss: 0.7963376641273499\n",
      "Iteration 47028 Loss: 0.9518985748291016\n",
      "Iteration 47029 Loss: 0.7860537171363831\n",
      "Iteration 47029 Loss: 1.0215747356414795\n",
      "Iteration 47030 Loss: 1.07834792137146\n",
      "Iteration 47031 Loss: 0.8591077923774719\n",
      "Iteration 47032 Loss: 0.9218133091926575\n",
      "Iteration 47033 Loss: 0.9348695874214172\n",
      "Iteration 47034 Loss: 0.6421095132827759\n",
      "Iteration 47035 Loss: 0.7776304483413696\n",
      "Iteration 47036 Loss: 1.0528478622436523\n",
      "Iteration 47037 Loss: 0.8131425976753235\n",
      "Iteration 47038 Loss: 0.8227249979972839\n",
      "Iteration 47039 Loss: 1.042103886604309\n",
      "Iteration 47039 Loss: 0.8944698572158813\n",
      "Iteration 47040 Loss: 0.9418474435806274\n",
      "Iteration 47041 Loss: 1.2477285861968994\n",
      "Iteration 47042 Loss: 0.880169689655304\n",
      "Iteration 47043 Loss: 0.8752574920654297\n",
      "Iteration 47044 Loss: 1.0086588859558105\n",
      "Iteration 47045 Loss: 1.0353206396102905\n",
      "Iteration 47046 Loss: 1.0811126232147217\n",
      "Iteration 47047 Loss: 1.180002212524414\n",
      "Iteration 47048 Loss: 1.058732509613037\n",
      "Iteration 47049 Loss: 0.8758212327957153\n",
      "Iteration 47049 Loss: 1.0184651613235474\n",
      "Iteration 47050 Loss: 1.0300006866455078\n",
      "Iteration 47051 Loss: 0.9847000241279602\n",
      "Iteration 47052 Loss: 1.0336354970932007\n",
      "Iteration 47053 Loss: 0.6632239818572998\n",
      "Iteration 47054 Loss: 0.8359066247940063\n",
      "Iteration 47055 Loss: 1.0470836162567139\n",
      "Iteration 47056 Loss: 0.9925765991210938\n",
      "Iteration 47057 Loss: 0.9688655138015747\n",
      "Iteration 47058 Loss: 1.1932698488235474\n",
      "Iteration 47059 Loss: 0.7925883531570435\n",
      "Iteration 47059 Loss: 0.9541850090026855\n",
      "Iteration 47060 Loss: 1.0490610599517822\n",
      "Iteration 47061 Loss: 1.011939287185669\n",
      "Iteration 47062 Loss: 0.7759193181991577\n",
      "Iteration 47063 Loss: 0.8236069679260254\n",
      "Iteration 47064 Loss: 1.1584854125976562\n",
      "Iteration 47065 Loss: 0.7839590311050415\n",
      "Iteration 47066 Loss: 1.0496277809143066\n",
      "Iteration 47067 Loss: 0.7806280255317688\n",
      "Iteration 47068 Loss: 0.9562287926673889\n",
      "Iteration 47069 Loss: 1.2646526098251343\n",
      "Iteration 47069 Loss: 0.9654108285903931\n",
      "Iteration 47070 Loss: 1.0331746339797974\n",
      "Iteration 47071 Loss: 0.8299567699432373\n",
      "Iteration 47072 Loss: 1.0987493991851807\n",
      "Iteration 47073 Loss: 0.8270646929740906\n",
      "Iteration 47074 Loss: 0.7732366323471069\n",
      "Iteration 47075 Loss: 0.8125072717666626\n",
      "Iteration 47076 Loss: 0.5719311833381653\n",
      "Iteration 47077 Loss: 0.8684160709381104\n",
      "Iteration 47078 Loss: 1.096866250038147\n",
      "Iteration 47079 Loss: 1.259361982345581\n",
      "Iteration 47079 Loss: 0.9171263575553894\n",
      "Iteration 47080 Loss: 0.8092007637023926\n",
      "Iteration 47081 Loss: 0.8601131439208984\n",
      "Iteration 47082 Loss: 0.8043299317359924\n",
      "Iteration 47083 Loss: 1.1812516450881958\n",
      "Iteration 47084 Loss: 0.8917632102966309\n",
      "Iteration 47085 Loss: 1.026637077331543\n",
      "Iteration 47086 Loss: 0.6935094594955444\n",
      "Iteration 47087 Loss: 0.7609857320785522\n",
      "Iteration 47088 Loss: 1.1113262176513672\n",
      "Iteration 47089 Loss: 1.0348418951034546\n",
      "Iteration 47089 Loss: 0.9173958897590637\n",
      "Iteration 47090 Loss: 0.9963564276695251\n",
      "Iteration 47091 Loss: 0.6132213473320007\n",
      "Iteration 47092 Loss: 0.9665140509605408\n",
      "Iteration 47093 Loss: 0.8233253359794617\n",
      "Iteration 47094 Loss: 0.8717201352119446\n",
      "Iteration 47095 Loss: 0.6909483075141907\n",
      "Iteration 47096 Loss: 0.9546914100646973\n",
      "Iteration 47097 Loss: 0.9044647216796875\n",
      "Iteration 47098 Loss: 0.9805207252502441\n",
      "Iteration 47099 Loss: 0.9329099059104919\n",
      "Iteration 47099 Loss: 0.8734672665596008\n",
      "Iteration 47100 Loss: 0.5586763620376587\n",
      "Iteration 47101 Loss: 1.217354416847229\n",
      "Iteration 47102 Loss: 1.005760669708252\n",
      "Iteration 47103 Loss: 1.2211737632751465\n",
      "Iteration 47104 Loss: 1.2082589864730835\n",
      "Iteration 47105 Loss: 1.0136406421661377\n",
      "Iteration 47106 Loss: 1.240372657775879\n",
      "Iteration 47107 Loss: 0.9243242740631104\n",
      "Iteration 47108 Loss: 1.0773307085037231\n",
      "Iteration 47109 Loss: 1.2457709312438965\n",
      "Iteration 47109 Loss: 1.0712662935256958\n",
      "Iteration 47110 Loss: 1.051925778388977\n",
      "Iteration 47111 Loss: 0.828565239906311\n",
      "Iteration 47112 Loss: 1.0490471124649048\n",
      "Iteration 47113 Loss: 0.7643218040466309\n",
      "Iteration 47114 Loss: 0.8489975333213806\n",
      "Iteration 47115 Loss: 0.6479203104972839\n",
      "Iteration 47116 Loss: 1.1238120794296265\n",
      "Iteration 47117 Loss: 1.0800639390945435\n",
      "Iteration 47118 Loss: 0.7653307914733887\n",
      "Iteration 47119 Loss: 0.7930599451065063\n",
      "Iteration 47119 Loss: 0.8953045010566711\n",
      "Iteration 47120 Loss: 0.8057137131690979\n",
      "Iteration 47121 Loss: 0.8262098431587219\n",
      "Iteration 47122 Loss: 1.1059316396713257\n",
      "Iteration 47123 Loss: 1.0016710758209229\n",
      "Iteration 47124 Loss: 0.8465370535850525\n",
      "Iteration 47125 Loss: 1.1665115356445312\n",
      "Iteration 47126 Loss: 0.7611988186836243\n",
      "Iteration 47127 Loss: 1.1633284091949463\n",
      "Iteration 47128 Loss: 0.8997278809547424\n",
      "Iteration 47129 Loss: 0.8921855092048645\n",
      "Iteration 47129 Loss: 0.9469015002250671\n",
      "Iteration 47130 Loss: 0.9349051713943481\n",
      "Iteration 47131 Loss: 0.8427505493164062\n",
      "Iteration 47132 Loss: 1.0434834957122803\n",
      "Iteration 47133 Loss: 0.8530870079994202\n",
      "Iteration 47134 Loss: 0.9630692005157471\n",
      "Iteration 47135 Loss: 0.7002036571502686\n",
      "Iteration 47136 Loss: 1.3707165718078613\n",
      "Iteration 47137 Loss: 0.9289526343345642\n",
      "Iteration 47138 Loss: 0.8848031163215637\n",
      "Iteration 47139 Loss: 0.8224990963935852\n",
      "Iteration 47139 Loss: 0.9344469904899597\n",
      "Iteration 47140 Loss: 1.0563921928405762\n",
      "Iteration 47141 Loss: 0.6805468201637268\n",
      "Iteration 47142 Loss: 1.134426474571228\n",
      "Iteration 47143 Loss: 1.0698423385620117\n",
      "Iteration 47144 Loss: 0.8735623955726624\n",
      "Iteration 47145 Loss: 0.8475528955459595\n",
      "Iteration 47146 Loss: 1.1100198030471802\n",
      "Iteration 47147 Loss: 0.8193197846412659\n",
      "Iteration 47148 Loss: 0.910407543182373\n",
      "Iteration 47149 Loss: 0.8665699362754822\n",
      "Iteration 47149 Loss: 0.9368640184402466\n",
      "Iteration 47150 Loss: 1.1186167001724243\n",
      "Iteration 47151 Loss: 0.5126417279243469\n",
      "Iteration 47152 Loss: 0.9364356994628906\n",
      "Iteration 47153 Loss: 0.7866502404212952\n",
      "Iteration 47154 Loss: 1.2140134572982788\n",
      "Iteration 47155 Loss: 1.0081435441970825\n",
      "Iteration 47156 Loss: 0.6603443622589111\n",
      "Iteration 47157 Loss: 0.7421274185180664\n",
      "Iteration 47158 Loss: 1.0806646347045898\n",
      "Iteration 47159 Loss: 0.8485389351844788\n",
      "Iteration 47159 Loss: 0.8908176422119141\n",
      "Iteration 47160 Loss: 0.8586944341659546\n",
      "Iteration 47161 Loss: 1.0239129066467285\n",
      "Iteration 47162 Loss: 0.9081317782402039\n",
      "Iteration 47163 Loss: 0.9502738118171692\n",
      "Iteration 47164 Loss: 1.2075588703155518\n",
      "Iteration 47165 Loss: 0.9289695024490356\n",
      "Iteration 47166 Loss: 0.7571340203285217\n",
      "Iteration 47167 Loss: 0.8333938121795654\n",
      "Iteration 47168 Loss: 1.3191794157028198\n",
      "Iteration 47169 Loss: 0.9604232907295227\n",
      "Iteration 47169 Loss: 0.9747672080993652\n",
      "Iteration 47170 Loss: 1.0978355407714844\n",
      "Iteration 47171 Loss: 0.8260973691940308\n",
      "Iteration 47172 Loss: 0.6766552329063416\n",
      "Iteration 47173 Loss: 0.931818425655365\n",
      "Iteration 47174 Loss: 0.7448881268501282\n",
      "Iteration 47175 Loss: 1.2218914031982422\n",
      "Iteration 47176 Loss: 0.9379692673683167\n",
      "Iteration 47177 Loss: 0.5683027505874634\n",
      "Iteration 47178 Loss: 1.0082427263259888\n",
      "Iteration 47179 Loss: 1.0396653413772583\n",
      "Iteration 47179 Loss: 0.9053366780281067\n",
      "Iteration 47180 Loss: 0.9993265867233276\n",
      "Iteration 47181 Loss: 1.2649482488632202\n",
      "Iteration 47182 Loss: 0.9133350849151611\n",
      "Iteration 47183 Loss: 0.9029777646064758\n",
      "Iteration 47184 Loss: 1.063683271408081\n",
      "Iteration 47185 Loss: 1.2781932353973389\n",
      "Iteration 47186 Loss: 0.8703912496566772\n",
      "Iteration 47187 Loss: 1.1709851026535034\n",
      "Iteration 47188 Loss: 1.07073974609375\n",
      "Iteration 47189 Loss: 1.0850001573562622\n",
      "Iteration 47189 Loss: 1.0619580745697021\n",
      "Iteration 47190 Loss: 0.568598747253418\n",
      "Iteration 47191 Loss: 1.200770378112793\n",
      "Iteration 47192 Loss: 1.0646811723709106\n",
      "Iteration 47193 Loss: 0.9689704775810242\n",
      "Iteration 47194 Loss: 1.2712093591690063\n",
      "Iteration 47195 Loss: 0.841686487197876\n",
      "Iteration 47196 Loss: 0.9885766506195068\n",
      "Iteration 47197 Loss: 0.8523567914962769\n",
      "Iteration 47198 Loss: 0.9586118459701538\n",
      "Iteration 47199 Loss: 0.6033599376678467\n",
      "Iteration 47199 Loss: 0.9318822026252747\n",
      "Iteration 47200 Loss: 0.6698705554008484\n",
      "Iteration 47201 Loss: 1.2160487174987793\n",
      "Iteration 47202 Loss: 0.9212502241134644\n",
      "Iteration 47203 Loss: 1.1597092151641846\n",
      "Iteration 47204 Loss: 0.8953162431716919\n",
      "Iteration 47205 Loss: 0.8554810285568237\n",
      "Iteration 47206 Loss: 0.6881064772605896\n",
      "Iteration 47207 Loss: 1.3043779134750366\n",
      "Iteration 47208 Loss: 0.8852590918540955\n",
      "Iteration 47209 Loss: 0.7736203670501709\n",
      "Iteration 47209 Loss: 0.9369039535522461\n",
      "Iteration 47210 Loss: 1.104219675064087\n",
      "Iteration 47211 Loss: 0.609883725643158\n",
      "Iteration 47212 Loss: 1.1238363981246948\n",
      "Iteration 47213 Loss: 0.7847846746444702\n",
      "Iteration 47214 Loss: 1.1904833316802979\n",
      "Iteration 47215 Loss: 0.9901997447013855\n",
      "Iteration 47216 Loss: 0.9143223762512207\n",
      "Iteration 47217 Loss: 0.9655717611312866\n",
      "Iteration 47218 Loss: 0.8190395832061768\n",
      "Iteration 47219 Loss: 0.9288952946662903\n",
      "Iteration 47219 Loss: 0.9431236386299133\n",
      "Iteration 47220 Loss: 0.8335942029953003\n",
      "Iteration 47221 Loss: 0.926594078540802\n",
      "Iteration 47222 Loss: 1.2403112649917603\n",
      "Iteration 47223 Loss: 0.6513558626174927\n",
      "Iteration 47224 Loss: 0.900688886642456\n",
      "Iteration 47225 Loss: 0.8472867012023926\n",
      "Iteration 47226 Loss: 1.1360896825790405\n",
      "Iteration 47227 Loss: 0.6604540944099426\n",
      "Iteration 47228 Loss: 0.9014626145362854\n",
      "Iteration 47229 Loss: 1.1634520292282104\n",
      "Iteration 47229 Loss: 0.9261289834976196\n",
      "Iteration 47230 Loss: 0.9993658065795898\n",
      "Iteration 47231 Loss: 0.6640089154243469\n",
      "Iteration 47232 Loss: 1.0998777151107788\n",
      "Iteration 47233 Loss: 1.197953462600708\n",
      "Iteration 47234 Loss: 0.814591109752655\n",
      "Iteration 47235 Loss: 1.0971544981002808\n",
      "Iteration 47236 Loss: 0.9239407777786255\n",
      "Iteration 47237 Loss: 0.7197205424308777\n",
      "Iteration 47238 Loss: 0.8395804166793823\n",
      "Iteration 47239 Loss: 1.092881202697754\n",
      "Iteration 47239 Loss: 0.9449074864387512\n",
      "Iteration 47240 Loss: 0.791711151599884\n",
      "Iteration 47241 Loss: 0.9434781074523926\n",
      "Iteration 47242 Loss: 1.088805079460144\n",
      "Iteration 47243 Loss: 1.0133816003799438\n",
      "Iteration 47244 Loss: 1.0153472423553467\n",
      "Iteration 47245 Loss: 0.8154873251914978\n",
      "Iteration 47246 Loss: 1.107714295387268\n",
      "Iteration 47247 Loss: 1.0360273122787476\n",
      "Iteration 47248 Loss: 0.9315577745437622\n",
      "Iteration 47249 Loss: 0.8603255748748779\n",
      "Iteration 47249 Loss: 0.9603835344314575\n",
      "Iteration 47250 Loss: 0.74847412109375\n",
      "Iteration 47251 Loss: 0.8953056931495667\n",
      "Iteration 47252 Loss: 1.0298131704330444\n",
      "Iteration 47253 Loss: 1.0295792818069458\n",
      "Iteration 47254 Loss: 1.034550666809082\n",
      "Iteration 47255 Loss: 0.9008185267448425\n",
      "Iteration 47256 Loss: 0.8839275240898132\n",
      "Iteration 47257 Loss: 0.8567730188369751\n",
      "Iteration 47258 Loss: 1.1515288352966309\n",
      "Iteration 47259 Loss: 0.7934798002243042\n",
      "Iteration 47259 Loss: 0.9324251413345337\n",
      "Iteration 47260 Loss: 0.9646742343902588\n",
      "Iteration 47261 Loss: 1.3430321216583252\n",
      "Iteration 47262 Loss: 0.7487040758132935\n",
      "Iteration 47263 Loss: 0.8219006657600403\n",
      "Iteration 47264 Loss: 0.967826247215271\n",
      "Iteration 47265 Loss: 1.2010982036590576\n",
      "Iteration 47266 Loss: 1.1484886407852173\n",
      "Iteration 47267 Loss: 0.9488282203674316\n",
      "Iteration 47268 Loss: 0.9551899433135986\n",
      "Iteration 47269 Loss: 0.9776588082313538\n",
      "Iteration 47269 Loss: 1.0077402591705322\n",
      "Iteration 47270 Loss: 0.6795195937156677\n",
      "Iteration 47271 Loss: 1.006171464920044\n",
      "Iteration 47272 Loss: 1.1139761209487915\n",
      "Iteration 47273 Loss: 0.6717299818992615\n",
      "Iteration 47274 Loss: 1.1983883380889893\n",
      "Iteration 47275 Loss: 0.869307279586792\n",
      "Iteration 47276 Loss: 1.0598114728927612\n",
      "Iteration 47277 Loss: 0.9762804508209229\n",
      "Iteration 47278 Loss: 1.394486427307129\n",
      "Iteration 47279 Loss: 0.6311177611351013\n",
      "Iteration 47279 Loss: 0.9600788950920105\n",
      "Iteration 47280 Loss: 0.7866902351379395\n",
      "Iteration 47281 Loss: 1.1360276937484741\n",
      "Iteration 47282 Loss: 1.2020829916000366\n",
      "Iteration 47283 Loss: 0.8240396976470947\n",
      "Iteration 47284 Loss: 0.8408722877502441\n",
      "Iteration 47285 Loss: 0.9530907869338989\n",
      "Iteration 47286 Loss: 0.8394052386283875\n",
      "Iteration 47287 Loss: 0.8594273328781128\n",
      "Iteration 47288 Loss: 0.9572162628173828\n",
      "Iteration 47289 Loss: 0.8739964962005615\n",
      "Iteration 47289 Loss: 0.9272848963737488\n",
      "Iteration 47290 Loss: 0.7875425219535828\n",
      "Iteration 47291 Loss: 0.8798526525497437\n",
      "Iteration 47292 Loss: 0.8645948171615601\n",
      "Iteration 47293 Loss: 0.9839963316917419\n",
      "Iteration 47294 Loss: 0.7721080183982849\n",
      "Iteration 47295 Loss: 0.9179463982582092\n",
      "Iteration 47296 Loss: 0.7697620391845703\n",
      "Iteration 47297 Loss: 0.8412595391273499\n",
      "Iteration 47298 Loss: 0.8944653272628784\n",
      "Iteration 47299 Loss: 0.7036405205726624\n",
      "Iteration 47299 Loss: 0.8415168523788452\n",
      "Iteration 47300 Loss: 0.8174986839294434\n",
      "Iteration 47301 Loss: 0.9796127676963806\n",
      "Iteration 47302 Loss: 0.9676068425178528\n",
      "Iteration 47303 Loss: 0.9854414463043213\n",
      "Iteration 47304 Loss: 1.110754132270813\n",
      "Iteration 47305 Loss: 0.7743474841117859\n",
      "Iteration 47306 Loss: 0.8930081129074097\n",
      "Iteration 47307 Loss: 0.985061764717102\n",
      "Iteration 47308 Loss: 0.8993801474571228\n",
      "Iteration 47309 Loss: 0.668137788772583\n",
      "Iteration 47309 Loss: 0.9080848693847656\n",
      "Iteration 47310 Loss: 1.0528504848480225\n",
      "Iteration 47311 Loss: 0.8094767928123474\n",
      "Iteration 47312 Loss: 0.7122747898101807\n",
      "Iteration 47313 Loss: 1.0943981409072876\n",
      "Iteration 47314 Loss: 0.8633430004119873\n",
      "Iteration 47315 Loss: 0.9212552309036255\n",
      "Iteration 47316 Loss: 0.8034549355506897\n",
      "Iteration 47317 Loss: 0.8487116098403931\n",
      "Iteration 47318 Loss: 1.0910625457763672\n",
      "Iteration 47319 Loss: 0.9126384258270264\n",
      "Iteration 47319 Loss: 0.9109466671943665\n",
      "Iteration 47320 Loss: 1.0438950061798096\n",
      "Iteration 47321 Loss: 0.8892270922660828\n",
      "Iteration 47322 Loss: 0.7336534261703491\n",
      "Iteration 47323 Loss: 1.1556733846664429\n",
      "Iteration 47324 Loss: 0.9942222833633423\n",
      "Iteration 47325 Loss: 0.9942117929458618\n",
      "Iteration 47326 Loss: 0.7810829281806946\n",
      "Iteration 47327 Loss: 1.095768928527832\n",
      "Iteration 47328 Loss: 0.944778561592102\n",
      "Iteration 47329 Loss: 1.132087230682373\n",
      "Iteration 47329 Loss: 0.9764600992202759\n",
      "Iteration 47330 Loss: 0.9631637334823608\n",
      "Iteration 47331 Loss: 1.2844889163970947\n",
      "Iteration 47332 Loss: 1.0785540342330933\n",
      "Iteration 47333 Loss: 0.8851401209831238\n",
      "Iteration 47334 Loss: 0.9005311727523804\n",
      "Iteration 47335 Loss: 0.7655559182167053\n",
      "Iteration 47336 Loss: 0.8428928852081299\n",
      "Iteration 47337 Loss: 0.8848604559898376\n",
      "Iteration 47338 Loss: 0.6991381049156189\n",
      "Iteration 47339 Loss: 1.2195727825164795\n",
      "Iteration 47339 Loss: 0.9523898363113403\n",
      "Iteration 47340 Loss: 0.814745306968689\n",
      "Iteration 47341 Loss: 0.655839204788208\n",
      "Iteration 47342 Loss: 0.7261815071105957\n",
      "Iteration 47343 Loss: 0.8784193396568298\n",
      "Iteration 47344 Loss: 1.0180253982543945\n",
      "Iteration 47345 Loss: 0.5810663104057312\n",
      "Iteration 47346 Loss: 1.1159098148345947\n",
      "Iteration 47347 Loss: 1.0820590257644653\n",
      "Iteration 47348 Loss: 0.9748525619506836\n",
      "Iteration 47349 Loss: 0.9059450626373291\n",
      "Iteration 47349 Loss: 0.8753043413162231\n",
      "Iteration 47350 Loss: 0.6264806389808655\n",
      "Iteration 47351 Loss: 0.9966583847999573\n",
      "Iteration 47352 Loss: 0.7274724841117859\n",
      "Iteration 47353 Loss: 1.590118646621704\n",
      "Iteration 47354 Loss: 1.1844921112060547\n",
      "Iteration 47355 Loss: 1.018141269683838\n",
      "Iteration 47356 Loss: 0.8077107667922974\n",
      "Iteration 47357 Loss: 0.8110844492912292\n",
      "Iteration 47358 Loss: 0.7608456611633301\n",
      "Iteration 47359 Loss: 1.0331801176071167\n",
      "Iteration 47359 Loss: 0.9556185007095337\n",
      "Iteration 47360 Loss: 1.0801509618759155\n",
      "Iteration 47361 Loss: 1.1486133337020874\n",
      "Iteration 47362 Loss: 0.937651515007019\n",
      "Iteration 47363 Loss: 1.1322953701019287\n",
      "Iteration 47364 Loss: 0.9671459197998047\n",
      "Iteration 47365 Loss: 0.7872927188873291\n",
      "Iteration 47366 Loss: 0.5894981622695923\n",
      "Iteration 47367 Loss: 0.712895929813385\n",
      "Iteration 47368 Loss: 0.796388566493988\n",
      "Iteration 47369 Loss: 0.979141354560852\n",
      "Iteration 47369 Loss: 0.9131075143814087\n",
      "Iteration 47370 Loss: 0.8812204003334045\n",
      "Iteration 47371 Loss: 1.1871435642242432\n",
      "Iteration 47372 Loss: 0.933478832244873\n",
      "Iteration 47373 Loss: 1.122743010520935\n",
      "Iteration 47374 Loss: 0.6330085396766663\n",
      "Iteration 47375 Loss: 0.5404682159423828\n",
      "Iteration 47376 Loss: 1.0144848823547363\n",
      "Iteration 47377 Loss: 0.6102380156517029\n",
      "Iteration 47378 Loss: 0.7988792061805725\n",
      "Iteration 47379 Loss: 0.9672753214836121\n",
      "Iteration 47379 Loss: 0.8688939809799194\n",
      "Iteration 47380 Loss: 1.101690649986267\n",
      "Iteration 47381 Loss: 1.038024663925171\n",
      "Iteration 47382 Loss: 1.0888965129852295\n",
      "Iteration 47383 Loss: 1.124576210975647\n",
      "Iteration 47384 Loss: 0.9344779253005981\n",
      "Iteration 47385 Loss: 0.6466200947761536\n",
      "Iteration 47386 Loss: 0.5379101634025574\n",
      "Iteration 47387 Loss: 0.9916232824325562\n",
      "Iteration 47388 Loss: 0.8212828636169434\n",
      "Iteration 47389 Loss: 0.7093377113342285\n",
      "Iteration 47389 Loss: 0.8994439840316772\n",
      "Iteration 47390 Loss: 0.8314167857170105\n",
      "Iteration 47391 Loss: 0.7934767603874207\n",
      "Iteration 47392 Loss: 0.711047887802124\n",
      "Iteration 47393 Loss: 1.0087145566940308\n",
      "Iteration 47394 Loss: 1.1202079057693481\n",
      "Iteration 47395 Loss: 0.9691045880317688\n",
      "Iteration 47396 Loss: 0.8698511123657227\n",
      "Iteration 47397 Loss: 1.354946255683899\n",
      "Iteration 47398 Loss: 0.7992645502090454\n",
      "Iteration 47399 Loss: 0.9671763777732849\n",
      "Iteration 47399 Loss: 0.9425207376480103\n",
      "Iteration 47400 Loss: 0.8179448246955872\n",
      "Iteration 47401 Loss: 1.1307967901229858\n",
      "Iteration 47402 Loss: 1.1415234804153442\n",
      "Iteration 47403 Loss: 0.8784833550453186\n",
      "Iteration 47404 Loss: 0.7095232605934143\n",
      "Iteration 47405 Loss: 1.295003890991211\n",
      "Iteration 47406 Loss: 0.7297901511192322\n",
      "Iteration 47407 Loss: 1.0299361944198608\n",
      "Iteration 47408 Loss: 1.0679999589920044\n",
      "Iteration 47409 Loss: 0.8218177556991577\n",
      "Iteration 47409 Loss: 0.9622818827629089\n",
      "Iteration 47410 Loss: 0.9192724823951721\n",
      "Iteration 47411 Loss: 0.6878753304481506\n",
      "Iteration 47412 Loss: 1.1255279779434204\n",
      "Iteration 47413 Loss: 0.8584652543067932\n",
      "Iteration 47414 Loss: 1.1432478427886963\n",
      "Iteration 47415 Loss: 0.986629068851471\n",
      "Iteration 47416 Loss: 0.8195252418518066\n",
      "Iteration 47417 Loss: 1.0407631397247314\n",
      "Iteration 47418 Loss: 0.880504310131073\n",
      "Iteration 47419 Loss: 0.5355474352836609\n",
      "Iteration 47419 Loss: 0.8997357487678528\n",
      "Iteration 47420 Loss: 0.9634964466094971\n",
      "Iteration 47421 Loss: 1.1934844255447388\n",
      "Iteration 47422 Loss: 1.1055386066436768\n",
      "Iteration 47423 Loss: 1.06937837600708\n",
      "Iteration 47424 Loss: 0.7294302582740784\n",
      "Iteration 47425 Loss: 0.9463681578636169\n",
      "Iteration 47426 Loss: 1.051874041557312\n",
      "Iteration 47427 Loss: 1.235345482826233\n",
      "Iteration 47428 Loss: 0.8575459718704224\n",
      "Iteration 47429 Loss: 0.8875002264976501\n",
      "Iteration 47429 Loss: 1.0039961338043213\n",
      "Iteration 47430 Loss: 1.0272679328918457\n",
      "Iteration 47431 Loss: 0.9255702495574951\n",
      "Iteration 47432 Loss: 0.7744203209877014\n",
      "Iteration 47433 Loss: 0.9798737168312073\n",
      "Iteration 47434 Loss: 0.8657128810882568\n",
      "Iteration 47435 Loss: 1.0445560216903687\n",
      "Iteration 47436 Loss: 0.7150387167930603\n",
      "Iteration 47437 Loss: 1.2735519409179688\n",
      "Iteration 47438 Loss: 0.9705320000648499\n",
      "Iteration 47439 Loss: 1.0247267484664917\n",
      "Iteration 47439 Loss: 0.9601250886917114\n",
      "Iteration 47440 Loss: 0.7690570950508118\n",
      "Iteration 47441 Loss: 0.9660232663154602\n",
      "Iteration 47442 Loss: 0.7237934470176697\n",
      "Iteration 47443 Loss: 1.338200330734253\n",
      "Iteration 47444 Loss: 1.2081458568572998\n",
      "Iteration 47445 Loss: 0.8520379662513733\n",
      "Iteration 47446 Loss: 0.9290385246276855\n",
      "Iteration 47447 Loss: 0.8829421401023865\n",
      "Iteration 47448 Loss: 1.0658247470855713\n",
      "Iteration 47449 Loss: 0.8221243619918823\n",
      "Iteration 47449 Loss: 0.9557188153266907\n",
      "Iteration 47450 Loss: 0.8883008360862732\n",
      "Iteration 47451 Loss: 0.9609991908073425\n",
      "Iteration 47452 Loss: 0.9677563309669495\n",
      "Iteration 47453 Loss: 0.9943439960479736\n",
      "Iteration 47454 Loss: 0.8760999441146851\n",
      "Iteration 47455 Loss: 0.9737248420715332\n",
      "Iteration 47456 Loss: 0.9530594944953918\n",
      "Iteration 47457 Loss: 0.5012243986129761\n",
      "Iteration 47458 Loss: 1.0041561126708984\n",
      "Iteration 47459 Loss: 1.0898035764694214\n",
      "Iteration 47459 Loss: 0.9209468960762024\n",
      "Iteration 47460 Loss: 0.9783156514167786\n",
      "Iteration 47461 Loss: 1.0757420063018799\n",
      "Iteration 47462 Loss: 1.131820559501648\n",
      "Iteration 47463 Loss: 0.827263593673706\n",
      "Iteration 47464 Loss: 0.6897961497306824\n",
      "Iteration 47465 Loss: 1.1387261152267456\n",
      "Iteration 47466 Loss: 1.0835992097854614\n",
      "Iteration 47467 Loss: 0.9475317597389221\n",
      "Iteration 47468 Loss: 0.962096095085144\n",
      "Iteration 47469 Loss: 1.0047540664672852\n",
      "Iteration 47469 Loss: 0.9839645624160767\n",
      "Iteration 47470 Loss: 0.9242623448371887\n",
      "Iteration 47471 Loss: 1.0654635429382324\n",
      "Iteration 47472 Loss: 0.832785964012146\n",
      "Iteration 47473 Loss: 0.9334903359413147\n",
      "Iteration 47474 Loss: 0.7936350107192993\n",
      "Iteration 47475 Loss: 0.8691186904907227\n",
      "Iteration 47476 Loss: 0.7864742279052734\n",
      "Iteration 47477 Loss: 1.0219533443450928\n",
      "Iteration 47478 Loss: 0.9469981789588928\n",
      "Iteration 47479 Loss: 1.1829087734222412\n",
      "Iteration 47479 Loss: 0.9357089996337891\n",
      "Iteration 47480 Loss: 1.0945732593536377\n",
      "Iteration 47481 Loss: 0.7965971827507019\n",
      "Iteration 47482 Loss: 1.595223069190979\n",
      "Iteration 47483 Loss: 1.1137022972106934\n",
      "Iteration 47484 Loss: 0.8291894793510437\n",
      "Iteration 47485 Loss: 1.2314420938491821\n",
      "Iteration 47486 Loss: 1.1268028020858765\n",
      "Iteration 47487 Loss: 0.976498007774353\n",
      "Iteration 47488 Loss: 0.9821198582649231\n",
      "Iteration 47489 Loss: 0.7424871921539307\n",
      "Iteration 47489 Loss: 1.048863410949707\n",
      "Iteration 47490 Loss: 1.230577826499939\n",
      "Iteration 47491 Loss: 0.9177838563919067\n",
      "Iteration 47492 Loss: 1.0553536415100098\n",
      "Iteration 47493 Loss: 1.1684563159942627\n",
      "Iteration 47494 Loss: 1.3259527683258057\n",
      "Iteration 47495 Loss: 0.9905474781990051\n",
      "Iteration 47496 Loss: 0.7944603562355042\n",
      "Iteration 47497 Loss: 0.8496690392494202\n",
      "Iteration 47498 Loss: 1.0787169933319092\n",
      "Iteration 47499 Loss: 0.9452277421951294\n",
      "Iteration 47499 Loss: 1.0356745719909668\n",
      "Iteration 47500 Loss: 0.9497144222259521\n",
      "Iteration 47501 Loss: 0.7471275925636292\n",
      "Iteration 47502 Loss: 0.8187373280525208\n",
      "Iteration 47503 Loss: 0.8806535601615906\n",
      "Iteration 47504 Loss: 0.8320567011833191\n",
      "Iteration 47505 Loss: 1.0848243236541748\n",
      "Iteration 47506 Loss: 1.0760445594787598\n",
      "Iteration 47507 Loss: 1.553404688835144\n",
      "Iteration 47508 Loss: 0.7329462170600891\n",
      "Iteration 47509 Loss: 1.020047664642334\n",
      "Iteration 47509 Loss: 0.9695557355880737\n",
      "Iteration 47510 Loss: 1.087433099746704\n",
      "Iteration 47511 Loss: 0.9210870862007141\n",
      "Iteration 47512 Loss: 0.9011711478233337\n",
      "Iteration 47513 Loss: 1.3162873983383179\n",
      "Iteration 47514 Loss: 0.8868811726570129\n",
      "Iteration 47515 Loss: 1.1250947713851929\n",
      "Iteration 47516 Loss: 0.9649105668067932\n",
      "Iteration 47517 Loss: 1.1447944641113281\n",
      "Iteration 47518 Loss: 1.19723641872406\n",
      "Iteration 47519 Loss: 0.9338093400001526\n",
      "Iteration 47519 Loss: 1.0478705167770386\n",
      "Iteration 47520 Loss: 0.8696974515914917\n",
      "Iteration 47521 Loss: 0.9043139815330505\n",
      "Iteration 47522 Loss: 1.031765103340149\n",
      "Iteration 47523 Loss: 0.9894452691078186\n",
      "Iteration 47524 Loss: 0.8081016540527344\n",
      "Iteration 47525 Loss: 0.8923352956771851\n",
      "Iteration 47526 Loss: 0.8282861113548279\n",
      "Iteration 47527 Loss: 1.0504045486450195\n",
      "Iteration 47528 Loss: 0.8901554942131042\n",
      "Iteration 47529 Loss: 0.8089088797569275\n",
      "Iteration 47529 Loss: 0.9073413610458374\n",
      "Iteration 47530 Loss: 1.1320292949676514\n",
      "Iteration 47531 Loss: 0.6505203247070312\n",
      "Iteration 47532 Loss: 0.8624590039253235\n",
      "Iteration 47533 Loss: 0.6569027900695801\n",
      "Iteration 47534 Loss: 1.0342485904693604\n",
      "Iteration 47535 Loss: 1.2530404329299927\n",
      "Iteration 47536 Loss: 1.0574491024017334\n",
      "Iteration 47537 Loss: 1.011871099472046\n",
      "Iteration 47538 Loss: 0.7025642991065979\n",
      "Iteration 47539 Loss: 0.7551071047782898\n",
      "Iteration 47539 Loss: 0.9116193056106567\n",
      "Iteration 47540 Loss: 1.0084174871444702\n",
      "Iteration 47541 Loss: 1.1590931415557861\n",
      "Iteration 47542 Loss: 0.8074683547019958\n",
      "Iteration 47543 Loss: 0.7450269460678101\n",
      "Iteration 47544 Loss: 1.2922582626342773\n",
      "Iteration 47545 Loss: 1.0180459022521973\n",
      "Iteration 47546 Loss: 0.9784550070762634\n",
      "Iteration 47547 Loss: 1.1235750913619995\n",
      "Iteration 47548 Loss: 1.0311392545700073\n",
      "Iteration 47549 Loss: 0.41419360041618347\n",
      "Iteration 47549 Loss: 0.9577673077583313\n",
      "Iteration 47550 Loss: 0.896096408367157\n",
      "Iteration 47551 Loss: 0.802629292011261\n",
      "Iteration 47552 Loss: 0.9289232492446899\n",
      "Iteration 47553 Loss: 0.8867769241333008\n",
      "Iteration 47554 Loss: 0.8503088355064392\n",
      "Iteration 47555 Loss: 1.182053804397583\n",
      "Iteration 47556 Loss: 0.987260639667511\n",
      "Iteration 47557 Loss: 0.9097937345504761\n",
      "Iteration 47558 Loss: 0.9920915961265564\n",
      "Iteration 47559 Loss: 1.0483099222183228\n",
      "Iteration 47559 Loss: 0.9484244585037231\n",
      "Iteration 47560 Loss: 0.836559534072876\n",
      "Iteration 47561 Loss: 0.9975917339324951\n",
      "Iteration 47562 Loss: 1.2064570188522339\n",
      "Iteration 47563 Loss: 1.3391233682632446\n",
      "Iteration 47564 Loss: 1.0512874126434326\n",
      "Iteration 47565 Loss: 0.8068402409553528\n",
      "Iteration 47566 Loss: 1.0705631971359253\n",
      "Iteration 47567 Loss: 0.9040395021438599\n",
      "Iteration 47568 Loss: 0.9609648585319519\n",
      "Iteration 47569 Loss: 0.9523833990097046\n",
      "Iteration 47569 Loss: 1.0125809907913208\n",
      "Iteration 47570 Loss: 1.03736412525177\n",
      "Iteration 47571 Loss: 1.01042640209198\n",
      "Iteration 47572 Loss: 0.7785322070121765\n",
      "Iteration 47573 Loss: 0.9652909636497498\n",
      "Iteration 47574 Loss: 0.9194115400314331\n",
      "Iteration 47575 Loss: 0.9859361052513123\n",
      "Iteration 47576 Loss: 0.44138792157173157\n",
      "Iteration 47577 Loss: 0.9683968424797058\n",
      "Iteration 47578 Loss: 0.9376252293586731\n",
      "Iteration 47579 Loss: 1.0516959428787231\n",
      "Iteration 47579 Loss: 0.9096067547798157\n",
      "Iteration 47580 Loss: 0.9878434538841248\n",
      "Iteration 47581 Loss: 0.792598307132721\n",
      "Iteration 47582 Loss: 1.072283148765564\n",
      "Iteration 47583 Loss: 0.8546661138534546\n",
      "Iteration 47584 Loss: 1.033504605293274\n",
      "Iteration 47585 Loss: 1.219496726989746\n",
      "Iteration 47586 Loss: 0.8697070479393005\n",
      "Iteration 47587 Loss: 1.0447099208831787\n",
      "Iteration 47588 Loss: 1.2176679372787476\n",
      "Iteration 47589 Loss: 0.7877854704856873\n",
      "Iteration 47589 Loss: 0.9880262613296509\n",
      "Iteration 47590 Loss: 1.069711685180664\n",
      "Iteration 47591 Loss: 1.082332968711853\n",
      "Iteration 47592 Loss: 1.0667328834533691\n",
      "Iteration 47593 Loss: 1.1589107513427734\n",
      "Iteration 47594 Loss: 1.1322613954544067\n",
      "Iteration 47595 Loss: 0.6842940449714661\n",
      "Iteration 47596 Loss: 0.864584743976593\n",
      "Iteration 47597 Loss: 1.1219940185546875\n",
      "Iteration 47598 Loss: 1.0837111473083496\n",
      "Iteration 47599 Loss: 0.8525879383087158\n",
      "Iteration 47599 Loss: 1.0117121934890747\n",
      "Iteration 47600 Loss: 1.0547295808792114\n",
      "Iteration 47601 Loss: 1.3038694858551025\n",
      "Iteration 47602 Loss: 1.2090171575546265\n",
      "Iteration 47603 Loss: 0.9857153296470642\n",
      "Iteration 47604 Loss: 0.8558664321899414\n",
      "Iteration 47605 Loss: 1.1158026456832886\n",
      "Iteration 47606 Loss: 0.9502293467521667\n",
      "Iteration 47607 Loss: 0.8016309142112732\n",
      "Iteration 47608 Loss: 0.9721729159355164\n",
      "Iteration 47609 Loss: 0.9308624863624573\n",
      "Iteration 47609 Loss: 1.0179897546768188\n",
      "Iteration 47610 Loss: 0.7025469541549683\n",
      "Iteration 47611 Loss: 0.8983191847801208\n",
      "Iteration 47612 Loss: 1.0359387397766113\n",
      "Iteration 47613 Loss: 1.036557674407959\n",
      "Iteration 47614 Loss: 0.8983354568481445\n",
      "Iteration 47615 Loss: 0.7070952653884888\n",
      "Iteration 47616 Loss: 1.0996760129928589\n",
      "Iteration 47617 Loss: 1.006615400314331\n",
      "Iteration 47618 Loss: 0.83195960521698\n",
      "Iteration 47619 Loss: 0.8847407698631287\n",
      "Iteration 47619 Loss: 0.9101784825325012\n",
      "Iteration 47620 Loss: 0.8100084066390991\n",
      "Iteration 47621 Loss: 0.7313041090965271\n",
      "Iteration 47622 Loss: 0.878572940826416\n",
      "Iteration 47623 Loss: 0.6290732622146606\n",
      "Iteration 47624 Loss: 0.6698758602142334\n",
      "Iteration 47625 Loss: 1.0146032571792603\n",
      "Iteration 47626 Loss: 0.7755917906761169\n",
      "Iteration 47627 Loss: 0.7061885595321655\n",
      "Iteration 47628 Loss: 0.7012547254562378\n",
      "Iteration 47629 Loss: 0.9656415581703186\n",
      "Iteration 47629 Loss: 0.788211464881897\n",
      "Iteration 47630 Loss: 1.0319510698318481\n",
      "Iteration 47631 Loss: 0.831836998462677\n",
      "Iteration 47632 Loss: 0.9873666763305664\n",
      "Iteration 47633 Loss: 0.689850389957428\n",
      "Iteration 47634 Loss: 1.047482967376709\n",
      "Iteration 47635 Loss: 0.8850492238998413\n",
      "Iteration 47636 Loss: 0.9564313292503357\n",
      "Iteration 47637 Loss: 0.7902620434761047\n",
      "Iteration 47638 Loss: 0.6687504649162292\n",
      "Iteration 47639 Loss: 0.6914451718330383\n",
      "Iteration 47639 Loss: 0.8580425977706909\n",
      "Iteration 47640 Loss: 0.8440248370170593\n",
      "Iteration 47641 Loss: 0.7550777792930603\n",
      "Iteration 47642 Loss: 1.2224938869476318\n",
      "Iteration 47643 Loss: 1.050880789756775\n",
      "Iteration 47644 Loss: 1.2316126823425293\n",
      "Iteration 47645 Loss: 0.7845625281333923\n",
      "Iteration 47646 Loss: 0.6273149847984314\n",
      "Iteration 47647 Loss: 0.8245250582695007\n",
      "Iteration 47648 Loss: 0.8638119101524353\n",
      "Iteration 47649 Loss: 1.0460724830627441\n",
      "Iteration 47649 Loss: 0.925037682056427\n",
      "Iteration 47650 Loss: 0.7166346907615662\n",
      "Iteration 47651 Loss: 1.0879491567611694\n",
      "Iteration 47652 Loss: 1.0796089172363281\n",
      "Iteration 47653 Loss: 0.6600590944290161\n",
      "Iteration 47654 Loss: 0.874702513217926\n",
      "Iteration 47655 Loss: 0.6084728240966797\n",
      "Iteration 47656 Loss: 0.9164656400680542\n",
      "Iteration 47657 Loss: 0.9070646166801453\n",
      "Iteration 47658 Loss: 0.9220508337020874\n",
      "Iteration 47659 Loss: 1.2084341049194336\n",
      "Iteration 47659 Loss: 0.8981442451477051\n",
      "Iteration 47660 Loss: 0.809837281703949\n",
      "Iteration 47661 Loss: 1.123252511024475\n",
      "Iteration 47662 Loss: 0.5898311734199524\n",
      "Iteration 47663 Loss: 1.07057523727417\n",
      "Iteration 47664 Loss: 0.8466147780418396\n",
      "Iteration 47665 Loss: 0.7028864026069641\n",
      "Iteration 47666 Loss: 0.7848352789878845\n",
      "Iteration 47667 Loss: 0.9546909332275391\n",
      "Iteration 47668 Loss: 0.7870995998382568\n",
      "Iteration 47669 Loss: 0.9962344765663147\n",
      "Iteration 47669 Loss: 0.8665858507156372\n",
      "Iteration 47670 Loss: 0.8483583331108093\n",
      "Iteration 47671 Loss: 0.9036644697189331\n",
      "Iteration 47672 Loss: 0.7160713076591492\n",
      "Iteration 47673 Loss: 0.8836700916290283\n",
      "Iteration 47674 Loss: 0.6474217176437378\n",
      "Iteration 47675 Loss: 0.9754700660705566\n",
      "Iteration 47676 Loss: 0.660873532295227\n",
      "Iteration 47677 Loss: 0.9030753374099731\n",
      "Iteration 47678 Loss: 1.007346749305725\n",
      "Iteration 47679 Loss: 0.9913927316665649\n",
      "Iteration 47679 Loss: 0.8537344932556152\n",
      "Iteration 47680 Loss: 0.5768631100654602\n",
      "Iteration 47681 Loss: 1.1909326314926147\n",
      "Iteration 47682 Loss: 1.1212794780731201\n",
      "Iteration 47683 Loss: 0.7181428670883179\n",
      "Iteration 47684 Loss: 0.7042782306671143\n",
      "Iteration 47685 Loss: 0.6110025644302368\n",
      "Iteration 47686 Loss: 0.8617474436759949\n",
      "Iteration 47687 Loss: 0.9390392303466797\n",
      "Iteration 47688 Loss: 0.8156044483184814\n",
      "Iteration 47689 Loss: 0.7706232070922852\n",
      "Iteration 47689 Loss: 0.8309513330459595\n",
      "Iteration 47690 Loss: 0.8144044280052185\n",
      "Iteration 47691 Loss: 0.5750795602798462\n",
      "Iteration 47692 Loss: 1.242721676826477\n",
      "Iteration 47693 Loss: 0.5785040855407715\n",
      "Iteration 47694 Loss: 0.7047640681266785\n",
      "Iteration 47695 Loss: 0.9070807695388794\n",
      "Iteration 47696 Loss: 0.9940302968025208\n",
      "Iteration 47697 Loss: 1.0366801023483276\n",
      "Iteration 47698 Loss: 0.5629157423973083\n",
      "Iteration 47699 Loss: 1.095563530921936\n",
      "Iteration 47699 Loss: 0.8511743545532227\n",
      "Iteration 47700 Loss: 0.9868460297584534\n",
      "Iteration 47701 Loss: 0.8311780095100403\n",
      "Iteration 47702 Loss: 0.81923508644104\n",
      "Iteration 47703 Loss: 0.9343574643135071\n",
      "Iteration 47704 Loss: 1.1707450151443481\n",
      "Iteration 47705 Loss: 1.0058574676513672\n",
      "Iteration 47706 Loss: 0.6143813133239746\n",
      "Iteration 47707 Loss: 0.722955584526062\n",
      "Iteration 47708 Loss: 0.9704610109329224\n",
      "Iteration 47709 Loss: 0.8575283885002136\n",
      "Iteration 47709 Loss: 0.8913545608520508\n",
      "Iteration 47710 Loss: 1.4081904888153076\n",
      "Iteration 47711 Loss: 1.1570841073989868\n",
      "Iteration 47712 Loss: 0.8083735704421997\n",
      "Iteration 47713 Loss: 0.6183376908302307\n",
      "Iteration 47714 Loss: 1.2089309692382812\n",
      "Iteration 47715 Loss: 0.8659420609474182\n",
      "Iteration 47716 Loss: 1.2935006618499756\n",
      "Iteration 47717 Loss: 0.7761908769607544\n",
      "Iteration 47718 Loss: 1.2161481380462646\n",
      "Iteration 47719 Loss: 1.1636205911636353\n",
      "Iteration 47719 Loss: 1.0516319274902344\n",
      "Iteration 47720 Loss: 1.2489206790924072\n",
      "Iteration 47721 Loss: 0.8643395900726318\n",
      "Iteration 47722 Loss: 0.903407096862793\n",
      "Iteration 47723 Loss: 0.7238103151321411\n",
      "Iteration 47724 Loss: 0.8788456916809082\n",
      "Iteration 47725 Loss: 1.2376545667648315\n",
      "Iteration 47726 Loss: 1.2006281614303589\n",
      "Iteration 47727 Loss: 0.8349144458770752\n",
      "Iteration 47728 Loss: 0.9162984490394592\n",
      "Iteration 47729 Loss: 1.1010308265686035\n",
      "Iteration 47729 Loss: 0.9909849166870117\n",
      "Iteration 47730 Loss: 0.7151378989219666\n",
      "Iteration 47731 Loss: 0.8220646977424622\n",
      "Iteration 47732 Loss: 1.1840704679489136\n",
      "Iteration 47733 Loss: 1.1453814506530762\n",
      "Iteration 47734 Loss: 1.1911152601242065\n",
      "Iteration 47735 Loss: 1.0557564496994019\n",
      "Iteration 47736 Loss: 1.2108709812164307\n",
      "Iteration 47737 Loss: 1.145440936088562\n",
      "Iteration 47738 Loss: 0.9082337021827698\n",
      "Iteration 47739 Loss: 0.9989657998085022\n",
      "Iteration 47739 Loss: 1.0377037525177002\n",
      "Iteration 47740 Loss: 0.8675680756568909\n",
      "Iteration 47741 Loss: 1.110734462738037\n",
      "Iteration 47742 Loss: 1.0361981391906738\n",
      "Iteration 47743 Loss: 1.0720018148422241\n",
      "Iteration 47744 Loss: 1.1093502044677734\n",
      "Iteration 47745 Loss: 1.0714472532272339\n",
      "Iteration 47746 Loss: 0.9290693998336792\n",
      "Iteration 47747 Loss: 1.0586003065109253\n",
      "Iteration 47748 Loss: 1.3182624578475952\n",
      "Iteration 47749 Loss: 0.9502442479133606\n",
      "Iteration 47749 Loss: 1.0523476600646973\n",
      "Iteration 47750 Loss: 0.805844783782959\n",
      "Iteration 47751 Loss: 1.2118935585021973\n",
      "Iteration 47752 Loss: 1.0485936403274536\n",
      "Iteration 47753 Loss: 1.2035603523254395\n",
      "Iteration 47754 Loss: 0.9275088906288147\n",
      "Iteration 47755 Loss: 0.7896945476531982\n",
      "Iteration 47756 Loss: 1.1398123502731323\n",
      "Iteration 47757 Loss: 0.9783907532691956\n",
      "Iteration 47758 Loss: 1.1614043712615967\n",
      "Iteration 47759 Loss: 0.9325170516967773\n",
      "Iteration 47759 Loss: 1.0199220180511475\n",
      "Iteration 47760 Loss: 0.9996405839920044\n",
      "Iteration 47761 Loss: 0.8831343054771423\n",
      "Iteration 47762 Loss: 0.835144579410553\n",
      "Iteration 47763 Loss: 0.9389288425445557\n",
      "Iteration 47764 Loss: 0.8829542398452759\n",
      "Iteration 47765 Loss: 0.9502618312835693\n",
      "Iteration 47766 Loss: 1.0655027627944946\n",
      "Iteration 47767 Loss: 0.8111099600791931\n",
      "Iteration 47768 Loss: 0.9361509680747986\n",
      "Iteration 47769 Loss: 1.0245883464813232\n",
      "Iteration 47769 Loss: 0.932741641998291\n",
      "Iteration 47770 Loss: 0.7483614087104797\n",
      "Iteration 47771 Loss: 1.0382263660430908\n",
      "Iteration 47772 Loss: 0.7872283458709717\n",
      "Iteration 47773 Loss: 1.0221843719482422\n",
      "Iteration 47774 Loss: 1.341399908065796\n",
      "Iteration 47775 Loss: 0.6810875535011292\n",
      "Iteration 47776 Loss: 0.5738525390625\n",
      "Iteration 47777 Loss: 0.7366617321968079\n",
      "Iteration 47778 Loss: 0.6555142402648926\n",
      "Iteration 47779 Loss: 0.9188739657402039\n",
      "Iteration 47779 Loss: 0.8503390550613403\n",
      "Iteration 47780 Loss: 0.8775428533554077\n",
      "Iteration 47781 Loss: 0.9023023843765259\n",
      "Iteration 47782 Loss: 1.0441933870315552\n",
      "Iteration 47783 Loss: 1.050410509109497\n",
      "Iteration 47784 Loss: 0.5151368379592896\n",
      "Iteration 47785 Loss: 0.8077378869056702\n",
      "Iteration 47786 Loss: 0.5298835039138794\n",
      "Iteration 47787 Loss: 0.874782919883728\n",
      "Iteration 47788 Loss: 0.8400394916534424\n",
      "Iteration 47789 Loss: 0.817310094833374\n",
      "Iteration 47789 Loss: 0.8259339332580566\n",
      "Iteration 47790 Loss: 0.6569159030914307\n",
      "Iteration 47791 Loss: 0.8558706641197205\n",
      "Iteration 47792 Loss: 0.6241096258163452\n",
      "Iteration 47793 Loss: 0.5129619240760803\n",
      "Iteration 47794 Loss: 0.8877409100532532\n",
      "Iteration 47795 Loss: 0.9778811931610107\n",
      "Iteration 47796 Loss: 0.8169553875923157\n",
      "Iteration 47797 Loss: 1.0642273426055908\n",
      "Iteration 47798 Loss: 1.1264477968215942\n",
      "Iteration 47799 Loss: 0.8534039258956909\n",
      "Iteration 47799 Loss: 0.8376514315605164\n",
      "Iteration 47800 Loss: 0.9274477958679199\n",
      "Iteration 47801 Loss: 0.8277904987335205\n",
      "Iteration 47802 Loss: 0.7745395302772522\n",
      "Iteration 47803 Loss: 1.0045108795166016\n",
      "Iteration 47804 Loss: 1.073527455329895\n",
      "Iteration 47805 Loss: 0.7435176372528076\n",
      "Iteration 47806 Loss: 1.2864298820495605\n",
      "Iteration 47807 Loss: 1.0608950853347778\n",
      "Iteration 47808 Loss: 1.186643362045288\n",
      "Iteration 47809 Loss: 0.5859519243240356\n",
      "Iteration 47809 Loss: 0.9471253156661987\n",
      "Iteration 47810 Loss: 0.8781412243843079\n",
      "Iteration 47811 Loss: 0.8881027102470398\n",
      "Iteration 47812 Loss: 1.0473887920379639\n",
      "Iteration 47813 Loss: 0.9954445362091064\n",
      "Iteration 47814 Loss: 1.128436803817749\n",
      "Iteration 47815 Loss: 0.7566011548042297\n",
      "Iteration 47816 Loss: 0.8011752963066101\n",
      "Iteration 47817 Loss: 1.0156970024108887\n",
      "Iteration 47818 Loss: 0.9263111352920532\n",
      "Iteration 47819 Loss: 0.9988654851913452\n",
      "Iteration 47819 Loss: 0.943616509437561\n",
      "Iteration 47820 Loss: 1.0099899768829346\n",
      "Iteration 47821 Loss: 0.9209210872650146\n",
      "Iteration 47822 Loss: 1.0616276264190674\n",
      "Iteration 47823 Loss: 1.047366976737976\n",
      "Iteration 47824 Loss: 1.3998188972473145\n",
      "Iteration 47825 Loss: 0.7699892520904541\n",
      "Iteration 47826 Loss: 0.7687052488327026\n",
      "Iteration 47827 Loss: 0.8914214372634888\n",
      "Iteration 47828 Loss: 0.7909735441207886\n",
      "Iteration 47829 Loss: 1.1085690259933472\n",
      "Iteration 47829 Loss: 0.9769383668899536\n",
      "Iteration 47830 Loss: 0.7582384347915649\n",
      "Iteration 47831 Loss: 0.8029385209083557\n",
      "Iteration 47832 Loss: 1.1387808322906494\n",
      "Iteration 47833 Loss: 0.6994885206222534\n",
      "Iteration 47834 Loss: 0.6424137353897095\n",
      "Iteration 47835 Loss: 0.7659070491790771\n",
      "Iteration 47836 Loss: 0.7932985424995422\n",
      "Iteration 47837 Loss: 1.1464213132858276\n",
      "Iteration 47838 Loss: 1.2470711469650269\n",
      "Iteration 47839 Loss: 0.9243070483207703\n",
      "Iteration 47839 Loss: 0.8918865323066711\n",
      "Iteration 47840 Loss: 0.9713955521583557\n",
      "Iteration 47841 Loss: 1.1670788526535034\n",
      "Iteration 47842 Loss: 0.6731747388839722\n",
      "Iteration 47843 Loss: 0.8359502553939819\n",
      "Iteration 47844 Loss: 0.7283229231834412\n",
      "Iteration 47845 Loss: 0.9818544983863831\n",
      "Iteration 47846 Loss: 1.027628779411316\n",
      "Iteration 47847 Loss: 0.9844207763671875\n",
      "Iteration 47848 Loss: 0.7255908250808716\n",
      "Iteration 47849 Loss: 0.9603396058082581\n",
      "Iteration 47849 Loss: 0.9055756330490112\n",
      "Iteration 47850 Loss: 1.0561068058013916\n",
      "Iteration 47851 Loss: 0.8291281461715698\n",
      "Iteration 47852 Loss: 1.0365549325942993\n",
      "Iteration 47853 Loss: 0.6694915294647217\n",
      "Iteration 47854 Loss: 0.9238959550857544\n",
      "Iteration 47855 Loss: 1.0524909496307373\n",
      "Iteration 47856 Loss: 0.5786071419715881\n",
      "Iteration 47857 Loss: 1.3511723279953003\n",
      "Iteration 47858 Loss: 0.9977181553840637\n",
      "Iteration 47859 Loss: 1.3628650903701782\n",
      "Iteration 47859 Loss: 0.9858031272888184\n",
      "Iteration 47860 Loss: 0.846511721611023\n",
      "Iteration 47861 Loss: 0.73587965965271\n",
      "Iteration 47862 Loss: 0.9337177872657776\n",
      "Iteration 47863 Loss: 1.0147535800933838\n",
      "Iteration 47864 Loss: 1.2825497388839722\n",
      "Iteration 47865 Loss: 1.1352641582489014\n",
      "Iteration 47866 Loss: 1.0968704223632812\n",
      "Iteration 47867 Loss: 0.7091695070266724\n",
      "Iteration 47868 Loss: 1.090269923210144\n",
      "Iteration 47869 Loss: 0.8462857604026794\n",
      "Iteration 47869 Loss: 0.9691272974014282\n",
      "Iteration 47870 Loss: 1.2026019096374512\n",
      "Iteration 47871 Loss: 0.7896236181259155\n",
      "Iteration 47872 Loss: 0.7368307113647461\n",
      "Iteration 47873 Loss: 1.1588479280471802\n",
      "Iteration 47874 Loss: 1.5686949491500854\n",
      "Iteration 47875 Loss: 0.8365799188613892\n",
      "Iteration 47876 Loss: 1.0771799087524414\n",
      "Iteration 47877 Loss: 0.9434077739715576\n",
      "Iteration 47878 Loss: 0.7154902815818787\n",
      "Iteration 47879 Loss: 0.822274386882782\n",
      "Iteration 47879 Loss: 0.985153079032898\n",
      "Iteration 47880 Loss: 1.0440609455108643\n",
      "Iteration 47881 Loss: 1.1459686756134033\n",
      "Iteration 47882 Loss: 0.7585372924804688\n",
      "Iteration 47883 Loss: 0.7127788662910461\n",
      "Iteration 47884 Loss: 1.0922757387161255\n",
      "Iteration 47885 Loss: 1.0409071445465088\n",
      "Iteration 47886 Loss: 1.3183163404464722\n",
      "Iteration 47887 Loss: 1.099530816078186\n",
      "Iteration 47888 Loss: 0.8926361203193665\n",
      "Iteration 47889 Loss: 0.8643439412117004\n",
      "Iteration 47889 Loss: 0.9969356656074524\n",
      "Iteration 47890 Loss: 0.8615137338638306\n",
      "Iteration 47891 Loss: 0.9446163773536682\n",
      "Iteration 47892 Loss: 0.6720312237739563\n",
      "Iteration 47893 Loss: 0.9620103240013123\n",
      "Iteration 47894 Loss: 0.8251226544380188\n",
      "Iteration 47895 Loss: 0.9347042441368103\n",
      "Iteration 47896 Loss: 0.7186927199363708\n",
      "Iteration 47897 Loss: 0.766372561454773\n",
      "Iteration 47898 Loss: 0.7037468552589417\n",
      "Iteration 47899 Loss: 0.962161660194397\n",
      "Iteration 47899 Loss: 0.8350973129272461\n",
      "Iteration 47900 Loss: 1.0857927799224854\n",
      "Iteration 47901 Loss: 0.9079543352127075\n",
      "Iteration 47902 Loss: 0.7836545705795288\n",
      "Iteration 47903 Loss: 0.7884702086448669\n",
      "Iteration 47904 Loss: 0.8087997436523438\n",
      "Iteration 47905 Loss: 1.1376452445983887\n",
      "Iteration 47906 Loss: 0.903664767742157\n",
      "Iteration 47907 Loss: 0.6899009346961975\n",
      "Iteration 47908 Loss: 1.1299190521240234\n",
      "Iteration 47909 Loss: 1.1537766456604004\n",
      "Iteration 47909 Loss: 0.9389578700065613\n",
      "Iteration 47910 Loss: 1.0312310457229614\n",
      "Iteration 47911 Loss: 0.8782030344009399\n",
      "Iteration 47912 Loss: 0.9436880350112915\n",
      "Iteration 47913 Loss: 0.8565054535865784\n",
      "Iteration 47914 Loss: 1.2475216388702393\n",
      "Iteration 47915 Loss: 0.7906914949417114\n",
      "Iteration 47916 Loss: 0.6510818600654602\n",
      "Iteration 47917 Loss: 0.7293696999549866\n",
      "Iteration 47918 Loss: 0.8733600974082947\n",
      "Iteration 47919 Loss: 0.7999771237373352\n",
      "Iteration 47919 Loss: 0.8801630139350891\n",
      "Iteration 47920 Loss: 1.1052857637405396\n",
      "Iteration 47921 Loss: 0.8328502178192139\n",
      "Iteration 47922 Loss: 0.7797824740409851\n",
      "Iteration 47923 Loss: 0.7864753007888794\n",
      "Iteration 47924 Loss: 0.8358076810836792\n",
      "Iteration 47925 Loss: 1.0516177415847778\n",
      "Iteration 47926 Loss: 1.029874563217163\n",
      "Iteration 47927 Loss: 0.6865776181221008\n",
      "Iteration 47928 Loss: 1.17142915725708\n",
      "Iteration 47929 Loss: 0.9669971466064453\n",
      "Iteration 47929 Loss: 0.9246697425842285\n",
      "Iteration 47930 Loss: 0.9411030411720276\n",
      "Iteration 47931 Loss: 1.0143505334854126\n",
      "Iteration 47932 Loss: 1.1278679370880127\n",
      "Iteration 47933 Loss: 0.8009430170059204\n",
      "Iteration 47934 Loss: 1.0082266330718994\n",
      "Iteration 47935 Loss: 0.8690786957740784\n",
      "Iteration 47936 Loss: 0.7429004907608032\n",
      "Iteration 47937 Loss: 1.0444320440292358\n",
      "Iteration 47938 Loss: 1.0321413278579712\n",
      "Iteration 47939 Loss: 0.8258062601089478\n",
      "Iteration 47939 Loss: 0.940684974193573\n",
      "Iteration 47940 Loss: 1.136094093322754\n",
      "Iteration 47941 Loss: 0.8650145530700684\n",
      "Iteration 47942 Loss: 1.0749413967132568\n",
      "Iteration 47943 Loss: 1.149733304977417\n",
      "Iteration 47944 Loss: 0.8575270771980286\n",
      "Iteration 47945 Loss: 0.6157902479171753\n",
      "Iteration 47946 Loss: 1.1772626638412476\n",
      "Iteration 47947 Loss: 0.8488188982009888\n",
      "Iteration 47948 Loss: 1.0754969120025635\n",
      "Iteration 47949 Loss: 1.0799649953842163\n",
      "Iteration 47949 Loss: 0.9880644679069519\n",
      "Iteration 47950 Loss: 1.313632845878601\n",
      "Iteration 47951 Loss: 0.9395701289176941\n",
      "Iteration 47952 Loss: 0.9526210427284241\n",
      "Iteration 47953 Loss: 0.9793155193328857\n",
      "Iteration 47954 Loss: 1.2831604480743408\n",
      "Iteration 47955 Loss: 1.024459719657898\n",
      "Iteration 47956 Loss: 1.029661774635315\n",
      "Iteration 47957 Loss: 1.226318359375\n",
      "Iteration 47958 Loss: 1.1661394834518433\n",
      "Iteration 47959 Loss: 0.9011407494544983\n",
      "Iteration 47959 Loss: 1.0816019773483276\n",
      "Iteration 47960 Loss: 0.765937089920044\n",
      "Iteration 47961 Loss: 0.8732057809829712\n",
      "Iteration 47962 Loss: 0.6404696106910706\n",
      "Iteration 47963 Loss: 0.4498148262500763\n",
      "Iteration 47964 Loss: 1.2437098026275635\n",
      "Iteration 47965 Loss: 1.0036342144012451\n",
      "Iteration 47966 Loss: 0.7195472121238708\n",
      "Iteration 47967 Loss: 0.632773220539093\n",
      "Iteration 47968 Loss: 0.9320685863494873\n",
      "Iteration 47969 Loss: 1.0956732034683228\n",
      "Iteration 47969 Loss: 0.8356834650039673\n",
      "Iteration 47970 Loss: 1.3788937330245972\n",
      "Iteration 47971 Loss: 0.8801009654998779\n",
      "Iteration 47972 Loss: 0.9270301461219788\n",
      "Iteration 47973 Loss: 0.7256643176078796\n",
      "Iteration 47974 Loss: 1.027293086051941\n",
      "Iteration 47975 Loss: 0.9737516045570374\n",
      "Iteration 47976 Loss: 0.9150559306144714\n",
      "Iteration 47977 Loss: 1.031628966331482\n",
      "Iteration 47978 Loss: 0.8909176588058472\n",
      "Iteration 47979 Loss: 0.9370306134223938\n",
      "Iteration 47979 Loss: 0.9687366485595703\n",
      "Iteration 47980 Loss: 1.066962718963623\n",
      "Iteration 47981 Loss: 0.9260830879211426\n",
      "Iteration 47982 Loss: 0.6249625086784363\n",
      "Iteration 47983 Loss: 0.8277606964111328\n",
      "Iteration 47984 Loss: 1.1346406936645508\n",
      "Iteration 47985 Loss: 0.8392284512519836\n",
      "Iteration 47986 Loss: 0.7627921104431152\n",
      "Iteration 47987 Loss: 0.9142635464668274\n",
      "Iteration 47988 Loss: 1.3100125789642334\n",
      "Iteration 47989 Loss: 1.1619170904159546\n",
      "Iteration 47989 Loss: 0.9568623304367065\n",
      "Iteration 47990 Loss: 1.0418568849563599\n",
      "Iteration 47991 Loss: 0.8603429794311523\n",
      "Iteration 47992 Loss: 0.6992453932762146\n",
      "Iteration 47993 Loss: 0.936775267124176\n",
      "Iteration 47994 Loss: 1.4176833629608154\n",
      "Iteration 47995 Loss: 0.7658765912055969\n",
      "Iteration 47996 Loss: 0.7409734129905701\n",
      "Iteration 47997 Loss: 0.7464824318885803\n",
      "Iteration 47998 Loss: 1.2265677452087402\n",
      "Iteration 47999 Loss: 0.9864336848258972\n",
      "Iteration 47999 Loss: 0.942223846912384\n",
      "Iteration 48000 Loss: 1.0393859148025513\n",
      "Iteration 48001 Loss: 0.7247341275215149\n",
      "Iteration 48002 Loss: 0.8004918098449707\n",
      "Iteration 48003 Loss: 0.8673251867294312\n",
      "Iteration 48004 Loss: 0.7936466336250305\n",
      "Iteration 48005 Loss: 1.3645679950714111\n",
      "Iteration 48006 Loss: 1.0427085161209106\n",
      "Iteration 48007 Loss: 1.1058741807937622\n",
      "Iteration 48008 Loss: 0.8347983956336975\n",
      "Iteration 48009 Loss: 1.0912591218948364\n",
      "Iteration 48009 Loss: 0.9664791822433472\n",
      "Iteration 48010 Loss: 0.8666481971740723\n",
      "Iteration 48011 Loss: 1.1082675457000732\n",
      "Iteration 48012 Loss: 0.8968946933746338\n",
      "Iteration 48013 Loss: 0.6605482697486877\n",
      "Iteration 48014 Loss: 0.8629500865936279\n",
      "Iteration 48015 Loss: 1.26939058303833\n",
      "Iteration 48016 Loss: 0.7834744453430176\n",
      "Iteration 48017 Loss: 0.8150213360786438\n",
      "Iteration 48018 Loss: 0.9079654812812805\n",
      "Iteration 48019 Loss: 0.9675420522689819\n",
      "Iteration 48019 Loss: 0.9138703346252441\n",
      "Iteration 48020 Loss: 0.7636644244194031\n",
      "Iteration 48021 Loss: 0.8746066689491272\n",
      "Iteration 48022 Loss: 0.638769268989563\n",
      "Iteration 48023 Loss: 0.9206407070159912\n",
      "Iteration 48024 Loss: 0.7392631769180298\n",
      "Iteration 48025 Loss: 1.0528812408447266\n",
      "Iteration 48026 Loss: 0.6536403894424438\n",
      "Iteration 48027 Loss: 0.8078368306159973\n",
      "Iteration 48028 Loss: 0.8900845050811768\n",
      "Iteration 48029 Loss: 1.1394481658935547\n",
      "Iteration 48029 Loss: 0.84808349609375\n",
      "Iteration 48030 Loss: 1.0406235456466675\n",
      "Iteration 48031 Loss: 0.6471858620643616\n",
      "Iteration 48032 Loss: 0.8940706253051758\n",
      "Iteration 48033 Loss: 0.8177042007446289\n",
      "Iteration 48034 Loss: 0.8612445592880249\n",
      "Iteration 48035 Loss: 0.9028186202049255\n",
      "Iteration 48036 Loss: 0.7997986674308777\n",
      "Iteration 48037 Loss: 0.6833352446556091\n",
      "Iteration 48038 Loss: 0.8866564631462097\n",
      "Iteration 48039 Loss: 1.0512473583221436\n",
      "Iteration 48039 Loss: 0.8584685325622559\n",
      "Iteration 48040 Loss: 0.9806281924247742\n",
      "Iteration 48041 Loss: 0.7493141889572144\n",
      "Iteration 48042 Loss: 1.117711067199707\n",
      "Iteration 48043 Loss: 0.929713249206543\n",
      "Iteration 48044 Loss: 1.0639744997024536\n",
      "Iteration 48045 Loss: 0.7938827872276306\n",
      "Iteration 48046 Loss: 1.170498251914978\n",
      "Iteration 48047 Loss: 1.0906285047531128\n",
      "Iteration 48048 Loss: 0.7232125401496887\n",
      "Iteration 48049 Loss: 1.033324122428894\n",
      "Iteration 48049 Loss: 0.9652887582778931\n",
      "Iteration 48050 Loss: 0.9390077590942383\n",
      "Iteration 48051 Loss: 1.2091447114944458\n",
      "Iteration 48052 Loss: 0.7822998762130737\n",
      "Iteration 48053 Loss: 0.9198200702667236\n",
      "Iteration 48054 Loss: 0.7117719054222107\n",
      "Iteration 48055 Loss: 0.8310351371765137\n",
      "Iteration 48056 Loss: 0.7892191410064697\n",
      "Iteration 48057 Loss: 1.0454444885253906\n",
      "Iteration 48058 Loss: 0.8928027153015137\n",
      "Iteration 48059 Loss: 0.8821583986282349\n",
      "Iteration 48059 Loss: 0.9002704620361328\n",
      "Iteration 48060 Loss: 1.08505380153656\n",
      "Iteration 48061 Loss: 0.9036247134208679\n",
      "Iteration 48062 Loss: 0.9820417165756226\n",
      "Iteration 48063 Loss: 1.3786977529525757\n",
      "Iteration 48064 Loss: 0.8005412817001343\n",
      "Iteration 48065 Loss: 0.6308376789093018\n",
      "Iteration 48066 Loss: 0.9222847819328308\n",
      "Iteration 48067 Loss: 0.850492000579834\n",
      "Iteration 48068 Loss: 0.7719055414199829\n",
      "Iteration 48069 Loss: 1.0223042964935303\n",
      "Iteration 48069 Loss: 0.9347783923149109\n",
      "Iteration 48070 Loss: 1.1009312868118286\n",
      "Iteration 48071 Loss: 0.6908625960350037\n",
      "Iteration 48072 Loss: 1.0933324098587036\n",
      "Iteration 48073 Loss: 1.2147581577301025\n",
      "Iteration 48074 Loss: 0.9207972288131714\n",
      "Iteration 48075 Loss: 1.0482332706451416\n",
      "Iteration 48076 Loss: 0.9327461123466492\n",
      "Iteration 48077 Loss: 0.8018149137496948\n",
      "Iteration 48078 Loss: 1.2143045663833618\n",
      "Iteration 48079 Loss: 1.1100038290023804\n",
      "Iteration 48079 Loss: 1.012778401374817\n",
      "Iteration 48080 Loss: 1.0330605506896973\n",
      "Iteration 48081 Loss: 0.7030075788497925\n",
      "Iteration 48082 Loss: 0.8068522214889526\n",
      "Iteration 48083 Loss: 1.299710750579834\n",
      "Iteration 48084 Loss: 0.611840009689331\n",
      "Iteration 48085 Loss: 0.9743414521217346\n",
      "Iteration 48086 Loss: 0.620510995388031\n",
      "Iteration 48087 Loss: 0.9894075989723206\n",
      "Iteration 48088 Loss: 1.1278626918792725\n",
      "Iteration 48089 Loss: 1.2185522317886353\n",
      "Iteration 48089 Loss: 0.9385145902633667\n",
      "Iteration 48090 Loss: 0.9124426245689392\n",
      "Iteration 48091 Loss: 1.0441563129425049\n",
      "Iteration 48092 Loss: 0.8474743366241455\n",
      "Iteration 48093 Loss: 0.7508034706115723\n",
      "Iteration 48094 Loss: 0.963097095489502\n",
      "Iteration 48095 Loss: 0.9752113819122314\n",
      "Iteration 48096 Loss: 1.2273207902908325\n",
      "Iteration 48097 Loss: 0.9182614684104919\n",
      "Iteration 48098 Loss: 1.017693042755127\n",
      "Iteration 48099 Loss: 1.0088880062103271\n",
      "Iteration 48099 Loss: 0.9665347933769226\n",
      "Iteration 48100 Loss: 0.8749839067459106\n",
      "Iteration 48101 Loss: 0.9811179637908936\n",
      "Iteration 48102 Loss: 0.9696680307388306\n",
      "Iteration 48103 Loss: 0.9865021705627441\n",
      "Iteration 48104 Loss: 1.0930638313293457\n",
      "Iteration 48105 Loss: 0.893378734588623\n",
      "Iteration 48106 Loss: 1.0032681226730347\n",
      "Iteration 48107 Loss: 1.1002569198608398\n",
      "Iteration 48108 Loss: 1.051138162612915\n",
      "Iteration 48109 Loss: 1.0363714694976807\n",
      "Iteration 48109 Loss: 0.9989749193191528\n",
      "Iteration 48110 Loss: 0.7723798155784607\n",
      "Iteration 48111 Loss: 0.9412176012992859\n",
      "Iteration 48112 Loss: 0.7935231328010559\n",
      "Iteration 48113 Loss: 0.6199353933334351\n",
      "Iteration 48114 Loss: 1.0476207733154297\n",
      "Iteration 48115 Loss: 0.9206179976463318\n",
      "Iteration 48116 Loss: 0.8260232210159302\n",
      "Iteration 48117 Loss: 0.7760105729103088\n",
      "Iteration 48118 Loss: 0.6241133213043213\n",
      "Iteration 48119 Loss: 0.9055672883987427\n",
      "Iteration 48119 Loss: 0.8227009773254395\n",
      "Iteration 48120 Loss: 0.9293716549873352\n",
      "Iteration 48121 Loss: 1.190988540649414\n",
      "Iteration 48122 Loss: 1.0109943151474\n",
      "Iteration 48123 Loss: 0.8107644319534302\n",
      "Iteration 48124 Loss: 1.0337754487991333\n",
      "Iteration 48125 Loss: 0.8034688234329224\n",
      "Iteration 48126 Loss: 0.9333996772766113\n",
      "Iteration 48127 Loss: 0.8350106477737427\n",
      "Iteration 48128 Loss: 0.9076283574104309\n",
      "Iteration 48129 Loss: 0.8001307845115662\n",
      "Iteration 48129 Loss: 0.9255532026290894\n",
      "Iteration 48130 Loss: 1.0722376108169556\n",
      "Iteration 48131 Loss: 0.7882351279258728\n",
      "Iteration 48132 Loss: 0.6629496812820435\n",
      "Iteration 48133 Loss: 0.7806456089019775\n",
      "Iteration 48134 Loss: 0.7896333336830139\n",
      "Iteration 48135 Loss: 0.9004324674606323\n",
      "Iteration 48136 Loss: 0.8812476992607117\n",
      "Iteration 48137 Loss: 0.9866281151771545\n",
      "Iteration 48138 Loss: 0.8699913024902344\n",
      "Iteration 48139 Loss: 0.8903322219848633\n",
      "Iteration 48139 Loss: 0.8622332811355591\n",
      "Iteration 48140 Loss: 0.759860098361969\n",
      "Iteration 48141 Loss: 0.8258985877037048\n",
      "Iteration 48142 Loss: 1.1014682054519653\n",
      "Iteration 48143 Loss: 0.874721109867096\n",
      "Iteration 48144 Loss: 1.32632315158844\n",
      "Iteration 48145 Loss: 0.9965311288833618\n",
      "Iteration 48146 Loss: 1.0049699544906616\n",
      "Iteration 48147 Loss: 1.043567180633545\n",
      "Iteration 48148 Loss: 0.9906404614448547\n",
      "Iteration 48149 Loss: 0.9933558702468872\n",
      "Iteration 48149 Loss: 0.9917335510253906\n",
      "Iteration 48150 Loss: 0.6403928995132446\n",
      "Iteration 48151 Loss: 0.833433985710144\n",
      "Iteration 48152 Loss: 0.7787266373634338\n",
      "Iteration 48153 Loss: 0.8227630853652954\n",
      "Iteration 48154 Loss: 0.892227292060852\n",
      "Iteration 48155 Loss: 0.8400536775588989\n",
      "Iteration 48156 Loss: 0.8836203813552856\n",
      "Iteration 48157 Loss: 0.9707774519920349\n",
      "Iteration 48158 Loss: 1.2975202798843384\n",
      "Iteration 48159 Loss: 0.8454508185386658\n",
      "Iteration 48159 Loss: 0.880496621131897\n",
      "Iteration 48160 Loss: 0.7061852216720581\n",
      "Iteration 48161 Loss: 0.8401675224304199\n",
      "Iteration 48162 Loss: 1.1055598258972168\n",
      "Iteration 48163 Loss: 0.8630737066268921\n",
      "Iteration 48164 Loss: 0.9122968912124634\n",
      "Iteration 48165 Loss: 0.6551575064659119\n",
      "Iteration 48166 Loss: 0.8651511669158936\n",
      "Iteration 48167 Loss: 1.1226816177368164\n",
      "Iteration 48168 Loss: 0.5086462497711182\n",
      "Iteration 48169 Loss: 0.7266525030136108\n",
      "Iteration 48169 Loss: 0.8305572271347046\n",
      "Iteration 48170 Loss: 0.9208152294158936\n",
      "Iteration 48171 Loss: 0.9779536724090576\n",
      "Iteration 48172 Loss: 1.451719045639038\n",
      "Iteration 48173 Loss: 0.9673348069190979\n",
      "Iteration 48174 Loss: 0.7748064398765564\n",
      "Iteration 48175 Loss: 0.6790655255317688\n",
      "Iteration 48176 Loss: 1.0045881271362305\n",
      "Iteration 48177 Loss: 0.7052280902862549\n",
      "Iteration 48178 Loss: 0.9052011966705322\n",
      "Iteration 48179 Loss: 0.9206194281578064\n",
      "Iteration 48179 Loss: 0.9307332038879395\n",
      "Iteration 48180 Loss: 0.9268863201141357\n",
      "Iteration 48181 Loss: 0.8389935493469238\n",
      "Iteration 48182 Loss: 0.6990357637405396\n",
      "Iteration 48183 Loss: 1.1127110719680786\n",
      "Iteration 48184 Loss: 0.7006869912147522\n",
      "Iteration 48185 Loss: 1.0550315380096436\n",
      "Iteration 48186 Loss: 1.0779340267181396\n",
      "Iteration 48187 Loss: 1.1152681112289429\n",
      "Iteration 48188 Loss: 1.0411467552185059\n",
      "Iteration 48189 Loss: 0.7972296476364136\n",
      "Iteration 48189 Loss: 0.9364923238754272\n",
      "Iteration 48190 Loss: 0.6756783723831177\n",
      "Iteration 48191 Loss: 0.8337212800979614\n",
      "Iteration 48192 Loss: 0.7661996483802795\n",
      "Iteration 48193 Loss: 0.8613849878311157\n",
      "Iteration 48194 Loss: 0.6997635960578918\n",
      "Iteration 48195 Loss: 1.08143150806427\n",
      "Iteration 48196 Loss: 0.8740566372871399\n",
      "Iteration 48197 Loss: 0.7061915993690491\n",
      "Iteration 48198 Loss: 1.3041175603866577\n",
      "Iteration 48199 Loss: 0.8739924430847168\n",
      "Iteration 48199 Loss: 0.8676537275314331\n",
      "Iteration 48200 Loss: 1.0619616508483887\n",
      "Iteration 48201 Loss: 1.1544533967971802\n",
      "Iteration 48202 Loss: 1.039627194404602\n",
      "Iteration 48203 Loss: 0.8419944643974304\n",
      "Iteration 48204 Loss: 0.6759663820266724\n",
      "Iteration 48205 Loss: 0.8295076489448547\n",
      "Iteration 48206 Loss: 0.9992098808288574\n",
      "Iteration 48207 Loss: 1.086479902267456\n",
      "Iteration 48208 Loss: 0.8527860641479492\n",
      "Iteration 48209 Loss: 0.901357889175415\n",
      "Iteration 48209 Loss: 0.9443345069885254\n",
      "Iteration 48210 Loss: 0.6927530765533447\n",
      "Iteration 48211 Loss: 1.011683464050293\n",
      "Iteration 48212 Loss: 0.7609186172485352\n",
      "Iteration 48213 Loss: 1.0425031185150146\n",
      "Iteration 48214 Loss: 1.2930907011032104\n",
      "Iteration 48215 Loss: 0.9240221977233887\n",
      "Iteration 48216 Loss: 0.6435626149177551\n",
      "Iteration 48217 Loss: 1.0606907606124878\n",
      "Iteration 48218 Loss: 0.92879319190979\n",
      "Iteration 48219 Loss: 0.8546410799026489\n",
      "Iteration 48219 Loss: 0.9212659001350403\n",
      "Iteration 48220 Loss: 0.7453415393829346\n",
      "Iteration 48221 Loss: 1.1194311380386353\n",
      "Iteration 48222 Loss: 0.733726978302002\n",
      "Iteration 48223 Loss: 0.8306959271430969\n",
      "Iteration 48224 Loss: 0.774498701095581\n",
      "Iteration 48225 Loss: 0.7164387702941895\n",
      "Iteration 48226 Loss: 0.9201058745384216\n",
      "Iteration 48227 Loss: 1.0953576564788818\n",
      "Iteration 48228 Loss: 0.7952744960784912\n",
      "Iteration 48229 Loss: 0.8655908107757568\n",
      "Iteration 48229 Loss: 0.859646201133728\n",
      "Iteration 48230 Loss: 0.9287617206573486\n",
      "Iteration 48231 Loss: 1.0829334259033203\n",
      "Iteration 48232 Loss: 0.8218303918838501\n",
      "Iteration 48233 Loss: 0.7872365713119507\n",
      "Iteration 48234 Loss: 0.6381763815879822\n",
      "Iteration 48235 Loss: 0.8567971587181091\n",
      "Iteration 48236 Loss: 0.8967036604881287\n",
      "Iteration 48237 Loss: 0.9281309247016907\n",
      "Iteration 48238 Loss: 0.6375707387924194\n",
      "Iteration 48239 Loss: 0.8931509852409363\n",
      "Iteration 48239 Loss: 0.847129225730896\n",
      "Iteration 48240 Loss: 0.8011496663093567\n",
      "Iteration 48241 Loss: 0.9456865191459656\n",
      "Iteration 48242 Loss: 0.8578305840492249\n",
      "Iteration 48243 Loss: 0.9347753524780273\n",
      "Iteration 48244 Loss: 0.561191737651825\n",
      "Iteration 48245 Loss: 1.052973985671997\n",
      "Iteration 48246 Loss: 0.9951016902923584\n",
      "Iteration 48247 Loss: 1.0309585332870483\n",
      "Iteration 48248 Loss: 0.8135489225387573\n",
      "Iteration 48249 Loss: 1.085961937904358\n",
      "Iteration 48249 Loss: 0.907917857170105\n",
      "Iteration 48250 Loss: 1.0923627614974976\n",
      "Iteration 48251 Loss: 0.9841588139533997\n",
      "Iteration 48252 Loss: 0.7044521570205688\n",
      "Iteration 48253 Loss: 1.2835687398910522\n",
      "Iteration 48254 Loss: 1.2633769512176514\n",
      "Iteration 48255 Loss: 0.9536352157592773\n",
      "Iteration 48256 Loss: 0.7535102963447571\n",
      "Iteration 48257 Loss: 1.0019481182098389\n",
      "Iteration 48258 Loss: 1.2619620561599731\n",
      "Iteration 48259 Loss: 1.0137720108032227\n",
      "Iteration 48259 Loss: 1.0312747955322266\n",
      "Iteration 48260 Loss: 1.0702500343322754\n",
      "Iteration 48261 Loss: 0.8409855365753174\n",
      "Iteration 48262 Loss: 0.9295568466186523\n",
      "Iteration 48263 Loss: 1.1184158325195312\n",
      "Iteration 48264 Loss: 1.0185658931732178\n",
      "Iteration 48265 Loss: 0.9998461008071899\n",
      "Iteration 48266 Loss: 0.861224353313446\n",
      "Iteration 48267 Loss: 0.7734638452529907\n",
      "Iteration 48268 Loss: 0.8893569707870483\n",
      "Iteration 48269 Loss: 0.8508331775665283\n",
      "Iteration 48269 Loss: 0.935249924659729\n",
      "Iteration 48270 Loss: 0.8039759397506714\n",
      "Iteration 48271 Loss: 1.2173491716384888\n",
      "Iteration 48272 Loss: 1.0354007482528687\n",
      "Iteration 48273 Loss: 0.8427153825759888\n",
      "Iteration 48274 Loss: 1.1982792615890503\n",
      "Iteration 48275 Loss: 0.9234915375709534\n",
      "Iteration 48276 Loss: 1.0843851566314697\n",
      "Iteration 48277 Loss: 1.2602571249008179\n",
      "Iteration 48278 Loss: 0.85393887758255\n",
      "Iteration 48279 Loss: 0.9843078851699829\n",
      "Iteration 48279 Loss: 1.0204100608825684\n",
      "Iteration 48280 Loss: 1.1425927877426147\n",
      "Iteration 48281 Loss: 0.7211437225341797\n",
      "Iteration 48282 Loss: 0.7485302686691284\n",
      "Iteration 48283 Loss: 0.6835206151008606\n",
      "Iteration 48284 Loss: 1.054917335510254\n",
      "Iteration 48285 Loss: 1.013993740081787\n",
      "Iteration 48286 Loss: 1.0436742305755615\n",
      "Iteration 48287 Loss: 0.9771730899810791\n",
      "Iteration 48288 Loss: 1.3380507230758667\n",
      "Iteration 48289 Loss: 0.8084038496017456\n",
      "Iteration 48289 Loss: 0.9532000422477722\n",
      "Iteration 48290 Loss: 1.0728144645690918\n",
      "Iteration 48291 Loss: 1.011239767074585\n",
      "Iteration 48292 Loss: 0.8211666941642761\n",
      "Iteration 48293 Loss: 1.0788615942001343\n",
      "Iteration 48294 Loss: 1.0301200151443481\n",
      "Iteration 48295 Loss: 0.9030643701553345\n",
      "Iteration 48296 Loss: 0.8526389002799988\n",
      "Iteration 48297 Loss: 0.7752251625061035\n",
      "Iteration 48298 Loss: 1.1429636478424072\n",
      "Iteration 48299 Loss: 1.0416450500488281\n",
      "Iteration 48299 Loss: 0.9729740023612976\n",
      "Iteration 48300 Loss: 1.013716459274292\n",
      "Iteration 48301 Loss: 0.8001103401184082\n",
      "Iteration 48302 Loss: 1.0064455270767212\n",
      "Iteration 48303 Loss: 0.8448635339736938\n",
      "Iteration 48304 Loss: 0.7698155641555786\n",
      "Iteration 48305 Loss: 0.8915106058120728\n",
      "Iteration 48306 Loss: 0.7999338507652283\n",
      "Iteration 48307 Loss: 1.19917631149292\n",
      "Iteration 48308 Loss: 0.9372450113296509\n",
      "Iteration 48309 Loss: 0.9497405886650085\n",
      "Iteration 48309 Loss: 0.9212557077407837\n",
      "Iteration 48310 Loss: 1.0293647050857544\n",
      "Iteration 48311 Loss: 1.0070570707321167\n",
      "Iteration 48312 Loss: 1.130336046218872\n",
      "Iteration 48313 Loss: 0.8154376149177551\n",
      "Iteration 48314 Loss: 0.7191441059112549\n",
      "Iteration 48315 Loss: 0.7710170745849609\n",
      "Iteration 48316 Loss: 1.0555282831192017\n",
      "Iteration 48317 Loss: 1.1046079397201538\n",
      "Iteration 48318 Loss: 1.138192057609558\n",
      "Iteration 48319 Loss: 1.0031770467758179\n",
      "Iteration 48319 Loss: 0.9773862957954407\n",
      "Iteration 48320 Loss: 1.1007436513900757\n",
      "Iteration 48321 Loss: 0.6729660630226135\n",
      "Iteration 48322 Loss: 0.5680896043777466\n",
      "Iteration 48323 Loss: 0.8580928444862366\n",
      "Iteration 48324 Loss: 0.8443053364753723\n",
      "Iteration 48325 Loss: 0.8907569050788879\n",
      "Iteration 48326 Loss: 0.9706718921661377\n",
      "Iteration 48327 Loss: 0.7775790095329285\n",
      "Iteration 48328 Loss: 0.941521406173706\n",
      "Iteration 48329 Loss: 0.8024301528930664\n",
      "Iteration 48329 Loss: 0.8427157402038574\n",
      "Iteration 48330 Loss: 0.8158614635467529\n",
      "Iteration 48331 Loss: 1.1041985750198364\n",
      "Iteration 48332 Loss: 0.8298851251602173\n",
      "Iteration 48333 Loss: 0.876096248626709\n",
      "Iteration 48334 Loss: 1.094745397567749\n",
      "Iteration 48335 Loss: 0.689383327960968\n",
      "Iteration 48336 Loss: 0.7182478904724121\n",
      "Iteration 48337 Loss: 0.8616769313812256\n",
      "Iteration 48338 Loss: 1.3257697820663452\n",
      "Iteration 48339 Loss: 0.8736547827720642\n",
      "Iteration 48339 Loss: 0.9189519882202148\n",
      "Iteration 48340 Loss: 0.5629270076751709\n",
      "Iteration 48341 Loss: 0.9237836599349976\n",
      "Iteration 48342 Loss: 0.9732540845870972\n",
      "Iteration 48343 Loss: 0.6492847800254822\n",
      "Iteration 48344 Loss: 0.8679297566413879\n",
      "Iteration 48345 Loss: 0.9233722686767578\n",
      "Iteration 48346 Loss: 0.6405607461929321\n",
      "Iteration 48347 Loss: 1.3156218528747559\n",
      "Iteration 48348 Loss: 1.0895590782165527\n",
      "Iteration 48349 Loss: 0.9408182501792908\n",
      "Iteration 48349 Loss: 0.888711154460907\n",
      "Iteration 48350 Loss: 1.060245394706726\n",
      "Iteration 48351 Loss: 0.9182698726654053\n",
      "Iteration 48352 Loss: 1.1947569847106934\n",
      "Iteration 48353 Loss: 0.9445527791976929\n",
      "Iteration 48354 Loss: 0.8018210530281067\n",
      "Iteration 48355 Loss: 0.7732957005500793\n",
      "Iteration 48356 Loss: 1.033779263496399\n",
      "Iteration 48357 Loss: 0.8888540863990784\n",
      "Iteration 48358 Loss: 0.8411094546318054\n",
      "Iteration 48359 Loss: 1.0643401145935059\n",
      "Iteration 48359 Loss: 0.952102541923523\n",
      "Iteration 48360 Loss: 0.7110542058944702\n",
      "Iteration 48361 Loss: 0.6355576515197754\n",
      "Iteration 48362 Loss: 0.9271860718727112\n",
      "Iteration 48363 Loss: 1.304962158203125\n",
      "Iteration 48364 Loss: 0.7294192910194397\n",
      "Iteration 48365 Loss: 1.0901706218719482\n",
      "Iteration 48366 Loss: 0.8790313005447388\n",
      "Iteration 48367 Loss: 1.1348925828933716\n",
      "Iteration 48368 Loss: 0.8180460929870605\n",
      "Iteration 48369 Loss: 0.6085096597671509\n",
      "Iteration 48369 Loss: 0.8838828802108765\n",
      "Iteration 48370 Loss: 1.1499539613723755\n",
      "Iteration 48371 Loss: 0.8540653586387634\n",
      "Iteration 48372 Loss: 1.0424426794052124\n",
      "Iteration 48373 Loss: 0.8926104307174683\n",
      "Iteration 48374 Loss: 1.040108561515808\n",
      "Iteration 48375 Loss: 0.9953197240829468\n",
      "Iteration 48376 Loss: 0.7784316539764404\n",
      "Iteration 48377 Loss: 0.8966332674026489\n",
      "Iteration 48378 Loss: 1.1127338409423828\n",
      "Iteration 48379 Loss: 0.9274227619171143\n",
      "Iteration 48379 Loss: 0.9689723253250122\n",
      "Iteration 48380 Loss: 0.7695506811141968\n",
      "Iteration 48381 Loss: 0.8403612971305847\n",
      "Iteration 48382 Loss: 0.8241313695907593\n",
      "Iteration 48383 Loss: 0.7381657361984253\n",
      "Iteration 48384 Loss: 1.1176501512527466\n",
      "Iteration 48385 Loss: 1.1411231756210327\n",
      "Iteration 48386 Loss: 0.8799683451652527\n",
      "Iteration 48387 Loss: 1.101418375968933\n",
      "Iteration 48388 Loss: 1.015561580657959\n",
      "Iteration 48389 Loss: 0.8640317916870117\n",
      "Iteration 48389 Loss: 0.9291962385177612\n",
      "Iteration 48390 Loss: 0.8704485893249512\n",
      "Iteration 48391 Loss: 1.1376307010650635\n",
      "Iteration 48392 Loss: 0.9781084656715393\n",
      "Iteration 48393 Loss: 1.113329529762268\n",
      "Iteration 48394 Loss: 0.8205698132514954\n",
      "Iteration 48395 Loss: 0.896958589553833\n",
      "Iteration 48396 Loss: 0.803424060344696\n",
      "Iteration 48397 Loss: 0.9155682325363159\n",
      "Iteration 48398 Loss: 0.8936542272567749\n",
      "Iteration 48399 Loss: 1.038840889930725\n",
      "Iteration 48399 Loss: 0.9468533396720886\n",
      "Iteration 48400 Loss: 0.9512593150138855\n",
      "Iteration 48401 Loss: 0.9832906723022461\n",
      "Iteration 48402 Loss: 0.6735338568687439\n",
      "Iteration 48403 Loss: 0.9035643935203552\n",
      "Iteration 48404 Loss: 0.7743738293647766\n",
      "Iteration 48405 Loss: 0.8086344599723816\n",
      "Iteration 48406 Loss: 1.046362042427063\n",
      "Iteration 48407 Loss: 0.8287462592124939\n",
      "Iteration 48408 Loss: 0.807988703250885\n",
      "Iteration 48409 Loss: 1.1332039833068848\n",
      "Iteration 48409 Loss: 0.891095757484436\n",
      "Iteration 48410 Loss: 0.9987103343009949\n",
      "Iteration 48411 Loss: 1.032328724861145\n",
      "Iteration 48412 Loss: 0.993971049785614\n",
      "Iteration 48413 Loss: 0.9465140700340271\n",
      "Iteration 48414 Loss: 0.8804663419723511\n",
      "Iteration 48415 Loss: 1.0089796781539917\n",
      "Iteration 48416 Loss: 1.2041518688201904\n",
      "Iteration 48417 Loss: 0.7405268549919128\n",
      "Iteration 48418 Loss: 0.9838096499443054\n",
      "Iteration 48419 Loss: 0.7759584188461304\n",
      "Iteration 48419 Loss: 0.9565417170524597\n",
      "Iteration 48420 Loss: 0.9940659999847412\n",
      "Iteration 48421 Loss: 0.9512625932693481\n",
      "Iteration 48422 Loss: 1.0161356925964355\n",
      "Iteration 48423 Loss: 0.7730062007904053\n",
      "Iteration 48424 Loss: 1.15200936794281\n",
      "Iteration 48425 Loss: 0.8291491270065308\n",
      "Iteration 48426 Loss: 1.1052402257919312\n",
      "Iteration 48427 Loss: 0.7832047939300537\n",
      "Iteration 48428 Loss: 1.0044909715652466\n",
      "Iteration 48429 Loss: 0.8490158915519714\n",
      "Iteration 48429 Loss: 0.9457581639289856\n",
      "Iteration 48430 Loss: 0.6760591864585876\n",
      "Iteration 48431 Loss: 0.9872097373008728\n",
      "Iteration 48432 Loss: 1.0154469013214111\n",
      "Iteration 48433 Loss: 0.854005753993988\n",
      "Iteration 48434 Loss: 1.142008662223816\n",
      "Iteration 48435 Loss: 1.1466559171676636\n",
      "Iteration 48436 Loss: 0.9195664525032043\n",
      "Iteration 48437 Loss: 0.8767932653427124\n",
      "Iteration 48438 Loss: 1.2609761953353882\n",
      "Iteration 48439 Loss: 0.8564414978027344\n",
      "Iteration 48439 Loss: 0.9735163450241089\n",
      "Iteration 48440 Loss: 0.5884212851524353\n",
      "Iteration 48441 Loss: 0.5403928160667419\n",
      "Iteration 48442 Loss: 0.8132509589195251\n",
      "Iteration 48443 Loss: 0.8087376952171326\n",
      "Iteration 48444 Loss: 1.0814921855926514\n",
      "Iteration 48445 Loss: 0.9462538361549377\n",
      "Iteration 48446 Loss: 1.0917247533798218\n",
      "Iteration 48447 Loss: 0.9667766690254211\n",
      "Iteration 48448 Loss: 1.2629890441894531\n",
      "Iteration 48449 Loss: 0.9586406350135803\n",
      "Iteration 48449 Loss: 0.9058679342269897\n",
      "Iteration 48450 Loss: 0.6473094820976257\n",
      "Iteration 48451 Loss: 1.1905996799468994\n",
      "Iteration 48452 Loss: 0.8641290664672852\n",
      "Iteration 48453 Loss: 0.9013792276382446\n",
      "Iteration 48454 Loss: 0.7697637677192688\n",
      "Iteration 48455 Loss: 0.8729772567749023\n",
      "Iteration 48456 Loss: 0.9699382781982422\n",
      "Iteration 48457 Loss: 0.9713903069496155\n",
      "Iteration 48458 Loss: 0.9234158992767334\n",
      "Iteration 48459 Loss: 1.1060569286346436\n",
      "Iteration 48459 Loss: 0.9216960072517395\n",
      "Iteration 48460 Loss: 1.044490933418274\n",
      "Iteration 48461 Loss: 0.8721771240234375\n",
      "Iteration 48462 Loss: 1.2244656085968018\n",
      "Iteration 48463 Loss: 0.9061911106109619\n",
      "Iteration 48464 Loss: 0.8188098669052124\n",
      "Iteration 48465 Loss: 1.3402612209320068\n",
      "Iteration 48466 Loss: 0.9922277927398682\n",
      "Iteration 48467 Loss: 0.7332812547683716\n",
      "Iteration 48468 Loss: 0.9564024806022644\n",
      "Iteration 48469 Loss: 0.8343332409858704\n",
      "Iteration 48469 Loss: 0.9722639918327332\n",
      "Iteration 48470 Loss: 1.0639266967773438\n",
      "Iteration 48471 Loss: 1.0749388933181763\n",
      "Iteration 48472 Loss: 1.0207585096359253\n",
      "Iteration 48473 Loss: 0.9958352446556091\n",
      "Iteration 48474 Loss: 0.6717416644096375\n",
      "Iteration 48475 Loss: 1.1131064891815186\n",
      "Iteration 48476 Loss: 0.9579334259033203\n",
      "Iteration 48477 Loss: 1.0363901853561401\n",
      "Iteration 48478 Loss: 0.9004587531089783\n",
      "Iteration 48479 Loss: 0.8888554573059082\n",
      "Iteration 48479 Loss: 0.972394585609436\n",
      "Iteration 48480 Loss: 0.7579376697540283\n",
      "Iteration 48481 Loss: 0.8691357374191284\n",
      "Iteration 48482 Loss: 0.9940685033798218\n",
      "Iteration 48483 Loss: 0.8909204602241516\n",
      "Iteration 48484 Loss: 0.8971349000930786\n",
      "Iteration 48485 Loss: 0.6939929723739624\n",
      "Iteration 48486 Loss: 0.7738207578659058\n",
      "Iteration 48487 Loss: 0.9340470433235168\n",
      "Iteration 48488 Loss: 1.224589467048645\n",
      "Iteration 48489 Loss: 0.7898940443992615\n",
      "Iteration 48489 Loss: 0.8825541734695435\n",
      "Iteration 48490 Loss: 1.177561640739441\n",
      "Iteration 48491 Loss: 0.9455559849739075\n",
      "Iteration 48492 Loss: 0.7692002058029175\n",
      "Iteration 48493 Loss: 0.9500300288200378\n",
      "Iteration 48494 Loss: 0.8500179052352905\n",
      "Iteration 48495 Loss: 0.6154349446296692\n",
      "Iteration 48496 Loss: 1.1120398044586182\n",
      "Iteration 48497 Loss: 1.0950204133987427\n",
      "Iteration 48498 Loss: 0.8723962306976318\n",
      "Iteration 48499 Loss: 1.2079013586044312\n",
      "Iteration 48499 Loss: 0.9595158696174622\n",
      "Iteration 48500 Loss: 1.2696417570114136\n",
      "Iteration 48501 Loss: 1.031985878944397\n",
      "Iteration 48502 Loss: 0.953184962272644\n",
      "Iteration 48503 Loss: 1.068774700164795\n",
      "Iteration 48504 Loss: 0.7588202357292175\n",
      "Iteration 48505 Loss: 0.9869489669799805\n",
      "Iteration 48506 Loss: 0.4862107038497925\n",
      "Iteration 48507 Loss: 0.9625740051269531\n",
      "Iteration 48508 Loss: 0.6189857125282288\n",
      "Iteration 48509 Loss: 0.8695745468139648\n",
      "Iteration 48509 Loss: 0.9006701707839966\n",
      "Iteration 48510 Loss: 1.1838586330413818\n",
      "Iteration 48511 Loss: 0.6817623972892761\n",
      "Iteration 48512 Loss: 1.0930700302124023\n",
      "Iteration 48513 Loss: 0.8614258766174316\n",
      "Iteration 48514 Loss: 0.9039406180381775\n",
      "Iteration 48515 Loss: 0.7998232245445251\n",
      "Iteration 48516 Loss: 0.5535480976104736\n",
      "Iteration 48517 Loss: 0.7296674251556396\n",
      "Iteration 48518 Loss: 0.8591111898422241\n",
      "Iteration 48519 Loss: 0.9196361303329468\n",
      "Iteration 48519 Loss: 0.8585844039916992\n",
      "Iteration 48520 Loss: 0.4740407168865204\n",
      "Iteration 48521 Loss: 0.6098467707633972\n",
      "Iteration 48522 Loss: 0.8676725625991821\n",
      "Iteration 48523 Loss: 0.9272155165672302\n",
      "Iteration 48524 Loss: 0.8093799352645874\n",
      "Iteration 48525 Loss: 1.0921857357025146\n",
      "Iteration 48526 Loss: 0.9401930570602417\n",
      "Iteration 48527 Loss: 1.0904580354690552\n",
      "Iteration 48528 Loss: 0.7135485410690308\n",
      "Iteration 48529 Loss: 1.1382790803909302\n",
      "Iteration 48529 Loss: 0.8662819862365723\n",
      "Iteration 48530 Loss: 0.9370126128196716\n",
      "Iteration 48531 Loss: 0.9521316885948181\n",
      "Iteration 48532 Loss: 0.7605419754981995\n",
      "Iteration 48533 Loss: 0.8074907660484314\n",
      "Iteration 48534 Loss: 0.9834550619125366\n",
      "Iteration 48535 Loss: 0.9879565238952637\n",
      "Iteration 48536 Loss: 0.9752436876296997\n",
      "Iteration 48537 Loss: 0.9378600120544434\n",
      "Iteration 48538 Loss: 1.2137370109558105\n",
      "Iteration 48539 Loss: 1.2537208795547485\n",
      "Iteration 48539 Loss: 0.9809150695800781\n",
      "Iteration 48540 Loss: 1.0145612955093384\n",
      "Iteration 48541 Loss: 0.931091845035553\n",
      "Iteration 48542 Loss: 0.9312826991081238\n",
      "Iteration 48543 Loss: 1.0910570621490479\n",
      "Iteration 48544 Loss: 1.0952355861663818\n",
      "Iteration 48545 Loss: 1.0193347930908203\n",
      "Iteration 48546 Loss: 0.9402578473091125\n",
      "Iteration 48547 Loss: 0.7077289819717407\n",
      "Iteration 48548 Loss: 0.5441174507141113\n",
      "Iteration 48549 Loss: 1.1321086883544922\n",
      "Iteration 48549 Loss: 0.9406776428222656\n",
      "Iteration 48550 Loss: 1.4222851991653442\n",
      "Iteration 48551 Loss: 1.0240085124969482\n",
      "Iteration 48552 Loss: 0.9741517901420593\n",
      "Iteration 48553 Loss: 0.7285334467887878\n",
      "Iteration 48554 Loss: 1.0509028434753418\n",
      "Iteration 48555 Loss: 1.1690330505371094\n",
      "Iteration 48556 Loss: 0.8958560228347778\n",
      "Iteration 48557 Loss: 1.0608762502670288\n",
      "Iteration 48558 Loss: 0.9980764985084534\n",
      "Iteration 48559 Loss: 0.9744420647621155\n",
      "Iteration 48559 Loss: 1.0298163890838623\n",
      "Iteration 48560 Loss: 0.6640751361846924\n",
      "Iteration 48561 Loss: 1.0108741521835327\n",
      "Iteration 48562 Loss: 0.805860161781311\n",
      "Iteration 48563 Loss: 0.9033023715019226\n",
      "Iteration 48564 Loss: 0.732503354549408\n",
      "Iteration 48565 Loss: 0.6939278841018677\n",
      "Iteration 48566 Loss: 0.9947237968444824\n",
      "Iteration 48567 Loss: 0.7389284372329712\n",
      "Iteration 48568 Loss: 0.8420558571815491\n",
      "Iteration 48569 Loss: 0.9420485496520996\n",
      "Iteration 48569 Loss: 0.8328299522399902\n",
      "Iteration 48570 Loss: 0.8421437740325928\n",
      "Iteration 48571 Loss: 1.0265944004058838\n",
      "Iteration 48572 Loss: 1.0617038011550903\n",
      "Iteration 48573 Loss: 0.9172861576080322\n",
      "Iteration 48574 Loss: 1.1127740144729614\n",
      "Iteration 48575 Loss: 1.2605087757110596\n",
      "Iteration 48576 Loss: 1.1585216522216797\n",
      "Iteration 48577 Loss: 0.9167162179946899\n",
      "Iteration 48578 Loss: 0.7995009422302246\n",
      "Iteration 48579 Loss: 0.8286861181259155\n",
      "Iteration 48579 Loss: 0.9924435615539551\n",
      "Iteration 48580 Loss: 0.8482306003570557\n",
      "Iteration 48581 Loss: 1.4784927368164062\n",
      "Iteration 48582 Loss: 0.8661400079727173\n",
      "Iteration 48583 Loss: 0.39864811301231384\n",
      "Iteration 48584 Loss: 0.972359836101532\n",
      "Iteration 48585 Loss: 1.2909315824508667\n",
      "Iteration 48586 Loss: 0.9099972248077393\n",
      "Iteration 48587 Loss: 0.8356045484542847\n",
      "Iteration 48588 Loss: 0.8241319060325623\n",
      "Iteration 48589 Loss: 0.890907883644104\n",
      "Iteration 48589 Loss: 0.9315444827079773\n",
      "Iteration 48590 Loss: 0.8033068776130676\n",
      "Iteration 48591 Loss: 0.6898159384727478\n",
      "Iteration 48592 Loss: 0.8268415927886963\n",
      "Iteration 48593 Loss: 0.6376705765724182\n",
      "Iteration 48594 Loss: 1.2101686000823975\n",
      "Iteration 48595 Loss: 0.8162298202514648\n",
      "Iteration 48596 Loss: 1.2070308923721313\n",
      "Iteration 48597 Loss: 0.8777204751968384\n",
      "Iteration 48598 Loss: 0.8677809834480286\n",
      "Iteration 48599 Loss: 0.7672486305236816\n",
      "Iteration 48599 Loss: 0.8703814744949341\n",
      "Iteration 48600 Loss: 0.6069238781929016\n",
      "Iteration 48601 Loss: 0.7214301228523254\n",
      "Iteration 48602 Loss: 0.995627760887146\n",
      "Iteration 48603 Loss: 1.036094307899475\n",
      "Iteration 48604 Loss: 1.0788356065750122\n",
      "Iteration 48605 Loss: 1.014497995376587\n",
      "Iteration 48606 Loss: 0.8619071841239929\n",
      "Iteration 48607 Loss: 0.6924314498901367\n",
      "Iteration 48608 Loss: 1.099794626235962\n",
      "Iteration 48609 Loss: 0.8015709519386292\n",
      "Iteration 48609 Loss: 0.8909112811088562\n",
      "Iteration 48610 Loss: 0.9729024767875671\n",
      "Iteration 48611 Loss: 0.8567186594009399\n",
      "Iteration 48612 Loss: 0.8216992616653442\n",
      "Iteration 48613 Loss: 1.2020816802978516\n",
      "Iteration 48614 Loss: 1.4102147817611694\n",
      "Iteration 48615 Loss: 0.845356285572052\n",
      "Iteration 48616 Loss: 1.2375874519348145\n",
      "Iteration 48617 Loss: 0.9492635726928711\n",
      "Iteration 48618 Loss: 0.7703493237495422\n",
      "Iteration 48619 Loss: 0.8384355306625366\n",
      "Iteration 48619 Loss: 0.9904608726501465\n",
      "Iteration 48620 Loss: 1.0438460111618042\n",
      "Iteration 48621 Loss: 1.0129753351211548\n",
      "Iteration 48622 Loss: 0.6831256151199341\n",
      "Iteration 48623 Loss: 0.8426493406295776\n",
      "Iteration 48624 Loss: 1.0328937768936157\n",
      "Iteration 48625 Loss: 0.840120792388916\n",
      "Iteration 48626 Loss: 0.8395671248435974\n",
      "Iteration 48627 Loss: 0.9045827984809875\n",
      "Iteration 48628 Loss: 1.0220123529434204\n",
      "Iteration 48629 Loss: 0.6423133611679077\n",
      "Iteration 48629 Loss: 0.8864086270332336\n",
      "Iteration 48630 Loss: 0.9031084775924683\n",
      "Iteration 48631 Loss: 0.9037021994590759\n",
      "Iteration 48632 Loss: 1.133412480354309\n",
      "Iteration 48633 Loss: 0.8926727175712585\n",
      "Iteration 48634 Loss: 0.7282941937446594\n",
      "Iteration 48635 Loss: 0.7654351592063904\n",
      "Iteration 48636 Loss: 0.906898558139801\n",
      "Iteration 48637 Loss: 1.1573076248168945\n",
      "Iteration 48638 Loss: 0.7015624642372131\n",
      "Iteration 48639 Loss: 1.0601109266281128\n",
      "Iteration 48639 Loss: 0.9152504801750183\n",
      "Iteration 48640 Loss: 0.9353384971618652\n",
      "Iteration 48641 Loss: 0.8451774716377258\n",
      "Iteration 48642 Loss: 1.0248430967330933\n",
      "Iteration 48643 Loss: 0.7753332257270813\n",
      "Iteration 48644 Loss: 1.2629096508026123\n",
      "Iteration 48645 Loss: 0.9165438413619995\n",
      "Iteration 48646 Loss: 1.0455925464630127\n",
      "Iteration 48647 Loss: 1.0422544479370117\n",
      "Iteration 48648 Loss: 0.8687941431999207\n",
      "Iteration 48649 Loss: 0.6608392596244812\n",
      "Iteration 48649 Loss: 0.9377626180648804\n",
      "Iteration 48650 Loss: 1.2617697715759277\n",
      "Iteration 48651 Loss: 1.0775188207626343\n",
      "Iteration 48652 Loss: 0.8237979412078857\n",
      "Iteration 48653 Loss: 0.4774390757083893\n",
      "Iteration 48654 Loss: 0.8923447132110596\n",
      "Iteration 48655 Loss: 0.8564466238021851\n",
      "Iteration 48656 Loss: 0.9620733857154846\n",
      "Iteration 48657 Loss: 0.7634058594703674\n",
      "Iteration 48658 Loss: 0.5833388566970825\n",
      "Iteration 48659 Loss: 1.0416369438171387\n",
      "Iteration 48659 Loss: 0.8739771842956543\n",
      "Iteration 48660 Loss: 0.9543357491493225\n",
      "Iteration 48661 Loss: 0.8983421325683594\n",
      "Iteration 48662 Loss: 0.9458215236663818\n",
      "Iteration 48663 Loss: 1.0445202589035034\n",
      "Iteration 48664 Loss: 0.9448055624961853\n",
      "Iteration 48665 Loss: 1.204087495803833\n",
      "Iteration 48666 Loss: 0.41246935725212097\n",
      "Iteration 48667 Loss: 1.2515079975128174\n",
      "Iteration 48668 Loss: 0.894517719745636\n",
      "Iteration 48669 Loss: 0.8925105333328247\n",
      "Iteration 48669 Loss: 0.9442917704582214\n",
      "Iteration 48670 Loss: 0.848357081413269\n",
      "Iteration 48671 Loss: 0.9282969832420349\n",
      "Iteration 48672 Loss: 0.7760523557662964\n",
      "Iteration 48673 Loss: 1.019735336303711\n",
      "Iteration 48674 Loss: 1.0944445133209229\n",
      "Iteration 48675 Loss: 0.8475305438041687\n",
      "Iteration 48676 Loss: 0.7872806191444397\n",
      "Iteration 48677 Loss: 1.0376724004745483\n",
      "Iteration 48678 Loss: 0.5863874554634094\n",
      "Iteration 48679 Loss: 0.6460835933685303\n",
      "Iteration 48679 Loss: 0.857184112071991\n",
      "Iteration 48680 Loss: 0.9415384531021118\n",
      "Iteration 48681 Loss: 0.971643328666687\n",
      "Iteration 48682 Loss: 0.7987506985664368\n",
      "Iteration 48683 Loss: 0.8459999561309814\n",
      "Iteration 48684 Loss: 1.1893242597579956\n",
      "Iteration 48685 Loss: 0.8957073092460632\n",
      "Iteration 48686 Loss: 0.8333067893981934\n",
      "Iteration 48687 Loss: 1.223793625831604\n",
      "Iteration 48688 Loss: 1.3494170904159546\n",
      "Iteration 48689 Loss: 0.792045533657074\n",
      "Iteration 48689 Loss: 0.9841526746749878\n",
      "Iteration 48690 Loss: 0.949891984462738\n",
      "Iteration 48691 Loss: 1.0108829736709595\n",
      "Iteration 48692 Loss: 0.7540628910064697\n",
      "Iteration 48693 Loss: 0.7242293357849121\n",
      "Iteration 48694 Loss: 0.8648791313171387\n",
      "Iteration 48695 Loss: 1.0213183164596558\n",
      "Iteration 48696 Loss: 0.9907865524291992\n",
      "Iteration 48697 Loss: 0.9592053890228271\n",
      "Iteration 48698 Loss: 0.9361920356750488\n",
      "Iteration 48699 Loss: 0.9685013294219971\n",
      "Iteration 48699 Loss: 0.9179950952529907\n",
      "Iteration 48700 Loss: 0.8958509564399719\n",
      "Iteration 48701 Loss: 0.9263721108436584\n",
      "Iteration 48702 Loss: 0.7224776744842529\n",
      "Iteration 48703 Loss: 0.86639404296875\n",
      "Iteration 48704 Loss: 0.913850724697113\n",
      "Iteration 48705 Loss: 0.9607928395271301\n",
      "Iteration 48706 Loss: 1.149967908859253\n",
      "Iteration 48707 Loss: 1.1836600303649902\n",
      "Iteration 48708 Loss: 0.9133580923080444\n",
      "Iteration 48709 Loss: 1.3601614236831665\n",
      "Iteration 48709 Loss: 0.9892885088920593\n",
      "Iteration 48710 Loss: 0.9234236478805542\n",
      "Iteration 48711 Loss: 1.0566564798355103\n",
      "Iteration 48712 Loss: 0.5153580904006958\n",
      "Iteration 48713 Loss: 0.7627233266830444\n",
      "Iteration 48714 Loss: 0.7951755523681641\n",
      "Iteration 48715 Loss: 1.0241212844848633\n",
      "Iteration 48716 Loss: 1.1619168519973755\n",
      "Iteration 48717 Loss: 0.8484655022621155\n",
      "Iteration 48718 Loss: 0.9859014749526978\n",
      "Iteration 48719 Loss: 0.6899580955505371\n",
      "Iteration 48719 Loss: 0.8763700723648071\n",
      "Iteration 48720 Loss: 0.9300559759140015\n",
      "Iteration 48721 Loss: 1.1966986656188965\n",
      "Iteration 48722 Loss: 0.9055240154266357\n",
      "Iteration 48723 Loss: 0.6309313774108887\n",
      "Iteration 48724 Loss: 0.7614756226539612\n",
      "Iteration 48725 Loss: 0.9031734466552734\n",
      "Iteration 48726 Loss: 0.682714581489563\n",
      "Iteration 48727 Loss: 1.050178050994873\n",
      "Iteration 48728 Loss: 0.967263400554657\n",
      "Iteration 48729 Loss: 0.8813966512680054\n",
      "Iteration 48729 Loss: 0.8909411430358887\n",
      "Iteration 48730 Loss: 1.13887357711792\n",
      "Iteration 48731 Loss: 1.105172872543335\n",
      "Iteration 48732 Loss: 0.9970689415931702\n",
      "Iteration 48733 Loss: 0.981153130531311\n",
      "Iteration 48734 Loss: 0.892292320728302\n",
      "Iteration 48735 Loss: 0.9380584955215454\n",
      "Iteration 48736 Loss: 1.037492036819458\n",
      "Iteration 48737 Loss: 0.7219429612159729\n",
      "Iteration 48738 Loss: 1.195067286491394\n",
      "Iteration 48739 Loss: 0.9355452656745911\n",
      "Iteration 48739 Loss: 0.9942666888237\n",
      "Iteration 48740 Loss: 0.8258757591247559\n",
      "Iteration 48741 Loss: 1.038034200668335\n",
      "Iteration 48742 Loss: 1.0256184339523315\n",
      "Iteration 48743 Loss: 0.8591497540473938\n",
      "Iteration 48744 Loss: 1.1653329133987427\n",
      "Iteration 48745 Loss: 0.8067721128463745\n",
      "Iteration 48746 Loss: 0.8397634625434875\n",
      "Iteration 48747 Loss: 0.8804057836532593\n",
      "Iteration 48748 Loss: 0.709640622138977\n",
      "Iteration 48749 Loss: 1.109378457069397\n",
      "Iteration 48749 Loss: 0.9259971380233765\n",
      "Iteration 48750 Loss: 1.0684998035430908\n",
      "Iteration 48751 Loss: 1.1027029752731323\n",
      "Iteration 48752 Loss: 0.984872579574585\n",
      "Iteration 48753 Loss: 0.9694332480430603\n",
      "Iteration 48754 Loss: 1.104960560798645\n",
      "Iteration 48755 Loss: 0.8798853158950806\n",
      "Iteration 48756 Loss: 1.3299140930175781\n",
      "Iteration 48757 Loss: 1.0003657341003418\n",
      "Iteration 48758 Loss: 0.6311376094818115\n",
      "Iteration 48759 Loss: 1.148145318031311\n",
      "Iteration 48759 Loss: 1.0219917297363281\n",
      "Iteration 48760 Loss: 0.7851122617721558\n",
      "Iteration 48761 Loss: 1.113322138786316\n",
      "Iteration 48762 Loss: 0.8667203187942505\n",
      "Iteration 48763 Loss: 1.1047914028167725\n",
      "Iteration 48764 Loss: 1.2107689380645752\n",
      "Iteration 48765 Loss: 1.280364990234375\n",
      "Iteration 48766 Loss: 0.9772879481315613\n",
      "Iteration 48767 Loss: 0.7833133339881897\n",
      "Iteration 48768 Loss: 0.8046845197677612\n",
      "Iteration 48769 Loss: 0.888910174369812\n",
      "Iteration 48769 Loss: 0.9815276265144348\n",
      "Iteration 48770 Loss: 1.0273809432983398\n",
      "Iteration 48771 Loss: 0.6896349191665649\n",
      "Iteration 48772 Loss: 1.0354137420654297\n",
      "Iteration 48773 Loss: 0.8113411068916321\n",
      "Iteration 48774 Loss: 0.8249732255935669\n",
      "Iteration 48775 Loss: 0.9503042697906494\n",
      "Iteration 48776 Loss: 1.2289882898330688\n",
      "Iteration 48777 Loss: 0.9790127277374268\n",
      "Iteration 48778 Loss: 0.7981827855110168\n",
      "Iteration 48779 Loss: 0.9368085265159607\n",
      "Iteration 48779 Loss: 0.9282040596008301\n",
      "Iteration 48780 Loss: 0.9721189737319946\n",
      "Iteration 48781 Loss: 0.7264492511749268\n",
      "Iteration 48782 Loss: 0.6640457510948181\n",
      "Iteration 48783 Loss: 0.5859116911888123\n",
      "Iteration 48784 Loss: 1.446635127067566\n",
      "Iteration 48785 Loss: 0.8727244734764099\n",
      "Iteration 48786 Loss: 1.1052101850509644\n",
      "Iteration 48787 Loss: 0.777715265750885\n",
      "Iteration 48788 Loss: 0.8308168649673462\n",
      "Iteration 48789 Loss: 1.0312232971191406\n",
      "Iteration 48789 Loss: 0.9012851715087891\n",
      "Iteration 48790 Loss: 0.8982996940612793\n",
      "Iteration 48791 Loss: 0.856582522392273\n",
      "Iteration 48792 Loss: 0.9153328537940979\n",
      "Iteration 48793 Loss: 1.042434811592102\n",
      "Iteration 48794 Loss: 0.9769411087036133\n",
      "Iteration 48795 Loss: 1.0156797170639038\n",
      "Iteration 48796 Loss: 1.1958009004592896\n",
      "Iteration 48797 Loss: 0.8342866897583008\n",
      "Iteration 48798 Loss: 0.9849151372909546\n",
      "Iteration 48799 Loss: 1.0644829273223877\n",
      "Iteration 48799 Loss: 0.9784756898880005\n",
      "Iteration 48800 Loss: 0.8092496395111084\n",
      "Iteration 48801 Loss: 1.159579873085022\n",
      "Iteration 48802 Loss: 0.9829921722412109\n",
      "Iteration 48803 Loss: 0.7401421666145325\n",
      "Iteration 48804 Loss: 1.0415300130844116\n",
      "Iteration 48805 Loss: 0.9344395995140076\n",
      "Iteration 48806 Loss: 1.0088711977005005\n",
      "Iteration 48807 Loss: 0.7215501070022583\n",
      "Iteration 48808 Loss: 1.4036232233047485\n",
      "Iteration 48809 Loss: 1.4865654706954956\n",
      "Iteration 48809 Loss: 1.0288543701171875\n",
      "Iteration 48810 Loss: 0.7578940987586975\n",
      "Iteration 48811 Loss: 0.8894057869911194\n",
      "Iteration 48812 Loss: 1.2920849323272705\n",
      "Iteration 48813 Loss: 0.7792079448699951\n",
      "Iteration 48814 Loss: 0.6092891693115234\n",
      "Iteration 48815 Loss: 1.217053771018982\n",
      "Iteration 48816 Loss: 0.9503219723701477\n",
      "Iteration 48817 Loss: 0.918293297290802\n",
      "Iteration 48818 Loss: 0.9924774765968323\n",
      "Iteration 48819 Loss: 0.8437793850898743\n",
      "Iteration 48819 Loss: 0.9249807596206665\n",
      "Iteration 48820 Loss: 0.947400689125061\n",
      "Iteration 48821 Loss: 1.2606043815612793\n",
      "Iteration 48822 Loss: 1.313281536102295\n",
      "Iteration 48823 Loss: 0.9695778489112854\n",
      "Iteration 48824 Loss: 0.8176397085189819\n",
      "Iteration 48825 Loss: 1.1585239171981812\n",
      "Iteration 48826 Loss: 0.8823757767677307\n",
      "Iteration 48827 Loss: 0.8615049123764038\n",
      "Iteration 48828 Loss: 0.9283730983734131\n",
      "Iteration 48829 Loss: 1.3563532829284668\n",
      "Iteration 48829 Loss: 1.0495635271072388\n",
      "Iteration 48830 Loss: 0.7315526604652405\n",
      "Iteration 48831 Loss: 0.9274717569351196\n",
      "Iteration 48832 Loss: 1.148306965827942\n",
      "Iteration 48833 Loss: 0.7247670888900757\n",
      "Iteration 48834 Loss: 0.7995291352272034\n",
      "Iteration 48835 Loss: 1.101165533065796\n",
      "Iteration 48836 Loss: 0.7265979647636414\n",
      "Iteration 48837 Loss: 1.0690780878067017\n",
      "Iteration 48838 Loss: 0.7937605977058411\n",
      "Iteration 48839 Loss: 0.8334330916404724\n",
      "Iteration 48839 Loss: 0.8855663537979126\n",
      "Iteration 48840 Loss: 0.8521297574043274\n",
      "Iteration 48841 Loss: 0.6750279664993286\n",
      "Iteration 48842 Loss: 0.8569319844245911\n",
      "Iteration 48843 Loss: 0.8002971410751343\n",
      "Iteration 48844 Loss: 0.8431406021118164\n",
      "Iteration 48845 Loss: 0.8136141896247864\n",
      "Iteration 48846 Loss: 0.8843802809715271\n",
      "Iteration 48847 Loss: 0.7350106239318848\n",
      "Iteration 48848 Loss: 1.0901339054107666\n",
      "Iteration 48849 Loss: 0.7369400262832642\n",
      "Iteration 48849 Loss: 0.8287607431411743\n",
      "Iteration 48850 Loss: 0.7234036922454834\n",
      "Iteration 48851 Loss: 0.6279458999633789\n",
      "Iteration 48852 Loss: 0.7450792193412781\n",
      "Iteration 48853 Loss: 1.2226775884628296\n",
      "Iteration 48854 Loss: 0.7366727590560913\n",
      "Iteration 48855 Loss: 1.04398512840271\n",
      "Iteration 48856 Loss: 1.0526024103164673\n",
      "Iteration 48857 Loss: 0.8324060440063477\n",
      "Iteration 48858 Loss: 1.0024176836013794\n",
      "Iteration 48859 Loss: 0.8939096331596375\n",
      "Iteration 48859 Loss: 0.8881100416183472\n",
      "Iteration 48860 Loss: 0.7438495755195618\n",
      "Iteration 48861 Loss: 0.8720265626907349\n",
      "Iteration 48862 Loss: 0.9261636137962341\n",
      "Iteration 48863 Loss: 0.9557088017463684\n",
      "Iteration 48864 Loss: 0.9401106238365173\n",
      "Iteration 48865 Loss: 0.8568030595779419\n",
      "Iteration 48866 Loss: 0.7947716116905212\n",
      "Iteration 48867 Loss: 0.7187942862510681\n",
      "Iteration 48868 Loss: 1.0164445638656616\n",
      "Iteration 48869 Loss: 0.7326500415802002\n",
      "Iteration 48869 Loss: 0.855732262134552\n",
      "Iteration 48870 Loss: 1.0185223817825317\n",
      "Iteration 48871 Loss: 0.8930333852767944\n",
      "Iteration 48872 Loss: 0.8640919327735901\n",
      "Iteration 48873 Loss: 1.2082127332687378\n",
      "Iteration 48874 Loss: 0.9445177316665649\n",
      "Iteration 48875 Loss: 0.7967866659164429\n",
      "Iteration 48876 Loss: 0.7687774300575256\n",
      "Iteration 48877 Loss: 0.8234642148017883\n",
      "Iteration 48878 Loss: 1.0952908992767334\n",
      "Iteration 48879 Loss: 0.980027437210083\n",
      "Iteration 48879 Loss: 0.9392725229263306\n",
      "Iteration 48880 Loss: 1.0053176879882812\n",
      "Iteration 48881 Loss: 0.737758994102478\n",
      "Iteration 48882 Loss: 0.8399849534034729\n",
      "Iteration 48883 Loss: 1.183178186416626\n",
      "Iteration 48884 Loss: 0.880409300327301\n",
      "Iteration 48885 Loss: 0.9230241179466248\n",
      "Iteration 48886 Loss: 0.8798699975013733\n",
      "Iteration 48887 Loss: 0.9209300875663757\n",
      "Iteration 48888 Loss: 0.7970014214515686\n",
      "Iteration 48889 Loss: 0.7894424200057983\n",
      "Iteration 48889 Loss: 0.8956917524337769\n",
      "Iteration 48890 Loss: 0.7971862554550171\n",
      "Iteration 48891 Loss: 1.343895435333252\n",
      "Iteration 48892 Loss: 0.9288007616996765\n",
      "Iteration 48893 Loss: 1.2146742343902588\n",
      "Iteration 48894 Loss: 1.1330780982971191\n",
      "Iteration 48895 Loss: 1.0030733346939087\n",
      "Iteration 48896 Loss: 0.918387770652771\n",
      "Iteration 48897 Loss: 1.0624884366989136\n",
      "Iteration 48898 Loss: 0.8258271217346191\n",
      "Iteration 48899 Loss: 0.4809918999671936\n",
      "Iteration 48899 Loss: 0.9708402752876282\n",
      "Iteration 48900 Loss: 0.9400021433830261\n",
      "Iteration 48901 Loss: 0.9131451845169067\n",
      "Iteration 48902 Loss: 0.9976791143417358\n",
      "Iteration 48903 Loss: 0.8662557005882263\n",
      "Iteration 48904 Loss: 1.0857741832733154\n",
      "Iteration 48905 Loss: 0.8293143510818481\n",
      "Iteration 48906 Loss: 0.8364355564117432\n",
      "Iteration 48907 Loss: 0.7557864189147949\n",
      "Iteration 48908 Loss: 1.0969198942184448\n",
      "Iteration 48909 Loss: 0.8460712432861328\n",
      "Iteration 48909 Loss: 0.9167383313179016\n",
      "Iteration 48910 Loss: 0.7844768166542053\n",
      "Iteration 48911 Loss: 0.7875751256942749\n",
      "Iteration 48912 Loss: 0.8882035613059998\n",
      "Iteration 48913 Loss: 1.1369941234588623\n",
      "Iteration 48914 Loss: 1.234205722808838\n",
      "Iteration 48915 Loss: 0.8192006349563599\n",
      "Iteration 48916 Loss: 0.46417710185050964\n",
      "Iteration 48917 Loss: 1.1365638971328735\n",
      "Iteration 48918 Loss: 0.6677265167236328\n",
      "Iteration 48919 Loss: 0.9916730523109436\n",
      "Iteration 48919 Loss: 0.8910796046257019\n",
      "Iteration 48920 Loss: 1.0148591995239258\n",
      "Iteration 48921 Loss: 0.5304747819900513\n",
      "Iteration 48922 Loss: 1.0707327127456665\n",
      "Iteration 48923 Loss: 0.660395622253418\n",
      "Iteration 48924 Loss: 0.8542215824127197\n",
      "Iteration 48925 Loss: 0.5937592387199402\n",
      "Iteration 48926 Loss: 1.0806219577789307\n",
      "Iteration 48927 Loss: 1.2852189540863037\n",
      "Iteration 48928 Loss: 0.9512119889259338\n",
      "Iteration 48929 Loss: 0.9451733231544495\n",
      "Iteration 48929 Loss: 0.8986669778823853\n",
      "Iteration 48930 Loss: 1.4976757764816284\n",
      "Iteration 48931 Loss: 0.8287902474403381\n",
      "Iteration 48932 Loss: 0.7372049689292908\n",
      "Iteration 48933 Loss: 0.6491478681564331\n",
      "Iteration 48934 Loss: 0.4324406385421753\n",
      "Iteration 48935 Loss: 0.703605055809021\n",
      "Iteration 48936 Loss: 0.7506073117256165\n",
      "Iteration 48937 Loss: 0.7839458584785461\n",
      "Iteration 48938 Loss: 0.6154820322990417\n",
      "Iteration 48939 Loss: 0.9597449898719788\n",
      "Iteration 48939 Loss: 0.7958645224571228\n",
      "Iteration 48940 Loss: 1.1420044898986816\n",
      "Iteration 48941 Loss: 0.7917178273200989\n",
      "Iteration 48942 Loss: 0.6953094601631165\n",
      "Iteration 48943 Loss: 1.1059305667877197\n",
      "Iteration 48944 Loss: 0.8614516854286194\n",
      "Iteration 48945 Loss: 0.8916471004486084\n",
      "Iteration 48946 Loss: 0.7331215739250183\n",
      "Iteration 48947 Loss: 1.0707746744155884\n",
      "Iteration 48948 Loss: 0.6615194082260132\n",
      "Iteration 48949 Loss: 1.070409893989563\n",
      "Iteration 48949 Loss: 0.9023885726928711\n",
      "Iteration 48950 Loss: 1.0832055807113647\n",
      "Iteration 48951 Loss: 1.0146294832229614\n",
      "Iteration 48952 Loss: 0.8602626919746399\n",
      "Iteration 48953 Loss: 0.7606519460678101\n",
      "Iteration 48954 Loss: 0.9130491614341736\n",
      "Iteration 48955 Loss: 0.6862080097198486\n",
      "Iteration 48956 Loss: 1.113338828086853\n",
      "Iteration 48957 Loss: 1.1131783723831177\n",
      "Iteration 48958 Loss: 0.6630883812904358\n",
      "Iteration 48959 Loss: 0.7798839807510376\n",
      "Iteration 48959 Loss: 0.8987497091293335\n",
      "Iteration 48960 Loss: 1.0258601903915405\n",
      "Iteration 48961 Loss: 0.9000298976898193\n",
      "Iteration 48962 Loss: 0.9447132349014282\n",
      "Iteration 48963 Loss: 0.7467655539512634\n",
      "Iteration 48964 Loss: 0.977694034576416\n",
      "Iteration 48965 Loss: 0.6770645380020142\n",
      "Iteration 48966 Loss: 0.7524547576904297\n",
      "Iteration 48967 Loss: 0.811540424823761\n",
      "Iteration 48968 Loss: 0.8189700841903687\n",
      "Iteration 48969 Loss: 1.2471171617507935\n",
      "Iteration 48969 Loss: 0.8902209997177124\n",
      "Iteration 48970 Loss: 1.1134275197982788\n",
      "Iteration 48971 Loss: 1.0501766204833984\n",
      "Iteration 48972 Loss: 0.8336507081985474\n",
      "Iteration 48973 Loss: 0.7899095416069031\n",
      "Iteration 48974 Loss: 0.8235553503036499\n",
      "Iteration 48975 Loss: 0.7847374081611633\n",
      "Iteration 48976 Loss: 0.7913535833358765\n",
      "Iteration 48977 Loss: 0.9666187763214111\n",
      "Iteration 48978 Loss: 1.0110286474227905\n",
      "Iteration 48979 Loss: 0.8858205676078796\n",
      "Iteration 48979 Loss: 0.9050278663635254\n",
      "Iteration 48980 Loss: 1.0480786561965942\n",
      "Iteration 48981 Loss: 1.2118040323257446\n",
      "Iteration 48982 Loss: 0.8428831696510315\n",
      "Iteration 48983 Loss: 1.1547118425369263\n",
      "Iteration 48984 Loss: 0.9007392525672913\n",
      "Iteration 48985 Loss: 1.323560118675232\n",
      "Iteration 48986 Loss: 0.6236630082130432\n",
      "Iteration 48987 Loss: 0.9562343955039978\n",
      "Iteration 48988 Loss: 1.2034590244293213\n",
      "Iteration 48989 Loss: 1.1380733251571655\n",
      "Iteration 48989 Loss: 1.040320634841919\n",
      "Iteration 48990 Loss: 1.1741353273391724\n",
      "Iteration 48991 Loss: 0.6874087452888489\n",
      "Iteration 48992 Loss: 1.5200293064117432\n",
      "Iteration 48993 Loss: 0.922048807144165\n",
      "Iteration 48994 Loss: 1.1046932935714722\n",
      "Iteration 48995 Loss: 0.9132794737815857\n",
      "Iteration 48996 Loss: 0.9977147579193115\n",
      "Iteration 48997 Loss: 0.9139008522033691\n",
      "Iteration 48998 Loss: 0.9219128489494324\n",
      "Iteration 48999 Loss: 0.9815799593925476\n",
      "Iteration 48999 Loss: 1.0136703252792358\n",
      "Iteration 49000 Loss: 1.1720870733261108\n",
      "Iteration 49001 Loss: 0.9740812182426453\n",
      "Iteration 49002 Loss: 0.8933139443397522\n",
      "Iteration 49003 Loss: 1.1501230001449585\n",
      "Iteration 49004 Loss: 1.1037163734436035\n",
      "Iteration 49005 Loss: 1.0012569427490234\n",
      "Iteration 49006 Loss: 1.0132452249526978\n",
      "Iteration 49007 Loss: 0.9088083505630493\n",
      "Iteration 49008 Loss: 0.8039509057998657\n",
      "Iteration 49009 Loss: 1.0644327402114868\n",
      "Iteration 49009 Loss: 1.008501648902893\n",
      "Iteration 49010 Loss: 0.5986454486846924\n",
      "Iteration 49011 Loss: 1.1470732688903809\n",
      "Iteration 49012 Loss: 0.8882902264595032\n",
      "Iteration 49013 Loss: 1.0631872415542603\n",
      "Iteration 49014 Loss: 0.8721996545791626\n",
      "Iteration 49015 Loss: 1.100524663925171\n",
      "Iteration 49016 Loss: 0.5704503655433655\n",
      "Iteration 49017 Loss: 0.7943238615989685\n",
      "Iteration 49018 Loss: 0.7933499813079834\n",
      "Iteration 49019 Loss: 0.9450754523277283\n",
      "Iteration 49019 Loss: 0.8773120045661926\n",
      "Iteration 49020 Loss: 1.0306293964385986\n",
      "Iteration 49021 Loss: 1.2958252429962158\n",
      "Iteration 49022 Loss: 0.8919259309768677\n",
      "Iteration 49023 Loss: 0.5386759042739868\n",
      "Iteration 49024 Loss: 0.9101171493530273\n",
      "Iteration 49025 Loss: 0.9632771611213684\n",
      "Iteration 49026 Loss: 0.7252740263938904\n",
      "Iteration 49027 Loss: 0.9341374039649963\n",
      "Iteration 49028 Loss: 1.0572232007980347\n",
      "Iteration 49029 Loss: 1.0666626691818237\n",
      "Iteration 49029 Loss: 0.9413747787475586\n",
      "Iteration 49030 Loss: 0.6558724045753479\n",
      "Iteration 49031 Loss: 0.8481073975563049\n",
      "Iteration 49032 Loss: 0.6886425614356995\n",
      "Iteration 49033 Loss: 0.8297460675239563\n",
      "Iteration 49034 Loss: 1.019986629486084\n",
      "Iteration 49035 Loss: 0.8955588340759277\n",
      "Iteration 49036 Loss: 0.6249363422393799\n",
      "Iteration 49037 Loss: 1.0886311531066895\n",
      "Iteration 49038 Loss: 0.857391357421875\n",
      "Iteration 49039 Loss: 0.538474440574646\n",
      "Iteration 49039 Loss: 0.8047348260879517\n",
      "Iteration 49040 Loss: 0.7592629790306091\n",
      "Iteration 49041 Loss: 0.8590733408927917\n",
      "Iteration 49042 Loss: 0.6986052989959717\n",
      "Iteration 49043 Loss: 0.5959293842315674\n",
      "Iteration 49044 Loss: 0.852124035358429\n",
      "Iteration 49045 Loss: 0.788134753704071\n",
      "Iteration 49046 Loss: 1.1925981044769287\n",
      "Iteration 49047 Loss: 0.9412727355957031\n",
      "Iteration 49048 Loss: 0.8967506289482117\n",
      "Iteration 49049 Loss: 0.852598249912262\n",
      "Iteration 49049 Loss: 0.8436349630355835\n",
      "Iteration 49050 Loss: 0.9230667352676392\n",
      "Iteration 49051 Loss: 0.8014073371887207\n",
      "Iteration 49052 Loss: 0.9327310919761658\n",
      "Iteration 49053 Loss: 1.0726712942123413\n",
      "Iteration 49054 Loss: 1.2454533576965332\n",
      "Iteration 49055 Loss: 1.4587724208831787\n",
      "Iteration 49056 Loss: 0.7293094992637634\n",
      "Iteration 49057 Loss: 1.0186870098114014\n",
      "Iteration 49058 Loss: 0.9001310467720032\n",
      "Iteration 49059 Loss: 0.6558012366294861\n",
      "Iteration 49059 Loss: 0.9738031625747681\n",
      "Iteration 49060 Loss: 0.8366226553916931\n",
      "Iteration 49061 Loss: 1.01676607131958\n",
      "Iteration 49062 Loss: 0.6216997504234314\n",
      "Iteration 49063 Loss: 1.143251657485962\n",
      "Iteration 49064 Loss: 0.7851654887199402\n",
      "Iteration 49065 Loss: 0.7986924648284912\n",
      "Iteration 49066 Loss: 0.6551690101623535\n",
      "Iteration 49067 Loss: 0.9541285634040833\n",
      "Iteration 49068 Loss: 1.1534637212753296\n",
      "Iteration 49069 Loss: 0.9755244851112366\n",
      "Iteration 49069 Loss: 0.8940483927726746\n",
      "Iteration 49070 Loss: 0.950431764125824\n",
      "Iteration 49071 Loss: 0.8077808022499084\n",
      "Iteration 49072 Loss: 0.9447547793388367\n",
      "Iteration 49073 Loss: 1.3979616165161133\n",
      "Iteration 49074 Loss: 1.0303200483322144\n",
      "Iteration 49075 Loss: 0.7722716927528381\n",
      "Iteration 49076 Loss: 0.763418436050415\n",
      "Iteration 49077 Loss: 0.9277716279029846\n",
      "Iteration 49078 Loss: 0.9537868499755859\n",
      "Iteration 49079 Loss: 1.0986510515213013\n",
      "Iteration 49079 Loss: 0.9647148251533508\n",
      "Iteration 49080 Loss: 0.8963698148727417\n",
      "Iteration 49081 Loss: 1.121059775352478\n",
      "Iteration 49082 Loss: 0.9790539741516113\n",
      "Iteration 49083 Loss: 0.7571565508842468\n",
      "Iteration 49084 Loss: 0.9390953779220581\n",
      "Iteration 49085 Loss: 1.0162937641143799\n",
      "Iteration 49086 Loss: 1.126535177230835\n",
      "Iteration 49087 Loss: 0.773967444896698\n",
      "Iteration 49088 Loss: 0.8006864190101624\n",
      "Iteration 49089 Loss: 0.8934510946273804\n",
      "Iteration 49089 Loss: 0.9303669929504395\n",
      "Iteration 49090 Loss: 1.2885193824768066\n",
      "Iteration 49091 Loss: 0.6983375549316406\n",
      "Iteration 49092 Loss: 0.844420313835144\n",
      "Iteration 49093 Loss: 1.0113030672073364\n",
      "Iteration 49094 Loss: 1.178529977798462\n",
      "Iteration 49095 Loss: 1.0270624160766602\n",
      "Iteration 49096 Loss: 0.9661334753036499\n",
      "Iteration 49097 Loss: 1.006074070930481\n",
      "Iteration 49098 Loss: 0.71086585521698\n",
      "Iteration 49099 Loss: 1.1009817123413086\n",
      "Iteration 49099 Loss: 0.9832227826118469\n",
      "Iteration 49100 Loss: 0.8094003796577454\n",
      "Iteration 49101 Loss: 1.3154137134552002\n",
      "Iteration 49102 Loss: 0.8576796054840088\n",
      "Iteration 49103 Loss: 0.6263748407363892\n",
      "Iteration 49104 Loss: 0.7021000981330872\n",
      "Iteration 49105 Loss: 1.0568153858184814\n",
      "Iteration 49106 Loss: 1.1596850156784058\n",
      "Iteration 49107 Loss: 1.0358400344848633\n",
      "Iteration 49108 Loss: 0.9600499868392944\n",
      "Iteration 49109 Loss: 1.039783239364624\n",
      "Iteration 49109 Loss: 0.956314206123352\n",
      "Iteration 49110 Loss: 0.8413658142089844\n",
      "Iteration 49111 Loss: 0.8462327122688293\n",
      "Iteration 49112 Loss: 0.8286446332931519\n",
      "Iteration 49113 Loss: 0.7661088705062866\n",
      "Iteration 49114 Loss: 0.9138171672821045\n",
      "Iteration 49115 Loss: 1.1156648397445679\n",
      "Iteration 49116 Loss: 0.9873511791229248\n",
      "Iteration 49117 Loss: 0.8798325061798096\n",
      "Iteration 49118 Loss: 1.2117109298706055\n",
      "Iteration 49119 Loss: 1.0556185245513916\n",
      "Iteration 49119 Loss: 0.944634735584259\n",
      "Iteration 49120 Loss: 0.8662339448928833\n",
      "Iteration 49121 Loss: 0.8710613250732422\n",
      "Iteration 49122 Loss: 1.1633604764938354\n",
      "Iteration 49123 Loss: 0.9179626107215881\n",
      "Iteration 49124 Loss: 0.8702322840690613\n",
      "Iteration 49125 Loss: 1.007226586341858\n",
      "Iteration 49126 Loss: 1.0855380296707153\n",
      "Iteration 49127 Loss: 1.0087766647338867\n",
      "Iteration 49128 Loss: 0.9228876829147339\n",
      "Iteration 49129 Loss: 0.6929619312286377\n",
      "Iteration 49129 Loss: 0.9406241178512573\n",
      "Iteration 49130 Loss: 0.9301587343215942\n",
      "Iteration 49131 Loss: 1.0458132028579712\n",
      "Iteration 49132 Loss: 0.923660397529602\n",
      "Iteration 49133 Loss: 1.3436222076416016\n",
      "Iteration 49134 Loss: 0.6465697884559631\n",
      "Iteration 49135 Loss: 0.957464873790741\n",
      "Iteration 49136 Loss: 1.0770094394683838\n",
      "Iteration 49137 Loss: 1.1117355823516846\n",
      "Iteration 49138 Loss: 1.0117359161376953\n",
      "Iteration 49139 Loss: 0.928703784942627\n",
      "Iteration 49139 Loss: 0.9976472854614258\n",
      "Iteration 49140 Loss: 1.0263211727142334\n",
      "Iteration 49141 Loss: 0.807393491268158\n",
      "Iteration 49142 Loss: 1.3506218194961548\n",
      "Iteration 49143 Loss: 1.127361536026001\n",
      "Iteration 49144 Loss: 0.9279154539108276\n",
      "Iteration 49145 Loss: 1.0274103879928589\n",
      "Iteration 49146 Loss: 0.7487174868583679\n",
      "Iteration 49147 Loss: 0.7037903070449829\n",
      "Iteration 49148 Loss: 0.7614137530326843\n",
      "Iteration 49149 Loss: 0.9073839783668518\n",
      "Iteration 49149 Loss: 0.9388329386711121\n",
      "Iteration 49150 Loss: 0.9889538884162903\n",
      "Iteration 49151 Loss: 0.9174959659576416\n",
      "Iteration 49152 Loss: 0.7326467633247375\n",
      "Iteration 49153 Loss: 1.238140344619751\n",
      "Iteration 49154 Loss: 1.1089437007904053\n",
      "Iteration 49155 Loss: 0.7912872433662415\n",
      "Iteration 49156 Loss: 0.9036546349525452\n",
      "Iteration 49157 Loss: 0.9034733176231384\n",
      "Iteration 49158 Loss: 0.8981496095657349\n",
      "Iteration 49159 Loss: 0.7819238901138306\n",
      "Iteration 49159 Loss: 0.9264669418334961\n",
      "Iteration 49160 Loss: 0.5981466174125671\n",
      "Iteration 49161 Loss: 1.0447508096694946\n",
      "Iteration 49162 Loss: 1.0789493322372437\n",
      "Iteration 49163 Loss: 0.5698534846305847\n",
      "Iteration 49164 Loss: 0.6994422078132629\n",
      "Iteration 49165 Loss: 1.0774534940719604\n",
      "Iteration 49166 Loss: 0.8674423098564148\n",
      "Iteration 49167 Loss: 0.8443917036056519\n",
      "Iteration 49168 Loss: 0.9618546962738037\n",
      "Iteration 49169 Loss: 1.1773982048034668\n",
      "Iteration 49169 Loss: 0.8919683694839478\n",
      "Iteration 49170 Loss: 0.6079697608947754\n",
      "Iteration 49171 Loss: 0.6118220090866089\n",
      "Iteration 49172 Loss: 1.1102321147918701\n",
      "Iteration 49173 Loss: 0.9801289439201355\n",
      "Iteration 49174 Loss: 0.9627698063850403\n",
      "Iteration 49175 Loss: 0.6772507429122925\n",
      "Iteration 49176 Loss: 1.202117681503296\n",
      "Iteration 49177 Loss: 1.1038540601730347\n",
      "Iteration 49178 Loss: 0.843844473361969\n",
      "Iteration 49179 Loss: 1.4148579835891724\n",
      "Iteration 49179 Loss: 0.9514847993850708\n",
      "Iteration 49180 Loss: 0.9953949451446533\n",
      "Iteration 49181 Loss: 0.6880751252174377\n",
      "Iteration 49182 Loss: 1.106794834136963\n",
      "Iteration 49183 Loss: 0.8506262302398682\n",
      "Iteration 49184 Loss: 0.8446822762489319\n",
      "Iteration 49185 Loss: 0.9745685458183289\n",
      "Iteration 49186 Loss: 0.7118794322013855\n",
      "Iteration 49187 Loss: 0.7554453611373901\n",
      "Iteration 49188 Loss: 0.5741323828697205\n",
      "Iteration 49189 Loss: 0.6426119804382324\n",
      "Iteration 49189 Loss: 0.8144210577011108\n",
      "Iteration 49190 Loss: 0.9247749447822571\n",
      "Iteration 49191 Loss: 0.9878637790679932\n",
      "Iteration 49192 Loss: 0.8927814364433289\n",
      "Iteration 49193 Loss: 0.7501069903373718\n",
      "Iteration 49194 Loss: 0.9403031468391418\n",
      "Iteration 49195 Loss: 0.9878852367401123\n",
      "Iteration 49196 Loss: 1.0150587558746338\n",
      "Iteration 49197 Loss: 0.9580857157707214\n",
      "Iteration 49198 Loss: 0.8837608695030212\n",
      "Iteration 49199 Loss: 0.9470073580741882\n",
      "Iteration 49199 Loss: 0.9287627935409546\n",
      "Iteration 49200 Loss: 0.8911929726600647\n",
      "Iteration 49201 Loss: 1.415035605430603\n",
      "Iteration 49202 Loss: 1.215788722038269\n",
      "Iteration 49203 Loss: 1.257714867591858\n",
      "Iteration 49204 Loss: 1.070899486541748\n",
      "Iteration 49205 Loss: 0.7351945042610168\n",
      "Iteration 49206 Loss: 0.8006230592727661\n",
      "Iteration 49207 Loss: 0.7295379638671875\n",
      "Iteration 49208 Loss: 0.800703227519989\n",
      "Iteration 49209 Loss: 0.8186179995536804\n",
      "Iteration 49209 Loss: 0.9735308885574341\n",
      "Iteration 49210 Loss: 0.7202668190002441\n",
      "Iteration 49211 Loss: 0.9151326417922974\n",
      "Iteration 49212 Loss: 1.013700008392334\n",
      "Iteration 49213 Loss: 1.1649245023727417\n",
      "Iteration 49214 Loss: 1.1332993507385254\n",
      "Iteration 49215 Loss: 0.6710774302482605\n",
      "Iteration 49216 Loss: 0.9512708187103271\n",
      "Iteration 49217 Loss: 1.0898069143295288\n",
      "Iteration 49218 Loss: 1.671103835105896\n",
      "Iteration 49219 Loss: 0.9393278360366821\n",
      "Iteration 49219 Loss: 1.0269910097122192\n",
      "Iteration 49220 Loss: 1.0365939140319824\n",
      "Iteration 49221 Loss: 1.0445431470870972\n",
      "Iteration 49222 Loss: 0.8634598851203918\n",
      "Iteration 49223 Loss: 0.7538161277770996\n",
      "Iteration 49224 Loss: 0.726820707321167\n",
      "Iteration 49225 Loss: 0.830612301826477\n",
      "Iteration 49226 Loss: 0.856805682182312\n",
      "Iteration 49227 Loss: 0.9853721261024475\n",
      "Iteration 49228 Loss: 0.8369450569152832\n",
      "Iteration 49229 Loss: 0.844651460647583\n",
      "Iteration 49229 Loss: 0.8779621124267578\n",
      "Iteration 49230 Loss: 0.7688589692115784\n",
      "Iteration 49231 Loss: 0.958962619304657\n",
      "Iteration 49232 Loss: 0.8931143879890442\n",
      "Iteration 49233 Loss: 1.021463394165039\n",
      "Iteration 49234 Loss: 0.6856653094291687\n",
      "Iteration 49235 Loss: 0.8064250349998474\n",
      "Iteration 49236 Loss: 0.9640050530433655\n",
      "Iteration 49237 Loss: 0.8785516619682312\n",
      "Iteration 49238 Loss: 0.8006020784378052\n",
      "Iteration 49239 Loss: 1.1430541276931763\n",
      "Iteration 49239 Loss: 0.8920702934265137\n",
      "Iteration 49240 Loss: 0.8367130160331726\n",
      "Iteration 49241 Loss: 0.9307430982589722\n",
      "Iteration 49242 Loss: 0.9500334858894348\n",
      "Iteration 49243 Loss: 1.364843726158142\n",
      "Iteration 49244 Loss: 1.0248043537139893\n",
      "Iteration 49245 Loss: 0.814010500907898\n",
      "Iteration 49246 Loss: 0.9601625204086304\n",
      "Iteration 49247 Loss: 1.00230073928833\n",
      "Iteration 49248 Loss: 0.8721988201141357\n",
      "Iteration 49249 Loss: 0.7848330140113831\n",
      "Iteration 49249 Loss: 0.9540643692016602\n",
      "Iteration 49250 Loss: 0.8058499693870544\n",
      "Iteration 49251 Loss: 0.7680518627166748\n",
      "Iteration 49252 Loss: 1.2691115140914917\n",
      "Iteration 49253 Loss: 0.90314120054245\n",
      "Iteration 49254 Loss: 0.7496783137321472\n",
      "Iteration 49255 Loss: 0.9715262055397034\n",
      "Iteration 49256 Loss: 0.9222508668899536\n",
      "Iteration 49257 Loss: 1.111431360244751\n",
      "Iteration 49258 Loss: 0.7196929454803467\n",
      "Iteration 49259 Loss: 0.7510213255882263\n",
      "Iteration 49259 Loss: 0.8971754908561707\n",
      "Iteration 49260 Loss: 0.9559838771820068\n",
      "Iteration 49261 Loss: 1.083247184753418\n",
      "Iteration 49262 Loss: 0.662933349609375\n",
      "Iteration 49263 Loss: 0.9642658233642578\n",
      "Iteration 49264 Loss: 1.0554604530334473\n",
      "Iteration 49265 Loss: 0.9588300585746765\n",
      "Iteration 49266 Loss: 0.9844197630882263\n",
      "Iteration 49267 Loss: 0.9726777672767639\n",
      "Iteration 49268 Loss: 1.1153974533081055\n",
      "Iteration 49269 Loss: 1.2617580890655518\n",
      "Iteration 49269 Loss: 1.001497507095337\n",
      "Iteration 49270 Loss: 1.0406248569488525\n",
      "Iteration 49271 Loss: 1.089345932006836\n",
      "Iteration 49272 Loss: 0.6943223476409912\n",
      "Iteration 49273 Loss: 1.0854898691177368\n",
      "Iteration 49274 Loss: 0.7976888418197632\n",
      "Iteration 49275 Loss: 0.9637037515640259\n",
      "Iteration 49276 Loss: 0.9395766258239746\n",
      "Iteration 49277 Loss: 0.6408889293670654\n",
      "Iteration 49278 Loss: 0.9914074540138245\n",
      "Iteration 49279 Loss: 0.9578295350074768\n",
      "Iteration 49279 Loss: 0.9200878143310547\n",
      "Iteration 49280 Loss: 1.0878539085388184\n",
      "Iteration 49281 Loss: 0.6597952842712402\n",
      "Iteration 49282 Loss: 0.8022821545600891\n",
      "Iteration 49283 Loss: 0.7272589206695557\n",
      "Iteration 49284 Loss: 1.1305745840072632\n",
      "Iteration 49285 Loss: 0.9996216893196106\n",
      "Iteration 49286 Loss: 0.9130664467811584\n",
      "Iteration 49287 Loss: 1.0949548482894897\n",
      "Iteration 49288 Loss: 0.666125476360321\n",
      "Iteration 49289 Loss: 0.8172416090965271\n",
      "Iteration 49289 Loss: 0.8898774981498718\n",
      "Iteration 49290 Loss: 0.9002949595451355\n",
      "Iteration 49291 Loss: 1.2982653379440308\n",
      "Iteration 49292 Loss: 1.1512231826782227\n",
      "Iteration 49293 Loss: 1.405757188796997\n",
      "Iteration 49294 Loss: 0.8382056951522827\n",
      "Iteration 49295 Loss: 0.7286569476127625\n",
      "Iteration 49296 Loss: 1.1980167627334595\n",
      "Iteration 49297 Loss: 0.8709967136383057\n",
      "Iteration 49298 Loss: 1.1018810272216797\n",
      "Iteration 49299 Loss: 1.0508230924606323\n",
      "Iteration 49299 Loss: 1.0544121265411377\n",
      "Iteration 49300 Loss: 1.1194779872894287\n",
      "Iteration 49301 Loss: 0.8580201864242554\n",
      "Iteration 49302 Loss: 0.6755984425544739\n",
      "Iteration 49303 Loss: 0.8557159304618835\n",
      "Iteration 49304 Loss: 0.8656721711158752\n",
      "Iteration 49305 Loss: 0.9750218987464905\n",
      "Iteration 49306 Loss: 1.001150369644165\n",
      "Iteration 49307 Loss: 0.6661707162857056\n",
      "Iteration 49308 Loss: 1.1996406316757202\n",
      "Iteration 49309 Loss: 1.131397008895874\n",
      "Iteration 49309 Loss: 0.9347864985466003\n",
      "Iteration 49310 Loss: 0.946559488773346\n",
      "Iteration 49311 Loss: 0.7146143913269043\n",
      "Iteration 49312 Loss: 0.913863480091095\n",
      "Iteration 49313 Loss: 0.9837167859077454\n",
      "Iteration 49314 Loss: 0.8626094460487366\n",
      "Iteration 49315 Loss: 1.0233538150787354\n",
      "Iteration 49316 Loss: 0.721861720085144\n",
      "Iteration 49317 Loss: 0.9363774657249451\n",
      "Iteration 49318 Loss: 1.0271464586257935\n",
      "Iteration 49319 Loss: 0.9726901650428772\n",
      "Iteration 49319 Loss: 0.910279393196106\n",
      "Iteration 49320 Loss: 0.9298511147499084\n",
      "Iteration 49321 Loss: 0.5885036587715149\n",
      "Iteration 49322 Loss: 0.9279933571815491\n",
      "Iteration 49323 Loss: 0.6689985394477844\n",
      "Iteration 49324 Loss: 1.132773756980896\n",
      "Iteration 49325 Loss: 1.177719235420227\n",
      "Iteration 49326 Loss: 0.8034586906433105\n",
      "Iteration 49327 Loss: 1.1052197217941284\n",
      "Iteration 49328 Loss: 0.8765602111816406\n",
      "Iteration 49329 Loss: 1.1667096614837646\n",
      "Iteration 49329 Loss: 0.9377787709236145\n",
      "Iteration 49330 Loss: 0.880715012550354\n",
      "Iteration 49331 Loss: 0.9720918536186218\n",
      "Iteration 49332 Loss: 0.6551483273506165\n",
      "Iteration 49333 Loss: 0.9852302074432373\n",
      "Iteration 49334 Loss: 0.7407784461975098\n",
      "Iteration 49335 Loss: 1.0540025234222412\n",
      "Iteration 49336 Loss: 1.2051273584365845\n",
      "Iteration 49337 Loss: 1.1430646181106567\n",
      "Iteration 49338 Loss: 0.9744538068771362\n",
      "Iteration 49339 Loss: 1.2509537935256958\n",
      "Iteration 49339 Loss: 0.9861565828323364\n",
      "Iteration 49340 Loss: 0.8245754241943359\n",
      "Iteration 49341 Loss: 0.9312724471092224\n",
      "Iteration 49342 Loss: 0.8622548580169678\n",
      "Iteration 49343 Loss: 0.859052300453186\n",
      "Iteration 49344 Loss: 1.046718955039978\n",
      "Iteration 49345 Loss: 0.9444348216056824\n",
      "Iteration 49346 Loss: 0.9425309896469116\n",
      "Iteration 49347 Loss: 1.2770576477050781\n",
      "Iteration 49348 Loss: 1.3458725214004517\n",
      "Iteration 49349 Loss: 0.9058461785316467\n",
      "Iteration 49349 Loss: 0.9939616322517395\n",
      "Iteration 49350 Loss: 1.184694766998291\n",
      "Iteration 49351 Loss: 0.7260506749153137\n",
      "Iteration 49352 Loss: 0.9478904604911804\n",
      "Iteration 49353 Loss: 0.6590519547462463\n",
      "Iteration 49354 Loss: 1.0402045249938965\n",
      "Iteration 49355 Loss: 0.9357765316963196\n",
      "Iteration 49356 Loss: 1.2314000129699707\n",
      "Iteration 49357 Loss: 1.2676682472229004\n",
      "Iteration 49358 Loss: 0.8907776474952698\n",
      "Iteration 49359 Loss: 1.136903166770935\n",
      "Iteration 49359 Loss: 1.0020418167114258\n",
      "Iteration 49360 Loss: 0.7550237774848938\n",
      "Iteration 49361 Loss: 0.836929440498352\n",
      "Iteration 49362 Loss: 0.8272566795349121\n",
      "Iteration 49363 Loss: 0.9103545546531677\n",
      "Iteration 49364 Loss: 1.0824726819992065\n",
      "Iteration 49365 Loss: 0.9369211792945862\n",
      "Iteration 49366 Loss: 1.2820645570755005\n",
      "Iteration 49367 Loss: 1.1180437803268433\n",
      "Iteration 49368 Loss: 1.0155259370803833\n",
      "Iteration 49369 Loss: 0.7771914005279541\n",
      "Iteration 49369 Loss: 0.9541784524917603\n",
      "Iteration 49370 Loss: 0.9782125949859619\n",
      "Iteration 49371 Loss: 1.0971063375473022\n",
      "Iteration 49372 Loss: 0.7680001854896545\n",
      "Iteration 49373 Loss: 0.9206964373588562\n",
      "Iteration 49374 Loss: 0.9957293272018433\n",
      "Iteration 49375 Loss: 1.0992993116378784\n",
      "Iteration 49376 Loss: 0.9186522960662842\n",
      "Iteration 49377 Loss: 0.8837664723396301\n",
      "Iteration 49378 Loss: 0.9632347822189331\n",
      "Iteration 49379 Loss: 0.8366446495056152\n",
      "Iteration 49379 Loss: 0.9461342096328735\n",
      "Iteration 49380 Loss: 0.8910929560661316\n",
      "Iteration 49381 Loss: 1.0544462203979492\n",
      "Iteration 49382 Loss: 0.9186961054801941\n",
      "Iteration 49383 Loss: 1.1085224151611328\n",
      "Iteration 49384 Loss: 1.3883181810379028\n",
      "Iteration 49385 Loss: 0.7242479920387268\n",
      "Iteration 49386 Loss: 0.895587146282196\n",
      "Iteration 49387 Loss: 1.1059298515319824\n",
      "Iteration 49388 Loss: 1.0102534294128418\n",
      "Iteration 49389 Loss: 1.083898901939392\n",
      "Iteration 49389 Loss: 1.0180994272232056\n",
      "Iteration 49390 Loss: 0.615966796875\n",
      "Iteration 49391 Loss: 1.1969949007034302\n",
      "Iteration 49392 Loss: 1.1710320711135864\n",
      "Iteration 49393 Loss: 0.8472307324409485\n",
      "Iteration 49394 Loss: 1.0156142711639404\n",
      "Iteration 49395 Loss: 0.7641828060150146\n",
      "Iteration 49396 Loss: 0.932081937789917\n",
      "Iteration 49397 Loss: 0.6818541884422302\n",
      "Iteration 49398 Loss: 0.6525955200195312\n",
      "Iteration 49399 Loss: 0.7287447452545166\n",
      "Iteration 49399 Loss: 0.8606298565864563\n",
      "Iteration 49400 Loss: 0.9295846223831177\n",
      "Iteration 49401 Loss: 0.9326325058937073\n",
      "Iteration 49402 Loss: 1.022329330444336\n",
      "Iteration 49403 Loss: 0.9582323431968689\n",
      "Iteration 49404 Loss: 1.002535343170166\n",
      "Iteration 49405 Loss: 0.9541895985603333\n",
      "Iteration 49406 Loss: 1.0725822448730469\n",
      "Iteration 49407 Loss: 0.8936641812324524\n",
      "Iteration 49408 Loss: 0.5595766305923462\n",
      "Iteration 49409 Loss: 0.730004608631134\n",
      "Iteration 49409 Loss: 0.9055331349372864\n",
      "Iteration 49410 Loss: 0.41540443897247314\n",
      "Iteration 49411 Loss: 1.0046135187149048\n",
      "Iteration 49412 Loss: 1.1272823810577393\n",
      "Iteration 49413 Loss: 0.9190956354141235\n",
      "Iteration 49414 Loss: 0.9361310005187988\n",
      "Iteration 49415 Loss: 1.0750551223754883\n",
      "Iteration 49416 Loss: 1.0117228031158447\n",
      "Iteration 49417 Loss: 0.8079009652137756\n",
      "Iteration 49418 Loss: 0.8570823073387146\n",
      "Iteration 49419 Loss: 0.45149362087249756\n",
      "Iteration 49419 Loss: 0.8605782389640808\n",
      "Iteration 49420 Loss: 0.8467016816139221\n",
      "Iteration 49421 Loss: 0.8172922730445862\n",
      "Iteration 49422 Loss: 0.8511995673179626\n",
      "Iteration 49423 Loss: 0.8904390931129456\n",
      "Iteration 49424 Loss: 0.957664966583252\n",
      "Iteration 49425 Loss: 1.2295818328857422\n",
      "Iteration 49426 Loss: 0.8834736347198486\n",
      "Iteration 49427 Loss: 1.209577202796936\n",
      "Iteration 49428 Loss: 0.727144181728363\n",
      "Iteration 49429 Loss: 0.8588550686836243\n",
      "Iteration 49429 Loss: 0.9271929860115051\n",
      "Iteration 49430 Loss: 0.6446812748908997\n",
      "Iteration 49431 Loss: 0.799887478351593\n",
      "Iteration 49432 Loss: 0.8939920663833618\n",
      "Iteration 49433 Loss: 1.0360455513000488\n",
      "Iteration 49434 Loss: 1.0050017833709717\n",
      "Iteration 49435 Loss: 0.9764600396156311\n",
      "Iteration 49436 Loss: 0.7737647891044617\n",
      "Iteration 49437 Loss: 1.031043529510498\n",
      "Iteration 49438 Loss: 1.0463074445724487\n",
      "Iteration 49439 Loss: 1.0356743335723877\n",
      "Iteration 49439 Loss: 0.924285888671875\n",
      "Iteration 49440 Loss: 0.5910277366638184\n",
      "Iteration 49441 Loss: 1.1252098083496094\n",
      "Iteration 49442 Loss: 0.8796713948249817\n",
      "Iteration 49443 Loss: 0.8956305980682373\n",
      "Iteration 49444 Loss: 0.9292258620262146\n",
      "Iteration 49445 Loss: 1.0605294704437256\n",
      "Iteration 49446 Loss: 1.0673364400863647\n",
      "Iteration 49447 Loss: 1.0639359951019287\n",
      "Iteration 49448 Loss: 0.6318273544311523\n",
      "Iteration 49449 Loss: 0.7419579029083252\n",
      "Iteration 49449 Loss: 0.8986352682113647\n",
      "Iteration 49450 Loss: 0.7260216474533081\n",
      "Iteration 49451 Loss: 1.0417757034301758\n",
      "Iteration 49452 Loss: 1.139204978942871\n",
      "Iteration 49453 Loss: 0.8387701511383057\n",
      "Iteration 49454 Loss: 0.9371755123138428\n",
      "Iteration 49455 Loss: 0.9487221240997314\n",
      "Iteration 49456 Loss: 0.8585104942321777\n",
      "Iteration 49457 Loss: 0.9448618292808533\n",
      "Iteration 49458 Loss: 0.7188908457756042\n",
      "Iteration 49459 Loss: 0.8417561054229736\n",
      "Iteration 49459 Loss: 0.8995688557624817\n",
      "Iteration 49460 Loss: 1.008736252784729\n",
      "Iteration 49461 Loss: 0.800729513168335\n",
      "Iteration 49462 Loss: 0.9303497672080994\n",
      "Iteration 49463 Loss: 1.233541488647461\n",
      "Iteration 49464 Loss: 0.8808850646018982\n",
      "Iteration 49465 Loss: 1.0458606481552124\n",
      "Iteration 49466 Loss: 1.0066462755203247\n",
      "Iteration 49467 Loss: 0.8845181465148926\n",
      "Iteration 49468 Loss: 1.0600402355194092\n",
      "Iteration 49469 Loss: 0.7336531281471252\n",
      "Iteration 49469 Loss: 0.95849609375\n",
      "Iteration 49470 Loss: 1.11006498336792\n",
      "Iteration 49471 Loss: 0.8199903964996338\n",
      "Iteration 49472 Loss: 0.9763944745063782\n",
      "Iteration 49473 Loss: 1.2104074954986572\n",
      "Iteration 49474 Loss: 0.9787851572036743\n",
      "Iteration 49475 Loss: 0.8662378191947937\n",
      "Iteration 49476 Loss: 0.8882682919502258\n",
      "Iteration 49477 Loss: 0.7944828271865845\n",
      "Iteration 49478 Loss: 0.864994466304779\n",
      "Iteration 49479 Loss: 0.8906737565994263\n",
      "Iteration 49479 Loss: 0.9400299191474915\n",
      "Iteration 49480 Loss: 0.8884828686714172\n",
      "Iteration 49481 Loss: 0.9669259190559387\n",
      "Iteration 49482 Loss: 1.1244397163391113\n",
      "Iteration 49483 Loss: 0.9958526492118835\n",
      "Iteration 49484 Loss: 1.014817476272583\n",
      "Iteration 49485 Loss: 1.3308769464492798\n",
      "Iteration 49486 Loss: 0.7985748052597046\n",
      "Iteration 49487 Loss: 0.5269212126731873\n",
      "Iteration 49488 Loss: 0.8373148441314697\n",
      "Iteration 49489 Loss: 1.0660213232040405\n",
      "Iteration 49489 Loss: 0.9550226926803589\n",
      "Iteration 49490 Loss: 1.1030043363571167\n",
      "Iteration 49491 Loss: 1.153381109237671\n",
      "Iteration 49492 Loss: 1.0174938440322876\n",
      "Iteration 49493 Loss: 1.1330492496490479\n",
      "Iteration 49494 Loss: 0.7075712084770203\n",
      "Iteration 49495 Loss: 0.8405524492263794\n",
      "Iteration 49496 Loss: 0.7555696964263916\n",
      "Iteration 49497 Loss: 0.9441413879394531\n",
      "Iteration 49498 Loss: 1.1493144035339355\n",
      "Iteration 49499 Loss: 0.8453455567359924\n",
      "Iteration 49499 Loss: 0.9649422764778137\n",
      "Iteration 49500 Loss: 0.9169616103172302\n",
      "Iteration 49501 Loss: 0.9774131774902344\n",
      "Iteration 49502 Loss: 0.948823094367981\n",
      "Iteration 49503 Loss: 0.7907566428184509\n",
      "Iteration 49504 Loss: 0.7210688591003418\n",
      "Iteration 49505 Loss: 0.6993459463119507\n",
      "Iteration 49506 Loss: 0.6547881364822388\n",
      "Iteration 49507 Loss: 0.995757520198822\n",
      "Iteration 49508 Loss: 0.9583820700645447\n",
      "Iteration 49509 Loss: 1.031821608543396\n",
      "Iteration 49509 Loss: 0.8695119023323059\n",
      "Iteration 49510 Loss: 0.7428235411643982\n",
      "Iteration 49511 Loss: 0.9768226146697998\n",
      "Iteration 49512 Loss: 0.9644678235054016\n",
      "Iteration 49513 Loss: 0.8468993306159973\n",
      "Iteration 49514 Loss: 0.7566758394241333\n",
      "Iteration 49515 Loss: 0.8432185649871826\n",
      "Iteration 49516 Loss: 0.816519021987915\n",
      "Iteration 49517 Loss: 1.1582212448120117\n",
      "Iteration 49518 Loss: 1.1729620695114136\n",
      "Iteration 49519 Loss: 0.8532779216766357\n",
      "Iteration 49519 Loss: 0.9131887555122375\n",
      "Iteration 49520 Loss: 0.9840112924575806\n",
      "Iteration 49521 Loss: 1.3198375701904297\n",
      "Iteration 49522 Loss: 1.0292617082595825\n",
      "Iteration 49523 Loss: 0.6980247497558594\n",
      "Iteration 49524 Loss: 1.0837846994400024\n",
      "Iteration 49525 Loss: 0.7958691120147705\n",
      "Iteration 49526 Loss: 0.9250636696815491\n",
      "Iteration 49527 Loss: 0.9001308083534241\n",
      "Iteration 49528 Loss: 0.7495940327644348\n",
      "Iteration 49529 Loss: 0.8124145865440369\n",
      "Iteration 49529 Loss: 0.9297992587089539\n",
      "Iteration 49530 Loss: 0.983837366104126\n",
      "Iteration 49531 Loss: 1.2728875875473022\n",
      "Iteration 49532 Loss: 0.8901826739311218\n",
      "Iteration 49533 Loss: 0.7799625992774963\n",
      "Iteration 49534 Loss: 1.010832667350769\n",
      "Iteration 49535 Loss: 0.6407670974731445\n",
      "Iteration 49536 Loss: 0.9498220086097717\n",
      "Iteration 49537 Loss: 0.884406328201294\n",
      "Iteration 49538 Loss: 1.3259758949279785\n",
      "Iteration 49539 Loss: 0.9533153772354126\n",
      "Iteration 49539 Loss: 0.9691988825798035\n",
      "Iteration 49540 Loss: 0.8784649968147278\n",
      "Iteration 49541 Loss: 1.2511759996414185\n",
      "Iteration 49542 Loss: 0.764863133430481\n",
      "Iteration 49543 Loss: 1.0818485021591187\n",
      "Iteration 49544 Loss: 0.8617827892303467\n",
      "Iteration 49545 Loss: 0.8771237730979919\n",
      "Iteration 49546 Loss: 0.9550082683563232\n",
      "Iteration 49547 Loss: 0.8153387308120728\n",
      "Iteration 49548 Loss: 0.8363957405090332\n",
      "Iteration 49549 Loss: 0.7439291477203369\n",
      "Iteration 49549 Loss: 0.9065931439399719\n",
      "Iteration 49550 Loss: 0.9556117653846741\n",
      "Iteration 49551 Loss: 0.7954692840576172\n",
      "Iteration 49552 Loss: 1.133561611175537\n",
      "Iteration 49553 Loss: 1.1624516248703003\n",
      "Iteration 49554 Loss: 0.9567658305168152\n",
      "Iteration 49555 Loss: 1.2130835056304932\n",
      "Iteration 49556 Loss: 0.9732463359832764\n",
      "Iteration 49557 Loss: 1.2247934341430664\n",
      "Iteration 49558 Loss: 1.0429171323776245\n",
      "Iteration 49559 Loss: 1.0918917655944824\n",
      "Iteration 49559 Loss: 1.0549792051315308\n",
      "Iteration 49560 Loss: 1.0165189504623413\n",
      "Iteration 49561 Loss: 0.8297168612480164\n",
      "Iteration 49562 Loss: 0.8870338201522827\n",
      "Iteration 49563 Loss: 0.6644309163093567\n",
      "Iteration 49564 Loss: 0.9948617219924927\n",
      "Iteration 49565 Loss: 0.8787083029747009\n",
      "Iteration 49566 Loss: 1.021063208580017\n",
      "Iteration 49567 Loss: 0.8217008113861084\n",
      "Iteration 49568 Loss: 1.0283297300338745\n",
      "Iteration 49569 Loss: 0.7839463949203491\n",
      "Iteration 49569 Loss: 0.8926310539245605\n",
      "Iteration 49570 Loss: 1.272534728050232\n",
      "Iteration 49571 Loss: 0.9631062746047974\n",
      "Iteration 49572 Loss: 0.9431800842285156\n",
      "Iteration 49573 Loss: 0.98337721824646\n",
      "Iteration 49574 Loss: 0.9464113712310791\n",
      "Iteration 49575 Loss: 1.2788881063461304\n",
      "Iteration 49576 Loss: 0.6453700065612793\n",
      "Iteration 49577 Loss: 0.8309988975524902\n",
      "Iteration 49578 Loss: 0.7852917909622192\n",
      "Iteration 49579 Loss: 1.110532283782959\n",
      "Iteration 49579 Loss: 0.975969135761261\n",
      "Iteration 49580 Loss: 1.194216012954712\n",
      "Iteration 49581 Loss: 0.637631893157959\n",
      "Iteration 49582 Loss: 1.0918605327606201\n",
      "Iteration 49583 Loss: 0.6273295879364014\n",
      "Iteration 49584 Loss: 1.1174248456954956\n",
      "Iteration 49585 Loss: 1.0393736362457275\n",
      "Iteration 49586 Loss: 0.8818836808204651\n",
      "Iteration 49587 Loss: 0.9180002212524414\n",
      "Iteration 49588 Loss: 0.856410026550293\n",
      "Iteration 49589 Loss: 0.8328444361686707\n",
      "Iteration 49589 Loss: 0.9196974635124207\n",
      "Iteration 49590 Loss: 1.1389191150665283\n",
      "Iteration 49591 Loss: 0.8349796533584595\n",
      "Iteration 49592 Loss: 1.1999303102493286\n",
      "Iteration 49593 Loss: 1.037685513496399\n",
      "Iteration 49594 Loss: 0.9623648524284363\n",
      "Iteration 49595 Loss: 0.9701511859893799\n",
      "Iteration 49596 Loss: 0.736243724822998\n",
      "Iteration 49597 Loss: 1.3954102993011475\n",
      "Iteration 49598 Loss: 0.9765408039093018\n",
      "Iteration 49599 Loss: 0.6463932991027832\n",
      "Iteration 49599 Loss: 0.9898618459701538\n",
      "Iteration 49600 Loss: 0.9631885290145874\n",
      "Iteration 49601 Loss: 1.0040957927703857\n",
      "Iteration 49602 Loss: 1.1792806386947632\n",
      "Iteration 49603 Loss: 1.0247069597244263\n",
      "Iteration 49604 Loss: 0.8973429799079895\n",
      "Iteration 49605 Loss: 0.9113286137580872\n",
      "Iteration 49606 Loss: 0.6065635681152344\n",
      "Iteration 49607 Loss: 0.6407440304756165\n",
      "Iteration 49608 Loss: 0.8928892016410828\n",
      "Iteration 49609 Loss: 0.6881586909294128\n",
      "Iteration 49609 Loss: 0.880829930305481\n",
      "Iteration 49610 Loss: 0.7684573531150818\n",
      "Iteration 49611 Loss: 1.1612104177474976\n",
      "Iteration 49612 Loss: 1.0006300210952759\n",
      "Iteration 49613 Loss: 0.8769534826278687\n",
      "Iteration 49614 Loss: 0.9595358967781067\n",
      "Iteration 49615 Loss: 0.9981132745742798\n",
      "Iteration 49616 Loss: 0.8102337718009949\n",
      "Iteration 49617 Loss: 1.0484422445297241\n",
      "Iteration 49618 Loss: 0.7295802235603333\n",
      "Iteration 49619 Loss: 1.3906307220458984\n",
      "Iteration 49619 Loss: 0.9743787050247192\n",
      "Iteration 49620 Loss: 1.1913154125213623\n",
      "Iteration 49621 Loss: 0.6386606693267822\n",
      "Iteration 49622 Loss: 0.5376489162445068\n",
      "Iteration 49623 Loss: 0.8490661382675171\n",
      "Iteration 49624 Loss: 0.9117527008056641\n",
      "Iteration 49625 Loss: 1.0683478116989136\n",
      "Iteration 49626 Loss: 1.2771525382995605\n",
      "Iteration 49627 Loss: 1.0108540058135986\n",
      "Iteration 49628 Loss: 0.937391459941864\n",
      "Iteration 49629 Loss: 1.264893889427185\n",
      "Iteration 49629 Loss: 0.968708336353302\n",
      "Iteration 49630 Loss: 1.0224149227142334\n",
      "Iteration 49631 Loss: 0.915881872177124\n",
      "Iteration 49632 Loss: 0.946218729019165\n",
      "Iteration 49633 Loss: 1.1144394874572754\n",
      "Iteration 49634 Loss: 0.6147310733795166\n",
      "Iteration 49635 Loss: 1.1411947011947632\n",
      "Iteration 49636 Loss: 0.7116687297821045\n",
      "Iteration 49637 Loss: 0.6693398356437683\n",
      "Iteration 49638 Loss: 1.195270299911499\n",
      "Iteration 49639 Loss: 0.8722556233406067\n",
      "Iteration 49639 Loss: 0.9203416109085083\n",
      "Iteration 49640 Loss: 0.9002441763877869\n",
      "Iteration 49641 Loss: 0.9897711873054504\n",
      "Iteration 49642 Loss: 0.9853399395942688\n",
      "Iteration 49643 Loss: 1.0198791027069092\n",
      "Iteration 49644 Loss: 0.9703167676925659\n",
      "Iteration 49645 Loss: 0.8535369634628296\n",
      "Iteration 49646 Loss: 1.209313154220581\n",
      "Iteration 49647 Loss: 0.923922061920166\n",
      "Iteration 49648 Loss: 0.9121692180633545\n",
      "Iteration 49649 Loss: 1.0714174509048462\n",
      "Iteration 49649 Loss: 0.9835910797119141\n",
      "Iteration 49650 Loss: 0.8718075752258301\n",
      "Iteration 49651 Loss: 1.1541262865066528\n",
      "Iteration 49652 Loss: 0.8283223509788513\n",
      "Iteration 49653 Loss: 0.8431863784790039\n",
      "Iteration 49654 Loss: 1.0519335269927979\n",
      "Iteration 49655 Loss: 1.0083938837051392\n",
      "Iteration 49656 Loss: 0.7496393322944641\n",
      "Iteration 49657 Loss: 0.9852340221405029\n",
      "Iteration 49658 Loss: 0.6554728746414185\n",
      "Iteration 49659 Loss: 1.0286147594451904\n",
      "Iteration 49659 Loss: 0.9176731109619141\n",
      "Iteration 49660 Loss: 1.207201361656189\n",
      "Iteration 49661 Loss: 0.909203052520752\n",
      "Iteration 49662 Loss: 0.9368077516555786\n",
      "Iteration 49663 Loss: 0.7457690238952637\n",
      "Iteration 49664 Loss: 0.91610187292099\n",
      "Iteration 49665 Loss: 0.7942903637886047\n",
      "Iteration 49666 Loss: 0.9689334630966187\n",
      "Iteration 49667 Loss: 0.9565613865852356\n",
      "Iteration 49668 Loss: 0.9539405703544617\n",
      "Iteration 49669 Loss: 0.7960373163223267\n",
      "Iteration 49669 Loss: 0.9184845685958862\n",
      "Iteration 49670 Loss: 0.8869775533676147\n",
      "Iteration 49671 Loss: 0.6577363610267639\n",
      "Iteration 49672 Loss: 1.0840386152267456\n",
      "Iteration 49673 Loss: 1.5720574855804443\n",
      "Iteration 49674 Loss: 0.8886604309082031\n",
      "Iteration 49675 Loss: 0.694956362247467\n",
      "Iteration 49676 Loss: 0.817285418510437\n",
      "Iteration 49677 Loss: 1.1026381254196167\n",
      "Iteration 49678 Loss: 1.0156830549240112\n",
      "Iteration 49679 Loss: 0.8074800968170166\n",
      "Iteration 49679 Loss: 0.9527513384819031\n",
      "Iteration 49680 Loss: 1.0754681825637817\n",
      "Iteration 49681 Loss: 0.5562024712562561\n",
      "Iteration 49682 Loss: 1.1138838529586792\n",
      "Iteration 49683 Loss: 0.8553245067596436\n",
      "Iteration 49684 Loss: 0.8771831393241882\n",
      "Iteration 49685 Loss: 1.0350604057312012\n",
      "Iteration 49686 Loss: 1.047070026397705\n",
      "Iteration 49687 Loss: 1.056538701057434\n",
      "Iteration 49688 Loss: 0.6935070753097534\n",
      "Iteration 49689 Loss: 1.186095118522644\n",
      "Iteration 49689 Loss: 0.9496333003044128\n",
      "Iteration 49690 Loss: 1.1308053731918335\n",
      "Iteration 49691 Loss: 1.2132647037506104\n",
      "Iteration 49692 Loss: 1.0544534921646118\n",
      "Iteration 49693 Loss: 1.0423798561096191\n",
      "Iteration 49694 Loss: 1.0188157558441162\n",
      "Iteration 49695 Loss: 0.7877520322799683\n",
      "Iteration 49696 Loss: 0.8125515580177307\n",
      "Iteration 49697 Loss: 0.9501879811286926\n",
      "Iteration 49698 Loss: 0.8631043434143066\n",
      "Iteration 49699 Loss: 0.9479765295982361\n",
      "Iteration 49699 Loss: 0.9821292161941528\n",
      "Iteration 49700 Loss: 1.0463168621063232\n",
      "Iteration 49701 Loss: 1.5017491579055786\n",
      "Iteration 49702 Loss: 0.9228730797767639\n",
      "Iteration 49703 Loss: 0.7851012349128723\n",
      "Iteration 49704 Loss: 0.9710094928741455\n",
      "Iteration 49705 Loss: 0.9687275290489197\n",
      "Iteration 49706 Loss: 0.8242941498756409\n",
      "Iteration 49707 Loss: 1.0835826396942139\n",
      "Iteration 49708 Loss: 1.1136211156845093\n",
      "Iteration 49709 Loss: 1.0589823722839355\n",
      "Iteration 49709 Loss: 1.0276257991790771\n",
      "Iteration 49710 Loss: 0.6283561587333679\n",
      "Iteration 49711 Loss: 1.2988837957382202\n",
      "Iteration 49712 Loss: 1.0461245775222778\n",
      "Iteration 49713 Loss: 0.7703299522399902\n",
      "Iteration 49714 Loss: 0.9042798280715942\n",
      "Iteration 49715 Loss: 0.8541046977043152\n",
      "Iteration 49716 Loss: 1.2161897420883179\n",
      "Iteration 49717 Loss: 0.8664752840995789\n",
      "Iteration 49718 Loss: 0.8015522360801697\n",
      "Iteration 49719 Loss: 0.9142043590545654\n",
      "Iteration 49719 Loss: 0.9300500154495239\n",
      "Iteration 49720 Loss: 0.9764137268066406\n",
      "Iteration 49721 Loss: 0.6097689867019653\n",
      "Iteration 49722 Loss: 1.1512131690979004\n",
      "Iteration 49723 Loss: 0.9507074356079102\n",
      "Iteration 49724 Loss: 1.2652404308319092\n",
      "Iteration 49725 Loss: 0.7562891244888306\n",
      "Iteration 49726 Loss: 0.9416841864585876\n",
      "Iteration 49727 Loss: 1.3949830532073975\n",
      "Iteration 49728 Loss: 1.1173579692840576\n",
      "Iteration 49729 Loss: 0.8357250094413757\n",
      "Iteration 49729 Loss: 0.9999383091926575\n",
      "Iteration 49730 Loss: 0.8301458954811096\n",
      "Iteration 49731 Loss: 1.1637767553329468\n",
      "Iteration 49732 Loss: 1.108074426651001\n",
      "Iteration 49733 Loss: 1.0592986345291138\n",
      "Iteration 49734 Loss: 0.6922962665557861\n",
      "Iteration 49735 Loss: 0.9796059131622314\n",
      "Iteration 49736 Loss: 0.6181698441505432\n",
      "Iteration 49737 Loss: 1.1364641189575195\n",
      "Iteration 49738 Loss: 0.7053558826446533\n",
      "Iteration 49739 Loss: 0.9103804230690002\n",
      "Iteration 49739 Loss: 0.9203567504882812\n",
      "Iteration 49740 Loss: 0.9837989807128906\n",
      "Iteration 49741 Loss: 0.6211177110671997\n",
      "Iteration 49742 Loss: 0.8275002837181091\n",
      "Iteration 49743 Loss: 0.8970678448677063\n",
      "Iteration 49744 Loss: 0.9163945913314819\n",
      "Iteration 49745 Loss: 0.969377875328064\n",
      "Iteration 49746 Loss: 0.9219125509262085\n",
      "Iteration 49747 Loss: 0.7494321465492249\n",
      "Iteration 49748 Loss: 0.893681526184082\n",
      "Iteration 49749 Loss: 1.0111323595046997\n",
      "Iteration 49749 Loss: 0.879141628742218\n",
      "Iteration 49750 Loss: 0.8519648909568787\n",
      "Iteration 49751 Loss: 0.9610647559165955\n",
      "Iteration 49752 Loss: 0.8917378783226013\n",
      "Iteration 49753 Loss: 1.2032212018966675\n",
      "Iteration 49754 Loss: 1.0564504861831665\n",
      "Iteration 49755 Loss: 0.6731988787651062\n",
      "Iteration 49756 Loss: 0.6420787572860718\n",
      "Iteration 49757 Loss: 0.9303135871887207\n",
      "Iteration 49758 Loss: 0.8332259058952332\n",
      "Iteration 49759 Loss: 0.949286162853241\n",
      "Iteration 49759 Loss: 0.8992542028427124\n",
      "Iteration 49760 Loss: 0.8860588073730469\n",
      "Iteration 49761 Loss: 0.8343749046325684\n",
      "Iteration 49762 Loss: 0.9698373675346375\n",
      "Iteration 49763 Loss: 0.941795825958252\n",
      "Iteration 49764 Loss: 0.8497494459152222\n",
      "Iteration 49765 Loss: 0.8053045272827148\n",
      "Iteration 49766 Loss: 1.0469744205474854\n",
      "Iteration 49767 Loss: 0.4938742518424988\n",
      "Iteration 49768 Loss: 0.7284651398658752\n",
      "Iteration 49769 Loss: 0.8964746594429016\n",
      "Iteration 49769 Loss: 0.8452909588813782\n",
      "Iteration 49770 Loss: 1.2112807035446167\n",
      "Iteration 49771 Loss: 0.9919254183769226\n",
      "Iteration 49772 Loss: 0.9928359985351562\n",
      "Iteration 49773 Loss: 0.9137340188026428\n",
      "Iteration 49774 Loss: 0.9704065322875977\n",
      "Iteration 49775 Loss: 0.8105422258377075\n",
      "Iteration 49776 Loss: 1.0181413888931274\n",
      "Iteration 49777 Loss: 1.2803053855895996\n",
      "Iteration 49778 Loss: 0.7092118859291077\n",
      "Iteration 49779 Loss: 0.8124207854270935\n",
      "Iteration 49779 Loss: 0.9710804224014282\n",
      "Iteration 49780 Loss: 0.7954153418540955\n",
      "Iteration 49781 Loss: 0.7165664434432983\n",
      "Iteration 49782 Loss: 1.0133106708526611\n",
      "Iteration 49783 Loss: 1.1951388120651245\n",
      "Iteration 49784 Loss: 1.292095422744751\n",
      "Iteration 49785 Loss: 1.2034947872161865\n",
      "Iteration 49786 Loss: 0.7039446830749512\n",
      "Iteration 49787 Loss: 0.6873381733894348\n",
      "Iteration 49788 Loss: 1.1494224071502686\n",
      "Iteration 49789 Loss: 0.86268550157547\n",
      "Iteration 49789 Loss: 0.961941123008728\n",
      "Iteration 49790 Loss: 0.7905885577201843\n",
      "Iteration 49791 Loss: 1.0878214836120605\n",
      "Iteration 49792 Loss: 0.7613506317138672\n",
      "Iteration 49793 Loss: 0.9743309020996094\n",
      "Iteration 49794 Loss: 0.956807553768158\n",
      "Iteration 49795 Loss: 1.0506807565689087\n",
      "Iteration 49796 Loss: 0.9924405217170715\n",
      "Iteration 49797 Loss: 0.8224562406539917\n",
      "Iteration 49798 Loss: 0.9529829025268555\n",
      "Iteration 49799 Loss: 0.8676899671554565\n",
      "Iteration 49799 Loss: 0.9257149696350098\n",
      "Iteration 49800 Loss: 0.8105804920196533\n",
      "Iteration 49801 Loss: 0.769219160079956\n",
      "Iteration 49802 Loss: 0.8203459978103638\n",
      "Iteration 49803 Loss: 0.8407796621322632\n",
      "Iteration 49804 Loss: 0.8874560594558716\n",
      "Iteration 49805 Loss: 0.7746426463127136\n",
      "Iteration 49806 Loss: 0.9429855942726135\n",
      "Iteration 49807 Loss: 0.897123396396637\n",
      "Iteration 49808 Loss: 1.3105438947677612\n",
      "Iteration 49809 Loss: 0.6425427198410034\n",
      "Iteration 49809 Loss: 0.8696219325065613\n",
      "Iteration 49810 Loss: 1.2009896039962769\n",
      "Iteration 49811 Loss: 1.182192325592041\n",
      "Iteration 49812 Loss: 0.7315330505371094\n",
      "Iteration 49813 Loss: 1.2225151062011719\n",
      "Iteration 49814 Loss: 1.2276666164398193\n",
      "Iteration 49815 Loss: 0.966866672039032\n",
      "Iteration 49816 Loss: 0.8311130404472351\n",
      "Iteration 49817 Loss: 0.6560767292976379\n",
      "Iteration 49818 Loss: 0.7274996638298035\n",
      "Iteration 49819 Loss: 1.1588225364685059\n",
      "Iteration 49819 Loss: 0.9905274510383606\n",
      "Iteration 49820 Loss: 1.032218098640442\n",
      "Iteration 49821 Loss: 0.8528425097465515\n",
      "Iteration 49822 Loss: 1.0060278177261353\n",
      "Iteration 49823 Loss: 1.1454839706420898\n",
      "Iteration 49824 Loss: 0.9778733849525452\n",
      "Iteration 49825 Loss: 1.092822790145874\n",
      "Iteration 49826 Loss: 1.0483320951461792\n",
      "Iteration 49827 Loss: 0.8477301597595215\n",
      "Iteration 49828 Loss: 0.7317712306976318\n",
      "Iteration 49829 Loss: 0.9679222702980042\n",
      "Iteration 49829 Loss: 0.970302402973175\n",
      "Iteration 49830 Loss: 0.907412588596344\n",
      "Iteration 49831 Loss: 0.8111475706100464\n",
      "Iteration 49832 Loss: 0.870617151260376\n",
      "Iteration 49833 Loss: 0.8706617951393127\n",
      "Iteration 49834 Loss: 0.6614062190055847\n",
      "Iteration 49835 Loss: 0.6205480098724365\n",
      "Iteration 49836 Loss: 0.9293524026870728\n",
      "Iteration 49837 Loss: 1.0495117902755737\n",
      "Iteration 49838 Loss: 0.8449356555938721\n",
      "Iteration 49839 Loss: 1.1113678216934204\n",
      "Iteration 49839 Loss: 0.8676959872245789\n",
      "Iteration 49840 Loss: 1.108538269996643\n",
      "Iteration 49841 Loss: 0.7647506594657898\n",
      "Iteration 49842 Loss: 0.9852200746536255\n",
      "Iteration 49843 Loss: 0.8215844035148621\n",
      "Iteration 49844 Loss: 0.945083498954773\n",
      "Iteration 49845 Loss: 0.614314079284668\n",
      "Iteration 49846 Loss: 1.0875654220581055\n",
      "Iteration 49847 Loss: 0.7770355939865112\n",
      "Iteration 49848 Loss: 0.8463804721832275\n",
      "Iteration 49849 Loss: 0.851343035697937\n",
      "Iteration 49849 Loss: 0.880181610584259\n",
      "Iteration 49850 Loss: 0.7803165912628174\n",
      "Iteration 49851 Loss: 0.8170600533485413\n",
      "Iteration 49852 Loss: 1.2336806058883667\n",
      "Iteration 49853 Loss: 0.9317004084587097\n",
      "Iteration 49854 Loss: 1.0818557739257812\n",
      "Iteration 49855 Loss: 1.2400168180465698\n",
      "Iteration 49856 Loss: 0.7425025701522827\n",
      "Iteration 49857 Loss: 1.0778610706329346\n",
      "Iteration 49858 Loss: 1.1562221050262451\n",
      "Iteration 49859 Loss: 0.5550066232681274\n",
      "Iteration 49859 Loss: 0.9616222381591797\n",
      "Iteration 49860 Loss: 1.236950397491455\n",
      "Iteration 49861 Loss: 1.0414053201675415\n",
      "Iteration 49862 Loss: 1.0944161415100098\n",
      "Iteration 49863 Loss: 1.1327762603759766\n",
      "Iteration 49864 Loss: 1.1546988487243652\n",
      "Iteration 49865 Loss: 1.119640827178955\n",
      "Iteration 49866 Loss: 0.9021332859992981\n",
      "Iteration 49867 Loss: 0.8759932518005371\n",
      "Iteration 49868 Loss: 0.9693812727928162\n",
      "Iteration 49869 Loss: 0.808783769607544\n",
      "Iteration 49869 Loss: 1.0336179733276367\n",
      "Iteration 49870 Loss: 0.7764323353767395\n",
      "Iteration 49871 Loss: 0.7649015188217163\n",
      "Iteration 49872 Loss: 0.8571141958236694\n",
      "Iteration 49873 Loss: 0.8661494255065918\n",
      "Iteration 49874 Loss: 0.6418930292129517\n",
      "Iteration 49875 Loss: 0.9014294147491455\n",
      "Iteration 49876 Loss: 0.8165481090545654\n",
      "Iteration 49877 Loss: 0.4118945300579071\n",
      "Iteration 49878 Loss: 0.9613529443740845\n",
      "Iteration 49879 Loss: 0.5439425706863403\n",
      "Iteration 49879 Loss: 0.754165768623352\n",
      "Iteration 49880 Loss: 0.9456070065498352\n",
      "Iteration 49881 Loss: 0.8607587218284607\n",
      "Iteration 49882 Loss: 1.1196411848068237\n",
      "Iteration 49883 Loss: 1.0800018310546875\n",
      "Iteration 49884 Loss: 0.7401967644691467\n",
      "Iteration 49885 Loss: 0.6828513741493225\n",
      "Iteration 49886 Loss: 0.9498018622398376\n",
      "Iteration 49887 Loss: 1.1854454278945923\n",
      "Iteration 49888 Loss: 0.6648072004318237\n",
      "Iteration 49889 Loss: 0.8404057025909424\n",
      "Iteration 49889 Loss: 0.9069517254829407\n",
      "Iteration 49890 Loss: 0.6219934821128845\n",
      "Iteration 49891 Loss: 0.8736139535903931\n",
      "Iteration 49892 Loss: 1.3395001888275146\n",
      "Iteration 49893 Loss: 1.0754300355911255\n",
      "Iteration 49894 Loss: 0.7374893426895142\n",
      "Iteration 49895 Loss: 1.030585527420044\n",
      "Iteration 49896 Loss: 0.7669755816459656\n",
      "Iteration 49897 Loss: 0.6480591893196106\n",
      "Iteration 49898 Loss: 1.0969997644424438\n",
      "Iteration 49899 Loss: 0.9181936979293823\n",
      "Iteration 49899 Loss: 0.9108840227127075\n",
      "Iteration 49900 Loss: 1.1049814224243164\n",
      "Iteration 49901 Loss: 0.7009958624839783\n",
      "Iteration 49902 Loss: 0.9620391130447388\n",
      "Iteration 49903 Loss: 0.9156966209411621\n",
      "Iteration 49904 Loss: 1.1475114822387695\n",
      "Iteration 49905 Loss: 0.9841110706329346\n",
      "Iteration 49906 Loss: 0.7454785108566284\n",
      "Iteration 49907 Loss: 0.6690234541893005\n",
      "Iteration 49908 Loss: 0.6604152321815491\n",
      "Iteration 49909 Loss: 1.1637777090072632\n",
      "Iteration 49909 Loss: 0.9054030179977417\n",
      "Iteration 49910 Loss: 1.0641329288482666\n",
      "Iteration 49911 Loss: 0.8776647448539734\n",
      "Iteration 49912 Loss: 0.7590083479881287\n",
      "Iteration 49913 Loss: 0.8413464426994324\n",
      "Iteration 49914 Loss: 0.9946176409721375\n",
      "Iteration 49915 Loss: 0.8413895964622498\n",
      "Iteration 49916 Loss: 1.0420888662338257\n",
      "Iteration 49917 Loss: 0.8908063173294067\n",
      "Iteration 49918 Loss: 1.033320665359497\n",
      "Iteration 49919 Loss: 0.9477865099906921\n",
      "Iteration 49919 Loss: 0.929216206073761\n",
      "Iteration 49920 Loss: 1.1980550289154053\n",
      "Iteration 49921 Loss: 1.1235339641571045\n",
      "Iteration 49922 Loss: 1.089747667312622\n",
      "Iteration 49923 Loss: 0.8846185207366943\n",
      "Iteration 49924 Loss: 0.694791316986084\n",
      "Iteration 49925 Loss: 1.191068172454834\n",
      "Iteration 49926 Loss: 1.1584504842758179\n",
      "Iteration 49927 Loss: 1.2302566766738892\n",
      "Iteration 49928 Loss: 1.067401647567749\n",
      "Iteration 49929 Loss: 1.3561971187591553\n",
      "Iteration 49929 Loss: 1.0994120836257935\n",
      "Iteration 49930 Loss: 0.71111661195755\n",
      "Iteration 49931 Loss: 1.2123185396194458\n",
      "Iteration 49932 Loss: 0.5431632995605469\n",
      "Iteration 49933 Loss: 0.8178486227989197\n",
      "Iteration 49934 Loss: 1.062583327293396\n",
      "Iteration 49935 Loss: 0.801546037197113\n",
      "Iteration 49936 Loss: 0.8321232199668884\n",
      "Iteration 49937 Loss: 0.7158538699150085\n",
      "Iteration 49938 Loss: 0.6809914112091064\n",
      "Iteration 49939 Loss: 1.0315003395080566\n",
      "Iteration 49939 Loss: 0.8409045338630676\n",
      "Iteration 49940 Loss: 0.7076374888420105\n",
      "Iteration 49941 Loss: 0.8513580560684204\n",
      "Iteration 49942 Loss: 0.8311948180198669\n",
      "Iteration 49943 Loss: 0.824137270450592\n",
      "Iteration 49944 Loss: 0.982215940952301\n",
      "Iteration 49945 Loss: 1.0112206935882568\n",
      "Iteration 49946 Loss: 0.6296237111091614\n",
      "Iteration 49947 Loss: 0.7746537923812866\n",
      "Iteration 49948 Loss: 1.3044122457504272\n",
      "Iteration 49949 Loss: 0.8456011414527893\n",
      "Iteration 49949 Loss: 0.876205563545227\n",
      "Iteration 49950 Loss: 0.9691369533538818\n",
      "Iteration 49951 Loss: 1.0062222480773926\n",
      "Iteration 49952 Loss: 0.856177568435669\n",
      "Iteration 49953 Loss: 1.040756344795227\n",
      "Iteration 49954 Loss: 0.9201641082763672\n",
      "Iteration 49955 Loss: 0.5390427112579346\n",
      "Iteration 49956 Loss: 0.6798016428947449\n",
      "Iteration 49957 Loss: 0.7693861722946167\n",
      "Iteration 49958 Loss: 0.7791707515716553\n",
      "Iteration 49959 Loss: 0.7354376912117004\n",
      "Iteration 49959 Loss: 0.8295295834541321\n",
      "Iteration 49960 Loss: 0.9922915101051331\n",
      "Iteration 49961 Loss: 1.0958818197250366\n",
      "Iteration 49962 Loss: 0.8772479295730591\n",
      "Iteration 49963 Loss: 0.8189864158630371\n",
      "Iteration 49964 Loss: 0.8254361152648926\n",
      "Iteration 49965 Loss: 0.717699408531189\n",
      "Iteration 49966 Loss: 0.6502525210380554\n",
      "Iteration 49967 Loss: 1.1433894634246826\n",
      "Iteration 49968 Loss: 1.0633883476257324\n",
      "Iteration 49969 Loss: 0.8798990845680237\n",
      "Iteration 49969 Loss: 0.9064472317695618\n",
      "Iteration 49970 Loss: 0.7056999802589417\n",
      "Iteration 49971 Loss: 0.9474109411239624\n",
      "Iteration 49972 Loss: 1.0103689432144165\n",
      "Iteration 49973 Loss: 0.854764461517334\n",
      "Iteration 49974 Loss: 0.7233942747116089\n",
      "Iteration 49975 Loss: 1.065314531326294\n",
      "Iteration 49976 Loss: 0.8476552367210388\n",
      "Iteration 49977 Loss: 0.5368455648422241\n",
      "Iteration 49978 Loss: 1.027835488319397\n",
      "Iteration 49979 Loss: 1.1472499370574951\n",
      "Iteration 49979 Loss: 0.8866539001464844\n",
      "Iteration 49980 Loss: 0.8922610878944397\n",
      "Iteration 49981 Loss: 0.9944724440574646\n",
      "Iteration 49982 Loss: 0.7624423503875732\n",
      "Iteration 49983 Loss: 0.9976881742477417\n",
      "Iteration 49984 Loss: 0.8621240854263306\n",
      "Iteration 49985 Loss: 0.8779427409172058\n",
      "Iteration 49986 Loss: 1.4838720560073853\n",
      "Iteration 49987 Loss: 1.144948124885559\n",
      "Iteration 49988 Loss: 0.7558327317237854\n",
      "Iteration 49989 Loss: 1.0908236503601074\n",
      "Iteration 49989 Loss: 0.9862407445907593\n",
      "Iteration 49990 Loss: 1.034985065460205\n",
      "Iteration 49991 Loss: 0.8134111762046814\n",
      "Iteration 49992 Loss: 1.00752854347229\n",
      "Iteration 49993 Loss: 0.9934664368629456\n",
      "Iteration 49994 Loss: 0.9395864605903625\n",
      "Iteration 49995 Loss: 0.7352215051651001\n",
      "Iteration 49996 Loss: 0.8552371263504028\n",
      "Iteration 49997 Loss: 1.0109179019927979\n",
      "Iteration 49998 Loss: 1.0137628316879272\n",
      "Iteration 49999 Loss: 0.8918981552124023\n",
      "Iteration 49999 Loss: 0.9296015501022339\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "history = []\n",
    "loss_avg_block_size = 10\n",
    "\n",
    "#train_dataset = TextDataset(train_enc, CONTEXT_LEN)\n",
    "# Prefetch the first batch\n",
    "#train_in, train_target = get_batches(train_enc, BATCH_COUNT, CONTEXT_LEN)\n",
    "\n",
    "#load model\n",
    "#transformer = CustomTransformer(tokenizer.n_vocab, CONTEXT_LEN, EMBED_DIM, NUM_HEADS, BLOCK_COUNT)\n",
    "\n",
    "out_dir = \"../data/out22M/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "i = 0\n",
    "for current_train_in, current_train_target in train_loader:\n",
    "    start_time = datetime.now()\n",
    "    # Process the current batch\n",
    "    logits, loss = transformer(current_train_in, current_train_target)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    print (f\"Iteration {i} Loss: {history[-1]}\")\n",
    "    if (len(history) >= loss_avg_block_size):\n",
    "        loss_history.append(torch.tensor(history).mean().item())\n",
    "        history = []\n",
    "        print (f\"Iteration {i} Loss: {loss_history[-1]}\")\n",
    "    i += 1\n",
    "\n",
    "    if (not os.path.exists(out_dir)):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    torch.save(transformer.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"loss.txt\"), \"a+\") as f:\n",
    "        f.write(f\"Loss: {str(loss)}___________Time: {total_seconds}s\\n\")\n",
    "\n",
    "    if (i >= ITERATIONS):\n",
    "        break\n",
    "\n",
    "if (len(history) > 0):\n",
    "    loss_history.append(torch.tensor(history).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(loss_history * loss_avg_block_size) + 1, loss_avg_block_size), loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________SAMPLED_________________________:\n",
      "PluggableType pluggableType=BackendDescriptor.PluggableType.UNKNOWN;\n",
      "      if (cn != null && cn.endsWith(DATABASE_JE_MONITORING_ENTRY_SUFFIX)) {\n",
      "        pluggableType=BackendDescriptor.PluggableType.JE;\n",
      "        monitorBackendID=cn.substring(0,cn.length() - DATABASE_JE_MONITORING_ENTRY_SUFFIX.length());\n",
      "      }\n",
      "      if (cn != null && cn.endsWith(DATABASE_PDB_MONITORING_ENTRY_SUFFIX)) {\n",
      "        pluggableType=BackendDescriptor.PluggableType.PDB;\n",
      "        monitorBackendID=cn.substring(0,cn.length() - DATABASE_PDB_MONITORING_ENTRY_SUFFIX\n",
      "_________________________PREDICTED_______________________:\n",
      ",0,ASE_CONFRELTERERENCEMCAPPROVID,fs.variableCompospVED_RESS);\n",
      "        getSrc.start();\n",
      "      }\n",
      "       MessageType.set(asser_TYPE,ge);\n",
      "    }\n",
      "    return level;\n",
      "    AIREWORY;\n",
      "    else {\n",
      "    }\n",
      "    if (level != null) {\n",
      "      cn=(cmpS + 1) >>> 1) {\n",
      "           for (Cubst.nf : zplu > Colorphice\tfloat b.nDB.Counts) {\n",
      "        return NEqualledlyphaNeeds;\n",
      "  }\n",
      " catch (per) {\n",
      "    show.finishertra(bit.WAMPLEM_PROLIENTEREQUEST_ON_SYNER,b) - 1);\n",
      "    } }\n",
      "\n",
      "  }\n",
      "  }\n",
      "  entermin\",getLabelPfinal Client.getLabel() > 2) + 1 1);\n",
      "  readPrevFocusableByte((byte)data.getC\n"
     ]
    }
   ],
   "source": [
    "start = torch.randint(0, len(test_enc) - CONTEXT_LEN, (1,)).item()\n",
    "sampled_txt = test_enc.ids[start:start + CONTEXT_LEN]\n",
    "print(\"_________________________SAMPLED_________________________:\")\n",
    "print(tokenizer.decode(sampled_txt))\n",
    "print(\"_________________________PREDICTED_______________________:\")\n",
    "x = complete(sampled_txt, CONTEXT_LEN)\n",
    "print(\"\".join(remove_padding(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
